{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"pandas-table.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic model with Github Pull Request \n",
    "\n",
    "Nowadays software development process is a complex social network activity where a big amount of non structured documentation is generated. Over the past 15 years, there has been an explosion of empirical research in software engineering to explore questions related with software development process. Partly motivated by the availability of data from sites like GitHub. \n",
    " \n",
    "- Who should fix this bug? \n",
    "- How do you find documentation about a bug? \n",
    "\n",
    "The documentation in a software project is generated dynamically in multiple documents as it grows.\n",
    "The pull request is a source of very valuable documentation that is generated collaboratively by the development team when they want to integrate a new change in the project. Pull request is an __short not structured text__, this makes it difficult to manage.\n",
    "One social network technique to manage this kind of __short documentation__  is to define a set of topics with a keyword, usually called labels or tags. Then every __short documentation__, pullrequest in this case, is labeled with a label. The choice of keywords (labels) are selected by the participants of the social network throughout all their interventions. Then they apply a label to a new interaction with social network.\n",
    "\n",
    "In this work aims  to label the pull requests applying topic modelling that it is an  non supervised machine learning technique.\n",
    "\n",
    "\n",
    "##  Researcher\n",
    " - César Ignacio García Osorio\n",
    " - Mario Juez Gil\n",
    " - Carlos López Nozal\n",
    " - Álvar Arnaiz González\n",
    "\n",
    "## References \n",
    "- [Topic modeling evaluation](https://datascience.blog.wzb.eu/2017/11/09/topic-modeling-evaluation-in-python-with-tmtoolkit/)\n",
    "Likelihood and perplexity Evaluating the posterior distributions’ density or divergence\n",
    "- [Perplexity To Evaluate Topic Models](http://qpleple.com/perplexity-to-evaluate-topic-models/)\n",
    "- [Python tutorial topic model](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "Choose a better values of K (#number of topics)\n",
    "We started with understanding what topic modeling can do. We built a basic topic model using Gensim’s LDA and visualize the topics using pyLDAvis. Then we built mallet’s LDA implementation. You saw how to find the optimal number of topics using coherence scores and how you can come to a logical understanding of how to choose the optimal model.\n",
    "Finally we saw how to aggregate and present the results to generate insights that may be in a more actionable.\n",
    "- [Paper Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "- [wikipedia Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Pull request dataset](#dataset)\n",
    "2. [Load data file dataset](#files)\n",
    "3. [Dataset Tokenization](#tokenization)\n",
    "4. [Dataset normalization, stemming and lemmatization](#normalization)\n",
    "5. [LDAModel](#ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## Pull Request dataset\n",
    "### Definitions\n",
    "- [What is a Pull request in development software process?](https://github.com/features)\n",
    "- [An example of pull requets commented](https://github.com/google/WebFundamentals/pull/4136)\n",
    "### Github Repositories\n",
    "Our dataset from many github trending repositories, is updated to July 2017:\n",
    "- ChartJS\n",
    "- AngularJS\n",
    "- CakePHP\n",
    "- Play Framework\n",
    "- WebFundamentals\n",
    "- ElasticSearch\n",
    "\n",
    "### Data set structure\n",
    "We concat following data attributtes in a single attributte named `pull_request`.\n",
    "- repository_owner\n",
    "- repository_name\n",
    "- repository_language\n",
    "- pull_request_title\n",
    "- pull_request_body "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='files'></a>\n",
    "## Load data file dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/pullrequest/reviews_Leaflet_processed.csv\n",
      "./datasets/pullrequest/reviews_playframework_processed.csv\n",
      "./datasets/pullrequest/reviews_angular.js_processed.csv\n",
      "./datasets/pullrequest/reviews_WebFundamentals_processed.csv\n",
      "./datasets/pullrequest/reviews_appium_processed.csv\n",
      "./datasets/pullrequest/reviews_Chart.js_processed.csv\n",
      "./datasets/pullrequest/reviews_cakephp_processed.csv\n",
      "./datasets/pullrequest/reviews_elasticsearch_processed.csv\n",
      "Number of files: 8 Number of instances: 4303\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "#Read csv\n",
    "def loadCsvPullRequestFolder(path):\n",
    "    \"\"\"Load pullrequest data from  csv file and generate a \n",
    "    list with all pull requeste\"\"\"\n",
    "    _lprbt=list()\n",
    "    _totalfile=0\n",
    "    for filename in glob.glob(os.path.join(path, '*.csv')):        \n",
    "        print(filename)\n",
    "        df2=pd.read_csv(filename, error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "        df2[\"pull_request\"] = df2[\"repository_owner\"].map(str) + \" \" + \\\n",
    "        df2[\"repository_name\"].map(str) +  \" \" +  df2[\"repository_language\"].map(str) + \" \" +\\\n",
    "        df2[\"pull_request_body\"].map(str) + \" \" +  df2[\"pull_request_title\"].map(str)\n",
    "        [_lprbt.append(pr) for pr in df2.pull_request] \n",
    "        _totalfile+=1\n",
    "        #print(lprbt[len(lprbt)-1])\n",
    "    return _totalfile, len(_lprbt), _lprbt\n",
    "\n",
    "totalfiles,totalinstances,lprbt=loadCsvPullRequestFolder(path=\"./datasets/pullrequest\")   \n",
    "print(\"Number of files: {} Number of instances: {}\".format(totalfiles,totalinstances))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tokenization'></a>\n",
    "## Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/pullrequest/reviews_Leaflet_processed.csv\n",
      "./datasets/pullrequest/reviews_playframework_processed.csv\n",
      "./datasets/pullrequest/reviews_angular.js_processed.csv\n",
      "./datasets/pullrequest/reviews_WebFundamentals_processed.csv\n",
      "./datasets/pullrequest/reviews_appium_processed.csv\n",
      "./datasets/pullrequest/reviews_Chart.js_processed.csv\n",
      "./datasets/pullrequest/reviews_cakephp_processed.csv\n",
      "./datasets/pullrequest/reviews_elasticsearch_processed.csv\n"
     ]
    }
   ],
   "source": [
    "totalfiles,totalinstances,lprbt=loadCsvPullRequestFolder(path=\"./datasets/pullrequest\")\n",
    "#Clean text \\\\nn\n",
    "\n",
    "import re\n",
    "def textNormalization(lpr):\n",
    "    #remove character space\n",
    "    lpr=[re.sub('\\s+',' ',pr) for pr in lpr]\n",
    "    #TODO remove \\\\n \\\\r\n",
    "    #lpt=[re.sub('[/\\\\n/\\\\n]',' ',pr) for pr in lpr]\n",
    "    return lpr\n",
    "\n",
    "lprbt=textNormalization(lprbt)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of tokens in pullrequest: 4303 \n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def pr_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc True remove puntactions\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))\n",
    "                \n",
    "prwords=list(pr_to_words(lprbt))\n",
    "print(\"Numbers of tokens in pullrequest: {} \".format(len(prwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "## Dataset stopwords, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clopezno/anaconda3/envs/TextMining/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaflet', 'leaflet', 'javascript', 'nan', 'fix', 'webpack', 'using', 'valid', 'image', 'file', 'for', 'default', 'icon', 'path']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(prwords, min_count=5, threshold=500) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[prwords], threshold=500)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[prwords[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/clopezno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Numbers of tokens in pullrequest after normalization: 4303 \n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['\\\\n\\\\n', '\\\\n\\\\r'])\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(prwords)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#!python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(\"Numbers of tokens in pullrequest after normalization: {} \".format(len(data_lemmatized)))\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ldamodel'></a>\n",
    "## Topic Models\n",
    "[Gensim tutorial](https://radimrehurek.com/gensim/tut2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Model algorithms\n",
    "- __Term Frequency__ count number of a word is in a document doc2bow\n",
    "- __Inverse Document Frequency__, Tf-Idf expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality, except that features which were rare in the training corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving the number of dimensions intact. It can also optionally normalize the resulting vectors to (Euclidean) unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating disctionary and corpus with pullrequest preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "import gensim.models.tfidfmodel as tfidmodel\n",
    "\n",
    "def createCorpus(data_lemmatized):\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    return corpus,id2word\n",
    "\n",
    "import gensim.models.tfidfmodel as tfidf\n",
    "def createCorpusTfid(corpus):\n",
    "    tfidf = tfidmodel.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printHumanCorpus(corpus, id2word, instance):\n",
    "    \"\"\"Print length of corpus and human legible instance \n",
    "    instance int index of pullrequest \"\"\"\n",
    "    print(\"Corpus lenght {}\".format(len(corpus)))\n",
    "    print(\"Instance corpus {}\".format(corpus[instance-1:instance]))\n",
    "    print(\"Instance corpus {}\")\n",
    "    [[print(id2word[id], freq) for id, freq in cp] for cp in corpus[instance-1:instance]]\n",
    "    return\n",
    "\n",
    "\n",
    "corpus,id2word= createCorpus(data_lemmatized)\n",
    "#printHumanCorpus(corpus,id2word,3)\n",
    "tfidf_corpus= createCorpusTfid(corpus)\n",
    "#printHumanCorpus(corpus,tfidf_corpus,3)\n",
    "\n",
    "#serialize corpus\n",
    "corpora.MmCorpus.serialize(\"./models/prcorpus.mm\", corpus)\n",
    "corpora.MmCorpus.serialize(\"./models/tfidfprcorpus.mm\", tfidf_corpus)\n",
    "\n",
    "#save dictionary\n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "dictionary.save('/models/dict_pullrequets')\n",
    "dictionary.save_as_text('./models/dict_pullrequets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a library to preprocces data\n",
    "All step in just a method preproccessData\n",
    "- textNormalization - now not implemented\n",
    "- stopWords\n",
    "- bigrams\n",
    "- speech tagging  ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/pullrequest/reviews_Leaflet_processed.csv\n",
      "./datasets/pullrequest/reviews_playframework_processed.csv\n",
      "./datasets/pullrequest/reviews_angular.js_processed.csv\n",
      "./datasets/pullrequest/reviews_WebFundamentals_processed.csv\n",
      "./datasets/pullrequest/reviews_appium_processed.csv\n",
      "./datasets/pullrequest/reviews_Chart.js_processed.csv\n",
      "./datasets/pullrequest/reviews_cakephp_processed.csv\n",
      "./datasets/pullrequest/reviews_elasticsearch_processed.csv\n",
      "Number of files: 8 Number of instances: 4303\n",
      "Numbers of tokens in pullrequest: 283706 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clopezno/anaconda3/envs/TextMining/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of tokens nostops in pullrequest: 191337 \n",
      "Numbers of tokens in pullrequest after lemmatization: 177226 \n"
     ]
    }
   ],
   "source": [
    "import LibraryTopicModel as ltm\n",
    "## process all csv in directory\n",
    "path=\"./datasets/pullrequest\"\n",
    "## process only one csv\n",
    "#path=\"./datasets/pullrequest/reviews_cakephp_processed.csv\"\n",
    "data_lemmatized=ltm.preprocessData(path)\n",
    "corpus,id2word=ltm.createCorpus(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating topic models and saving in diectory ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projections\n",
    "RP aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness. Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.rpmodel as rpmodel\n",
    "# Build Random Projections model\n",
    "#rpmodelpr = rpmodel.RpModel(tfidf_corpus, num_topics=20)\n",
    "rpmodelpr = rpmodel.RpModel(corpus, num_topics=10)\n",
    "#rpmodelpr.print_topics(2)\n",
    "\n",
    "rpmodelpr.save(\"./models/rpmodelpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "import gensim.models.ldamodel as ldamodel\n",
    "ldamodelpr = ldamodel.LdaModel(corpus, id2word=id2word, num_topics=10)\n",
    "ldamodelpr.print_topics(2)\n",
    "ldamodelpr.save(\"./models/ldamodelpr\")\n",
    "\n",
    "##lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "##                                           id2word=id2word,\n",
    "##                                           num_topics=5, \n",
    "##                                           random_state=100,\n",
    "##                                           update_every=1,\n",
    "##                                           chunksize=100,\n",
    "##                                           passes=10,\n",
    "##                                           alpha='auto',\n",
    "##                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Dirichlet Process\n",
    "__HDP__ is a non-parametric bayesian method (note the missing number of requested topics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Hdp model\n",
    "import gensim.models.hdpmodel as hdpmodel\n",
    "hpdmodelpr = hdpmodel.HdpModel(corpus, id2word=id2word)\n",
    "hpdmodelpr.print_topics(2)\n",
    "hpdmodelpr.save(\"./models/hpdmodelpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "LSI (or sometimes LSA) transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into a latent space of a lower dimensionality.\n",
    "LSI training is unique in that we can continue “training” at any point, simply by providing more training documents. This is done by incremental updates to the underlying model, in a process called online training. Because of this feature, the input document stream may even be infinite – just keep feeding LSI new documents as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build LSI model\n",
    "import gensim.models.lsimodel as lsimodel   \n",
    "#lsimodelpr = lsimodel.LsiModel(tfidf_corpus, id2word=id2word, num_topics=20)\n",
    "lsimodelpr = lsimodel.LsiModel(corpus, id2word=id2word, num_topics=10)\n",
    "lsimodelpr.print_topics(2)\n",
    "lsimodelpr.save(\"./models/lsimodelpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA mallet version\n",
    "So far you have seen Gensim’s inbuilt version of the LDA algorithm. Mallet’s version, however, often gives a better quality of topics.\n",
    "\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. You only need to download the zipfile, unzip it and provide the path to mallet in the unzipped directory to gensim.models.wrappers.LdaMallet. \n",
    "[download](https://www.machinelearningplus.com/wp-content/uploads/2018/03/mallet-2.0.8.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.wrappers.ldamallet as ldamallet\n",
    "\n",
    "mallet_path=\"./mallet-2.0.8/bin/mallet\"\n",
    "ldamalletmodelpr = ldamallet.LdaMallet(mallet_path, corpus, num_topics=10, id2word=id2word)\n",
    "ldamalletmodelpr.print_topics(2)\n",
    "ldamalletmodelpr.save(\"./models/ldamalletmodelpr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
