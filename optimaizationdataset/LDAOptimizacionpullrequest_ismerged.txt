,text
0,WORK-IN-PROGRESS.  Do **NOT** merge.  More work needs to be done and the tests are currently broken.\n- JSONP should require trusted resource URLs.  This would be a breaking\n  change but maybe not too onerous since same origin URLs are trusted in\n  the default config and you can easily whitelist any 3rd party URLs you\n  trust in one single place (your app/module config.)\n- fix a bug where $http can't handle $sce wrapper URLs.\n\nCloses #11352\nCloses #11328\n sec(http): JSONP should require trusted resource URLs >>> 0
1,"JSONP requests allow full access to the browser and the JavaScript context. So allowing a malicious attacker to make a JSONP request to an evil server could have bad results.\n\nBy requiring that JSONP urls are trusted we make it easier for developers to see that their app is only able to make JSONP requests to urls that they have audited.\n\nThere are two ways to trust:\n- whitelisting the url\n- explicitly marking a url as trusted by calling `$sce.trustAs`\n\nThe first commit in this PR ensures that all JSONP requests use a URL that is validated against the `$sce` ResourceUrl whitelist/blacklist checking. This commit creates a Breaking Change for app developers as they must now add all of their JSONP endpoints to the whitelist before their app will work. Although as significant breaking change, it is fairly simple to fix: just search for all calls to `$http.jsonp` and identify the url. Of course if the url is being generated dynamically then this is harder to fix but also indicates that perhaps there is a security vulnerability.\n\nThe second commit allows developers to provide explicitly trusted urls as a result of calling `$sce.trustAsResourceUrl`. This second approach has a few quirks:\n- the `$http` service needs to append parameters to the url, which means that the trusted url must be unwrapped, modified and then re-wrapped as trusted before passing it to `$httpBackend`.\n- it may be possible (??) that there might be an attack vector if an attacker is able to access a different endpoint by changing the parameters. But perhaps since the domain and path cannot be modified by the param serializer this is not an issue?\n\n_Is it acceptable (from a security point of view) to allow parameters to be added to a trusted url? If so then we could refactor the checks to happen in the `$http` service instead of `$httpBackend` which would remove the need for the re-wrapping of the built url._\n\nCloses #11352\nCloses #11328\n feat(http): JSONP requests now require trusted resource URLs >>> 0"
2,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nrefactor\n\n**What is the current behavior? (You can also link to an open issue here)**\nTests are duplicated and have non-optimal expects\n\n**What is the new behavior (if this is a feature change)?**\nTests use they and have been moved to one place\n refactor(compileSpec): make tests consistent >>> 1"
3,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\n**What is the current behavior? (You can also link to an open issue here)**\n\n**What is the new behavior (if this is a feature change)?**\n\n**Does this PR introduce a breaking change?**\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n Updated README.md - Add additional introductory paragraph >>> 0"
4,This PR replaces #15143 which is too complicated.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n feat($httpBackend): JSONP requests now require trusted resource  >>> 0
5,"This PR fixes the issue in bug #15144. If the page specifies a base URL using `<base>` tag, Angular may start to reject relative resource URL if the base URL domain does not match the loading domain. This PR adjusts matchUrl() to also consider the base URL (`document.baseURI`) in addition to `location.href` when matching URLs against the 'self' URl policy.\n\nThis PR does not introduce any breaking changes.\n\nSecurity note: SCE policy can be circumvented by rogue `<base>` tags on the page. However, I believe this is not too much of a concern. If an attacker can inject `<base>` tags, then it's very likely they can inject arbitrary HTML, which is a way worse vulnerability.\n fix($sce): Consider document base URL for 'self' URL policy. >>> 0"
6,"**What kind of change does this PR introduce?** Docs\n\n**What is the current behavior?** Incorrect wording\n\n**What is the new behavior (if this is a feature change)?** Correct wording\n\n**Does this PR introduce a breaking change?** No\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nI don't think ""Idempotence"" is a property that filters need to fulfill: https://en.wikipedia.org/wiki/Idempotence. Idempotence means that `f(f(x)) == f(x)`, i.e. that applying a filter to the result of itself gives the same value as only applying it once. Many common filters are not idempotent. For example, applying reverse twice is not the same as applying it once.\n docs(guide/Filters): Fix incorrect wording >>> 0"
7,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nRefactors & breaking API changes.\n\n**What is the current behavior? (You can also link to an open issue here)**\nBoolean attribute getters take properties into account and setters modify them. All falsy values for boolean attribute setters remove the attribute and set the property to false. The `null` value in a setter (for all attributes, not only boolean ones) sets the attribute value to the string `""null""` instead of removing it.\n\n**What is the new behavior (if this is a feature change)?**\nBoolean attribute getters/setters don't touch properties. The `null` value in a setter (for all attributes, not only boolean ones) removes the attribute. The `false` value removes it only for boolean attributes. Other defined values for boolean attributes set the value to the lowercased attribute name.\n\n**Does this PR introduce a breaking change?**\nYes.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~ - only in a form of commit messages with their `BREAKING CHANGES` sections; is anything else needed?\n\n**Other information**:\n Align jqLite's attr method with jQuery >>> 1"
8,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\nFix/clarification of how synchronous validators work.\n\n**What is the current behavior? (You can also link to an open issue here)**\n#14734 All values except `false` literal and `undefined` are treated as a pass. `undefined` is treated as pending, which makes no sense for synchronous operation.\n\n**What is the new behavior (if this is a feature change)?**\n\nAll falsy value returns to a synchronous validator are treated as fails, including `undefined`.\n\n**Does this PR introduce a breaking change?**\n\nYes, see commit message.\n\n**Please check if the PR fulfills these requirements**\n- [ x ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ x ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nI believe a change to docs in unnecessary. We still expect `true` and `false` to be the primary returns and don't want to encourage otherwise. This is mostly clarifying a niche, undocumented, edge-case to behave more intuitively.\n Patch 14734 boolean sync validators >>> 0"
9,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nRefactor/Fix (depending on your point of view).\n\n**What is the current behavior? (You can also link to an open issue here)**\nErrors thrown in a promise's `onFulfilled` or `onRejected` handlers are treated in a\nslightly different manner than regular rejections:  \nThey are passed to the `$exceptionHandler()` (in addition to being converted to rejections).\nSee also #3174 and #14745.\n\n**What is the new behavior (if this is a feature change)?**\nThe distinction is removed, by skipping the call to `$exceptionHandler()`, thus treating\nthrown errors as regular rejections.\n\n**Does this PR introduce a breaking change?**\nYes.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\nPreviously, errors thrown in a promise's `onFulfilled` or `onRejected` handlers were treated in a\nslightly different manner than regular rejections:\nThey were passed to the `$exceptionHandler()` (in addition to being converted to rejections).\n\nThe reasoning for this behavior was that an uncaught error is different than a regular rejection, as\nit can be caused by a programming error, for example. In practice, this turned out to be confusing\nor undesirable for users, since neither native promises nor any other popular promise library\ndistinguishes thrown errors from regular rejections.\n(Note: While this behavior does not go against the Promises/A+ spec, it is not prescribed either.)\n\nThis commit removes the distinction, by skipping the call to `$exceptionHandler()`, thus treating\nthrown errors as regular rejections.\n\n**Note:**\nUnless explicitly turned off, possibly unhandled rejections will still be caught and passed to the\n`$exceptionHandler()`, so errors thrown due to programming errors and not otherwise handled (with a\nsubsequent `onRejected` handler) will not go unnoticed.\n\nBREAKING CHANGE:\n\nPreviously, throwing an error from a promise's `onFulfilled` or `onRejection` handlers, would result\nin passing the error to the `$exceptionHandler()` (in addition to rejecting the promise with the\nerror as reason).\n\nNow, a thrown error is treated exactly the same as a regular rejection. This applies to all\nservices/controllers/filters etc that rely on `$q` (including built-in services, such as `$http` and\n`$route`). For example, `$http`'s `transformRequest/Response` functions or a route's `redirectTo`\nfunction as well as functions specified in a route's `resolve` object, will no longer result in a\ncall to `$exceptionHandler()` if they throw an error. Other than that, everything will continue to\nbehave in the same way; i.e. the promises will be rejected, route transition will be cancelled,\n`$routeChangeError` events will be broadcasted etc.\n\nFixes #3174\nFixes #14745\n fix($q): treat thrown errors as regular rejections >>> 0"
10,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**: performance improvement\n\n**What is the current behavior? (You can also link to an open issue here)**: it makes copies of very large objects:\n\n``` javascript\n    it('should not copy verylargeobject', inject(function($rootScope, $compile) {\n      var getProp = jasmine.createSpy('getProp');\n      var verylargeobject = {};\n      Object.defineProperty(verylargeobject, 'prop', {\n        get: getProp,\n        enumerable: true\n      });\n      element = $compile('<div ng-class=""{foo: verylargeobject}""></div>')($rootScope);\n      $rootScope.verylargeobject = verylargeobject;\n      $rootScope.$digest();\n\n      expect(getProp).not.toHaveBeenCalled();\n    }));\n```\n\n**What is the new behavior (if this is a feature change)?**: the user perceives no change but that it is faster with very large objects\n\n**Does this PR introduce a breaking change?**: no\n\n**Please check if the PR fulfills these requirements**\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [X] Tests for the changes have been added (for bug fixes / features)\n- [X] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\nThis PR responds to https://github.com/angular/angular.js/pull/14394#issuecomment-207812797 \nIt is alternative to https://github.com/angular/angular.js/pull/14394\n perf(ngClass): refactor to optimize the case of static map of classes with large objects >>> 0"
11,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nrefactor / cleanup, maybe performance gains when creating promises\n\nThis makes `Deferred` a simple wrapper of `Promise`. `Promise`s can now be constructed, chained etc without creating any `Deferred` objects.\n\nBest viewed [ignoring whitespace](https://github.com/angular/angular.js/pull/15064/files/?w=1)\n refactor($q): separate Deferred out of Promise implementation >>> 0"
12,"…dules\n\nThis feature allows the developer to store additional meta data about a\nmodule when it is defined by adding an additional parameter containing\nan object to the `angular.module(moduleName, deps, info, configFn)` call.\n\nDevelopers can then access this information by calling `angular.info(moduleName)`\n\nSee this https://github.com/angular/material/issues/3842 for more background.\n feat(angular.info): optionally store and access additional info on mo… >>> 0"
13,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\nfeature\n\n**What is the current behavior? (You can also link to an open issue here)**\n\nNo way to find information about the currently loaded modules\n\n**What is the new behavior (if this is a feature change)?**\n\nTwo new features:\n- a method on `Module` (`info()`) that lets developers add arbitrary info about their modules.\n- a property on `$injector` (`modules`) that lets developers access the module objects that were loaded into the injector at bootstrap.\n\nThis second item could also be useful to @ocombe for his ocLazyLoad project.\n\n**Does this PR introduce a breaking change?**\n\nNo\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nReplaces the previous #12465 PR\n Module info >>> 0"
14,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFix, Test, Refactor, Perf\n\n**What is the current behavior? (You can also link to an open issue here)**\nBroken, inefficient.\n\n**What is the new behavior (if this is a feature change)?**\nFixed, efficient.\n\n**Does this PR introduce a breaking change?**\nNo (to the best of my knowledge).\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\nSupercedes (and partly based on) #14404. Mad props to @drpicox for the initial idea and implementation.\n\nThis PR has been broken up in smaller, focused commits for easier reviewing. Better to review commit by commit (and probably in order). Most commits are (more or less) trivial - except for the last one.\n\nRefactoring commits:\n- 8822006: refactor(ngClass): remove unnecessary dependency on `$animate`\n- 0409589: refactor(ngClass): remove redundant `$observe`r\n- 2871a11: refactor(ngClass): simplify conditions\n- a7e69f7: refactor(ngClass): move helper functions outside the closure\n- 37cec7a: refactor(ngClass): exit `arrayDifference()` early if an input is empty\n\nTest commits:\n- 2705085: test(ngClass): add some tests about one-time bindings and objects inside arrays\n\nFix commits:\n- 0ba4be6: fix(ngClassOdd/Even): keep track of classes even if odd/even do not match index\n\nPerf commits:\n- 89268af: perf(ngClass): only access the element's `data` once\n- 5708424: perf(ngClass): avoid deep-watching (if possible) and unnecessary copies\n\nCloses #14404\n ngClass[Odd/Even] overhaul >>> 0"
15,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFeature\n\n**What is the current behavior? (You can also link to an open issue here)**\n`input[range]` is not supported.\n\n**What is the new behavior (if this is a feature change)?**\nSupport for `input[range]` can be opted into, by using the `ng-range-input` attribute.\n\n**Does this PR introduce a breaking change?**\nNo.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\nThis is a POC. Docs need to be updated.\n feat(input): add opt-in support for `input[range]` >>> 0"
16,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFix\n\n**What is the current behavior? (You can also link to an open issue here)**\nInternationalized Domain Name Urls (IDN), e.g. with an Umlaut, in Edge lead to an infinite digest on page load.\n\n**What is the new behavior (if this is a feature change)?**\nNo Infdig\n\n**Does this PR introduce a breaking change?**\nNo\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n\nFixes #15217\n\nNote:\nWe don't test on Edge yet, but I've tested this locally with the https://github.com/nickmccurdy/karma-edge-launcher \n fix($location): prevent infinite digest with IDN urls in Edge >>> 1"
17,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFeature\n\n**What is the current behavior? (You can also link to an open issue here)**\n`jqLite(callback)` is not supported.\n\n**What is the new behavior (if this is a feature change)?**\n`jqLite(callback)` is an alias to `jqLite(document).ready(callback)`. The latter is now deprecated.\n\n**Does this PR introduce a breaking change?**\nNo.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n feat(jqLite): implement jqLite(f) as alias to jqLite(document).ready(f) (also: deprecate the latter in the second commit) >>> 0"
18,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nA behavior change.\n\n**What is the current behavior? (You can also link to an open issue here)**\n`jqLite#data` uses keys unchanged.\n\n**What is the new behavior (if this is a feature change)?**\n`jqLite#data` camelCases keys in its getters/setters. This aligns jqLite with jQuery.\n\n**Does this PR introduce a breaking change?**\nYes.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nThis is WIP; this might slightly decrease performance so I wanted to discuss it first.\n\nAlso, finishing this would require making our `camelCase` function more strict so I'd like to discuss it first but I think we should do that anyway; the `-moz-` hack doesn't really belong here.\n refactor(data): camelCase keys in jqLite#data >>> 0"
19,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**: benchmark\nAdds a benchmark to measure the impact of changes in ngClass.\n\n**What is the current behavior? (You can also link to an open issue here)**: https://github.com/angular/angular.js/pull/15228#commitcomment-19347288 \n\n**What is the new behavior (if this is a feature change)?** no change\n\n**Does this PR introduce a breaking change?** no\n\n**Please check if the PR fulfills these requirements**\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] ~~Tests for the changes have been added (for bug fixes / features)~~\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\nIt adds a benchmark in `benchmarks/ng-class-bp`\nThe benchmark was designed to measure the changes introduced by: https://github.com/angular/angular.js/pull/15228\n\nIt has the following steps:\n- _setup_: cleanups preview, and create structures, not worth to measure\n- _create_: creation of a table/list with the selected number of items and applied classes\n- _$apply_: an apply without changes (compute digest time)\n- _update_: cost of updating classes (changes ng-classes values)\n- _unclass_: cost of removing all classes (falses all ngClass values)\n- _class_: cost of apply classes (applies all classes given already constructed elements)\n- _destroy_: cost of removing everything from dom (usually not worth to measure)\n\nThere are three versions to paint:\n- **optimized table**: a table with optimized ngClass values `ng-class=""'success' && todo.completed""`\n- **unoptimized table**: a table with object ngClass syntaxis: `ng-class=""{success: todo.completed}""`\n- **list**: list of todos with an object ngClass and all cases: `ng-class=""{completed: todo.completed, important: todo.important, urgent: todo.urgent}""`\n- **single optimized**: it computes a single ngClass using a complex object but optimizing the access `ng-class=""{success: !!$ctrl.todos, danger: !$ctrl.todos}""`\n- **single**: it computes a single ngClass using a complex object `ng-class=""{success: $ctrl.todos, danger: !$ctrl.todos}""`\n\nI ran them over the previous version of ngClass and the version of #15228 , the results are:\n- create, update, unclass, class:\n  - variability in create, update, unclass and class seems to have more or less the same performance\n  - the variability seems bigger in the #15228 implementation\n- $apply:\n  - optimized table:\n    - #15228 is around a 25% slower than previous version\n    - in my computer previous version took 0.75ms for 1000 rows, and for  #15228 around 1ms\n    - I think that this implementation is not often used\n  - unoptimized table and list:\n    - #15228 is faster than previous version, around 5x and 10x faster\n    - I ran the benchmark many times just to be sure\n    - previous version has high variability from 7ms ~ 13ms\n    -  #15228 maintains the speed around 1ms\n  - single optimized:\n    - #15228 seems to be slower but there is high variability\n    - times are around 0.02ms ~ 0.08ms\n  - single:\n    - #15228 is really really faster (around more than 100x faster)\n    - times of #15228 are close to single optimized\n    - previous version takes around 1.5ms\n\nI also modified #15228 to use compile instead of link (commented in https://github.com/angular/angular.js/pull/15228#commitcomment-19360023 ). Results are:\n- create:\n  - it improves the performance around 5%~10% (faster compile time)\n- other cases:\n  - no appreciable changes\n\nSo, as a conclusion, it seems that the **#15228  optimizes automatically code in the same fashion that an experienced angular programmers would do** manually. \nIn the other hand, given that 1 frame is around 16ms, we should give 8ms to the browser to update, it leaves 8ms for the application to work. Experiments show that digest loop times moves from values like 8 or 10ms to just 1ms in large lists, or 0.01ms in a single big object condition.\n chore(benchpress): add a ngClass benchmark >>> 0"
20,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nbug,  solving Issue https://github.com/angular/angular.js/issues/14990\n\nCloses #14990\n fix($parse): treat falsy values as defined in assignment expressions >>> 0"
21,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFix, Test, Refactor, Perf\n\n**What is the current behavior? (You can also link to an open issue here)**\nBroken, inefficient.\n\n**What is the new behavior (if this is a feature change)?**\nFixed, efficient.\n\n**Does this PR introduce a breaking change?**\nNo (to the best of my knowledge).\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\nThis is a cleaned up version of #15228 (I didn't want to rebase #15228 to avoid losing existing comments or their context).\n\nSo, #15243 applies to this PR as well.\n ngClass[Odd/Even] overhaul (2) >>> 0"
22,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nRefactor\n\n**What is the current behavior? (You can also link to an open issue here)**\njqLite's camelCasing logic converts not only dashes but also underscores & colons; it also collapses multiple dashes etc.\n\n**What is the new behavior (if this is a feature change)?**\njqLite's camelCasing logic is separated from the one used in compile/sce and more strict, making jqLite more similar to the jQuery behavior. This is also needed to make the new planned data key camelCasing work like in jQuery 3.\n\n**Does this PR introduce a breaking change?**\nYes.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\njqLite needs camelCase for it's css method; it should only convert one dash\nfollowed by a lowercase letter to an uppercase one; it shouldn't touch\nunderscores, colons or collapse multiple dashes into one. This is behavior\nof jQuery 3 as well.\n\nThis commit separates jqLite camelCasing from the $compile one (and $sce but\nthat's an internal-only use). The $compile version should behave as before.\n\nAlso, jqLite's css camelCasing logic was put in a separate function and\nrefactored: now the properties starting from an uppercase letter are used by\ndefault (i.e. Webkit, not webkit) and the only exception is for the -ms- prefix\nthat is converted to ms, not Ms. This makes the logic clearer as we're just\nalways changing a dash followed by a lowercase letter by an uppercase one; this\nis also how it works in jQuery.\n\nRef #15126\nFix #7744\n refactor(*): separate jqLite/compile/sce camelCasing logic >>> 0"
23,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)** Feature.\n\n**What is the current behavior? (You can also link to an open issue here)**: \nWith restrict you can reference parent controllers from a children, but if you want to reference children components you need to find them with _jqLike_ and then get the controller.\n\n**What is the new behavior (if this is a feature change)?**: \nIt adds a new directive that assigns the controller of the current DOM element component to any expression. If the element is destroyed (ex: it was inside ngIf or ngRepeat) it assigns a null.\n\n``` html\n<toggle ng-as=""burger""></toggle>\n<menu ng-if=""burger.isVisible""></menu>\n...\n<countdown-timer ng-as=""$ctrl.countdownTimer""></countdown-timer>\n<button ng-click=""$ctrl.relaunch()""></button>\n```\n\n**Does this PR introduce a breaking change?** \nNo.\n\n**Please check if the PR fulfills these requirements**\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [X] Tests for the changes have been added (for bug fixes / features)\n- [X] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nI present in this PR the implementation of a directive that I use in almost all my projects (and other projects in which I'm consultant) since the last year.\n### Basic use\n\nIt is really simple: it publishes the controller of component in the current scope. Ex:\n\n``` html\n<toggle ng-as=""myToggle""></toggle>\n<button ng-click=""myToggle.toggle();"">Toggle</button>\n<p ng-show=""myToggle.isOpened()"">The toggle is active</p>\n```\n\nso it can be accessed from enclosing components. \nUntil this point, it is a kind of custom `controllerAs` property for parent scope.\n### Using with parent components\n\nIn addition, as following the recommendation did by @petebacondarwin in https://github.com/angular/angular.js/issues/10007#issuecomment-168425866 it is able to assign it to any value, event enclosing controller. Ex:\n\n``` html\n<toggle ng-as=""$ctrl.myToggle""></toggle>\n```\n### ~~Keep track of DOM elements~~\n\nIt also serves to look for exact DOM elements instead of relaying in _jqLite_ searches. In case that there is no a controller, the as directive assigns the current DOM element to the expression set.\n\n``` html\n    <br>From <a href=""https://developer.mozilla.org/en/docs/Web/HTML/Element/audio"">here</a>.<br>\n    <player></player>\n    <script>\n        angular.module('demo').component('player', {\n            template: '' +\n                '<audio as=""$ctrl.audio"" src=""http://developer.mozilla.org/@api/deki/files/2926/=AudioTest_(1).ogg""></audio>' +\n                '<button ng-click=""$ctrl.play()"">Play</button>' +\n                '',\n            controller: function() {\n                this.play = function() {\n                    this.audio.play();\n                };\n            },\n        });\n    </script>\n```\n### _Deprecating_ ngController\n#### Why?\n\nngController is great for SEO webpages, but it has the inconvenience that it does not integrates well with bindings and many things have to be done ""manually"". For example:\n\n``` html\n    <script>\n        angular\n            .module('demo', [])\n            .service('booksService', function() {\n                this.getBook = function(id) {\n                    return { name: 'name'+id, abstract: 'abstract...'+id };\n                };\n            });\n    </script>\n\n    <div ng-controller=""BookDetailController as bookDetailCtrl"" book-id=""1"">\n        <h1>{{bookDetailCtrl.book.name}}</h1>\n        <p>{{bookDetailCtrl.book.abstract}}</p>\n    </div>\n    <script>\n        angular.module('demo').controller('BookDetailController', function(booksService,$attrs) {\n            this.book = booksService.getBook($attrs.bookId);\n        });\n    </script>\n```\n\nIn this example it needs to access to `$attrs` manually to find the bookId. You can imagine more complex functionalities, but I think that this is just enough to expose.\n\nNow, imagine that we do a directive to handle the same behaviour:\n\n``` html\n    <static-book-detail book-id=""2"">\n        <h1>{{bookDetailCtrl.book.name}}</h1>\n        <p>{{bookDetailCtrl.book.abstract}}</p>\n    </static-book-detail>\n    <script>\n        angular.module('demo').directive('staticBookDetail', function() {\n            return {\n                scope: true,\n                controller: function(booksService) {\n                    this.book = booksService.getBook(this.bookId);\n                },\n                controllerAs: 'bookDetailCtrl',\n                bindToController: {\n                    bookId: '@',\n                },\n            };\n            this.book = booksService.getBook($attrs.bookId);\n        });\n    </script>\n```\n\nIt has a conceptual problem: you must know by heart that static-book-detail publishes the `bookDetailCtrl`. \n\nAlternatively you can use a component, which always use `$ctrl` like:\n\n``` html\n    <p>Does not works as expected:</p>\n    <static-book-component book-id=""3"">\n        <h1>{{$ctrl.book.name}}</h1>\n        <p>{{$ctrl.book.abstract}}</p>\n    </static-book-component>\n    <script>\n        angular.module('demo').component('staticBookComponent', {\n            bindings: {\n                bookId: '@',\n            },\n            controller: function(booksService) {\n                this.book = booksService.getBook(this.bookId);\n            },\n        });\n    </script>\n```\n\nbut this naive solution does not works because `$ctrl` is not defined in the same scope that code relies.\n#### Popossal\n\nFinally, using as you can have this solution that seems cleaner and more easy to understand:\n\n``` html\n    <p>It dows works (and no implicit):</p>\n    <static-book-component ng-as=""$ctrl"" book-id=""4"">\n        <h1>{{$ctrl.book.name}}</h1>\n        <p>{{$ctrl.book.abstract}}</p>\n    </static-book-component>\n\n    <script>\n        angular.module('demo').component('staticBookComponent', {\n            bindings: {\n                bookId: '@',\n            },\n            controller: function(booksService) {\n                this.book = booksService.getBook(this.bookId);\n            },\n        });\n    </script>\n```\n\n_Additional Notes_: `ngAs` directive behaviour is very close to **Angular2** `#` template variables. It also copies concepts from _RiotJS_ and _Polymer_ (at least first versions).\n feat(ngAsDirective): new as directive to publish component controllers into current scope  >>> 0"
24,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix\n\n**What is the current behavior? (You can also link to an open issue here)**\nresult of ngResource.toJSON still include $cancelRequest\n\n**What is the new behavior (if this is a feature change)?**\nremove delete $cancelRequest from ngResource.toJSON's result.\n\n**Does this PR introduce a breaking change?**\nNo\n\n**Please check if the PR fulfills these requirements**\n- [ Y ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ N ] Tests for the changes have been added (for bug fixes / features)\n- [ N ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n fix($resource): Delete $cancelRequest from toJSON() >>> 0"
25,nan docs($rootScope.Scope): grammar >>> 0
26,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix.\n\n**What is the current behavior? (You can also link to an open issue here)**\n`step` validation (for `input[number]`/`input[range]`) fails on some occasions due to limitations of Floating Point Arithmetic (as described in #15257) and does not fully account for a `stepBase` as descussed in https://github.com/angular/angular.js/commit/9a8b8aa#commitcomment-19108436. \n\n**What is the new behavior (if this is a feature change)?**\nIt works as expected!\nBTW, according to [the spec](https://www.w3.org/TR/html5/forms.html#concept-input-min-zero), when there is no `min` attribute (to determine the step base), but there is a value attribute, then the step base is set to the value of that. In our case this doesn't apply very well, because the value attribute is not compatible with `ngModel` and is essentially ignored.\n\n**Does this PR introduce a breaking change?**\nNo\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\n- [ ] Needs to be backported to 1.5.x for `[ng-input-range]`.\n\nFixes https://github.com/angular/angular.js/commit/9a8b8aa#commitcomment-19108436\nFixes #15257\n fix(input): fix `step` validation for `input[number]`/`input[range]` >>> 0"
27,"This moves the object/array-literal watching logic from $compile to $parse so it can by used by watchers anywhere (not only bindings).\n\nBy moving this to `$parse` we can potentially improve it more. Maybe pushing the `equals` check into `expressionInputDirtyCheck` for literals, then we can avoid creating the object and avoid calling `equals` on the full object each digest?\n\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nbug fix\n\n**What is the current behavior? (You can also link to an open issue here)**\nwatching object/array literals throws infdig if any inputs are non-simple values\n\n**What is the new behavior (if this is a feature change)?**\nwatching object/array literals does what compile previously did to solve this\n\n**Does this PR introduce a breaking change?**\nno\n\n**Please check if the PR fulfills these requirements**\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n fix($parse): allow watching object/array literals >>> 0"
28,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nfix in the docs app\n\n**What is the current behavior? (You can also link to an open issue here)**\nOn https://docs.angularjs.org/api, the text currently says \n\n> These pages contain the AngularJS reference materials for version 1.5.8 arbitrary-fallbacks.\n\nbut the default docs are for the master / snapshot version. \n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n\n**Other information**:\n\nPreviously, the index would show the version of Angular that runs on\nthe page, not the version for which the docs are. This meant that in\nthat snapshot docs the stable version was displayed.\n\nThe `$scope.docsVersion` value was used in the plnkr opening code, but\nhas not been used since https://github.com/angular/angular.js/commit/bdec35cebc89e0d80a04eeffbd71ad999fc7e61a.\n fix(docsApp): show correct version number in api index >>> 1"
29,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nFeature for the docs app\n\n**What is the current behavior? (You can also link to an open issue here)**\nYou cannot switch to a newer version from an older version in the docs app.\n\n**What is the new behavior (if this is a feature change)?**\nFor docs app versions that contain this commit, switchting is possible. The docs app\nwill request the new versions-data.json file from the snapshot folder and fill the version switcher with\nthis data.\n\n**Does this PR introduce a breaking change?**\nNo\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- ~[] Tests for the changes have been added (for bug fixes / features)~\n- ~[ ] Docs have been added / updated (for bug fixes / features)~\n\n**Other information**:\n\nWith this patch, the docs app will request available versions data from the snapshot,\nwhich are now stored in a separate versions-data.json file.\n\nThis only affects docs apps that contain this commit, so older doc apps\nwill still only display the versions up to their own version.\n chore(docs-app): allow switching to newer versions >>> 0"
30,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\ndocs\n\n**Does this PR introduce a breaking change?**\nputs the warning for a future one\n\nAnything else that needs to be done to prepare for removing this in 1.7?\n docs($controller): deprecate the use of $controllerProvider#allowGlobals >>> 0"
31,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix\n\n**What is the current behavior? (You can also link to an open issue here)**\nhttps://github.com/angular/angular.js/issues/15268\n\n**What is the new behavior (if this is a feature change)?**\nCleans up attributes observer on change.\n\n**Does this PR introduce a breaking change?**\nNo.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n fix($compile): Cleanup attribute-binding observers >>> 0"
32,"Documentation updates to clarify default sort behavior. See\nhttps://github.com/angular/angular.js/issues/15293 for more information\n\n(These changes are slightly different than the ones in #15302, since I started from master rather than 1.3.x docs)\n docs(orderBy): Clarify behavior of default comparator >>> 0"
33,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\ndocs update\n\n**What is the current behavior? (You can also link to an open issue here)**\n\n**What is the new behavior (if this is a feature change)?**\n\n**Does this PR introduce a breaking change?**\n\n**Please check if the PR fulfills these requirements**\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nI think it is a typing error.\nI wonder it is.\nreview this PR.\nthx.\n docs($rootScope.Scope): correct a typing error >>> 0"
34,"I think this is a better alternative to #15274, but has a breaking change (which didn't break any tests!).\n\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix, perf\n\n**What is the current behavior? (You can also link to an open issue here)**\nWatching literals doesn't always work, literals passed to components are deep watched to work around this.\n\n**What is the new behavior (if this is a feature change)?**\nWatching literals now does a watch of the inputs.\n\n**Does this PR introduce a breaking change?**\nYES - see below or second commit\n\n**Please check if the PR fulfills these requirements**\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\nBREAKING CHANGE (65ff861):\nPreviously when a literal value was passed into a directive/component via one-way binding it would be watched with a deep watcher.\n\nFor example, for `<my-component input=""[a]"">`, a new instance of the array would be passed into the directive/component (and trigger $onChanges) not only if `a` changed but also if any sub property of `a` changed such as `a.b` or `a.b.c.d.e` etc.\n\nThis also means a new but equal value for `a` would NOT trigger such a change.\n\nNow literal values use a non-deep watch similar to other directive/component one-way bindings. Changes are only trigger when the value itself changes.\n\nAvoiding deep watchers for array/object literals will improve watcher performance of all literals passed as one-way bindings, especially those containing references to large/complex objects.\n fix+perf($parse): allow watching array/object literal values, disable deep watch for one-way bindings >>> 0"
35,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\n**What is the current behavior? (You can also link to an open issue here)**\n\n**What is the new behavior (if this is a feature change)?**\n\n**Does this PR introduce a breaking change?**\n\n**Please check if the PR fulfills these requirements**\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n Changes test >>> 1"
36,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\n**What is the current behavior? (You can also link to an open issue here)**\n\n**What is the new behavior (if this is a feature change)?**\n\n**Does this PR introduce a breaking change?**\n\n**Please check if the PR fulfills these requirements**\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nWe have two fields in package.json for checking the current version:\n- branchVersion\n- branchPattern\n\nThe `branchVersion` field is used to work out what version to use in the\ndocs application, so we should not update this to the most recent version\nuntil that version is on the Google CDN. Otherwise the docs app will break.\n\nThe `branchPattern` is used to determine what branch we are currently\nworking from and is generally used as a gate-keeper to prevent invalid\nreleases from the wrong branch.\n\nThe `getTaggedVersion()` method was using the `branchVersion` to check\nthat the tagged commit was valid but this fails when we are moving to a\nnew minor version with release candidates.\n\nThis fix avoids the problem by doing a custom comparison against the\n`branchPattern` instead.\n chore(version-info): use branchPattern to check tag >>> 1"
37,"I understand that you want the commit to eventually have `style` as the type, I'm happy to roll up this series as a single commit w/ that as the main line.\n\nFor now, I'm more interested in feedback on the content of my commits.\n style: Spelling fixes >>> 1"
38,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\ndocs update\n\n**What is the current behavior? (You can also link to an open issue here)**\n\nno hint that `*` is a valid scope -- it hints you could omit the `(...)` bit, but most commits of late don't\n\n**What is the new behavior (if this is a feature change)?**\n\ndocumentation of `*`\n\n**Does this PR introduce a breaking change?**\n\nno\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n\n**Other information**:\n docs(CONTRIBUTING.md): add note about scope wildcard >>> 1"
39,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\ndocs\n\n**What is the current behavior? (You can also link to an open issue here)**\nExample is broken in the snapshot because of changes to JSONP\n\n**Other information**:\n\nThe example will still be broken when opened in a plnkr, because they use the stable version instead of the snapshot.\n docs(guide/Conceptual Overview): fix external api example >>> 1"
40,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ndocs update\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe commit types went out of sync with https://github.com/angular/angular/blob/master/CONTRIBUTING.md#type\r\nWhat prompted me to do this fix is that there wasn't 100% clear that test fixes (e.g. for flaky tests) should come in the ""test"" commits.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nCommit types are the same as in Angular repo.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nExpanded ""test"" to also mean test fixes, added ""build"" and ""ci"".\r\nThis is to mirror the documentation in Angular (without JS) repo. docs(CONTRIBUTING.md): Updated commit types. >>> 1"
41,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nPerformance\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nIn some cases IE11/Edge calls to element.value are very slow when the element.value has not been set. Normally, these calls are usualy 0-3 ms but in these cases it can take 200-300 ms. This can easily add 3 or more seconds to the load time on a view that has 10 or more select tags using ngOptions.\r\n\r\nThe line this pull request is changing not only suffers from the performance issue described above but it also appears to be broken. The code is checking that option.value does not equal element.value but then sets element.value to option.selectValue. I don't believe option.value is actually defined anywhere and likely it was always intended to be option.selectValue. This means that check would always be true and since this code has been this way for quite a while and is causing a performance issue I've just removed the check. This way a call to element.value is never made prior to it's value being set.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n perf(ngOptions): avoid calls to element.value >>> 0"
42,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nperf\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nStructural animations such as ngIf trigger an additional digest after $animate.leave completes\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- ~[ ] Docs have been added / updated (for bug fixes / features)~\r\n\r\n perf(*): don't trigger digests after enter/leave of structural directives >>> 1"
43,"Extension URIs (`resource://...`) bypass Content-Security-Policy in Chrome and\r\nFirefox and can always be loaded. Now if a site already has a XSS bug, and uses\r\nCSP to protect itself, but the user has an extension installed that uses\r\nAngular, an attacked can load Angular from the extension, and Angular's\r\nauto-bootstrapping can be used to bypass the victim site's CSP protection.\r\n\r\nNotes:\r\n- `isAutoBootstrapAllowed` must be initialized on load, so that `currentScript`\r\n  is set correctly.\r\n- The tests are a bit indirect as reproducing the actual scenario is too\r\n  complicated to reproduce (requires signing an extension etc). I have confirmed\r\n  this to be working manually. fix(security): do not auto-bootstrap when loaded from an extension. >>> 0"
44,"**What is the current behavior? (You can also link to an open issue here)**\r\nCurrently, plnkrs are opened with the cdnVersion of the built docs app, which means\r\nthat snapshot plunkers might not work because they use the cdn (stable version).\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n\r\n**Other information**:\r\n\r\nShould we update this info for the other deployments too?\r\n\r\n chore(docs-gen): create plnkr examples with the correct version >>> 1"
45,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\n fix($location): don't remove multiple slashes from urls >>> 0"
46,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfix\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nsee the commit message\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nsee the commit message\r\n\r\n**Does this PR introduce a breaking change?**\r\nThe previous behaviour would have led to a broken app, so it is highly\r\nunlikely that any app relied on this behaviour. So this doesn't constitute\r\na breaking change.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\nAlternative solution #15359\r\n Location path 2 >>> 1"
47,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nHiding a property and overriding a property seem to me fundamentally different things. I thought while reading that the '(hides)' did not offer any more clarification than just saying overrides, and caused me to pause unnecessarily to understand it. docs(guide/Controllers): remove the word 'hides' >>> 0"
48,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nchore\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nVersions in the version picker are hard-coded into each build\r\nThis means that newer versions are never shown in older versions of the docs\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nNow the version information has split into the current and the list of all versions.\r\nIn the production deployment the version list is now read from the snapshot build, so we always\r\nget the latest list.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nReplaces #15381 chore(docs): improve version picker >>> 1"
49,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nfeat\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n`ngModelOptions` cannot be inherited from ancestor `ngModelOptions`\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n`ngModelOptions` can inherit if specified via `$inherit` values\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nA small internal-ish change to the programmatic API for reading options.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Ng model options extend >>> 0"
50,nan docs($compile): add double compilation known issue >>> 1
51,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nNo migration docs for 1.5 -> 1.6.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nMigration docs for 1.5 -> 1.6.\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nThis needs to be updated with the changes that land on future RCs. WIP - docs(guide/migration): add ""Migrate 1.5 to 1.6"" section >>> 0"
52,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Ng model options fix >>> 0"
53,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nCloses #15351 chore(docs): deprecation notices for methods and properties >>> 0"
54,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nN/A\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nN/A\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n\r\n**Other information**:\r\n\r\n\r\n1. The conditions checking the msie variable value have been simplified.\r\nThere is e.g. no point to check if `msie <= 11` since there IE 12 won't ever\r\nexist.\r\n2. Edge UA-sniffing has been added to tests (only!) where appropriate\r\n3. Support comments for IE/Edge have been added. chore(*): cleanup msie handling; add support comments >>> 0"
55,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n-webkit-prefixed rules are sometimes added manually in ngAnimate tests and sometimes not accounted for at all.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n`ss.addPossiblyPrefixedRule` is used where applicable, automatically handling the `-webkit-` prefix. Manually added prefixes have been removed.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n\r\n**Other information**:\r\n\r\n\r\n1. Change all transition/transform/animation-related ss.addRule to\r\n   ss.addPossiblyPrefixedRule to account for the -webkit- prefix.\r\n2. Remove manually added -webkit-prefixed rules in favor of automatically\r\n   handling them in ss.addPossiblyPrefixedRule. chore(ngAnimate): cleanup vendor prefixes handling in tests >>> 0"
56,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nfeature\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nno obvious way to programmatically change model options for an ngModel directive\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nAdds `$overrideModelOptions` method\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\n feat(ngModel): add `$overrideModelOptions` support >>> 1"
57,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nbug fix\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen an option contains multiple interpolations, an ngValue on the option element is ignored. See #15413\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features) fix(select): let ngValue take precedence over option text with multip… >>> 1"
58,"Many browsers have some extension URL scheme. It is unclear how many of\r\nthose have the security issue of allowing parser-inserted loads of\r\nextension URLs.\r\n\r\nTo be conservative, this code whitelists the URL schemes that are known\r\nto be subject to CSP, i.e. the ones that are expected and safe.\r\n\r\nNote: there is no change in tests as behavior does not change for any known URL. feat(security): explicitly whitelist URL schemes for bootstrap. >>> 1"
59,Follow-up to #15427. There is no reason for allowing cross-origin automatic bootstrapping at URLs with an unknown scheme.\r\n\r\n@mprobst @petebacondarwin feat(security): do not bootstrap from unknown schemes with a different origin >>> 0
60,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfeature\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nhttps://github.com/angular/angular.js/issues/15402\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\ncloses #15402 feat(minErr):  set max depth for angular error JSON stringify >>> 0"
61,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nAlthough e1da4be introduced a breaking change, it wasn't documented as such.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe breaking change is documented.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n<sub>(In a way. ""Let me introduce you this breaking change. It has been around for 2 releases, but I don't think you've met.)""</sub>\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] Docs have been added / updated (for bug fixes / features) docs(*): document the breaking change introduced in e1da4be >>> 1"
62,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n chore(*): use Yarn for dependency management >>> 1"
63,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore/Refactor.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nDuring the `grunt minall` task, `ng-closure-runner` logs some warnings due to:\r\n1. Throwing errors that do not seem to be `minErr` instances.\r\n2. A `@this` annotation in a non-JSDoc comment. \r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nNo warnings during `grunt minall`.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n chore(*): silence ng-closure-runner warnings during `grunt minall` >>> 0"
64,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix (for docs app).\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe URLs used for scripts when opening an example from the snapshot version in plnkr is incorrect and the code does not work.\r\nSee #15437.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe correct script URLs are used and the examples work.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15437 chore(docs): use correct script-URL for plnkr on snapshot >>> 0"
65,- Add note recommending against watching `File` objects with deep watchers\r\n\r\nThis should address #14352 docs($rootScope): add note about watching File objects >>> 0
66,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfeat / fix to the docs app\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`@example` in  `@method` is not rendered\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n chore(doc-gen): render @example tag for ngdoc @method >>> 1"
67,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs Update\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nImproving the error doc with more appropriate error message.\r\nImproving the error message for ng:areq with more details and examples.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nAdded more examples\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nImproving the error message for ng:areq with more details and examples. docs(error/areq): describe your change... >>> 0"
68,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFixes two (potential) test failures on Chrome 53-57+.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n[These tests][1] can fail on Chrome, because the **reported** size of the inner divs is subject to global display settings (e.g. font size) and browser settings (e.g. default zoom level).\r\nSee also #15333.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe assertions are now more robust by comparing against the size of the equivalent, hand-written SVG instead of fixed widths/heights.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15333\r\n\r\n[1]: https://github.com/angular/angular.js/blob/f1db7d735b475a7954023cc63f2b3f0ef685ea7e/test/ng/compileSpec.js#L445-L469 test($compile): work around Chrome issue with reported size for `<foreignObject>` >>> 0"
69,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfix (regression)\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen an empty option has ngIf on it in an ngOptions select, and the ngIf expression is false, then\r\nan error is thrown, because we try to remove the ""selected"" attribute from the ngIf comment node.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- ~[ ] Docs have been added / updated (for bug fixes / features)~\r\n\r\n**Other information**:\r\n~The logic is different in master, and the bug should not be happening there.~ The same bug exists on master\r\n fix(ngOptions): ignore comment nodes when removing 'selected' attribute >>> 1"
70,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix (regression).\n\n**What is the current behavior? (You can also link to an open issue here)**\nParams inside the `hostname` part of a URL are ignored.\nSee #14542.\n\n**What is the new behavior (if this is a feature change)?**\nParams inside the `hostname` part of a URL are not ignored (except if the `hostname` is a `[...]` enclosed IPv6 address).\n\n**Does this PR introduce a breaking change?**\nHardly.\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\nSupport for IPv6 addresses (in b643f0d) was too aggressive and broke support for params in the\n`hostname` part of a URL.\nThis commit restores support for params in the `hostname`, as long as it is not an IPv6 address.\n\nFixes #14542\n fix($resource): allow params in `hostname` (except for IPv6 addresses) >>> 0"
71,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfix (regression)\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen an empty option has ngIf on it in an ngOptions select, and the ngIf expression is false, then\r\nan error is thrown, because we try to remove the ""selected"" attribute from the ngIf comment node.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- ~[ ] Docs have been added / updated (for bug fixes / features)~\r\n\r\n**Other information**:\r\n fix(ngOptions): ignore comment nodes when removing 'selected' attribute >>> 1"
72,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nchore\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nDocs app uses bower\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nNo use of bower in the docs application\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\n chore(docs): don't use bower for docs dependencies >>> 1"
73,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nfeat\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nSupporting unidirectional data-flow is rather verbose\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nSupporting unidirectional data-flow is less verbose\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNope\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\nIn order to reduce some of the verbosity associated with conforming to the\r\nideas behind unidirectional data-flow, we can introduce a special\r\nattribute syntax that will be automatically expanded in order to simulate\r\ntwo-way data binding.\r\n\r\nFor example,\r\n\r\n`<my-component ng-bindon-foo=""bar"">` will be expanded into\r\n`<my-component foo=""bar"" foo-changed=""bar = $event"">`\r\n\r\nFixes #15455 feat($compile): support expansion of special `ngBindon` attributes >>> 0"
74,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen removing `<option>` elements, we schedule some operations on the `ngModelCtrl`. For performance reasons, these operation are scheduled to be run as a post-digest operation. If the `<select>` element happens to be removed as well (and its scope destroyed), an error may be thrown due to certain values not being available on the `$scope`.\r\nSee #15466.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nNothing breaks.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15466. fix(select): do not break when removing the element (e.g. via `ngIf`) >>> 0"
75,"The use of hasBody would allow developers of REST interfaces to decide if a request body is necessary or not on a per method basis.\n\nThe way ngResource currently handles request bodies in custom requests make the definition of custom requests pretty useless. It’s not possible to transfere a request body through a  other requests than POST, PUT, PATCH. Or am I wrong?\n\nIt may be true that some browser implementations are also brocken when it comes to methods like DELETE but this broken behavior should not be enforced by angular.\n\nEven as i read the https://github.com/angular/angular.js/issues/3207 after creating the patch its somehow picking up pmariduenas comment on creating a enableDeleteRequestBody. Its just the generic version for all custom requests.\n Add hasBody to ngResource action configuration >>> 0"
76,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nUpdate documentation.\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nn/a\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nn/a\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nMy understanding is that ngDisabled is for form elements only so I propose that the documentation reflect this more clearly. Clarify where ngDisabled should be used >>> 0"
77,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe element was always assumed to have a parent and an error was thrown when that was not the case.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\njqLite is now consistent with jQuery, which silently ignores a call on elements that do not have a parent.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo. (This change only affects already broken code, that would otherwise throw an error.)\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15331\r\nCloses #15367 fix(jqLite): silently ignore `after()` if element has no parent >>> 0"
78,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nRefactoring.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSame old, same old!\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n`HashMap` is replaced with `NgMap`, which for the time being points to `NgMapShim`, an API-compatible `Map` implementation (for the features Angular needs). This will allow us to easily switch to native implementations once they become more stable.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nSee #13209 and #15114 for related discussions. refactor(*): replace `HashMap` with `NgMap` >>> 0"
79,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\ndocs update\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nngMessageExp has the same description as ngMessage\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nUpdated description\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n\r\nOld description was copied over from ngMessage. docs(ngMessageExp): Better description >>> 0"
80,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nIn order for `ngModel` and `ngModelOptions` to work correctly together, the latter's pre-linking function should be run before the former's pre-linking function. This is typically what happens, except when `ngModel` is used on an element which also has a `replace` directive, whose template includes `ngModelOptions`. In that case, the order is reversed.\r\nSee #15492 for more details.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe initialization logic of `ngModelOptions` is moved from its pre-linking function to its controller's `$onInit()` lifecycle hook. This way, it is always guaranteed to be run before `ngModel`'s pre-linking function, even in the `replace` scenario mentioned above. \r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nMaybe.\r\nThere is no breaking change wrt 1.5.x (where the initialization of `ngModelOptions` used to take place in the controller itself), but wrt to 1.6.0 the following usecase will no longer be possible:\r\nA sibling directive to `ngModelOptions`, let's call it `foo` could be modifying the model options definition (e.g. `ng-model-options=""modelOptionsDefinition""`) in its `$onInit()` lifecycle hook. If the `foo` directive had a lower priority that `ngModelOptions`, then it will no longer work after this PR, since its `$onInit()` method will be called _after_ `ngModelOptions` is initialized. \r\n(This can be worked around by changing the priority of `foo` to be higher than `ngModelOptions` or by overwriting `ngModelOptions#$options` from `foo#$onInit()`.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15492. fix(ngModelOptions): work correctly when on the template of `replace` directives >>> 0"
81,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nIf an async task is scheduled (via `$evalAsync()`) on a scope and `$digest()` happens to be called on another, unrelated scope (local digest), then the `asyncQueue` will be drained but `$rootScope.$digest()` will not be called (potentially preventing the changes to propagate correctly through the app).\r\nSee #15127.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nAngular keeps track on whether `$digest()` has been called on `$rootScope` or not and calls `$rootScope.$digest()` if necessary (even if the `asyncQueue` has been drained by a local `$digest()`).\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nThis is a more targeted alternative to #15494.\r\nFixes #15127. fix($rootScope): correctly propagate async changes after local `$digest()` >>> 0"
82,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ndocs update:\r\n\r\nIf no arguments were specified, angular passes the current iteration to the funtion ([here](https://github.com/angular/angular.js/blob/ec565ddd9c536aa7f7441e89f03ea08e23de1a42/src/ng/interval.js#L171))\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] Docs have been added / updated (for bug fixes / features)\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n docs($interval): improve fn description >>> 0"
83,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen updating the value of a `select[multiple]` element, all options are first set to `selected = false` and then the selected ones are set to `true`. By setting an already selected option to `selected = false` and then `true` again - essentially unselecting and reselecting it - causes some browsers (including Firefox, IE and under some circumstances Chrome) to unexpectedly scroll to the last selected option.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe `selected` property of the `<option>` elements is only set if its current value is different than the new one and even then it is set to its final value at once (i.e. without first setting it to `false`), thus avoiding the undesirable behavior.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15477 fix(ngOptions): do not unset the `selected` property unless necessary >>> 0"
84,"…ry docs\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nBad grammar\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nBetter grammar \r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(guide/Migrating from Previous Versions): improve grammar in jque… >>> 0"
85,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`$animate` decides whether an animation should be cancelled based on some assumption that don't hold in specific cases (e.g. when animating transcluded clones with `templateUrl` directives on them for the first time). As a result, the entering elements will not be animated in such cases. This affects commonly used, structural built-in directives (`ngIf`, `ngRepeat`, `ngSwitch` etc).\r\nSee also #15510.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nAll entering animations work as expected (from the first time).\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nBTW, I believe this is a side effect of lazily compiling transcluded content.\r\n\r\nIn addition to the last `fix` commit that fixes the bug, this PR also includes several `refactor` commits that simplify `$animate` internally, mainly by removing unnecessary wrapping/unwrapping of DOM nodes in `jqLite`/`jQuery` collections. This changes may also positively affect performance, especially in animation-heavy apps (~~although I haven't benchmarked it - yet~~ it does indeed :wink:).\r\n(It is probably easier to review one commit at a time.)\r\n\r\nParty addresses #14074 and #14124.\r\n\r\nFixes #15510.\r\n fix(ngAnimate): correctly animate transcluded clones with `templateUrl` >>> 0"
86,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nBug fix.\n\n**What is the current behavior? (You can also link to an open issue here)**\n1. `$animateProvider` fails to detect several invalid `classNameFilter` RegExps.\n2. Even if a `classNameFilter` RegExp is detected as invalid, `$animateProvider` will still use it (despite printing an error message saying that it _is prohibited_).\n\n**What is the new behavior (if this is a feature change)?**\n1. `$animateProvider` does a better job detecting invalid `classNameFilter` RegExps (it is still not perfect though).\n2. If a `classNameFilter` RegExp is detected as invalid, `classNameFilter` will be reset to `null` (to avoid using an invalid RegExp).\n\n**Does this PR introduce a breaking change?**\nNo (unless someone was hacking around the previous implementation in order to allow invalid RegExps - which they shouldn't do anyway).\n\n**Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\n\n**Other information**:\n<sub>This PR contains two separate commits - which fix two different things - so that they can be liked/disliked independently.</sub>\n fix($animate): improve detection and handling of invalid `classNameFilter` RegExp >>> 0"
87,Fix fixes an incorrect Markdown syntax in the CHANGELOG file. docs(CHANGELOG): fix typo >>> 0
88,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFeature\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nConsole is filled with meaningless:  `Possibly unhandled rejection: {}` errors\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nConsole is filled with juicy traceback info.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nI don't think so\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**: Add traceback to unhandled promise rejections, Fixes: #14631 >>> 0"
89,"Return an empty string when an error is catched while getting cookie.\r\n\r\nCloses #15523\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nBug fix\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nhttps://github.com/angular/angular.js/issues/15523\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [X] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($$cookieReader): safe get cookie >>> 0"
90,"This PR only demonstrates a fix, it still needs for someone to add tests.\r\n\r\nCloses #15536 Fix issue: 15536 >>> 0"
91,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nfeature\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nAngular doesn't recognise objects prototypically inherited from `Array.prototype` as arrays.\r\nSee #15533\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nIn particular, this change gets Angular to play nicely with MobX observable arrays. \r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nYes. `angular.isArray` is used throughout Angular. In particular, after this change, `angular.copy` will use the array-cloning logic for objects prototypically inherited from `Array.prototype` whereas now it uses the object-cloning logic for them. This, in turn, affects deep watchers and other parts of the framework that use `angular.copy`.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n feat(isArray): make angular.isArray support Array subclasses >>> 0"
92,Removing implicit bias from an example. Docs-only PR. @petebacondarwin \r\n docs(guide/Forms): update an old example to remove gender bias >>> 0
93,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSome browsers (e.g. Safari 9.x, PhantomJS) do not set `link.origin/protocol` correctly, when setting `link.href` to `null`, which prevents auto-bootstraping Angular from scripts without a `src` attribute (i.e. inline scripts).\r\nSee also #15567.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nAuto-bootstraping Angular from inline scripts is allowed.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nInline scripts are on the same origin as the loading page, so auto-bootstraping\r\nshould be allowed.\r\n\r\nFixes #15567. fix(angularInit): allow auto-bootstraping from inline script >>> 0"
94,"- **What kind of change does this PR introduce?** (Bug fix, feature, docs update, ...)\n\nMake angular count pending [`resolve`](https://github.com/angular/angular.js/blob/master/src/ngRoute/route.js#L96) values as pending request, for the purposes of making `whenStable` more accurate.\n- **What is the current behavior?** (You can also link to an open issue here)\n\nCurrently, if someone uses promises to asynchronously set a `resolve`, this is not counted as a pending request.  The result is that callbacks to `whenStable` can be invoked before these variables are available.  This was causing protractor users problems (see https://github.com/angular/protractor/issues/789#issuecomment-190983200 for details)\n- **What is the new behavior (if this is a feature change)?**\n\nWaiting for these variables now counts as a pending request.\n- **Does this PR introduce a breaking change?**\n\nI guess you could imagine a breaking change where someone wanted a `whenStable` callback to be invoked before the variables resolved.\n- **Please check if the PR fulfills these requirements**\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [ ] Tests for the changes have been added (for bug fixes / features)\n- [ ] Docs have been added / updated (for bug fixes / features)\n\nNot sure tests are necessary for such a small change.  And since this brings behavior in line with user expectations not sure about the docs either.\n- **Other information**:\n fix(ngRoute): make route.resolve count as a pending request >>> 0"
95,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nCleaner code.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n refactor($interpolate): remove unnecessary else >>> 1"
96,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix(es).\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n1. `$browser.$$checkUrlChange()` (which was run before each `$digest`) will only detect an external change (i.e. not via `$location`) to the browser URL. External changes to `history.state` will not be detected and propagated to `$location`.\r\n\r\n   This would not be a problem if changes were followed by a `popstate` or `hashchange` event (which would call `cacheStateAndFireUrlChange()`). But since `history.pushState()/replaceState()` do not fire any events, calling these methods manually will result in `$location` getting out-of-sync with the actual\r\nhistory state.\r\n\r\n   This is not detected in tests, because the mocked `window.history` will incorrectly trigger `popstate` when calling `pushState()/replaceState()`, which ""covers"" the bug.\r\n\r\n2. When the URL is changed directly (e.g. via `location.href`) during a `$digest` (e.g. via `scope.$evalAsync()` or `promise.then()`) the change is not handled correctly, unless a `popstate` or `hashchange` event is fired synchronously.\r\n\r\n   This is an issue when calling `history.pushState()/replaceState()` in all browsers, since these methods do not emit any event. This is also an issue when setting `location.href` in IE11, where (unlike other browsers) no `popstate` event is fired at all for hash-only changes ([known bug][1]) and the `hashchange` event is fired asynchronously (which is too late).\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n1. The first bug is fixed by ensuring `cacheState()` is always called, before looking for and propagating a URL/state change.\r\n\r\n2. The second bug(s) is fixed by:\r\n   1. Keeping track of `$location` setter methods being called and only processing a URL change if it originated from such a call. If there is a URL difference but no setter method has been called, this means that the browser URL/history has been updated directly and the change hasn't yet been propagated to `$location` (e.g. due to no event being fired synchronously or at all).\r\n   2. Checking for URL/state changes at the end of the `$digest`, in order to detect changes via `history` methods (that took place during the `$digest`).\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n<sub>Who k</sub>**NO**<sub>ws?</sub>\r\n(This PR fixes some uncommon usecases, but I am not 100% sure it was not possible to somehow ""exploit"" the bugs to support a usecase that is now broken.)\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #11075, #12571 and #15556.\r\n\r\n[1]: https://developer.microsoft.com/en-us/microsoft-edge/platform/issues/3740423/ fix($location): correctly handle external URL change during `$digest` >>> 0"
97,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFeature.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThis commit adds a new `customFilter()` function on `$animateProvider` (similar to `classNameFilter()`), which can be used to filter animations (i.e. decide whether they are allowed or not), based on the return value of a custom filter function.\r\nThis allows to easily create arbitrarily complex rules for filtering animations, such as allowing specific events only, or enabling animations on specific subtrees of the DOM etc.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nFixes #14891.\r\n\r\nNeeds docs:\r\n- [x] Complete the API docs.\r\n- [x] Update the Developer Guide.\r\n feat($animate): add support for `customFilter` >>> 0"
98,Based on #14159. Just triggering CI. WIP - feat($route): wait for resolves >>> 0
99,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nDocument update\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nThe documentation for $log doesn't mention blackboxing, by which a developer can see the originating line of calls to $log.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nA brief, browser-agnostic mention of blackboxing was added to the page.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs($log): describe your change... >>> 1"
100,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen using an alias for an isolated scope property using ""$"" as part of the alias, the $compile throws an exception\r\n\r\n> Error: [$compile:iscp] Invalid controller bindings definition for directive 'foo'. Definition: {... factory: '<$fooFactory' ...}\r\n\r\nFixes: #15586\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThis commit removes the error by changing the regex to allow zero or one ""$"" character both at the beginning and the end of the alias.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($compile): allow the usage of ""$"" in isolated scope property alias >>> 0"
101,"This will put the same security constraints to bindings on base#ref as on iframes or script srcs, since it changes the behavior of relative URLs across the page. Currently, they don't have a $sce context. Also, it's more generally a good idea in Angular itself, as since #15144 and its fix #15145 we'll consider that baseURI as a trusted origin.\r\n\r\n**Does this PR introduce a breaking change?**\r\nYup: something like `<base href=""{{myUrl}}""/>` will send myUrl to the $sce's RESOURCE_URL checks. By default, it will break if myUrl isn't same-origin. Also, concatenation in trusted contexts is disabled: ""/{{myPathComponent}}/something"" won't work at all.\r\n\r\nThis follows the discussion in #15537 and the initial issue in #15144, fixed in #15145. fix($compile) : Add base#href to the list of RESOURCE_URL context attrs >>> 0"
102,"The fix from #13124 enabled ngMock and ngMockE2E to work together but\r\ndid it in a way that meant that the ""real"" `$httpBackend` service that\r\nwas used in pass-through depended upon a different `$browser` service\r\nto the rest of the app.\r\n\r\nThis broke Protractor since it watches the `$browser` for outstanding\r\nrequests and the pass through requests were being tracked by the wrong\r\n`$browser` instance.\r\n\r\nCloses #15593\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(ngMockE2E): ensure that mocked $httpBackend uses correct $browser >>> 1"
103,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSetting the binding to undefined calls jqLite/jQuery's `prop()` function, passing undefined as the second argument: `element.prop('value', undefined)`.\r\nThis is identical to `element.prop('value')` and will retrieve the dom value instead of updating it.\r\n\r\nSee: https://github.com/angular/angular.js/issues/15603\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(ngValue): setting binding to undefined should update the DOM value to an empty string >>> 0"
104,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\ndocs update\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nna\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nna\r\n\r\n**Does this PR introduce a breaking change?**\r\nno\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n component architecture results in many isolate scopes >>> 0"
105,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(*): ensure naming is correct for Angular(JS) versions >>> 0"
106,…peat\r\n\r\nFixes #15630\r\n fix(select): keep ngModel when selected option is recreated with ngRe… >>> 1
107,"Adding the missing watches for ES6 object property which added in #14407\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`$scope.$watch` can not work as expected.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n`$scope.$watch` works (NOT a feature change)\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNO\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($parse): Make sure ES6 object computed property to be watched >>> 0"
108,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nAdding param xhrStatus to response and separate handlers for `onerror`, `onabort` and `ontimeout`\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSame handler for all error events\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nSeparate handlers\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nProbably not\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nSome additional tests might be required in `httpSpec.js` and the docs would need updating as well. $http: allow differentiation between Request Error, Abort and Timeout >>> 0"
109,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`sniffer` incorrectly detects NW.js applications as `chromePackagedApp` disallowing them to make use of the history API.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nCorrectly detecting NW.js applications allowing them to make use of the History API.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n#15474\r\n fix($snifferProvider): allow history for nwjs applications >>> 0"
110,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nIt doesn't work correctly according to [docs](https://docs.angularjs.org/api/ngResource/service/$resource) if url params are dynamic. So, for instance, this is gonna work: \r\n\r\n```javascript\r\n// In SomeResourceName factory:\r\n$resouce('/path/\.json',  ...).save({});\r\n```\r\n\r\nbut almost the same code works incorrectly: \r\n\r\n```javascript\r\n// In SomeResourceName factory:\r\n$resouce('/path/:json',  {json: '\.json'}, ...).save({});\r\n```\r\nit sends request to ` :path/%5C.json`\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nMight be\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix (ngResource) dynamic params escaping fix >>> 0"
111,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nAddresses bug identified in #15624.\r\n\r\n**No error message is output in the console:**\r\n```\r\nvar User = $resource('users/:id', {id: '@id'});\r\nvar user User.get({id: 123}, function (res) {\r\n    y  = x + z; // this line not log any exception on the console\r\n}, function () {\r\n    console.log(arguments);\r\n});\r\n```\r\n**Error message is output in the console:**\r\n\r\n```\r\nvar User = $resource('users/:id', {id: '@id'});\r\nvar user User.get({id: 123}, function (res) {\r\n    y  = x + z; // and this log the exception in the console\r\n});\r\n```\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nNow any exceptions thrown in the success callback will be thrown and not eaten.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(ngResource): do not eat exceptions in success callback >>> 0"
112,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\nThis is an improvement to  uniformize the secure context code, by making the $sce handle URL-context in roughly the same way it handles HTML-context. That also required a more silent change, to have $sce-contexts where concatenation of values is allowed (described below).\n\n**What is the current behavior? (You can also link to an open issue here)**\n\nCurrently, the whole $sce.URL context is unused. Dynamically-set URLs that are sanitized through various codepaths (mostly in compile.js, $set function). \n\nUsually, if one wants to force a value through Angular's security mechanisms, the $sce.trustAs function should be used. There's no straightforward way to do it in a single place for URLs, as `<a href=""{{sce.trustAsUrl(foo)}}"">` sanitizes silently its link. \n\nAlso, an attribute being a $sce context means that concatenations are blocked: `<iframe src=""generate.php?{{params}}"">` would throw for instance. However, since it's not a $sce context, `<img src=""generate.php?{{params}}""/>` is fine.\n\n**What is the new behavior (if this is a feature change)?**\n\nThe new behavior makes Angular handle URLs with the $sce service, similarly to HTML context. $sce.trustAsUrl is now useful, as it allows bypassing the sanitization. If one uses plain strings in URL-context attributes, they will be sanitized through the existing $$sanitizeUri service.\n\nI also have tweaked $interpolate to work as expected with concatenated values in select secure contexts. In the listed contexts (URL only for now, though that could be expanded to anything), trustAs'd values with no concatenations are not sanitized (eg `""{{trustedVar}}""` works as you'd expect), and anything else is downgraded to string, concatenated if needed, and then passed to $sce.getTrusted that handles the sanitization as usual. For instance, `""java{{trustAsUrl('script:evil();')}}""` is sanitized as it should.\n\n**Does this PR introduce a breaking change?**\n\nYes: I've merged both URL sanitization methods in $$sanitizeUri (since there's a single $sce.URL context). This will break the applications that edit one of these whitelists, but I believe there's security benefits in doing so, as they will be able to use trustAsUrl in specific parts of their application instead of having to blanket-approve the special scheme they rely on. Otherwise, I believe the changes are backwards-compatible unless you're really hooking into Angular internals.\n\n**Please check if the PR fulfills these requirements**\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [X] Tests for the changes have been added (for bug fixes / features)\n- [X] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nI'm from Google security, but I'm not right all the time, so please ask about anything: I might have missed something, or this might not be something you want. I had to modifiy components I'm not that familiar with. Things to check:\n- If you had special reasons to split the URL sanitization in two parts, I haven't been able to find it. I just merged the whitelists. \n- I'm not entirely sure of the $interpolate change. I've had several issues with infinite digests with arrays while developping, the latest changes seem to pass, but there might still be dragons hidden in there.\n- I'm not sure either I got all the doc modified. \n- Finally, $sce.HTML / $sce.RESOURCE_URL could also be concatenated. At least for HTML it works, and you could have something like the commented-out test in sceSpecs.js, but I'm not sure it brings much value here, so I've left it disabled for now. RESOURCE_URL could follow the same path with either completely-trusted URLs, or concatenated-then-treated-as-string URLs that would be checked against the whitelist/blacklist mechanism. \n feat($sce): handle URLs through the $sce service, plus allow concatenation in that $sce contexts. >>> 0"
113,"This patch adds request and requestError interception for ngResource actions,\nas per the documentation found for $http interceptors. It is important to\nnote that returning an error at this stage of the request - before the\ncall to $http - will completely bypass any global interceptors and/or recovery\nhandlers, as those are added to a separate context. This is intentional; intercepting\na request before it is passed to the HTTP stack indicates that the resource itself\nhas made a decision, and that it accepts the responsibility for recovery.\n\nCloses #5146\n Add request and requestError interception in ngResource >>> 0"
114,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen an object has keys which are not of type string, the `filter` would throw an exception for trying to call `charAt`, which is not defined on a none-string type.\r\n\r\n#15644\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThis commit checks whether `charAt` is defined before calling it.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(filter): Don't throw `key.charAt is not a function` when object's keys are not of type `string`. >>> 0"
115,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ndocs update docs(guide/Components): should reference Angular 2 >>> 0"
116,"This updates the docs app to the new header style found on angularjs.org.\r\n\r\nI've also taken the liberty to update the menu entries on the doc site - pending approval I'll update the org site, too (would fix https://github.com/angular/angular.js/issues/14351)\r\n\r\nAnd fixes https://github.com/angular/angular.js/issues/14963\r\n\r\nNotable differences between org / docs:\r\n- search icon\r\n- font family / weight (should we update?)\r\n- no drop shadow on navbar because there's a secondary bar on docs\r\n\r\nNotable differences between docs old / new:\r\n- mobile: the top header + search are fixed, sub header is relative (previously all relative)\r\n- mobile search close icon has moved over the search bar, because I had some problems giving it it's previous position while keeping the search results scrollable (oh my)\r\n\r\nImplementation notes:\r\n- angular-topnav.css is included as is from org site\r\n- several overrides necessary because org uses bootstrap 2 / docs uses bs 3 (oh my)\r\n- org site uses one breakpoint at 800px, docs uses one at 768 and one at 992. I tested it and it doesn't affect the common screen sizes\r\n\r\nFuture work:\r\n- clean up the css and maybe implement sass for the docs app\r\n\r\nScreens:\r\n\r\n![fireshot screen capture 014 - angularjs_ api_ api reference - localhost_8000_build_docs_api desktop dropdown open](https://cloud.githubusercontent.com/assets/1153097/22548685/f47fb29c-e947-11e6-8f41-a6ab683a4037.png)\r\n![fireshot screen capture 015 - angularjs_ api_ api reference - localhost_8000_build_docs_api desktop search](https://cloud.githubusercontent.com/assets/1153097/22548686/f481f354-e947-11e6-8ea6-735012fe9c9c.png)\r\n![fireshot screen capture 016 - angularjs_ api_ api reference - localhost_8000_build_docs_api phone dropdown](https://cloud.githubusercontent.com/assets/1153097/22548683/f47f66ac-e947-11e6-87b3-8317404394a1.png)\r\n![fireshot screen capture 017 - angularjs_ api_ api reference - localhost_8000_build_docs_api phone search](https://cloud.githubusercontent.com/assets/1153097/22548684/f47fad88-e947-11e6-80c6-9beb87f70c53.png)\r\n docs app style updates >>> 1"
117,"… Compiler.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Add casts to avoid some 'non-existent property' warnings from Closure… >>> 0"
118,"…in IE/Edge\r\n\r\nIn IE9-11 + Edge, the selected options were previously incorrect under the following\r\ncircumstances:\r\n- at least two options are selected\r\n- shift+click or shift+down/up is used to add to the selection (any number of options)\r\n\r\nIn these cases, only the last of the previously selected options and the newly selected\r\noptions would be selected.\r\n\r\nThe problems seems to be that the render engine gets confused when an option that\r\nalready has selected = true gets selected = true set again.\r\n\r\nNote that this is not testable via unit test because it's not possible to simulate\r\nclick / keyboard events on option elements (the events are delegated to the select element\r\nchange event).\r\n\r\nFixes #15675\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBugfix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSee #15675\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- ~~[ ] Tests for the changes have been added (for bug fixes / features)~~\r\n- ~~[ ] Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n fix(select): keep original selection when using shift to add options … >>> 1"
119,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nhttps://github.com/angular/angular.js/issues/15685\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nN/A\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] <s>Docs have been added / updated (for bug fixes / features)</s>\r\n\r\n**Other information**: fix($window): allow $window to be mocked in unit tests >>> 0"
120,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nminor refactoring\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nno refactor($rootScope): asyncQueue is pre-$parsed, no need to call $eval >>> 0"
121,"…r handler\r\n\r\nSince https://github.com/angular/angular.js/commit/e13eeabd7e34a78becec06cfbe72c23f2dcb85f9,\r\nerrors thrown from onFulfilled and onRejected handlers are passed to the regular http\r\nerror handlers. Before this, JSON deserialization errors lead to hard application errors, and could\r\nnot be handled by application code. This behavior was introduced in https://github.com/angular/angular.js/commit/7b6c1d08aceba6704a40302f373400aed9ed0e0b, and originally, a malformed JSON string was forwarded\r\nas the data to the http success response handler.\r\n\r\nThis commit adds a specifc test case, even though the behavior is unlikely to break in the future without\r\na change in the $q rejection handling.\r\n\r\nRelated #11433\r\n\r\n test($http): ensure json deserialization errors are forwarded to erro… >>> 1"
122,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nBug fix\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nhttps://github.com/angular/angular.js/issues/15692\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nnot sure if this can be called a breaking change\r\n fix(angular.copy): copy own non-enumerable properties >>> 0"
123,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nbug fix\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nbrowser can be frozen with bad HTML\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nan exception is thrown instead\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($sanitize): prevent clobbered elements from freezing the browser >>> 1"
124,"…essages configurable\r\n\r\nCloses #14744\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFeature\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSometimes AngularJS throw error with long messages, very long URL referenced error\r\nWill cause 414 Request-URI Too Large  as in Issue https://github.com/angular/angular.js/issues/14744\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nmake the Url reference error length configurable.\r\n\r\n        errorHandlingConfig({urlMaxLength: 500});\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNO\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nMicrosoft Internet Explorer has a maximum uniform resource locator (URL) length of 2,083 characters. Internet Explorer also has a maximum path length of 2,048 characters. This limit applies to both POST request and GET request URLs. https://support.microsoft.com/en-us/help/208427/maximum-url-length-is-2,083-characters-in-internet-explorer which is less than all the other browsers, so i added the default value for url max length to be 2000 characters.\r\n\r\nI don't know what should be the minimum length for URL reference error but i implemented it with a constrains to be more than 200.\r\n feat(errorHandlingConfig): make the length of URL refrence in error m… >>> 0"
125,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nbug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nthe copied Map or Set will fail to call functions with an error such as:\r\nTypeError: Method Map.prototype.size called on incompatible receiver #<Map>\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nangular.copy will now property create new Map or Set objects with the same values as the source.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ x] Tests for the changes have been added (for bug fixes / features)\r\n- [ x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(angular.copy) support for Map and Set >>> 0"
126,"Fixes #15695\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n#15695 \r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($http): JSON parse failure >>> 0"
127,"A big docs update around the $sce: there's a lot of content in there that's often misunderstood, and some of the documentation starts to get really old too. Plus some minor falsehoods in there... Overall, this isn't a major refactoring, but there's a lot of smaller changes.\r\n\r\ncc @mprobst for another set of eyes on the contents.\r\n\r\n**Does this PR introduce a breaking change?**\r\nNope, it's a doc-only CL.\r\n\r\n**Other information**:\r\n- I haven't tested how the doc renders, as I couldn't find a way to do so ?\r\n doc($sce): Overhaul the $sce service's documentation. >>> 0"
128,"They should go through regular sanitization, RESOURCE_URL is overkill there.\r\nThis does not change the context for the rest of xlink:href attributes.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nMinor breaking change. \r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nxlink:href are always RESOURCE_URL context, so they go through the $sce whitelist, but aren't sanitized. That context is intended for links to things that are potentially executed as scripts in your origin, which isn't the case here.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nimage and a xlink:href are now sanitized as link URLs (like a href), and won't complain on different-origin links anymore.\r\n\r\n**Does this PR introduce a breaking change?**\r\nThis is breaking, but only in marginal (and probably unsafe) cases. If you whitelisted something for a xlink:href binding on one of those two, you'll need to change the aHrefSanitizationWhitelist instead. In the overwhelming majority of cases, everything that works with the default settings will keep working, and those defaults aren't usually modified.\r\n\r\n**Other information**:\r\nI'm almost sure there are issues around ng-href in svg lying around, and probably general svg bindings security issues too :/ ng-href security context currently doesn't match the xlink:href ones. It can't be fixed without breaking changes though, so that's for another PR. Some simple tests: https://plnkr.co/edit/PbR1FH2v4luScSYdU16t?p=preview\r\n\r\n feat($compile): Lower the security context of SVG's a and image xlink:href >>> 0"
129,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Autobootstrap fixes >>> 0"
130,"In Chrome an empty `src` attribute will be ignored, but in Firefox it seems\r\nhappy to prepend the `base[href]` and try to load whatever that is.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix(Angular): do not autobootstrap if the `src` exists but is empty >>> 1"
131,"…onstructors on `window`\r\n\r\nThis also removes the likewise deprecated `$controllerProvider.allowGlobals()` method.\r\n\r\nCloses #15349\r\n\r\nBREAKING CHANGE:\r\n\r\nThe option to instantiate controllers from constructors on the global `window` object\r\nhas been removed. Likewise, the deprecated `$controllerProvider.allowGlobals()`\r\nmethod that could enable this behavior, has been removed.\r\n\r\nThis behavior had been deprecated since AngularJS v1.3.0, because polluting the global scope\r\nis bad. To migrate, remove the call to $controllerProvider.allowGlobals() in the config, and\r\nregister your controller via the Module API or the $controllerProvider, e.g.\r\n\r\n```\r\nangular.module('myModule', []).controller('myController', function() {...});\r\n\r\nangular.module('myModule', []).config(function($controllerProvider) {\r\n  $controllerProvider.register('myController', function() {...});\r\n});\r\n\r\n```\r\n fix($controller): remove the option to instantiate controllers from c… >>> 1"
132,"…ideEnabled()\r\n\r\nCloses #15755\r\n\r\nBREAKING CHANGE:\r\n\r\nThe `ngClick` directive from the ngTouch module has been removed, and with the also the\r\ncorresponding $touchProvider.ngClickOverrideEnabled() method.\r\n\r\nIf you have included ngTouch in your application with a version of 1.5.0 or higher, and have not\r\nchanged the value of $touchProvider.ngClickOverrideEnabled()`, then there are no migration steps.\r\n\r\nThe `ngClick` override directive had been deprecated and by default disabled since v1.5.0,\r\nbecause of buggy behavior in edge cases, and a general trend to avoid special touch based\r\noverrides of click events. If you still need a touch override, consider using\r\n[Fastclick](https://github.com/ftlabs/fastclick).\r\n\r\nNote that the `$touch` service still offers the `ngClickOverrideEnabled()` function for\r\nlibraries that want to detect if an app uses the click override. Starting with 1.7.0, it\r\nwill always return `false`.\r\n\r\n\r\nOther info:\r\n\r\nThis one is a bit tricky, as we have deprecated the ngClick directive, but at the same time introduced two new APIs: $touchProvider.ngClickOverrideEnabled and $touch.ngClickOverrideEnabled().\r\nI have removed the former, because it's directly tied to the directive. I think that's fine, as someone who uses the ngClick override must migrate to a different solution anyway, and will see an error when he updates and the provider fn is missing. I've kept the actual provider for the unlikely case that we introduce new config for the ngTouch module.\r\nI've also kept the $touch.ngClickOverrideEnabled() method, as this one is used by material to check if they should include their own touch stuff. I think if we removed this then they wouldn't be able to determine if the ngClick override is present (because ngTouch module but no $touch service could also mean a pre 1.5.0 version of ngTouch)\r\n fix(ngTouch): remove ngClick override and $touchProvider.ngClickOverr… >>> 1"
133,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfix/refactor\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nwe expose lowercase/uppercase on the public angular object\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nyes\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Remove angular.uppercase/lowercase >>> 1"
134,…ern and pattern breaking change\r\n\r\nIntroduced in commit 0e00108\r\n\r\nCloses #15758\r\n\r\n docs(guide/Migrating from Previous Versions): add info for 1.4 ngPatt… >>> 1
135,"IE/Edge display errors in such a way that it requires the user to click in\r\n4 places to see the stack trace. There is no way to feature-detect it so\r\nthere's a chance of the user agent sniffing to go wrong but since it's only\r\nabout logging, this shouldn't break apps. Other browsers display errors in\r\na sensible way and some of them map stack traces along source maps if available\r\nso it makes sense to let browsers display it as they want.\r\n\r\nFixes #15590\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nStack traces don't have source maps applied in any browser. The stack trace is formatted by Angular and printed as a string.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nStack traces have source maps applied in Chrome/Safari. The received error is printed as-is instead of printing a transformed stack trace.\r\n\r\n**Does this PR introduce a breaking change?**\r\nMaybe? The format of logged exceptions is not specified in the docs but there is a chance someone depends on it always being a string.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n fix($log): don't parse error stacks manually outside of IE/Edge >>> 0"
136,"After console.log the output from the computeCachedCssStaggerStyles function I realized that I need set animation-duration to 0s and animation-delay on my stagger class as I am using animations instead of transitions.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(ngAnimate): Improve Staggering config for animations >>> 0"
137,"…ndependent of minErr\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ntest\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThere is no test for errorHandlingConfig\r\n**What is the new behavior (if this is a feature change)?**\r\nAdd tests to errorHandlingConfig independent of minErr tests\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nno\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nThese tests was added in https://github.com/angular/angular.js/pull/15707 , but it looks like there are some problems in the PR, so i have added the missing tests in this PR. test(errorHandlingConfig): add test for errorHandlingConfig() to be i… >>> 0"
138,"Formatting a date across 2 lines fails:\r\n```html\r\n<span>{{ '2017-03-09' | date: 'MMM\nyyyy' }}</span>\r\n```\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe date format is only processed up to the newline.  i.e. as though the sample code was:\r\n\r\n```html\r\n<span>{{ '2017-03-09' | date: 'MMM' }}</span>\r\n```\r\n\r\nProducing:\r\n```html\r\n<span>Mar</span>\r\n```\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe full date format string is processed, producing:\r\n```html\r\n<span>Mar\r\n2017</span>\r\n```\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Fix newline bug in date-filter >>> 0"
139,"BREAKING CHANGE:\r\n\r\nPreviously, the $compileProvider.preAssignBindingsEnabled flag was supported.\r\nTo migrate your code:\r\n\r\n1) If you specified `$compileProvider.preAssignBindingsEnabled(true)`, you\r\ncan remove that statement - since AngularJS 1.6.0 this is the default so your\r\napp should still work even in AngularJS 1.6 after such removal. Afterwards,\r\nmigrating to AngularJS 1.7.0 shouldn't require any further action.\r\n\r\n2) If you specified `$compileProvider.preAssignBindingsEnabled(true)`, first\r\nmigrate your code so that the flag can be flipped to `false`. The instructions\r\non how to do that are available in the ""Migrating from 1.5 to 1.6"" guide:\r\nhttps://docs.angularjs.org/guide/migration#migrating-from-1-5-to-1-6\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFeature removal.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe `$compileProvider.preAssignBindingsEnabled` flag is supported.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe `$compileProvider.preAssignBindingsEnabled` flag doesn't exist.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nYes.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nIt is strongly advised to view this pull request with `?w=1` in the query string i.e. at https://github.com/angular/angular.js/pull/15782/files?w=1. Unfortunately, it's impossible to insert comments in this view.\r\n fix($compile): remove the preAssignBindingsEnabled flag >>> 0"
140,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe deprecation strategy is not specified.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe deprecation strategy is specified.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n\r\n**Other information**:\r\n\r\nFixes #15282 docs(faq): document the AngularJS/jqLite deprecation strategy >>> 1"
141,"Updates the issue template.\r\n- make it more like the Angular one\r\n- include extra information into comments, so the actual output is cleaner and you can find the relevant info faster when triaging\r\n- make it easier to differentiate between bugs and features\r\n- remove section that are only relevant for features ... this is often filled out with redundant info\r\n- make sections shorter and hopefully clearer chore(github): update the issue template >>> 1"
142,"The previous description does not make it clear how this parameter is used with other parameters\r\nand its purpose.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nThis is a doc update.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nN/A\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nN/A\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nThis PR does not introduce a breaking change.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(filter): clarify the comparator parameter >>> 0"
143,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nRefers to this issue: [ngOptions slow performance in IE due to option rerendering](https://github.com/angular/angular.js/issues/15801)\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nNow the options are loaded once when we create the `<select ng-options>` directive. Also, if we add an `<option>` element with an ng-if directive, it is not deleted now.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo. \r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nNone. perf(ngOptions): prevent initial options repaiting >>> 0"
144,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`copy` throws when copying a `NgModelController` instance. This will occur when deep watching such as when doing `<e ref=""[myNgModel]"">` (#15833)\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe internal `NgModelController` `$$scope` is hidden\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nno, unless someone is doing something crazy that depends on `$$scope` being enumerable?\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nI've split the commit up into 3.  I think the refactor + (modified) test should go into 1.7. I don't think the actual fix should go into 1.7.  The specific test case (watched a literal containing an ng-model) will no longer fail in 1.7. If we want to prevent other cases like manually deep-watching or manually `copy`ing an ng-model I think we should do it differently in 1.7...\r\n Allow NgModelController to be copied >>> 1"
145,"there was no error page for the $compile:noslot error\r\n\r\nthis resolves #15790\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs ($compile): add error documentation for noslot error in $compile >>> 1"
146,This got left behind after e269c14425a3209040f65c022658770e00a36f16 refactor($controller): remove unused injected $window >>> 1
147,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nInformation is not super clear\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nclarify info on:\r\n- When do features and breaking changes appear\r\n- Relationship with Semver\r\n- Compatibility of core and optional modules \r\n\r\n docs(faq): clarify the versioning strategy >>> 1"
148,... and deleted a couple other unused things. refactor($parse): move duplicate $parse interpreter/compiler logic into common location >>> 1
149,I think this could be used in some other places as well once it's available... refactor($compile): reuse shared simpleCompare method >>> 1
150,I swear that variable [wasn't there originally](https://github.com/angular/angular.js/commit/fca6be71274e537c7df86ae9e27a3bd1597e9ffa#diff-780de070ede30180f6aedb6c5f7d57caR1218)! refactor($parse): make use of local variable instead of refetching property >>> 1
151,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWe don't know when error docs are missing or obsolete\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n\r\n**Other information**:\r\nI have removed an obsolete animation error and moved the tpload error to the correct namespace. In the $templateRequest file, it looks like the error is created in the $compile namespace, but it actually ends up as the $templateRequest namespace, where it should be.\r\n Warn for missing / obsolete error docs >>> 1"
152,"The previous implementation of jqLite didn't use cleanData from the jqLite\r\nobject but instead used a cached version which maede it impossible to\r\nmonkey-patch jqLite.cleanData similarly to how you can do it in jQuery.\r\n\r\nThe cleanData method is not meant to be called directly by userland code;\r\nits purpose is mainly to be able to be monkey-patched; therefore, the previous\r\nimplementation didn't make a lot of sense.\r\n\r\nThis commit enables one of the tests so far run only with jQuery to run with\r\njqLite as well.\r\n\r\nRef #8486\r\nRef #8695\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n`jqLite.cleanData` is not used directly so it can't be monkey-patched like its jQuery equivalent.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe opposite.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nI don't think so.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nThis is a followup to #8695.\r\n fix(jqLite): make jqLite invoke jqLite.cleanData as a method >>> 0"
153,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nTest update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWe test with jQuery 2.1, 2.2 & 3.1.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nWe test with jQuery 2.1, 2.2 & 3.2.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nAlso, update AngularJS 1.5 mentions in the docs to 1.6.\r\n test(jQuery): run tests with jQuery 2.1, 2.2 & 3.2 >>> 0"
154,"This is a continuation of https://github.com/angular/angular.js/pull/15626 feature($http): allow differentiation between Request Error, Abort and Timeout  >>> 0"
155,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWe install an old Yarn verison manually.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nOn Travis we rely on built-in Yarn support and we only cache the Yarn cache, not `node_modules`. We install a new Yarn verison manually on Jenkins; the location of the install script changed.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n chore(yarn): rely on Travis built-in Yarn support, update Yarn in Jenkins >>> 1"
156,"I don't recall how or why I came across this issue, and it's confusing me, but I found this stashed and the tests do make sense...? fix($rootScope): provide correct value of one-time bindings in watchGroup >>> 1"
157,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\nbug fix\n\n**What is the current behavior? (You can also link to an open issue here)**\n#15068\n fix(ngRepeat): trigger move animation >>> 0"
158,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nWhen submitting a form, $submitted is not set on child forms\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nI don't think so.\r\nNo one in #10071 said that his might be breaking.\r\nAngularJS doesn't have nested form isolation, and ngForm is only for grouping sub-forms. I think that it's expected that a submission event will affect all sub-forms.\r\nProbably: https://github.com/angular/angular.js/pull/11023#issuecomment-143162373\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n\r\nFollow up to https://github.com/angular/angular.js/pull/11023 Fix: set $submitted on child forms when form is submitted >>> 0"
159,"Previously literal one-time bindings did not use the expression `inputs`, causing infinite digest issues with literal values. This often forces the use of deepEquals when watching one-time literals.\r\n\r\n`ng-class` is one example of deepEquals which is no longer required (and had to be updated to keep all tests passing).\r\n\r\nThis one-time/literal behavior is now also consistently propagated through interceptors (and had to be updated to keep all tests passing).\r\n\r\n----\r\n\r\nThe ngClass/interceptor part was not originally planned but had to be changed to keep tests passing. Maybe this can be split into smaller commits? But it's mainly just deleting...! fix($parse): standardize one-time literal vs non-literal and interceptors >>> 1"
160,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Update angular.css >>> 0"
161,"Adds a new `$shutdown` service that can be used to shutdown an app and the `$shutdownProvider` that can be used to register tasks that need to be executed when shutting down an app.\n\nAdded tasks for `$rootElement`, `$rootScope` and `$browser` to be able to shutdown an app\n\nRefactor `$interval` so it delegates to `$browser` just like `$timeout` does\n feat(shutdown): Add the ability for an app to shutdown >>> 0"
162,"The deprecation warning is no longer needed as the feature has been removed\r\nin 1.7.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nPreassigning bindings to controllers is described as deprecated but still working. This feature has been removed so the warning is no longer needed.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nN/A\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs($compile): remove a mention of preassigning bindings in controllers >>> 1"
163,I think this is an edge case of https://github.com/angular/angular.js/commit/7084deccaac5855d7148fb6d91dcb83c16b079c4 (and [1.6.x](https://github.com/angular/angular.js/commit/25f008f541d68b09efd7b428b648c6d4899e6972)) that was not handled. If those objects (in the literals) have a `.valueOf` function that returns something other then `this` it will cause an infinite digest.\r\n\r\nThis should probably also go into 1.6? fix($parse): fix infinite digest errors when watching objects with .valueOf in literals >>> 1
164,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\n\nfeature/bug\n\n**What is the current behavior? (You can also link to an open issue here)**\n\n\n**What is the new behavior (if this is a feature change)?**\n\nAngular should pass them on to the correct excetion handler, and there's an angular.isError function now.\n\n**Does this PR introduce a breaking change?**\n\nNo\n\n**Please check if the PR fulfills these requirements**\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\n- [x] Tests for the changes have been added (for bug fixes / features)\n- [x] Docs have been added / updated (for bug fixes / features)\n\n**Other information**:\n\nre-implimented https://github.com/yefremov/iserror/blob/master/index.js to avoid license and make code faster.\n\nFixes #15868 provide isError and use it in place of instanceof Error >>> 0"
165,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSee #15880.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nCode that is distributed as part of both `angular.js` and `angular-loader.js` does not depend on ""closure"" globals that may not be available in `angular-loader`.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15880. fix(angular-loader): do not depend on ""closure"" globals that may not be available >>> 0"
166,"The `manualLowercase` & `manualUppercase` functions were inspired by Google Caja\r\ncode. Caja is written in Java, though, where problems with `toLowerCase`\r\nworking differently in Turkish locale are well known[1]. In JavaScript\r\n`String#toLowerCase` is defined in the ECMAScript spec and all implementations\r\nare required to lowercase I in the same way, regardless of the current locale.\r\nDifferences may (and do) happen only in `String#toLocaleLowerCase`.\r\n\r\nOther libraries doing string normalization, like jQuery or DOMPurify don't\r\napply special lowercasing logic in a Turkish environment.\r\n\r\nTherefore, the `manualLowercase` & `manualUppercase` logic is dead code in\r\nAngularJS and can be removed.\r\n\r\nAlso, the `manualLowercase` & `manualUppercase` functions are incomplete; they\r\nonly lowercase ASCII characters which is different to native\r\n`String#toLowerCase`. Since those functions are used in many places in the\r\nlibrary, they would break a lot of code. For example, the lowercase filter would\r\nnot lowercase Ω to ω but leave it as Ω.\r\n\r\n[1] https://garygregory.wordpress.com/2015/11/03/java-lowercase-conversion-turkey/\r\n\r\nRef #11387\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDead code removal.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThe `manualLowercase` & `manualUppercase` functions are defined & used if `'I'.toLowerCase() !== 'i'`.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n`String#toLowerCase` is used everywhere.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n chore(*): remove manualLowercase & manualUppercase functions >>> 0"
167,"I saw that the uppercase filter had no example so I decided to add a minimal example to explain how the uppercase filter works.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nThere is currently no example for the `uppercase` filter, so I added one.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nN/a\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [X] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(uppercase): Added an example >>> 1"
168,"- baddata error described incorrect http behavior, and workarounds\r\n- httpProvider defaults were missing transformResponse / transformRequest\r\n- http was not clear about JSON detection strategy\r\n\r\nCloses #15897\r\n\r\n docs($http): correct and clarify default transforms >>> 1"
169,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nTest config update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nOnly one iOS version can be tested via BrowserStack.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\niOS 8.3, 9.3 & 10.0 can be tested.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nAngular (2+) [supports iOS 7+](https://angular.io/docs/ts/latest/guide/browser-support.html) but iOS 7 is very flakey on BrowserStack (see https://github.com/angular/angular/issues/15916). It'd be good to at least try to support the same versions; making it possible to test via BrowserStack is the first step. chore(browserstack): Update OS X, make iOS 8-10 available to test >>> 1"
170,Closes #13687\n chore(travis): test on IE Edge >>> 1
171,"1. Remove remanining workarounds for IE <9\r\n2. Log all parameters in IE 9, not just the first two.\r\n3. Update IE/Edge-related support comments.\r\n4. ES6 classes now require Edge 14 or newer to work.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nA refactor.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nSome code targeting IE <=8 still exists in the code base, support comments don't account for Edge 15.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThe opposite. Also, `$log` is now able to log more than 2 parameters on IE and all browsers use one code path for passing parameters to a proper logging function.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo, unless we consider breaking support for classes in Edge 13 (current version is 15) a breaking change.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n refactor(*): remove workarounds for IE <9, log all parameters in IE 9 >>> 1"
172,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nFeature\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n#14881 \r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nFalls back on built-in comparison function if user-provided comparison function fails to break a tie between elements being ordered.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nYes\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\nBREAKING CHANGE: Using `orderBy` with a custom `comparator` function while\r\nsimultaneously setting `reverse` to `true` results in a potentially different\r\nlist order than previously.\r\n\r\nSpecifically, when the custom `comparator` fails to sort two objects (compares\r\nthem as being ""equal""), the objects would previously have remained in their\r\noriginal order in the array relative to each other. They will now switch places\r\nas the `reverse` will be more accurately respected.\r\n\r\nTo migrate, make sure that your custom `comparator` properly differentiates\r\nbetween all cases that your code base cares about and that ""equal"" cases are\r\nnot order dependent. We expect that due to the nature of this change, since the\r\nentries are considered ""equal"", that very few projects will be affected by this\r\nchange in all but a minor visual manor that still respects the expected\r\noperation being performed. Update(orderBy): Guaranteed stable sort >>> 0"
173,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFeature\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n$scope.$broadcast events cannot be cancelled and do not have a stopPropagation() function/method\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nThis change allows any scope to prevent it's children from receiving that\r\nevent by calling stopPropagation on the event object. Other listeners on the scope which called\r\nstopPropagation will continue to receive the event, as will siblings of that scope and any\r\nchildren of those siblings.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n feat($rootScope): Implement stopPropatagion for $broadcast events >>> 0"
174,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nbugfixes\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n- (a) when the unknown option is selected and the select is ""required"", the required error is not set\r\n- (b) ngOptions: when the model is unmatched and an empty option exists, the empty option is always selected, even if the model is not null / undefined.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n- (a) the error is set\r\n- (b) the unknown option is selected if the model is not null / undefined\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nPossibly? For (a), the following situation is possible: The model is programatically set to a value that is no longer in select options, and the select is required. With this patch, the user cannot keep the unmatched value, because of the required error, which considers this value invalid (just realized this, so it's not in the commit message).\r\n\r\nFor (b), it's unlikely that anyone relied on the fact that the empty option is selected instead of the unmatched option - the fixd behavior was also present in regular select since the beginning.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n Fixes to select and ngOptions - ""required"" validation and behavior with empty / unknown option >>> 1"
175,"The current implementation of $httpBackend.verifyNoOutstandingRequest\r\ngives an integer number describing how many requests are unflushed.\r\n\r\nWhile it's superficially easy to solve test errors from that message\r\nby simply adding an additional $httpBackend.flush(), if a developer\r\nis truly not expecting the code to make further requests this is\r\nnot ideal.\r\n\r\nThis change explicitly prints out which additional requests remain\r\nunflushed in the error message, helping her determine if the code\r\nneeds changing, or if an additional flush is appropriate.\r\n\r\nBefore this change:\r\n\r\n    Unflushed requests: 1\r\n\r\nAfter this change:\r\n\r\n    Unflushed requests:\r\n      GET /some\r\n\r\nCloses #10596\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nA feature improving the output of ngMocks ""Unflushed requests"" error message.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nThe error message includes the number of requests remaining unflushed.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nThe error message now includes a brief description of which requests remain unflushed.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo - unless the error messages are considered public API.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n feat(ngMocks): Describe unflushed http requests >>> 1"
176,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nfeat\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nselect directive behavior is unflexible\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nexposes methods to enable needed flexibility\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n Feat: select api >>> 1"
177,"docs(currency): Add sample of currency from scope\r\n\r\nAdded a sample line showing the use of a scope variable to set currency symbol.  E.g. for central i18n configuration.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nThis adds a docs update (example code).\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nShows currency symbols pulled from text strings.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nShows currency symbol pulled from scope.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [*] Tests for the changes have been added (for bug fixes / features)\r\n- [*] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(currency): Add sample of currency from scope >>> 0"
178,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nTranscluding multi-element directives (e.g. `foo-start`/`foo-end`) is not supported on elements with multi-slot transclusion (a `uterdir` error is thrown).\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nTranscluded nodes are put into a DocumentFragment, where they can be traversed via `.nextSibling`. This way, transcluding multi-element directives on elements with multi-slot transclusion works correctly.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15554. fix($compile): support transcluding multi-element directives >>> 0"
179,"This re-commits https://github.com/angular/angular.js/commit/4c8d8ad5083d9dd17c0b8480339d5f95943f1b71 which was [reverted](https://github.com/angular/angular.js/commit/36fd167e1d6668ab497970f491a6d95344e97e41) (with [tests added](https://github.com/angular/angular.js/commit/da75d138b144d196fc2b397c7a86a74528c151a5)).  This is possible now because of 189461f9bf6fda18ddbd16c42f2e959cf939c3da, although note that change is currently causing #15905 so maybe we should hold off on merging this one for now?\r\n\r\nFYI @gdi2290 perf(ngStyleDirective): use $watchCollection >>> 0"
180,"…ment equivalent to zero\r\n\r\nEnable test interval calls with delay set to zero without infinite loop.\r\nIf a spec has `$interval.flush(ms)` and the code to test has\r\n`$interval(callback, 0)` then instead of running an infinite while loop\r\nit would move forward `ms` executing the $interval's callback.\r\n\r\nFixes #15952\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nhttps://github.com/angular/angular.js/issues/15952\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($interval): change $interval ngMock service to handle second argu… >>> 0"
181,"These are various cases that I felt were missing tests, mainly around constant/literal/one-time $watch/$watchCollection. test($rootScope): add $watch/$watchCollection tests >>> 1"
182,"Reverts 60394a9d91dad8932fa900af7c8529837f1d4557 to fix #15905 in 1.6. Currently plan is to NOT revert in 1.7.\r\n\r\nFixes #15905  Revert ""fix($parse): standardize one-time literal vs non-literal and interceptors"" >>> 1"
183,"The correct <script> tag in template\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(guide/Templates): describe your change... >>> 0"
184,"The updated karma-chrome-launcher adds support for ChromeHeadless &\r\nChromeCanaryHeadless launchers; test with:\r\n\r\n    karma start karma-jqlite.conf.js --browsers=ChromeCanaryHeadless\r\n\r\nThe updated karma-firefox-launcher disables multi-process which may increase\r\nstability on Jenkins.\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nChore.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nN/A\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nN/A\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n chore(*): update all Karma-related packages except Karma >>> 1"
185,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFollow jQuery when serializing `null` and `undefined`.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n```js\r\n$httpParamSerializerJQLike({items:['foo', 'bar', null, undefined]}) ==\r\n  'items%5B%5D=foo&items%5B%5D=bar';\r\n```\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n```js\r\n$httpParamSerializerJQLike({items:['foo', 'bar', null, undefined]}) ==\r\n  'items%5B%5D=foo&items%5B%5D=bar&items%5B%5D=&items%5B%5D=';\r\n```\r\n\r\n**Does this PR introduce a breaking change?**\r\nYes, but does so to follow the implementation in jQuery.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [X] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [X] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\nCloses: #15969 fix(httpParamSerializerJQLike): Follow jQuery for `null` and `undefined` >>> 0"
186,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nTest fixes.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nThere was a syntax error in class-related tests; the test wasn't failing only because it was disabled everywhere outside of Chrome and Chrome <59 incorrectly accepted it.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n1. Wrap an evaled class definition in parens; previously they weren't.\r\n2. The classes support test was modified to check not only if a class definition parses but also if it stringifies correctly which is required by AngularJS. This restriction disables class-related tests in current Firefox (53) but will work in v55 or newer.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [ ] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n test(*): run class-related tests everywhere; fix eval syntax >>> 1"
187,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\ndocs update\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nBad docs for `FormController.$setValidity()`.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nBetter docs for `FormController.$setValidity()`.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] ~~Tests for the changes have been added (for bug fixes / features)~~\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nFixes #15963. docs(form): improve the docs for `FormController.$setValidity()` >>> 0"
188,"When calling $setPristine on the nested form or control,\nform becomes $pristine of all the nested controls are pristine\n\nCloses #13715\n fix(form): make ngForm $pristine after nested control.$setPristine() (counter version) >>> 0"
189,"Fixes #15964 \r\n\r\nThis changes the [compareObjectIdentity](https://github.com/angular/angular.js/blob/v1.6.4/src/ng/parse.js#L1785) flag from being per-expression to per-input.  This allows inputs to filters to not compare object entity, but other inputs such as `- x` or `{x: x}` to only compare object entity.  This also expands this `compareObjectIdentity = true` beyond just object literals. fix($parse): always re-evaluate filters within literals when an input is an object >>> 1"
190,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nTests update.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nIE 9 mode (missing `console.log.apply`) has a separate more restricted describe block.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nAll tests tha define `console.log` properly are run in both modes: with `apply` and without one.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\nIn IE 9 console methods don't inherit from Function.prototype and, hence, don't\r\nhave apply. Until recently IE 9 logging in AngularJS was restricted to the\r\nfirst 2 parameters but that changed as we could just reuse\r\nFunction.prototype.apply everywhere, creating one code path for all browsers.\r\nTherefore, we can now run all tests in modes where apply exists on logging\r\nmethods and where it doesn't.\r\n\r\nRef #15911\r\nRef b277e3ead7296ae27106fe7ac37696635c6bfda1 test($log): run all $log tests in IE9 & non-IE9 logging mode >>> 1"
191,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nFix, i18n update.\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nLocale files based on CLDR 29 and broken `i18n/` scripts.\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nLocale files based on CLDR 30.0.1 and working `i18n/` scripts.\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo.\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\nFixes #15976. chore(i18n): fix and update CLDR to v30.0.1 >>> 0"
192,"This should help to prevent issues such as #8671, #12452, #16004.\r\n\r\nCloses #12643\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ndocs\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nunclear docs\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nhopefully clearer docs :)\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs($rootScope.Scope): clarify $watchGroup ""oldValues"" argument >>> 1"
193,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\n\r\nNew tests.\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\nIt's not tested whether browsers pass or fail specific support tests used to skip particular test blocks.\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\nAll browsers have verified support tests results.\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- ~~Docs have been added / updated (for bug fixes / features)~~\r\n\r\n**Other information**:\r\n\r\n test(support): verify support tests results in all tested browsers >>> 1"
194,Both Firefox and Safari are vulnerable to XSS if we use an inert document\r\ncreated via `document.implementation.createHTMLDocument()`.\r\n\r\nNow we check for those vulnerabilities and then use a DOMParser or XHR\r\nstrategy if needed.\r\n\r\nThanks to @cure53 for the heads up on this issue.\r\n fix($sanitize): use appropriate inert document strategy for Firefox and Safari >>> 1
195,"I think this was a mistake [from the start](https://github.com/angular/angular.js/commit/fca6be71274e537c7df86ae9e27a3bd1597e9ffa?diff=unified#diff-44d3307b66bf51b52e110307fa57e196R518).  This interceptor doesn't actually store any state, it just reads the inner/complex state of its input.\r\n\r\nRemoving the `$stateful` flag will allow some expressions (those with `inputs` such as array/object literals) to avoid invoking `$watchCollectionInterceptor` and avoid creating the literals etc. on each digest. perf($rootScope): do not mark $watchCollectionInterceptor as $stateful >>> 0"
196,"Fixes #15905 \r\n\r\nThis basically applies the same as b5118ac6a9e0a327b31094b3fdcdc0432b23ad2f to interceptors.\r\n\r\nFor example normally when watching `[var]` the `var` only needs to be shallow-watched. But if that expression then gets wrapped in an interceptor such as `$parse('[var]', interceptor)` we must assume the `interceptor` is non-pure and might read state from within the `var` (just like filters/functions...).\r\n\r\nThis tries to preserve shallow watching of things behind operators such as `!` by distinguishing if an expression is pure due to an operator such as `!` (""absolute"") or pure only if no parent operation wants to deep watch it (""relative"").  This way when wrapped in an interceptor the ""absolute"" ones can still be shallow watched, while the ""relative"" ones that the interceptor has access to can no longer be shallow watched.\r\n\r\nThis still doesn't fully restore the 1.6.3 ng-class functionality because it still doesn't do a deep-watch for one-time bindings like 1.6.3 did. The one (famous last words...) remaining regression from 1.6.3 is demonstrated by the modified ng-class spec (objects-in-literals-with-interceptors-in-one-time bindings).  This is due to [this annoying condition](https://github.com/angular/angular.js/blob/b5118ac6a9e0a327b31094b3fdcdc0432b23ad2f/src/ng/parse.js#L1946-L1948) which I really wish I could find a better method of doing... fix($parse): do not shallow-watch inputs when wrapped in an interceptor fn >>> 1"
197,"This also deletes the incorrect info that a missing attribute\r\nin a non-optional binding will throw.\r\n\r\nCloses #15989\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\ndocs\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nmissing / incorrect info\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\ninfo!\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n101% not\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs($compile): add more info about optional bindings >>> 1"
198,This actually depends on #16009 because that (unnecessary) `$stateful` flag was previously being ignored in the case of the failing test. fix($parse): respect the interceptor.$stateful flag >>> 0
199,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix: microsoft edge does not support the origin property on anchor elements, so bootstrapping fails\r\nfor edge extensions. this fix simply changes the bootstrap origin check to use supported\r\nanchor properties while having the same effect\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\ncloses #16030 \r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nn/a\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix: bootstrap check with alternate anchor properties >>> 0"
200,"This function has problems with special object types but since it's not used in core,\r\nit is not worth implementing fixes for these cases.\r\nA general purpose library like lodash (provides `merge`) should be used instead.\r\n\r\nCloses #12653\r\nCloses #14941\r\nCloses #15180\r\nCloses #15992\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDeprecation\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\nUnclear API support\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\nDeprecation / docs update\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\n\r\nNo\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- ~[ ] Tests for the changes have been added (for bug fixes / features)~\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\nNot sure if this should be marked fix or feat\r\n~If we deprecate only in 1.7.0, then the known issue section can still go in 1.6.x~\r\n fix(Angular): deprecate angular.merge >>> 1"
201,While the firewall continues to block the update ports\r\nwe will not try to publish there.\r\n chore(jenkins): do not publish to code.angularjs.org snapshot >>> 1
202,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nBug fix\r\n\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n#16016 \r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [ ] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n fix($resource): Check if timeoutDeferred is null inside $cancelRequest >>> 0"
203,"AngularJS docs update: Currently, the link to Pluralsight at the bottom of the page https://docs.angularjs.org/guide/external-resources is broken. This will edit the link to a generic list of all AngularJs course on Pluralsight. This is an isolated UI change to a public facing document, therefore there are no breaking changes anticipated.\r\n Fix broken link - point to list of Pluralsight courses on AngularJS >>> 0"
204,This test keeps causing Firefox 47 (currently used on Travis) to crash and fail the build. The test passes locally (on Firefox 53). Lowering the loop count from 1000 to 100 seems to fix the issue.\r\n(Note: The crach only affects the mocked implementation of `$interval` and does not happen locally.) test(ngMock): fix Firefox crashes on Travis >>> 0
205,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nchore: \r\n- it hides the dots for unit tests on Travis, which makes it easier to find the separate test tasks and results.\r\n- avoids building package for unit tests (makes the unit test job slightly faster)\r\n- re-arranges the grunt task list to make it easier to read\r\n\r\n Improve readability of logging on Travis >>> 1"
206,"I realized that https://github.com/angular/angular.js/commit/189461f9bf6fda18ddbd16c42f2e959cf939c3da changed this a little - previously the second test failed for two reasons\r\n1. one-time literals [did not use .inputs](https://github.com/angular/angular.js/blob/93879b3c721f4c0273c90e9bfeb368425b0078c4/src/ng/parse.js#L1881) unlike the normal [one-time delegate](https://github.com/angular/angular.js/blob/93879b3c721f4c0273c90e9bfeb368425b0078c4/src/ng/parse.js#L1856), so the `identity` interceptor would throw an infdig because the literal recreated each digest was being watched\r\n2. one-time literals with interceptors [only check `isDefined`](https://github.com/angular/angular.js/blob/93879b3c721f4c0273c90e9bfeb368425b0078c4/src/ng/parse.js#L1933) to determine when the one time is ""done"" which is unlike [the one-time watch delegate](https://github.com/angular/angular.js/blob/93879b3c721f4c0273c90e9bfeb368425b0078c4/src/ng/parse.js#L1881) that checks all the values in the literal, so the interceptor thought the ""one-time"" was done right away for literals\r\n\r\nI think both of these changes are correct and want to make sure we don't revert them. This should probably only go into 1.7 since this change will [probably be reverted](https://github.com/angular/angular.js/pull/15958) in 1.6. test($parse): add one-time/interceptor tests >>> 1"
207,"…nge...\r\n\r\n**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nDocs update - changed file extension\r\n\r\n**What is the current behavior? (You can also link to an open issue here)**\r\n\r\n\r\n\r\n**What is the new behavior (if this is a feature change)?**\r\n\r\n\r\n\r\n**Does this PR introduce a breaking change?**\r\nNo\r\n\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [ ] Tests for the changes have been added (for bug fixes / features)\r\n- [x] Docs have been added / updated (for bug fixes / features)\r\n\r\n**Other information**:\r\n\r\n docs(tutorial/4 - Directory and File Organization): describe your cha… >>> 0"
208,Previously `.catch(noop)` was used on the rejected timeout/interval to prevent an unhandled rejection error. However this would schedule a deferred task to run the `noop`. If the cancelling was outside a digest this could cause a new digest such as with the ng-model `debounce` option.\r\n\r\nFixes #16057 fix($timeout/$interval): do not trigger a digest when cancelling a $timeout/$interval >>> 1
209,"Removing\r\n* cases accessing `constructor` that previously had extra ""security"" applied\r\n* the `bmPeWatchLiteral` directive that was making literals `$watch`-able, now they can be watched normally\r\n\r\nAdding\r\n* some `$watchCollection` tests benchmarks - remove no longer applicable, add $watchCollection >>> 1"
210,"**What kind of change does this PR introduce? (Bug fix, feature, docs update, ...)**\r\nSee #15812\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features)\r\n- ~[ ] Docs have been added / updated (for bug fixes / features)~\r\n\r\n**Other information**:\r\n\r\n perf(ngOptions): prevent initial options repainting >>> 1"
211,"Don't throw error if http response has json-like data but content-type is not ""application/json"". Instead return data unprocessed.\r\n\r\nFixes https://github.com/angular/angular.js/issues/16027\r\nThis is not a breaking change.\r\n\r\n**Please check if the PR fulfills these requirements**\r\n- [x] The commit message follows our guidelines: https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit-message-format\r\n- [x] Tests for the changes have been added (for bug fixes / features) fix($http) No error for json-like repsonses without ""application/json"" content-type >>> 0"
0,"## Proposed changes\n\nEnable specifying the WebDriverAgent port as a server parameter, so multiple Appium XCUI tests can be run concurrently.\n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [ ] Bugfix (non-breaking change which fixes an issue)\n- [x] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [x] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] I have added necessary documentation (if appropriate)\n- [ ] Any dependent changes have been merged and published in downstream modules\n### Reviewers: @imurchie, @jlipps, ...\n Add server parameter for WebDriverAgent local port >>> 1"
1,"## Proposed changes\n- Implemented UiAutomator2 [Driver](https://github.com/appium/appium-uiautomator2-driver) and [Server](https://github.com/appium/appium-uiautomator2-server)\n- This module is targeted to re implement appium-android-bootstrap's bootstrap module using Google's UIAutomator V2 API.\n- Supports on Android API above 19\n- [Technical Details](https://github.com/appium/appium-uiautomator2-server/wiki)\n- From Appium server it is requested by specifying the desired capability `automationName`of `uiautomator2` when starting a session.\n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [ ] Bugfix (non-breaking change which fixes an issue)\n- [x] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [x] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [x] I have added necessary documentation (if appropriate)\n- [x] Any dependent changes have been merged and published in downstream modules\n## Further comments\n\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\n### Reviewers: @imurchie, @jlipps, ...\n Added UiAutomator2 Driver >>> 1"
2,add some helpful info for people new to xcuitest support. cc @imurchie \n add xcuitest migration doc (fix #6869) >>> 1
3,"## Proposed changes\n\nWith newer npm versions we run into a problem installing locally, where the system fails to install Appium when doing a local install, because authorize-ios can't be linked.\n\nIt is already a global package, so just change instructions to manually install and run.\n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [x] Bugfix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [x] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] I have added necessary documentation (if appropriate)\n- [ ] Any dependent changes have been merged and published in downstream modules\n### Reviewers: @imurchie, @jlipps, ...\n Move authorize-ios to a global program only >>> 1"
4,"## Proposed changes\n\nChangelog and dependency changes for 1.6.0.\n\nThis is the first of the 1.6 line, so after this is in I will make a branch and publish. \n\n😱 😱 😱\n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [ ] Bugfix (non-breaking change which fixes an issue)\n- [x] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [x] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] I have added necessary documentation (if appropriate)\n- [ ] Any dependent changes have been merged and published in downstream modules\n### Reviewers: @jlipps, ...\n Prepare for release of 1.6.0 >>> 1"
5,"## Proposed changes\n\nUpon receiving SIGTERM appium exits with code 143 which is line with the documentation presented [here](https://nodejs.org/api/process.html#process_signal_events). However it is not an expected result from the point of view of many existing system tools. A SIGTERM would usually result in a graceful shutdown and an exit code of 0. \n\nI've installed basic signal handers for SIGTERM and SIGINT that shutdown express when they are met. The code then exits gracefully with an exit code of 0.\n\nThe specific problem I encountered was met when managing appium using launchctl on OSX which I use to manage appium process life. Shutdown is performed via SIGTERM and launchctl expects an exit code of 0 as is typical. \n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [x] Bugfix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [x] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [x] I have added necessary documentation (if appropriate)\n- [x] Any dependent changes have been merged and published in downstream modules\n## Further comments\n\nNA\n### Reviewers: @imurchie, @jlipps\n graceful handling of SIGTERM and SIGINT such that return codes are mo… >>> 1"
6,"## Proposed changes\n\nFix previous PR https://github.com/appium/appium/pull/7064\nSelenium 3 has changed the format of grid config files, but not the configuration object that must be sent to the grid hub.\nThe Selenium 3 config file does not contain 'configuration' section, but request data must contain a 'configuration' section with all the properties.\nTested with Selenium 2.53.1 and Selenium 3.0.1.\n## Types of changes\n\nWhat types of changes does your code introduce to Appium?\n_Put an `x` in the boxes that apply_\n- [X] Bugfix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n## Checklist\n\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\n- [X] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\n- [X] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\n- [X] Lint and unit tests pass locally with my changes\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] I have added necessary documentation (if appropriate)\n- [ ] Any dependent changes have been merged and published in downstream modules\n Fix grid register with Selenium 3 hubs >>> 1"
7,"cc @imurchie @dpgraham\n basic refresh of docs, just small edits for accuracy and completeness >>> 1"
8,## Proposed changes\n\nAdd recommendations regarding xpath usage in XCUITest mode\n Migration doc xpath >>> 1
9,"## Proposed changes\r\n\r\nFix js example of scroll\r\n\r\n## Types of changes\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nCurrent example for js throws an error like:\r\n```bash\r\n{ [Error: undefined is not an object (evaluating 'direction[0].toUpperCase')] message: 'undefined is not an object (evaluating \'direction[0].toUpperCase\')', type: 'RuntimeError', seleniumStack: { message: 'An error occurred while executing user supplied JavaScript.', orgStatusMessage: 'undefined is not an object (evaluating \'direction[0].toUpperCase\')', status: 17, type: 'JavaScriptError' } }\r\n```\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n\r\n Update js example in touch-actions.md >>> 1"
10,"## Proposed changes\r\n\r\nUpdated documentation to show usage of -ios predicate string locator strategy\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [ ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [ ] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n\r\nIncludes a Java example showing how to use predicate string in appium; documents the iOS version support Document '-ios predicate string' locator strategy >>> 1"
11,"## Proposed changes\r\n\r\nAdd note about lock, shake and location being unimplemented because of recalcitrant Apple. These are also in the `appium-xcuitest-driver` README.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [*] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [*] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [*] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [*] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @jlipps, @dpgraham \r\n\r\n Add note about lock, shake, location in xcuitest >>> 1"
12,"## Proposed changes\r\n\r\nNote that XCUITest pinch does not work.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n\r\n### Reviewers: @jlipps, @dpgraham \r\n\r\n Add note about pinch in xcuitest >>> 1"
13,"## Proposed changes\r\n\r\nAdd note to say that `autoAcceptAlerts` and `autoDismissAlerts` do not work for XCUITest.\r\n\r\nSecond part of #6864.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n\r\n### Reviewers: @jlipps, @dpgraham \r\n\r\n Add note about autoAcceptAlerts and autoDismissAlerts for XCUITest >>> 1"
14,nan Add native back for iOS >>> 1
15,"## Proposed changes\r\n\r\nCurrently we catch every SIGINT and SIGTERM signal coming in and try to stop the http server. Unfortunately, if there are connections, it has to wait for them to be closed on both ends before Appium actually stops. \r\n\r\nThis is a quick solution, only adding a one-time event listener for the signals. So if someone cares that the connections are closed etc. they can wait, otherwise another signal will terminate the whole thing.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @jlipps\r\n Only catch signals once >>> 1"
16,"## Proposed changes\r\n\r\nThe [3.0.0.1 release of the Appium.WebDriver package](https://github.com/appium/appium-dotnet-driver/blob/master/RELEASE_NOTES.md#3001) provides a WindowsDriver class which can be used in place of IOSDriver when testing against Windows devices.  This PR replaces IOSDriver/IOSElement with WindowsDriver/WindowsElement in the `windows-app-testing.md` document.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Replace IOSDriver with WindowsDriver >>> 1"
17,"## Proposed changes\r\n\r\nMake people check if their keychain is unlocked.\r\n[Link](https://github.com/appium/appium/issues/7705) to issue inspiring this change.\r\n\r\n## Types of changes\r\n\r\nAddition to documentation.\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\nNone.\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Update troubleshooting.md >>> 1"
18,"## Proposed changes\r\n\r\nIt is useful, for debugging purposes, to know the version of the driver running. Currently we do it in an ad hoc fashion. Some drivers report, some do not.\r\n\r\nThis adds it to every session.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @jlipps, @dpgraham \r\n Add version string to driver announcement >>> 1"
19,"I kept it simple and just instructed the reader to replace 'mobile: scroll' with 'mobile: swipe'. Since the two have the exact same API, no sense in maintaining both sets of instructions.\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Add swiping documentations >>> 1"
20,Update version in master ahead of releasing 1.6.4-beta. @jlipps  Bump version for 1.6.4-beta >>> 1
21,"## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nSo you can close stale #7698\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Fix typo in running-on-osx.md >>> 1"
22,## Proposed changes\r\n\r\nFix some typos in the Issue Template and make some sentences easier to read.\r\n\r\n## Types of changes\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules Fix typos in Issue Template >>> 1
23,"## Proposed changes\r\n\r\nWe still have the old Google-based CLA in the PR template checklist. Update to the generic JSF one. See https://github.com/appium/appium/pull/7822\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://docs.google.com/forms/d/1lOfXRw_0VCk7gYzjj4WLetGu7yelDVo5LWh0z7pGftE/viewform)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @jlipps, ...\r\n Fix CLA link in PR template >>> 1"
24,"argParse breaks if process.argv[1] is undefined, which is a problem when Appium is being run as a binary (because there's only one arg). Added check so that if process.argv[1] is undefined, set the prog name to 'Appium'.\r\n\r\n## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Don't allow undefined process.argv[1] in parser >>> 1"
25,## Types of changes\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules XCUITest now supports swipe gestures >>> 1
26,"## Proposed changes\r\n\r\nCall me irrational, but I _hate_ how the default capabilities get printed within the server args section of the Appium startup logging.\r\n\r\nHow it currently is:\r\n```\r\n$ node . --log-timestamp --default-capabilities '{""showIOSLog"": true, ""nativeInstrumentsLib"": true, ""platformName"": ""Android"", ""timeouts"": {""global"": 10000}}'\r\n2017-02-14 16:22:15:233 - [Appium] Welcome to Appium v1.6.4-beta (REV b57d1a3171355c791a7f0024cfcdc797ce89d56a)\r\n2017-02-14 16:22:15:235 - [Appium] Non-default server args:\r\n2017-02-14 16:22:15:236 - [Appium]   logTimestamp: true\r\n2017-02-14 16:22:15:249 - [Appium]   defaultCapabilities: { showIOSLog: true,\r\n  nativeInstrumentsLib: true,\r\n  platformName: 'Android',\r\n  timeouts: { global: 10000 } }\r\n2017-02-14 16:22:15:250 - [Appium] Default capabilities, which will be added to each request unless overridden by desired capabilities:\r\n2017-02-14 16:22:15:252 - [Appium]   showIOSLog: true\r\n2017-02-14 16:22:15:252 - [Appium]   nativeInstrumentsLib: true\r\n2017-02-14 16:22:15:252 - [Appium]   platformName: 'Android'\r\n2017-02-14 16:22:15:253 - [Appium]   timeouts: { global: 10000 }\r\n2017-02-14 16:22:15:295 - [Appium] Appium REST http interface listener started on 0.0.0.0:4723\r\n```\r\n\r\nHow it should be (and is, with this PR):\r\n```\r\n$ node . --log-timestamp --default-capabilities '{""showIOSLog"": true, ""nativeInstrumentsLib"": true, ""platformName"": ""Android"", ""timeouts"": {""global"": 10000}}'\r\n2017-02-14 16:16:31:567 - [Appium] Welcome to Appium v1.6.4-beta (REV b57d1a3171355c791a7f0024cfcdc797ce89d56a)\r\n2017-02-14 16:16:31:569 - [Appium] Non-default server args:\r\n2017-02-14 16:16:31:570 - [Appium]   logTimestamp: true\r\n2017-02-14 16:16:31:570 - [Appium]   defaultCapabilities: {\r\n2017-02-14 16:16:31:570 - [Appium]     showIOSLog: true\r\n2017-02-14 16:16:31:571 - [Appium]     nativeInstrumentsLib: true\r\n2017-02-14 16:16:31:571 - [Appium]     platformName: Android\r\n2017-02-14 16:16:31:571 - [Appium]     timeouts: {\r\n2017-02-14 16:16:31:572 - [Appium]       global: 10000\r\n2017-02-14 16:16:31:572 - [Appium]     }\r\n2017-02-14 16:16:31:573 - [Appium]   }\r\n2017-02-14 16:16:31:574 - [Appium] Default capabilities, which will be added to each request unless overridden by desired capabilities:\r\n2017-02-14 16:16:31:585 - [Appium]   showIOSLog: true\r\n2017-02-14 16:16:31:585 - [Appium]   nativeInstrumentsLib: true\r\n2017-02-14 16:16:31:586 - [Appium]   platformName: 'Android'\r\n2017-02-14 16:16:31:586 - [Appium]   timeouts: { global: 10000 }\r\n2017-02-14 16:16:31:634 - [Appium] Appium REST http interface listener started on 0.0.0.0:4723\r\n```\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nThis is minor but bugs me every time I see the logs.\r\n\r\n### Reviewers: @dpgraham , @jlipps, ...\r\n Fix formatting of default capabilities >>> 1"
27,"## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Update running-tests.md >>> 1"
28,"## Proposed changes\r\n\r\nStart using the logger from `appium-support` over the deprecated `appium-logger`. \r\n\r\nAlso, for fun, fix session handling in unit tests so `gulp once` returns after finishing running the tests.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n\r\n### Reviewers: @dpgraham, @jlipps, ...\r\n Move to logger from appium-support >>> 1"
29,"## Proposed changes\r\n\r\nAdd more detailed documentation for getting SafariLauncher to run, particularly when you cannot create a wildcard certificate.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nSee #7776 \r\n\r\n### Reviewers: @dpgraham , @jlipps\r\n Add documentation for SafariLauncher >>> 1"
30,"## Proposed changes\r\n\r\nThe documentation for real device setup is old and not particularly useful anymore. Better documentation exists in `appium-xcuitest-driver` for that particular system. Move that here, and integrate it.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nFulfills #7577\r\n\r\n### Reviewers: @dpgraham , @jlipps\r\n Clean up real device setup docs >>> 1"
31,nan Documentation for new desired cap: calendarAccessAuthorized >>> 1
32,"## Proposed changes\r\n\r\nThis python sample code occurs python NameError, so I fixed it.\r\n\r\n## Types of changes\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Fix wrong python sample code document >>> 1"
33,"Reviewers: @imurchie, @jlipps \r\n Document 'startIWDP' capability >>> 1"
34,nan Document 'startIWDP' desired capability >>> 0
35,Based on https://github.com/appium/appium-xcuitest-driver/pull/393 Add documentation for gestures available under “mobile: *” interface for XCTest-based driver >>> 1
36,"## Proposed changes\r\n\r\nIntegrating the Mac OS X app driver\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [ x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ x] Lint and unit tests pass locally with my changes\r\n- [ x] I have added tests that prove my fix is effective or that my feature works\r\n- [ x] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Add MacDriver >>> 1"
37,"What do you think @imurchie, @dpgraham? don't maintain two lists of xcuitest caps, they get out of sync >>> 1"
38,Get all our ducks in a row prior to release. Update CHANGELOG for release of 1.6.4 >>> 1
39,"## Proposed changes\r\n\r\nThere are memory leaks in Appium, but they are quite hard to catch. The heapdump tool allows to create node heap snapshot with one simple 'kill -SIGUSR2 &lt;NODEPID&gt;' command (works in *nix system only). This gives us a possibility to provide users with a better possibilities to debug the process and allows us to ask them to provide these dump to us if necessary for detailed investigation (and leaks fixing), since such kind of bugs is usually very hard to reproduce in dev environment.\r\n\r\n## Types of changes\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n### Reviewers: @imurchie, @jlipps  Add --enable-heapdump feature >>> 1"
40,nan Point download link to Appium Desktop >>> 1
41,@jlipps @imurchie  Remove name strategy from documentation >>> 1
42,"## Proposed changes\r\n\r\nPrepare for release of 1.6.5 beta so users can have access to the latest as it is released. Also perhaps give us some impetus to release 1.6.5 sooner rather than later.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nThe `1.6` branch is updated with the latest commits from master, and the shrinkwrap file removed. Once this is in it needs to be cherry picked into that branch and then the release can happen.\r\n\r\n### Reviewers: @dpgraham , @jlipps, @mykola-mokhnach \r\n Update version and changelog for 1.6.5 beta >>> 1"
43,See https://github.com/appium/appium.io/pull/90 for more details Update network connection adjustment information for Android >>> 1
44,"## Proposed changes\r\n\r\nRefer to You.i Engine driver for list of supported caps\r\n\r\n@imurchie, @jlipps \r\n Avoid You.i Engine caps getting out of sync >>> 1"
45,Please merge it after https://github.com/appium/appium-xcuitest-driver/pull/425/files is merged Add documentation for offset argument in selectPickerWheelValue call >>> 1
46,"* Removed date-utils and removed require\r\n* Replaced `call to date.toFormat(...)` with `dateformat(date, '...')` in `timestamp` function in `logsink.js`\r\n* Couldn't write any tests because timestamp is a private method, so did a quick sanity check from the command line to confirm that the behaviour is the same\r\n\r\n```\r\nDaniels-MacBook-Pro:appium danielgraham$ node --version\r\nv4.1.2\r\nDaniels-MacBook-Pro:appium danielgraham$ node\r\n> require('date-utils')\r\n{ language: [Function: language] }\r\n> var date = new Date()\r\nundefined\r\n> date = new Date(date.valueOf() + date.getTimezoneOffset() * 60000)\r\n2017-04-26T06:24:09.224Z\r\n> date.toFormat(""YYYY-MM-DD HH24:MI:SS:LL"")\r\n'2017-04-25 23:24:09:224'\r\n> var dateformat = require('dateformat')\r\nundefined\r\n> dateformat(date, ""yyyy-mm-dd H:M:ss:l"")\r\n'2017-04-25 23:24:09:224'\r\n```\r\n\r\n(reviewers\r\n Remove date-utils library and use node-dateformat >>> 1"
47,"## Proposed changes\r\n\r\nIt's a really small markdown syntax fix.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n- [x ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x ] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Fixed markdown title in ""running in windows"" doc >>> 1"
48,"## Proposed changes\r\n\r\nThe `enablePerformanceLogging` capability now works for iOS (https://github.com/appium/appium-ios-driver/commit/06699519b6349375b4b5a2bf566255f401148d99), so move it into the general capability section.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @dpgraham, @jlipps, ...\r\n Move enablePerformanceLogging cap to general >>> 1"
49,Please merge it after https://github.com/appium/appium-xcuitest-driver/pull/430 Add documentation on mobile: alert script >>> 1
50,"## Proposed changes\r\n\r\nNeed a space for markdown format.\r\n\r\n## Types of changes\r\n\r\n- fix documentation's format\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n a small fix for markdown format >>> 1"
51,This endpoint already exists in xcuitest driver. I've just forgotten to describe it here Add documentation for ‘mobile: touchAndHold’ endpoint >>> 1
52,(reviewers @jlipps @imurchie) Document various reset strategies >>> 1
53,"## Proposed changes\r\n\r\nUpdate Travis GCC version. This went in a while ago but the build was begin flakey and it was reverted. Turns out the flakiness had another source, so re-introduce this.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n\r\n\r\n### Reviewers: @dpgraham, @jlipps\r\n Use updated GCC in travis >>> 1"
54,"## Proposed changes\r\n\r\nAdd more information about Chromedriver. I had some time waiting for shopping family members :).\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @dpgraham, @jlipps, @filmaj \r\n Add documentation about Chromedriver >>> 1"
55,figured we should have this documented somewhere! add event timing docs >>> 1
56,"## Proposed changes\r\n\r\nWildcard and multiple waiting for activity is implemented by the followings. So, this PR adds examples for them.\r\n\r\n- https://github.com/appium/appium-adb/pull/196\r\n- https://github.com/appium/appium-adb/pull/224\r\n\r\n## Types of changes\r\n\r\nJust updating a documentation\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n update description for waitForActivity >>> 1"
57,"## Proposed changes\r\n\r\nSometimes, we should set `systemPort` to run parallel tests with uiautomator2 since without the capability, [Not able to run parallel tests using uiautomator2](https://github.com/appium/appium/issues/7745) is caused.\r\n\r\nSo, I've added the capability and the issue in `parallel_tests.md`. What do you think to add the description here?\r\n\r\n## Types of changes\r\n\r\nUpdate documentations.\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n add systemPort capability for uiautomator2 >>> 1"
58,"## Proposed changes\r\n\r\nIn preparation for the immanent release of Appium 1.6.5, update the CHANGELOG and bump the version\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @dpgraham , @jlipps\r\n Update CHANGELOG and version for 1.6.5 release >>> 1"
59,"## Proposed changes\r\n\r\nChanges will occur. So have a 1.6.6 beta release out to allow users to gain access to it without needing to do the whole `--no-shrinkwrap` dance.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n### Reviewers: @dpgraham, @jlipps, @mykola-mokhnach \r\n Prepare to release 1.6.6 BETA >>> 1"
60,"Fix the actual command for tagging. Mention `appium-desktop` in preference to appium.app/.exe.\r\n\r\n## Proposed changes\r\n\r\nMake it easier to release by not having to remember bits of steps.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @dpgraham, @jlipps\r\n Fix release instructions >>> 1"
61,"## Proposed changes\r\nAdd documentation for the new androidInstallPath capability for android devices. \r\n\r\n## Types of changes\r\nWhat types of changes does your code introduce to Appium?\r\nNone, it's just documentation.\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [x] Any dependent changes have been merged and published in downstream modules\r\n\r\nReviewers: @imurchie @jlipps \r\n add description for androidInstallPath >>> 1"
62,(related to https://github.com/appium/appium-android-driver/pull/241) Add remoteAdbHost to caps.md >>> 1
63,"## Proposed changes\r\n\r\nAdded documentation on how to set up and configure custom WDA server for appium xcuitest driver. This strategy should, hopefully, help to resolve known XCTest/WDA issues like https://github.com/facebook/WebDriverAgent/issues/507 and speed up automated tests initialization in general\r\n\r\n## Types of changes\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n Document how to set up custom WDA server >>> 1"
64,inspired by #8564 Advanced concept document: running android tests with docker >>> 0
65,"## Proposed changes\r\n\r\nAdding appium-doctor checks to appium repo. \r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [x ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [ x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x ] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n Add doctor checks to appium repo >>> 0"
66,I tested this in Appium Desktop and confirmed it work. Espresso Driver added to appium >>> 1
67,nan 1.6.6-beta.2 >>> 1
68,"## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [ ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [ ] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Espresso documentation >>> 1"
69,nan Add doc that links to xcuitest-driver docs >>> 1
70,"## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [ ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [ ] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Update broken link to valid Selenium platforms >>> 1"
71,"## Proposed changes\r\n\r\nLine 33 needs to have scrollObject instead of param\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [x] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [x] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [x] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [x] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n Line 33 need to have scrollObject instead of param >>> 0"
72,"## Proposed changes\r\n\r\nDescribe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\n## Types of changes\r\n\r\nWhat types of changes does your code introduce to Appium?\r\n_Put an `x` in the boxes that apply_\r\n\r\n- [ ] Bugfix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n## Checklist\r\n\r\n_Put an `x` in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code._\r\n\r\n- [ ] I have read the [CONTRIBUTING](/CONTRIBUTING.md) doc\r\n- [ ] I have signed the [CLA](https://cla.js.foundation/appium/appium)\r\n- [ ] Lint and unit tests pass locally with my changes\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] I have added necessary documentation (if appropriate)\r\n- [ ] Any dependent changes have been merged and published in downstream modules\r\n\r\n## Further comments\r\n\r\nIf this is a relatively large or complex change, kick off the discussion by explaining why you chose the solution you did and what alternatives you considered, etc...\r\n\r\n### Reviewers: @imurchie, @jlipps, ...\r\n 1.6 >>> 0"
0,Resolves https://github.com/cakephp/cakephp/issues/9405\n\nFeel free to add tests/commits on top.\n Login redirect fix for tab safe re-login using query string. >>> 1
1,Large float values were being truncated to scientific notation when being passed into PDO. This was caused by PHP's implicit float->string behavior that loses precision. Formatting into %F will retain all the necessary precision.\n\nRefs #9424\n Fix issues working with large float values. >>> 1
2,Updated boolean docblock Ref: https://github.com/cakephp/docs/issues/4147\n 3.x docblock: Validation boolean => mimic isTrue/isFalse >>> 0
3,"This works _without_ any external extension or lib, see \n\nPlease see http://php.net/manual/en/function.getimagesize.php\n\n> **Note:** This function does not require the GD image library.\n Adding image size validation >>> 1"
4,"While I remembered to include the error handling for errors raised during rendering, I forgot to include error logging in general. I've replicated the logic present in the ErrorHandler logging code except for the clientIP. This was omitted because it is non-trivial to extract the client IP from a PSR7 request.\n\nRefs #9469\n Fix missing logging in ErrorHandling Middleware >>> 1"
5,fixes #9195 \r\n\r\nAdds support for more clauses with MySQL for the UPDATE and DELETE queries. Specifically adds support for ORDER and LIMIT.\r\n\r\n**TODO**:\r\n\r\n[ ] Postgres support for ORDER in UPDATE with unit tests.\r\n[ ] Postgres support for LIMIT in UPDATE with unit tests.\r\n[ ] Postgres support for ORDER in DELETE with unit tests.\r\n[ ] Postgres support for LIMIT in DELETE with unit tests.\r\n\r\n[ ] SQLite support for ORDER in UPDATE with unit tests.\r\n[ ] SQLite support for LIMIT in UPDATE with unit tests.\r\n[ ] SQLite support for ORDER in DELETE with unit tests.\r\n[ ] SQLite support for LIMIT in DELETE with unit tests.\r\n\r\n[ ] SQL Server support for ORDER in UPDATE with unit tests.\r\n[ ] SQL Server support for LIMIT in UPDATE with unit tests.\r\n[ ] SQL Server support for ORDER in DELETE with unit tests.\r\n[ ] SQL Server support for LIMIT in DELETE with unit tests.\r\n\r\n[ ] MySQL support for ORDER in UPDATE with unit tests.\r\n[ ] MySQL support for LIMIT in UPDATE with unit tests.\r\n[ ] MySQL support for ORDER in DELETE with unit tests.\r\n[ ] MySQL support for LIMIT in DELETE with unit tests.\r\n\r\n MySQL support for UPDATE/DELETE to use ORDER and LIMIT >>> 0
6,"Under the new PSR7 HTTP stack, redirect routes would be missing their base directory as Router did not have the correct context set. Instead of making the Router request stack PSR7 aware, I've made the request context data PSR7 aware by introducing a new public method. Longer term I'd like to remove the request stack entirely, as its requestAction() will also be leaving.\n\nRefs #9470\n Fix missing base directories in redirect routes. >>> 1"
7,Due bad readability and avoiding parsing error in IDEs - no functional changes. Thank you\n Avoiding extract call in CakeSchema class >>> 1
8,"This is the first step towards https://github.com/cakephp/cakephp/issues/9310 and resolving URLs being needed unespaced for further processing.\n\nThis PR should start the discussion how to achieve that.\nI think this additional param is one option.\nIt is a rare use case, but easily doable if needed this way.\nBut I wonder if ""full => true"" and ""escape => false"" make any sense.\n\nIf this is not BC for extension, we need to target 3.4 instead here, or do in fact need a new method `buildRaw()` or just `raw()` here.\nThis method could then only have two arguments: $url and $escape.\n Allow unescaped URLs to be built. Other helpers need this internally. >>> 1"
9,"Since version [2.8.8](https://github.com/cakephp/cakephp/compare/2.8.7...2.8.8#diff-c02fdc21fc32c45375d9c450dc6bf062L3107) `$this->Form->_lastAction()` does not set the `_lastAction` to a correct value, when the url contains named parameters:\n- Output of [/users/index/page:1](https://3v4l.org/6ifYr) url\n- Output of [/users/index/foo:bar](https://3v4l.org/goqaN) url\n\nThe pull request the caused the behavior to change is #9458.\n\nI've added 2 tests that should succeed as far as I'm concerned.\n _lastAction() should also work with named parameters in the url >>> 1"
10,As discussed on irc with @hmic. Dropping the feature of fake ajax redirect. \n\nThis breaks my app. ($this->redirect() does not when doing an ajax request)\nIt is using the deprecated RequestActionTrait. Fixing it is unnecessary base it will be removed soon.\n Drop broken code for fake ajax redirect. >>> 0
11,"Simple option to preserve keys when using `Collection::chunk`.\nBecame very useful when fetching results as list:\n\n```\n$users = $this->Users->find('list')\n    ->chunk(10, true)\n    ->toArray();\n```\n\nThis way the `id => username` association gets reserved.\n Preserve keys option on Collection::chunk >>> 0"
12,Implement the header related method required for the PSR7 ServerRequestInterface. \n\nI've chosen to make the new methods read from the existing data as with the other changes I've done so far. This should hopefully make the new methods non-destructive to the existing ones.\n\nRefs #9325 \n Implement PSR7 header methods >>> 1
13,This new ResponseEmitter offers some improved ergonomics. It no longer:\n- Throws an exception when headers have been sent.\n- Truncates content when debug output has been generated in the  controller.\n\nIt also uses setcookie() which lets us remove the shims we had to apply to restore behavior of ext/session.\n\nRefs #9472\n Add a simpler response emitter. >>> 1
14,I missed a parameter and some behavior the first time I implemented withUri(). This shores up that gap and finishes the PSR7 implementation in Network/Request.\n\nRefs #9325 \n Finish implementing withUri() and the PSR7 interface. >>> 1
15,Add status codes from [Zend/Diactoros](https://github.com/zendframework/zend-diactoros/blob/215f47a73fb2a7f2d33230a3e23658acae234035/src/Response.php#L32-L96) and [httpstatuses.com](https://httpstatuses.com/).\n\nRefers https://github.com/cakephp/cakephp/issues/9516#issuecomment-249367130\n Add missing HTTP status codes/messages >>> 1
16,"The unit tests for requestAction all setup the necessary global state. Which is why they were passing while in reality the code failed. While this feature is deprecated and unloved, it should continue doing the thing it has always done.\n\nRefs #9505\n Fix requestAction() not working with middleware. >>> 1"
17,Ref: https://github.com/cakephp/cakephp/pull/9523\n Alternative HTTP STATUS code update >>> 0
18,I do still think that adding as inline comments the RFC and Section reference is highly helpful and also will make clear that we support the NON-STANDARD codes 499 and 599.\n\nThere are more than 16 RFCs ( https://github.com/cakephp/cakephp/pull/9528/files#diff-a57bc7407c64664db661cb46c465f363R36 ) and those won't change and its easy to discover what these then mean _by spec_. https://github.com/cakephp/cakephp/pull/9528/files#diff-a57bc7407c64664db661cb46c465f363R59\n Added un-merged improvements from #9528 >>> 1
19,"The beforeRedirect behavior has surprising behavior and is not something that we'd like to keep around long term. Deprecate the functionality, and provide a config flag for disabling it now.\n\nRefs #9525\nRefs #9505\n Allow RequestHandlerComponent::beforeRedirect to be disabled. >>> 1"
20,"As per cakephp/cakephp#9529.\n\nOn SQLServer this seems to be impossible to implement, as [a subquery is neccessary](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/f09d4166-2030-41fe-b86e-392fbc94db53/tsql-equivalent-for-groupconcat-function?forum=transactsql).\n\nIt's currently not possible to pass in a `ExpressionInterface`, to achive something like `GROUP_CONCAT(id+1), but it looks like that's not supported for the other functions either. (And I have no idea how to implement it).\n\n_I don't have tested the generated queries against a real DB server_ I can test against MySQL later, but I don't have access to the other types of servers.\n\nI'm still slightly confused by the Database system, so please tell me if there is a better way to achive this ;)\n Implement GROUP_CONCAT() function >>> 0"
21,"See #9531 \n\nThe fix was to add the template name to the cache key. However, there is one use-case where the defect is not fixed. If the caller explicitly sets a cache key, then they would always get the same results from `Cell::render()` regardless of the template.\n Bugfix/cached viewcells return the same results with different templates when rendering >>> 1"
22,"This is another ""special"" feature which IMHO needs to go.\n Deprecate ""ajaxLogin"" option of AuthComponent. >>> 1"
23,"Now that `Network\Request` implements the PSR7 interface, we can remove the usage of the `RequestTransformer`. I've not removed the transformer in case someone is referencing it in their code.\n\nI've removed the Bad Request error on malformed HTTP versions, instead we ignore the user input and assume they meant `1.1`.\n\nRefs #9325\n Use ServerRequest factory to create request instances >>> 1"
24,"This pull request is necessary to re-implement Explain button in debug toolbar.\n### What is changed\n- Added `Connection::canExplain()` method. Returns true If driver is supporting EXPLAIN statement for the specified query. Mysql, Postgres and Sqlite will return true if the specified query starts with SELECT, INSERT, UPDATE or DELETE. Sqlserver or other unknown drivers will always return false.\n- Added `Connection::explain()` method. Excecutes EXPLAIN query If driver is supporting EXPLAIN statement. If not supporting, throws an exception.\n- Added `LoggedQuery::$queryString` property because `LoggedQuery::$query` may be modified by QueryLogger. If LoggedQuery doesn't keep original query, DebugKit cannot execute correct EXPLAIN query.\n### Usage\n\n``` php\n$connection = ConnectionManager::get('default');\n$query = 'SELECT * FROM users WHERE id = :id';\nif ($connection->canExplan($query)) {\n    $stmt = $connection->explain($query, ['id' => 1], ['id' => 'integer']);\n    $result = $stmt->fetchAll();\n}\n```\n\n**Edit:** Fixed incorrect explanation \n Add support for EXPLAIN statement >>> 0"
25,apply TypeMap into matching/notMatching/innerJoinWith/leftJoinWith\ni just tested for innerJoinWith() and i am not sure these changes work well for all case\n\nfollow: #7506\n apply TypeMap into matching/notMatching/innerJoinWith... >>> 1
26,"Where possible use the the PSR7 & undeprecated methods in controller & components. I didn't change any of the operations that mutated the request to use the immutable interfaces as there is simply too much shared state in components and controllers right now.\n\nIn the future Components will need to stop holding on to references of the request, and instead fetch the request from the controller. This will allow PSR7 methods to be used in more places.\n\nRefs #9325\n Update controllers and components to use PSR7 methods >>> 1"
27,"This PR aims to introduce a `BreadcrumbsHelper` as discussed in #9542.\nIt exposes an API that allow precise insertions of crumbs (much like the way you'd manage your middlewares) to get around common issues related to the way Cake renders the views.\n\nIt also gives an easier way to customize the way the trail is rendered by using the `StringTemplateTrait`, the same way the `FormHelper` does.\n\nThe `render()` methods accepts two arguments : \n- the first argument will be used to customize the wrapper of the crumbs trail\n- the second argument to customize the separator between crumbs. By default, no separator will be added between each crumbs.\n\nFor the templates, I went with what I think are sane default. Feel free to comment on this.\n\nOnce this is approved, I'll document the docblocks a bit more (to give more details about the arguments of the `render()` methods for instance and how to customize the ""innerAttrs"") and write something for the book.\n\nRefs #9542 \n Introduces a BreadcrumbsHelper >>> 1"
28,Ref: https://github.com/cakephp/cakephp/issues/9211\n 3.x PaginatorHelper::defaultScope() and 'scope' / Consistency with PaginatorComponent >>> 0
29,Ref: https://github.com/cakephp/cakephp/issues/9463\n 3.x Add Validation::truthy() and Validation::falsey() >>> 1
30,Ref: https://github.com/cakephp/cakephp/issues/8618\n@dakota Let me know if this fits the bill @ https://github.com/cakephp/cakephp/issues/8618#issuecomment-207818465\n 3.x Validator::fieldsMatching() and Validator::fieldsNotMatching() >>> 0
31,"This allows you to do something like:\n\n```\n<?= $this->Html->image('cake.icon.png', ['base64' => true]); ?>\n```\n\nand get the following (long) output:\n\n```\n<img src=""data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA1FJREFUeNqsVF1IU2EYfraF5U951P0wnTidP/kzm5GZuygNDSWhQRnECMohhgnRnRcpKgQSqGhCEIhLJhkUGhGu0JwiXqSYQyddhG6EG7bQ/E1ccfq+D3fYULvyufgO53vf85zn/RXxPI+jxDF6iEQi4aK0tNRAHtdUKpUuOjpaJ5fLoVAomG1hYQFbW1twuVy2tbW1GXL1YnBwcMb/LRUnYgchpESxsbFtZWVl6vz8fIHkMFDyoaEhjIyMUPKHlJhFSw9C1mCxWPiDMDExwbe0tLDn5ubmPju9a2pq4glHgaCwtraWb25uPlRNTU0NUbwEj+cvpNIMpKVpQaNISkpidpoGk8k009fXl8NySPP0P1RVVaGr6xGKr4SSt0UsLzvR2/sBP5bFkCs0jDwiIoITikJzUVxcDK1WeyAhvc/IuEoUfoRSKYFKFYbUFBnCwmQQIRLfvtnhdrtZccR739jM5ka0traSvy8fSGo0GjE1KcGobQcu5x9IJCGQiEOoMkhjmDi7oJDg1+3b8SQXIeju7sTurgSJiUmIi1NBrVaD4yKxsrKKrKw8zM+P4uvX3xgetiM9XYnTaYlYX/dRjplAQrvF4jFUVuaguroQoaEq5rS46ITD4YDX62VOHMeRFhPDYEiAWMQR+y7mHN9JyG4mKpDQubq6i8eP36O8nEdJSRxIUyMmRhoU9tjYGN6988Fm8yL33Cmcz03FpYtSPO3sR3vHc1sQoUYTgdzcbFitn/HmzSdSOT30ej1rjfDwcObk8/mQnHwSUVFhGHg7T1IzhzPZqVhye51Bo0fjHx//SRxXUF9/DxsbxzE15cDAwAArEp0KP/R6OfLy4lFYcBY7OycwPe0iud8RCIXRM5mu8/HxMYzMaLwFnS6P5SsQVqsVHR0dNq1Woc6/kKnOyswgefSirf1VIxm9BsrlV0j6aNtcUXH5zvZ2JNrbn5F26EVRURE0Gg0CFwTB6JMn5kI6+0ql/YFcFlXgr3CQQuLAJSTIFuvq7nMyWQomJ+dZlWdnZwWFZG5p+HeJGnPAdlKTd6d/2yBwHxKj7ubNG196erp4j8e9bxH09/ezJXDYiAaFTLG323I2NrYML1++Fnai337YFAVCdNQbW4wjxj8BBgDSNqdSpFi+eQAAAABJRU5ErkJggg=="" alt=""""/>\n```\n\nI've included a test for this, and it passes the rest of the tests.\n\nConsiderations:\n\n1) No base64'ing a remote image. Can we rely on [allow-url-fopen](http://php.net/manual/en/filesystem.configuration.php#ini.allow-url-fopen) being enabled? Should I check for the flag and allow you to do it?\n\n2) There's no mime checking, but then again you can currently use image on anything you feel like, so should this code follow convention?\n\n3) How best to handle the case of being unable to file_get_contents? Ideally I'd like to throw, but this would introduce a thrown exception into something that doesn't already throw. \n\nIf this looks all ok, I'll forward-port this to 3.x (I did it into 2.x first as that's what I know better)\n\nRequested by cakephp/cakephp#9560\n (2.next) Adding Base64 support to the HtmlHelper::image() function >>> 1"
32,Resolves https://github.com/cakephp/cakephp/issues/9310\n\nFirst draft of yesterday evening.\nIf the idea is accepted I can add some tests later on.\n Add escape false to PaginationHelper >>> 1
33,Use undeprecated and PSR7 methods in helpers/view classes. I've also tried to remove the remaining ArrayAccess uses in the tests.\n\nI added in a few `...` operators in the Request class to avoid some `func_get_args()` calls as I know removing those is a minor goal for 3.4.0.\n\nRefs #9325 \n Use PSR7 request methods in helpers >>> 1
34,This is a:\n- [x] bug\n- [ ] enhancement\n- [ ] feature-discussion (RFC)\n- CakePHP Version: b03309f\n### What you did\n\nUsed `_method` in the body of the request to fake a `PATCH` request.\n### Expected Behavior\n\nRoute is correctly matched.\n### Actual Behavior\n\n`Cake\Core\Exception\Exception\MissingRouteException` is thrown.\n Checking for presence of _method and using over getMethod() in RoutingMiddleware >>> 1
35,"Work in progress, no need to merge it yet, just for review. :)\n Started making the reponse PSR7 compatible >>> 0"
36,Add an alias for backwards compatibility. I've not moved the tests yet as I wanted to ensure that backwards compatibility was not lost.\n\nRefs #9325\n Move request to http package. >>> 1
37,This is my attempt of fixing https://github.com/cakephp/cakephp/issues/9575.\n\nNote: I never worked with cakephp before :see_no_evil: \n [Bugfix] use Model::$cacheQueries as default for ->query(...) (#9575) >>> 1
38,"Add PSR7 `UploadedFileInterface` support to the various `Validation` methods that deal with files. This will help make transitioning to uploaded file objects simpler, as the marshalling/validation process will continue to work as it did with arrays.\n\nRefs #9325\n Psr7 file validation >>> 1"
39,"This moves the logic in `SelectableAssociationTrait` and `ExternalAssociationTrait` into separate classes that are used as collaborators in the association classes.\n\nI feel this makes the code more readable and easier to grep, as from previous experience, jumping in between the traits was quite a bad experience.\n Refactor association traits >>> 1"
40,Replacing #9583 with a clean start.\n Started making the reponse PSR7 compatible >>> 1
41,nan Add connectOptions to doc block >>> 1
42,Resolves https://github.com/cakephp/cakephp/issues/9601 for 3.next.\nWe still will need to backport this as a bugfix for 3.current where it currently would have to be fixed for session.\n Fix redirect to login with non GET requests and remembering location. >>> 1
43,Resolves https://github.com/cakephp/cakephp/issues/8840\n Localize pagination numbers. >>> 1
44,"Adds support for `<link rel=""canonical"" href=""..."">` for `Html->meta()`\n\nThis is a Google standard tag for indicating duplicate content.\n adds canonical to meta() >>> 1"
45,"Use of mime_content_type in function attachments.\n\nAllowing image inline in templates:\n\n``` php\n  <img src=""cid:/full/path/image"">\n  <img src=""cid:///full/path/image"">\n  <img src=""file:/full/path/image"">\n  <img src=""file:///full/path/image"">\n  echo $this->Html->image('cid:///full/path/image');\n  echo $this->Html->image('file:///full/path/image');\n```\n\nThis changes are compatible with CakePHP 2.x and 3.x\n\nhttps://github.com/fawno/FawnoEmail\n 3.next - Allow image inline in templates and use of mime_content_type in function attachments. >>> 0"
46,"I noticed in the `InflectorTest` class that the arguments in some of the assertion calls were in the incorrect order.  While this obviously doesn't cause failure, it's not semantically correct, and consistency in programming is usually nice.  This PR fixes the cases that I found.\n Reorder arguments in some assertions >>> 1"
47,"Use of mime_content_type in function attachments.\n\nAllowing image inline in templates:\n\n``` php\n  <img src=""cid:/full/path/image"">\n  <img src=""cid:///full/path/image"">\n  <img src=""file:/full/path/image"">\n  <img src=""file:///full/path/image"">\n  echo $this->Html->image('cid:///full/path/image');\n  echo $this->Html->image('file:///full/path/image');\n```\n\nThis changes are compatible with CakePHP 2.x and 3.x\n\nhttps://github.com/fawno/FawnoEmail\n 2.next - Allow image inline in templates and use of mime_content_type in function attachments. >>> 0"
48,Adds the following to the helper\n- moved core of generateUrl() to buildUrl() and returns a raw URL array of pagination parameters.\n- generateUrl calls buildUrl()\n- added total() which returns page count.\n- added to meta() support for generating first and last meta links\n\nThe object of the PR is to allow developers to generate pagination URLs in their array format for easier modification outside of the helper.\n\ncloses #9503 \n Improvements for PaginatorHelper >>> 0
49,Ref: https://github.com/cakephp/cakephp/issues/7977\n@ADmad is this what you had in mind / considered sane?\n 3.x Allow to specify 'method' and 'enctype' for FormHelper::create() >>> 1
50,"This change fixes support for intellisense with phpstorm.\n\nI found a number of open tickets for phpstorm to get class_alias working correctly, but they date back many years. So I don't know what the deal with it is.\n- adds a new file `deprecated.php` which contains all the aliases (for easier management).\n- aliased classes are now have a stub class.\n\nThe stub looks like this.\n\n```\nnamespace Cake\Network\Email;\n\nif (false) {\n    /**\n     * @deprecated Use Cake\Mailer\Email instead\n     */\n    class Email extends \Cake\Mailer\Email\n    {\n\n    }\n}\n```\n\nIt's a do nothing chunk of code that tells the IDE to mark the old classes as deprecated. So they are now shown with a dashed highlight to indicate it's deprecated.\n\nCode changes based upon this blog:\n\nhttp://126kr.com/article/3v5wd81w2nu\n refactored aliases to support IDE intellisense >>> 0"
51,Adds the following to the helper\n- moved core of generateUrl() to buildUrl() and returns a raw URL array of pagination parameters.\n- generateUrl calls buildUrl()\n- added total() which returns page count.\n- added to meta() support for generating first and last meta links\n- The object of the PR is to allow developers to generate pagination URLs in their array format for easier modification outside of the helper.\n\ncloses #9503\n Improvements for PaginatorHelper >>> 1
52,Refers to #9631 \n Adding $params argument to prefix() method of RouteBuilder. >>> 1
53,"When we have bundled translations but no changed fields, we need to mark the original fields dirty so we can save the root entity and the attached translations.\n\nRefs #9610\n Fix saving translations when only translations are modified. >>> 1"
54,Supports NAN/INF rendering in JsonView with PHP 5.5+\n\ncloses #6121\n Support for NAN and INF in JsonView >>> 1
55,The `fieldList` option for `newEntity` isn't consistent with other parts of the ORM. Where `fields` is used to select what fields will be used (queries as an example).\n\nThis PR deprecates `fieldList` and changes it to `fields`. With `fieldList` support working as before and to be removed at a later date.\n replaces fieldList with fields >>> 1
56,"Adds configurable masking of debugger output by property name and array key names. If you do not want to ever dump passwords, then you can add a global mask like this:\n\n```\nDebugger::configInstance('mask', ['password' => '[**********]']);\n```\n\nThe current defaults are to **not** mask anything. Later, I would like to update the cakephp/app to store a debugger config in `app.php` with some recommended masks (i.e, passwords, social media private keys, etc)\n\ncloses #5136 \n\nThere was another issue in the debug_kit about passwords being shown in the logs, but I could not find it again. I do not know if this will resolve that issue.\n Debugger: adds configurable masking of array/object properties >>> 1"
57,Resolves https://github.com/cakephp/cakephp/issues/8810\n\nAlso fixed a wrong text and made sure IDE autocomplete works on some arrays.\n Fix logic issue of required after optional arguments. >>> 1
58,In Component callbacks that return a response we can start using the PSR7 methods as the event dispatcher passes the returned response up the stack and eventually emits it.\n\nRefs #9636\n Use PSR7 response methods where possible. >>> 1
59,Resolves https://github.com/cakephp/cakephp/issues/9660 for 3.x\n\n...Checking if tests pass.\n Fix components enabled config option when merging with defaults. >>> 1
60,"This fixes #9634, however I have to admit I don't know if this will have any potential side effects I'm not considering. In the `testFormatResultsBelongsToMany` test I added a formatter to the originating table to make sure it gets called still and doesn't get the belongsToMany formatter applied to it, just in case.\n Proposed fix for #9634 >>> 1"
61,Implements https://github.com/cakephp/cakephp/issues/8757\n\nClean getter/setter API now.\nThe bad combined ones have been deprecated and can stay in place until 4.x.\n\nThe relations are now chainable and autocompletable:\n\n``` php\n        $articles->belongsTo('authors')\n            ->setForeignKey('author_id')\n            ->setName('Authors')\n            ->setTarget($authors)\n            ->setBindingKey('id')\n            ->setConditions([])\n            ->setFinder('list')\n            ->setProperty('authors')\n            ->setJoinType('inner');\n```\n\nSmall API changes it seems:\n- propertyName => setProperty()\n- className => setName()\n\nOpen questions:\n- through option setter/getter for belongsToMany missing? I added one now.\n 3.next table relations >>> 1
62,Implement more robust nonce handling in Digest authentication. Shore up some of the implementation gaps we have around nonces. Nonces should expire and should be generated by the server. I've followed the Symfony approach to generating nonces which results in short lived (5 minutes by default) nonces that are regenerated each time the client receives a 401.\n\nBy validating and requiring nonce rotation we can avoid replay attacks with our Digest authentication implementation.\n\nI've targetted 3.next as validating/requiring nonce rotation is a behavior change that I feel could trip up application developers. By including it in a minor release we can more effectively communicate the changes.\n\nRefs #9668\n Digest authentication improvements >>> 1
63,In issue #7830 it was requested to backport the feature.\n 2.next Support Stacking Flash Messages >>> 1
64,"Unlike for delete queries, possible aliases in the conditions of update queries are not being removed, which will fail for some database dialects. Given that update queries currently do not support joins, just like delete queries, removing the aliases here in the same way should be good enough for now I think.\n\nThe fact that expressions are being left untouched is something that maybe should go in the docs somewhere, it's something that affects all dependent, non-cascading associated deletes as well, which developers should generally be aware of.\n\nrefs #9404\n 3.x - Strip aliases from update query conditions where possible. >>> 1"
65,"if `$data['a_key']` exists and it is equal to `null` the `\Hash::get()` returns `false`, it should return `null`\r\n\Hash::get($data,'a_key',false); use array_key_exists() instead of isset() in \Hash::get() >>> 0"
66,This PR completes https://github.com/cakephp/cakephp/pull/9685\r\nThe feedback was that Schema should better be TableSchema\r\n\r\nEverything is changed expect for a few BC checks in tests and in the subclasses of BaseSchema - to proof that even extension with swapping the alias does not trigger any notices. Rename Schema/Table to Schema/TableSchema via class_alias() >>> 1
67,Use same kind of wording in each sub title Improved wording >>> 1
68,"I,ve fixed my problem on save Unicode JSON in Postgresql with this pull request.\r\n\r\n**Error Message:**\r\n```\r\nSQLSTATE[22P05]: Untranslatable character: 7 \r\nERROR:  unsupported Unicode escape sequence\r\nDETAIL:  Unicode escape values cannot be used for code point values above 007F when the server encoding is not UTF8.\r\nCONTEXT:  JSON data, line 1: {\""first_name\"":...\r\n``` \r\n\r\n Supprt Unicode escape sequence in Postgresql >>> 0"
69,"Backporting multiple paginators feature from cakephp 3.3.\r\n\r\nRefs #8488\r\n\r\nI used **queryScope** instead of **scope** as it is in Cakephp 3, because in Cakephp 2, paginate has a second parameter **$scope** for paginate conditions. 2.next - Backport multiple paginators >>> 1"
70,Other team members please add your name/email combinations to the file :) Add .mailmap >>> 1
71,Now that `Cake\Network\Response` implements the PSR7 interfaces we don't need the `ResponseTransformer` any more. This saves us some time and complexity as responses aren't transformed before being emitted.\r\n\r\nRefs #9636  3.next Stop using ResponseTransformer >>> 1
72,"Start the splitting of combined to separate setter/getter.\r\nScope is only the important ORM layers:\r\n- Database\r\n- Datasource\r\n- ORM\r\n\r\nKey features:\r\n- No more fatal errors or accidents will $null input that triggers the get instead of set call\r\n- Remove method complexity, allow stronger typehinting in the future, better IDE support\r\n- Better speaking API, booleans are more clear from the method name (`autoQuoting()` to `enableAutoQuoting()`/`isEnabledAutoQuoting` etc)\r\n- Chainable setters where it makes sense\r\n- Doc blocks (and therefore API doc) more readable as per their ""single responsibility"" now\r\n- Methods should never return `$this|otherType`\r\n\r\nE.g it removes the confusion around `selectTypeMap()` which does not mean ""to select a typeMap"" but to ""set the typeMap for the select clause"" by using `setSelectTypeMap()` and `getSelectTypeMap()`.\r\n\r\nEverything is BC, cleanup in 4.0 will be easy, also upgrading will be super easy as people have enough time in 3.x to slowly adjust to the new API.\r\n\r\nMethods are still used the old way a lot, to provide BC check and to keep diff readable. Can be done later.\r\n\r\nWorth mentioning:\r\n\r\n- `Query::bufferResults($enablel)` is now `enableBufferedResults()`/`isEnabledBufferedResults()`.\r\n- `Connection::useSavePoints($enable)` is now `enableSavePoints()`/`isEnabledSavePoints()`.\r\n\r\nBetter ideas for names welcome. This way it would be consistent, though - and `is...` shows that this is of bool result.\r\n\r\n---\r\n\r\nRemark for the future:\r\n- We must more declare properties in the tests, just `$this->foo = new Foo()` provides no typehinting etc. This makes the tests and refactoring very difficult. Each `$this->...` assignment must have a property defined IMO. Split combined into separate getter setter for cleaner API. >>> 1"
73,"Resolves https://github.com/cakephp/cakephp/issues/7444 for MySQL\n\nRFC, we can either add tests and merge or close.\n Allow basic mysql support for type year - mainly for lecagy DB support >>> 0"
74,resolves https://github.com/cakephp/cakephp/issues/9374\r\n\r\n accept webroot shell parameter >>> 1
75,As discussed in #9728 this will fix shell naming conflicts between plugins:\r\n\r\n- [x] Avoid reporting that shall name of PluginName conflicts with PluginName\r\n- [x] List all plugins that conflicts for a specific shall command False positives and conflicts list for shells >>> 1
76,"I found this exception a little obscure, especially as the things which it needs are detailed in the function. It could tell me what's actually missing. Improve descriptiveness of exception message >>> 1"
77,"Follow up on https://github.com/cakephp/cakephp/pull/9654\r\n\r\nI now confirmed with a real test application that it works together with a custom [YearType](https://github.com/dereuromark/cakephp-shim/blob/master/src/Database/Type/YearType.php).\r\n\r\nThis patch is required to properly store the values posted on the one side, and to output format it as Year dropdown on the other side for form display. Allow basic support for MySQL year type >>> 0"
78,fixes #9195 \r\n\r\nAdds support for more clauses with MySQL for the UPDATE and DELETE queries. Specifically adds support for ORDER and LIMIT.\r\n\r\n## ORM\r\n\r\n- [x] QueryTest verify UPDATE with ORDER/LIMIT works against fixtures on all drivers.\r\n- [x] QueryTest verify DELETE with ORDER/LIMIT works against fixtures on all drivers.\r\n\r\n## Postgres\r\n\r\n- [x] SQL dialect for UPDATE\r\n- [ ] SQL dialect for DELETE\r\n- [x] Support for ORDER in UPDATE with unit tests.\r\n- [x] Support for LIMIT in UPDATE with unit tests.\r\n- [x] Support for ORDER in DELETE with unit tests.\r\n- [x] Support for LIMIT in DELETE with unit tests.\r\n\r\n## SQLite\r\n\r\n- [x] SQL dialect for UPDATE\r\n- [ ] SQL dialect for DELETE\r\n- [x] Support for ORDER in UPDATE with unit tests.\r\n- [x] Support for LIMIT in UPDATE with unit tests.\r\n- [x] Support for ORDER in DELETE with unit tests.\r\n- [x] Support for LIMIT in DELETE with unit tests.\r\n\r\n## SQL Server\r\n\r\n- [ ] SQL dialect for UPDATE\r\n- [ ] SQL dialect for DELETE\r\n- [ ] Support for ORDER in UPDATE with unit tests.\r\n- [ ] Support for LIMIT in UPDATE with unit tests.\r\n- [ ] Support for ORDER in DELETE with unit tests.\r\n- [x] Support for LIMIT in DELETE with unit tests.\r\n\r\n## MySQL\r\n\r\n- [x] SQL dialect for UPDATE\r\n  - resolved by QueryCompiler::$_updateParts\r\n- [x] SQL dialect for DELETE\r\n  - resolved by QueryCompiler::$_deleteParts\r\n- [x] Support for ORDER in UPDATE with unit tests.\r\n- [x] Support for LIMIT in UPDATE with unit tests.\r\n- [x] Support for ORDER in DELETE with unit tests.\r\n- [x] Support for LIMIT in DELETE with unit tests. MySQL support for UPDATE/DELETE to use ORDER/LIMIT >>> 0
79,Simplified existing mappings and added few more.\r\n\r\n`git shortlog -sne` now returns **591** lines. Improve .mailmap >>> 1
80,Implement several `get*` methods on `ServerRequest`. These methods are more consistent with the PSR7 methods and each other as they all support dotted paths and default arguments now.\r\n\r\nI've left `getData()` able to return all data to help migration/upgrades be easier. `request->data()` is frequently used and I wanted the upgrade path to be a simple search/replace.\r\n\r\nRefs #9670 3.next - Add more consistent request getters >>> 1
81,By implementing the `engine` method just like in version 3\r\n\r\nFixes #9741 Expose Cache engines >>> 1
82,"> Method caching uses `md5` to construct cache keys. If you have problems with collisions, set DboSource::$cacheMethods to false.\r\n\r\nSeems like an easy fix for this problem. If you like this feature I can add some tests.\r\n\r\nThanks in advance Make it possible to configure the cacheMethod hashing algorithm in DboSource >>> 1"
83,"Doing debug($response) anywhere so far is always quite painful, especially in CLI.\r\nIt first outputs the whole internal property stack, including irrelevant long list [protected] _statusCodes as as well as [protected] _mimeTypes\r\n\r\nWith __debugInfo() it now focuses on the actual data. Add debugInfo to Response for better debugging. >>> 1"
84,"When err() started adding formatting for us, we forgot to remove the formatting added when handling option parser errors.\r\n\r\nRefs #9394 Fix double error tags when option parsing fails. >>> 1"
85,"Prevents ""Call to a member function toArray() on a non-object"" fatal error when generating an xml with strings containing class-names from your application. Checking if object before calling method on it >>> 1"
86,Refs https://github.com/cakephp/cakephp/pull/9756#issuecomment-260240864 Fix doc blocks and return values. Also fixed some missing typehinting. >>> 1
87,Refs https://github.com/FriendsOfCake/crud/pull/432#issuecomment-260230331\r\n\r\nFully BC Refactor into smaller chunks of single responsibility methods. >>> 0
88,"Completes further the goal of the clean API.\r\n\r\nThe mix of string+array vs array mix alone is really bongers, and the code so far was rather complex to understand the many use cases.\r\n\r\nI wonder if we could split the two cases above further?\r\n- setConfig ($key, $value)\r\n- setConfigFromArray(array $configArray)\r\n\r\nWhat do you think? Split Config trait modals into getter and setter. >>> 1"
89,"closes #9765 \r\n\r\n- removes the usage of regex in parsing extensions, and uses a simple ""string ends with"" check for extensions.\r\n- updates `extensions()` with `setExtensions()` and `getExtensions()`\r\n- added unit tests for changes\r\n- adds stub class to expose protected methods (unit test only) fixes greedy dots in extension parsing >>> 1"
90,closes #9727\r\n\r\n- adds a new deleteEach method to the Table interface\r\n- adds beforeDeleteEach  and afterDeleteEach events\r\n\r\nUnit tests are coming. Just wanted early review. Introduce a new deleteEach method for Table >>> 0
91,"Add `withLocation()` and `withType()`. These are immutable versions of `type()` and `location()`. Unfortunately this spiraled a bit out of control as I wanted to keep the tests passing. However, I've:\r\n\r\n* Fixed getHeaders() not always following PSR7 interface requirements  when headers are set with `header()`.\r\n* Restored behavior where the status code is set to 302 when a location  header is set and the current status is 200.\r\n* Eagerly set the 'Content-Type' header so that charset(), type() and  withType() all result in the content-type header being correctly set.   This also fixes an issue where the content type would not be correctly  set when the response was emitted.\r\n\r\nRefs #9636 3.next - Start adding Response helpers and fix some issues. >>> 1"
92,closes #9773 \r\n\r\n- adds a simple `_executeTransaction` for either transaction or none on a callback\r\n- adds a simple `_getCommitUsed` which returns `true` if there was a transaction committed.\r\n\r\nThis ended up being a smaller/simpler change then I originally thought it was going to be.\r\n\r\nWhich is a good thing 😄  Reduce use of duplicate code for transactions >>> 1
93,Many spots in the ORM reference the `ConnectionInterface` interface but proceed to call `Connection` object methods directly. These method calls are flagged as interface does not support method by PhpStorm.\r\n\r\nThis PR adds popular methods to the interface that all `Connection` objects need to implement since the core assumes the object has them. Updates the ConnectionInterface with missing methods. >>> 0
94,Also remove duplicate property definition that we don't need. Remove response from CookieComponent. >>> 1
95,It should forward the settings from ClassRegistry::init() so that aliases can be customized as needed.\r\n\r\nRefs #9766 Fix AclNode constructor. >>> 1
96,"Responses with these status codes do not have response bodies, by clearing the content-type header we can make responses more consistent with how CakePHP has historically performed.\r\n\r\nRefs #9636\r\n Fix clearing content-type when status is 204/304 >>> 1"
97,"This resolves issue #7324 as described in my comment there.\r\n\r\nYou can define the sort order of pagination in pagination settings array.  \r\nThese settings are unioned with the request query parameters, request query always comes first.  \r\n\r\nDue to the union, if the sort field of the request query is already defined by the settings array, the settings array will not overwrite the request query sort field.  \r\n\r\nIf no request query sort field is set (e.g. initial page 1), the pagination URL is created with the first settings entry, which has the same effect than ommiting it in the URL.\r\n Issue 7324 paginator sort multiple fields >>> 0"
98,Small fixes. Remove some cloaking for stricter null checks and fix a doc block. >>> 1
99,- Start fixing up ViewBuilder API. Also fixed a few wrong doc blocks.\r\n- Remove deprecated method usage in tests. Make typehinting possible which reveals coding errors.\r\n 3.next view builder >>> 1
100,Follow up on https://github.com/cakephp/cakephp/pull/9792 and the end() issue. Stricter treatment of end() returning false. >>> 1
101,"See https://github.com/cakephp/cakephp/issues/9798\r\n\r\nThe `render()` method should return `null` (or a blank string) if empty (now it returns the string `<ul></ul>`), in other words if you have not added any crumb. BreadcrumbsHelper::render() returns null with no crumbs >>> 1"
102,The default should be moved inside the class to allow simpler Application setup:\r\n```php\r\n\t\t$middleware\r\n\t\t\t->add(MaintenanceMiddleware::class)\r\n\t\t\t->add(ErrorHandlerMiddleware::class)\r\n\t\t\t->add(AssetMiddleware::class)\r\n\t\t\t->add(RoutingMiddleware::class);\r\n```\r\netc\r\n\r\nNo need to force instantiation here. Move default into class so simpler ::class syntax can be used. >>> 1
103,"Implements https://github.com/cakephp/cakephp/issues/9150\r\n\r\n      bin/cake plugin load Foo --cli\r\n\r\nNote that the regexp in unload task has to be fixed because / as delimiter won't work with plugin load calls like\r\n    \r\n     Plugin::load('ADmad/HybridAuth', ['routes' => true]);\r\n\r\nI found that out when testing it. Allow loading and unloading plugins for bootstrap_cli >>> 1"
104,"It's a bad idea to write tests that delete files in the TMP:\r\n\r\n```\r\n$ phpunit tests/TestCase/Shell/CacheShellTest.php\r\nPHPUnit 5.5.4 by Sebastian Bergmann and contributors.\r\n\r\n..E.E                                                               5 / 5 (100%)\r\n\r\nTime: 33 ms, Memory: 6.00MB\r\n\r\nThere were 2 errors:\r\n\r\n1) Cake\Test\TestCase\Shell\CacheShellTest::testClearValidPrefix\r\ndir(/tmp/pulse-PKdhtXMmr18n/): failed to open dir: Permission denied\r\n\r\n/home/mirko/Libs/cakephp/src/Cache/Engine/FileEngine.php:309\r\n/home/mirko/Libs/cakephp/src/Cache/Engine/FileEngine.php:286\r\n/home/mirko/Libs/cakephp/src/Cache/Cache.php:458\r\n/home/mirko/Libs/cakephp/src/Shell/CacheShell.php:77\r\n/home/mirko/Libs/cakephp/tests/TestCase/Shell/CacheShellTest.php:82\r\n/usr/share/php/PHPUnit/TextUI/Command.php:162\r\n/usr/share/php/PHPUnit/TextUI/Command.php:113\r\n\r\n2) Cake\Test\TestCase\Shell\CacheShellTest::testClearAll\r\ndir(/tmp/pulse-PKdhtXMmr18n/): failed to open dir: Permission denied\r\n\r\n/home/mirko/Libs/cakephp/src/Cache/Engine/FileEngine.php:309\r\n/home/mirko/Libs/cakephp/src/Cache/Engine/FileEngine.php:286\r\n/home/mirko/Libs/cakephp/src/Cache/Cache.php:458\r\n/home/mirko/Libs/cakephp/src/Shell/CacheShell.php:77\r\n/home/mirko/Libs/cakephp/src/Shell/CacheShell.php:101\r\n/home/mirko/Libs/cakephp/tests/TestCase/Shell/CacheShellTest.php:107\r\n/usr/share/php/PHPUnit/TextUI/Command.php:162\r\n/usr/share/php/PHPUnit/TextUI/Command.php:113\r\n\r\nERRORS!\r\nTests: 5, Assertions: 3, Errors: 2.\r\n\r\n``` CacheShellTest uses a TMP subdirectory >>> 1"
105,Add more response header helpers\r\n\r\n* withEtag\r\n* withNotModified\r\n* withSharable\r\n* withMaxAge\r\n\r\nRefs #9636 3.next - Add more Response helpers >>> 1
106,Fixes #9819 Fix redirectUrl issue when loginRedirect is empty >>> 1
107,Implement the remaining caching helpers. Only a few file/cookie methods are remaining.\r\n\r\nRefs #9636 3.next - More Response helper methods >>> 1
108,Now that Network\Response is PSR7 compliant we don't need to transform it into a Diactoros\Response. This should also fix the IntegrationTestCase errors people have when testing error responses.  3.next - Error middleware cleanup >>> 1
109,"@markstory as discussed in https://github.com/cakephp/cakephp/pull/9836#discussion_r90849991\r\n\r\nI went with a protected method as I don't see why it needs to be public. Can be changed later, if actually needed. Add IntegrationTestCase::_getBodyAsString() >>> 1"
110,Get started on PHP7.1 support for 2.x Add PHP7.1 to test matrix. >>> 1
111,"Hi, \r\nI didn't make an issue for this as it seemed too simple. \r\n\r\nStacking of Flash messages is a really nice feature, but when doing the following:\r\nclient_posts_data -> appcontroller_sets_flash -> a_controller_receives_post_and_redirects -> \r\nappcontroller_sets_flash =\r\nview rendered with duplicate flash messages. \r\n\r\nI added a configuration key ""duplicate"", when set to false, skips adding messages that are already in the stack. \r\n\r\n Flash messages stack and in cases can result in duplicate messages >>> 1"
112,"Came across a situation where testing the response body for a regular expression would be very useful, and it seemed like trivially easy functions to add. Add functions to test response body against regular expressions >>> 1"
113,Fixes #9848 #9848 PaginatorComponent.php ignores maxLimit >>> 1
114,"As far as I can tell, this commit `aa60b8791a213acc1a3ffbc5c6d41e5c12e74727` removed at least some support of the required attribute on multiple=checkbox. I am trying to use input type=""select"" multiple=""checkbox"" required=""required""\r\n\r\nMy change places the required attribute in place only for that scenario, because for instance I know doing it for all cases in _selectOptions caused tests to fail on, for example:\r\n\r\n```\r\n<select>\r\n<option required=""required""></option>\r\n<option required=""required""></option>\r\n</select>\r\n```\r\n\r\nPlease let me know if I'm off base here, or why support for that attribute should not be added. Also let me know if I need to change my base, I wasn't sure which was the best place to base 2.x changes from. Add support for select multiple=checkbox required=required >>> 0"
115,"Implement the last set of PSR-7-like methods. This is the last batch of this kind of work 🎉 \r\n\r\nRefs #9636 3.next - Response methods (file, cookie, download) >>> 1"
116,"Reverts #9854 as it copy & pasted the built-in message only.\r\n\r\nMakes use of:\r\nhttps://github.com/cakephp/cakephp/blob/3.3.10/src/View/Exception/MissingElementException.php#L28\r\n\r\nImproved tests to prevent that, too.\r\n Make use of built-in message template of MissingElementException >>> 1"
117,Fix missing HTML encoding when error messages contain HTML. This can happen when user data is used as an offset in an array in an unchecked way.\r\n\r\nThanks to Teppei Fukuda for reporting this issue via the responsible security disclosure process. 2.x - Debugger encoding >>> 1
118,"Just generally improving the consistency of docblocks in these files: method lines, capitalization, punctuation, added a few obvious comments. Improve consistency of docblocks >>> 1"
119,Just to follow RFC.\r\n\r\nhttp://www.ex-parrot.com/~pdw/Mail-RFC822-Address.html\r\nhttp://stackoverflow.com/questions/20771794/mailrfc822address-regex RFC-822 of mail address validation >>> 0
120,Closes #9857 \r\n\r\nMakes the Table::findOrCreate method null-safe and enables checking for array values in $search. Changed Table::findOrCreate to deal with null and array values in search >>> 0
121,Fixed few errors reported when trying out [phpstan/phpstan](https://github.com/phpstan/phpstan). Static analysis fixes >>> 1
122,"A while back I fixed the error code, but I forgot to also include the success code.\r\nThis rectifies it for clarity.\r\n\r\nOne should never use the magic numbers here directly as from looking at it they are exactly the opposite of what you would intuitively expect (and their boolean representation).\r\n\r\nResolves also some of the initial confusion that lead to\r\nhttps://github.com/cakephp/docs/pull/4532 Dont use magic numbers for success code either. >>> 1"
123,Fixes #9817 New Validation::(min|max)ByteLength() addition >>> 1
124,"Also some small IDE autocomplete helpers to get the code less ""yellow warning"" and clickable (= less bad magic). Fixing a few code smells detected by phpstan. >>> 1"
125,Resolves https://github.com/cakephp/cakephp/issues/9851 Implement necessary ExceptionRendererInterface for clean code. >>> 1
126,"See issue #7848 for discussion. This PR supersedes PR #8558, which got messed up due to incorrect git management, and was never checked with v3.3.\r\n\r\nI haven't yet made any changes in the documentation, pending acceptance of general implementation of this PR. If these changes look good but you want the docs updated before merging, let me know and I can take care of that. Allow SecurityComponent to check fields that are optional, but must have a specific value if included >>> 0"
127,Move the Response class into the Http namespace and required class names elsewhere in tests etc.\r\n\r\nRefs #9636 Move Response into Http namespace. >>> 1
128,"Implements #9890  Subtree split of Form namespace, solves #9890 >>> 1"
129,Small doc block fixes. Use FQCN for property annotations. >>> 1
130,"This finishes the PR #8452, as the initial creator hasn't updated his since March. New function to update config files at run time >>> 0"
131,This is related to this assignment.  #2058\r\n\r\nIt is possible to prepend a clamp to the addCrumb method.\r\nI think this will solve the view inheritance problem. Allow the prepend the addCrumb method >>> 1
132,I used `vendor/bin/phpstan analyse src --level 4`\r\n\r\nBefore: 672 errors\r\nAfter: 569 errors\r\n\r\nPlease review with IDE if possible. Fix some more doc blocks. >>> 1
133,"This is a unit test's bug.\r\n\r\n* CakePHP Version: 3.3.10\r\n* Platform and Target: Travis CI (Environment I found)\r\n\r\nI added a PR #9899. After checking the build result of Travis-CI, [`MemcachedEngineTest::testConfigurationConflict()`](https://github.com/cakephp/cakephp/blob/master/tests/TestCase/Cache/Engine/MemcachedEngineTest.php#L770) failed with only one of all the jobs. (It seems that it was manually rebuilt immediately.)\r\n\r\nProbably the same case: [testConfigurationConflict site:travis-ci.org - Google](https://www.google.co.jp/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=testConfigurationConflict+site%3Atravis-ci.org)\r\n\r\nIt seems that the test fails because the cache can not be read for a time shorter than the specified expiration date (maximum 1 second).\r\n\r\nFor example, in the following tests, It's only have 200 milliseconds waiting for a cache write, but sometimes the test fails. (Cache can't be read.)\r\n\r\n```php\r\n<?php\r\nnamespace App\Test\TestCase;\r\n\r\nuse Cake\Cache\Cache;\r\nuse Cake\TestSuite\TestCase;\r\n\r\nclass ErraticMemcachedEngineTest extends TestCase\r\n{\r\n    public function testSomeTimeFail()\r\n    {\r\n        usleep(rand(1, 1000000)); // start timing randomize\r\n        Cache::config('short_memcached', [\r\n            'engine' => 'Memcached',\r\n            'duration' => '+1 seconds',\r\n            'servers' => ['127.0.0.1:11211'],\r\n        ]);\r\n\r\n        $this->assertTrue(Cache::write('short_duration_test', 'boo', 'short_memcached'));\r\n        usleep(200000);\r\n        $this->assertEquals('boo', Cache::read('short_duration_test', 'short_memcached'), 'Value was not read %s');\r\n        Cache::drop('short_memcached');\r\n    }\r\n}\r\n```\r\n\r\nIn this PR, I adjusted the time according to the behavior of Memcached.\r\n\r\nBy the way, I checked [the commit with the test method added](https://github.com/cakephp/cakephp/commit/4425743bcd2c83431c8a500505e63cfcf4fb34ff#diff-f7b6371d1efb5cf5cd03dd8e1f3ff305) and [the commit on migration to Cake 3](https://github.com/cakephp/cakephp/pull/1606/commits/c2171e80e8aee91c1478cde21cde6ef53a67a635#diff-deb56b8077f4d5332e67d075877b3efdL444).\r\n\r\nAt the time of migration of Cake 3, the following code which seemed necessary was deleted, is not it a problem?\r\n\r\n`testConfigurationConflict()` DocComment is <q>test that configurations don't conflict, when a file engine is declared after a memcached one.</q>\r\n\r\n```php\r\nCache::config('some_file', array('engine' => 'File'));\r\n``` Fix MemcachedEngineTest::testConfigurationConflict() rarely fails >>> 1"
134,"The Email class was a huge mess so far.\r\n\r\nThe following methods did not return $this for reading:\r\n* charset()\r\n* headerCharset()\r\n\r\nThat can be dangerous when using in chaining, as all other methods do chain as expected, and those would suddenly break it.\r\n\r\nAlso\r\n* emailPattern() had to set false to read, null would reset.\r\n* readReceipt() sounds like it reads, but in fact is a setter in not null argument case...\r\n* template($template = false, $layout = false) has a weird double set/get of mixed template and layout, separation needed\r\n* viewRender() should be viewRenderer() or viewBuilder()\r\n\r\nSo this all cleans this up, makes it future proof, and that while retaining BC.\r\n\r\nAdditionally (in a separate commit after all tests verified BC):\r\n* Deprecated viewBuilder methods have also been replaced with the correct ones.\r\n* Deprecated Mailer and Email class method usage has been replace with the correct ones.\r\n\r\nPS: We can apply the self to $this doc block changes after this has been merged to avoid conflicts. Clean up Email API and combined setter getter methods. >>> 1"
135,"will solve issue https://github.com/cakephp/cakephp/issues/9915 \r\npaginate or find using virtual fields and param fields, return wrong results on virtual fields\r\n\r\ndocumentation should be updated:\r\nhttp://book.cakephp.org/2.0/en/models/virtual-fields.html#limitations-of-virtualfields\r\n\r\n Containing a virtualField - fixes #9915 >>> 0"
136,"Continuing the work from #9898. This is PR to integrate [PHPStan](https://github.com/phpstan/phpstan) into the CI build of CakePHP.\r\n\r\nIt's a work in progress since there are quite a lot of errors in the build log. `@property` annotations are already supported by PHPStan, but it still found some undefined properties.\r\n\r\nEven after merging this PR, I expect to continue to work with you on increasing the level of strictness of the analysis and also on updating to newer versions of PHPStan :)\r\n\r\nI'd like one of the maintainers to go through the error log and fix obvious errors in the code. We can then work together on the other, more questionable, errors. [WIP] Static analysis with PHPStan >>> 0"
137,"Refs https://github.com/auraphp/Aura.Intl/releases/tag/3.0.0-beta1\r\n\r\nBefore we decide to go ahead with the newer version it would be better to know what breaking API changes, if any, they plan for Aura.Intl 3.0. Bump Aura.Intl version to 3.0. >>> 1"
138,input() => control()\r\ninputs() => controls()\r\nallInputs() => allControls() Rename FormHelper methods to be less confusing. >>> 1
139,nan Improving PHPStan configuration >>> 1
140,Before this fix the cookies info in `Response::$_cookies` was never used to actually set cookies.\r\n\r\nI haven't added/updated tests as I am not 100% sure this is the correct way to fix this problem but testing in an actual app it does seem to fix the problem. Fix cookies sending by ResponseEmitter. >>> 1
141,"This PR improves the shell parser by adding the ability to have options that accepts multiple values, with a new `multiple` (a boolean) parameter e.g. :\r\n\r\n```php\r\n$parser->addOption('source', ['short' => 's', 'multiple' => true]);\r\n```\r\n\r\nWhen calling a shell :\r\n\r\n```\r\nbin/cake some_shell -n report -s mysql -s postgres\r\n```\r\n\r\nWhen you call that, the option will be rendered as an array, even if you only have one element.\r\n\r\nI went with `multiple` but I am opened to suggestions on the name of the option. I considered `array` at first but it was not clear enough in my opinion. Add support for multiple values option in the Shell system >>> 1"
142,This implements #9380  CounterCache should ignore dirty fields >>> 1
143,"This PR fixes an issue when we want to test a cake application with middlewares by acceptance tests.\r\n\r\nIf we create a middleware, that adds some extra headers to the request, then without this patch cake will lookup an old value when we call `$request->header('X-Api-Token')` for example. But with this patch, the following code will work correct:\r\n\r\n```php\r\n    public function __invoke(ServerRequestInterface $request, ResponseInterface $response, callable $next)\r\n    {\r\n        $request = $request->withHeader('X-Api-Token', 'FooBar');\r\n \r\n        return $next($request, $response);\r\n    }\r\n\r\n```\r\n Headers should win if they are provided when converting Psr7 request to Cake request >>> 1"
144,Worked on my fork so hopefully will work here also\r\n Run code sniffer against future v3 >>> 1
145,Partially resolves the left over tasks of https://github.com/cakephp/cakephp/issues/9978 Clean up ORM Query API. >>> 1
146,"By passing the request we can make it possible to add route conditions for more than just the URL. I've had to add new methods onto Router and RouteCollection to make this possible, but I feel that passing the object around is better long term anyways.\r\n\r\nThis requires deprecating `Router::parse()` which makes me a bit sad, but I feel this makes the code simpler/better long term.\r\n\r\nThis is currently an RFC as I want to see if people think the addition of the `_host` option is worth the churn in code. Also if there are any other similar types of conditions/matchers we should add.\r\n\r\nRefs #9917 RFC - Allow routes to restrict the hosts they should match on >>> 1"
147,"Widths for columns that contain an empty string are not added, and\r\nproduces a bug where there are fewer headings then columns per row. fixed empty string bug >>> 1"
148,"`Hash::maxDimensions` returns wrong value when the first element is false.\r\n```php\r\n$data = [\r\n   '1' => false,\r\n   '2' => ['2.1' => '2.1.1']\r\n];\r\n$result = Hash::maxDimensions($data); // $result is 0.\r\n```\r\nBecause `reset` returns false when empty array or the first element is false. Fix wrong dimensions when the first element is false. >>> 1"
149,"Resolves https://github.com/cakephp/cakephp/issues/9938\r\n\r\nI checked, laravel etc use also dd() function, so coming from them it will also feel a bit easier to work the way one might have used to work :) Add dd() as quick alternative for debug(); die();  >>> 1"
150,Refs #9989 Make location displayed by dd() identical to that by debug(). >>> 1
151,"The message of the `MissingRouteException` thrown when a route cannot be found now includes the method used to (attempt to) access the route, as well as the path of the route. This addresses a problem where the user (developer) could be presented with a message such as ""A route matching 'x' could not be found. List of available routes: 'x'"". It is now made explicit that e.g. ""A 'POST' route matching 'x' could not be found."" Update thrown message to indicate missing method/route combination, addressing #9994. >>> 1"
152,fixed #9992 Fixed Many regex in the Validation method miss a trailing newline >>> 0
153,Phpdoc fixes according to [#9987](https://github.com/cakephp/cakephp/issues/9987)\r\n Wrong return type hint in Controller::redirect #9987 >>> 1
154,"`EagerLoader::contain()` does return an array of Containments.\r\n\r\nthe old code of matching is\r\n```php\r\npublic function matching($assoc = null, callable $builder = null, $options = [])\r\n    {\r\n        if ($this->_matching === null) {\r\n            $this->_matching = new self();\r\n        }\r\n\r\n        if ($assoc === null) {\r\n            return $this->_matching->contain();\r\n        }\r\n        if (!isset($options['joinType'])) {\r\n            $options['joinType'] = 'INNER';\r\n        }\r\n\r\n        $assocs = explode('.', $assoc);\r\n        $last = array_pop($assocs);\r\n        $containments = [];\r\n        $pointer =& $containments;\r\n        $opts = ['matching' => true] + $options;\r\n        unset($opts['negateMatch']);\r\n\r\n        foreach ($assocs as $name) {\r\n            $pointer[$name] = $opts;\r\n            $pointer =& $pointer[$name];\r\n        }\r\n\r\n        $pointer[$last] = ['queryBuilder' => $builder, 'matching' => true] + $options;\r\n\r\n        return $this->_matching->contain($containments);\r\n    }\r\n```\r\n\r\nso it can't be replace by a return $this in a setter if I'm not mistaken.\r\nDoes `setMatching` is an appropriate name here ? 3.4 Fix setMatching return type >>> 1"
155,See #9879\r\n\r\nAlias on non-translation fields is enforced to keep alias consistency (contrary to the example shown in #9879). New translationField method. >>> 1
156,"Because we were using the stock translator from Aura, we had painful issues when:\r\n\r\n* A plural message was used with `__()` an array would be returned which\r\n  is never right.\r\n* When an unknown context is used the `__x()` method fails.\r\n\r\nI don't think the change in the translator type matters. If it bothers someone, I can move this to 3.4.0\r\n\r\nRefs #8833\r\nRefs #9985 Fix context/plural related issues in i18n >>> 1"
157,"### Before\r\n```php\r\n        $parser->addSubcommand('infos', [\r\n            'help' => 'Get some GitHub infos',\r\n            'parser' => [\r\n                //'description' => 'Get some GitHub infos' // Without declaring this again\r\n            ],\r\n        ]);\r\n```\r\n\r\nresults in no --help output (displaying main page again) and\r\n```\r\nUsage:\r\ncake release [subcommand] [-d] [-h] [-q] [-v]\r\n\r\nSubcommands:\r\n\r\ninfos  Get some GitHub infos\r\npr     Release PR\r\n\r\nTo see help on a subcommand use `cake release [subcommand] --help`\r\n```\r\n\r\nIt only works if you manually copy and paste the help text into the parser description (unDRY).\r\n\r\n### After\r\nThe above results in expected\r\n\r\n```\r\nGet some GitHub infos\r\n\r\nUsage:\r\ncake release infos [-h] [-q] [-v]\r\n\r\nOptions:\r\n\r\n--help, -h     Display this help.\r\n--quiet, -q    Enable quiet output.\r\n--verbose, -v  Enable verbose output.\r\n```\r\n\r\n Merge in defaults from subcommand for the parser description. >>> 1"
158,Return value description fix Fix docblock in CollectionInterface >>> 1
159,I don't think that I got all of them as there were really a lot of deprecated functions call/properties access and some of them are still necessary.\r\n\r\nI've only updated few tests where the Mock definition update was needed. 3.4 Remove more deprecated function calls >>> 1
160,An empty list should pass all conditions as it is the vacuous truth. We incorrectly made Collection::every() return false in #8670. Returning true makes Collection::every() match implementations in most other languages/libraries.\r\n\r\nAdd additional tests to other filter/member checks to verify empty set behavior there as well.\r\n\r\nRefs #10002 Make Collection::every() return true for empty sets. >>> 1
161,cc @dereuromark\r\n Update return type from `self` to `$this` >>> 0
162,"Small follow up on https://github.com/cakephp/cakephp/pull/10004\r\n\r\nOne question about `Translator::_fallbackLoader()`:\r\nIs the return type here really `\Aura\Intl\Translator`? It looks to me it is a ChainMessagesLoader, even after the callable.\r\n```php\r\n    /**\r\n     * Returns a new translator instance for the given name and locale\r\n     * based of conventions.\r\n     *\r\n     * @param string $name The translation package name.\r\n     * @param string $locale The locale to create the translator for.\r\n     * @return \Aura\Intl\Translator\r\n     */\r\n    protected function _fallbackLoader($name, $locale)\r\n    {\r\n        $chain = new ChainMessagesLoader([\r\n            new MessagesFileLoader($name, $locale, 'mo'),\r\n            new MessagesFileLoader($name, $locale, 'po')\r\n        ]);\r\n\r\n        // \Aura\Intl\Package by default uses formatter configured with key ""basic"".\r\n        // and we want to make sure the cake domain always uses the default formatter\r\n        $formatter = $name === 'cake' ? 'default' : $this->_defaultFormatter;\r\n        $chain = function () use ($formatter, $chain) {\r\n            $package = $chain();\r\n            $package->setFormatter($formatter);\r\n\r\n            return $package;\r\n        };\r\n\r\n        return $chain;\r\n    }\r\n``` Doc block types. >>> 1"
163,Fixes #9817 New Validation::(min|max)ByteLength() addition >>> 0
164,I found those mainly related to events and adjusted some docblocks 3.4 - Remove more deprecated function's call >>> 1
165,Implements https://github.com/cakephp/cakephp/pull/10018 respecting the static factories which actually have been correctly made non-$this returning.\r\n@antograssiot @thinkingmedia  Please double check. Fix up doc block return types for chaining. >>> 1
166,"I added contants representing the debug mode of CakePHP.\r\n\r\n1. This way it will be more easily to find with your IDE where you are doing something like this\r\n\r\n```\r\nif (Configure::read('debug') > CAKE_DEEP_DEBUG_MODE) {\r\n\r\n}\r\n```\r\n\r\n2. Makes it more verbose than having to know what 0,1,2 does all the time. Added debug mode constants >>> 0"
167,In reference of #9844 Router custom prefix path >>> 1
168,I removed PHPStan from `allow_failures` because I think it should pass now. Let's see if the build passes :) Updated PHPStan & related fixes >>> 1
169,See issue reported in #10040.\r\n\r\nNote: I have not run tests to verify these changes yet. Change Default Behavior in Postgres and SQLServer for DATETIME >>> 1
170,The old approach created a parser for every command that didn't have an explict option parser. This broke automatic option inheritance for subcommands. While there weren't any tests for this behavior many console tools rely on this behavior. eg. `orm_cache`\r\n\r\ncc @dereuromark  Shift subcommand propagation to be at help generation. >>> 1
171,"Using ab absolute path helps with tools that run in environments\r\nwhere the current working directory does not point to the root of the\r\nproject.\r\n\r\nFor example in my IDE I can do ""**Right click > Run test**"" in a test method body, on the test class, or even on a folder, in order to run single test methods, single test classes, or specific folders only. However, the CWD will point to the selected folder, respectively the folder that contains the test class file, so including the autoloader will fail. Use absolute autoloader path for better test tools integration. >>> 1"
172,nan minor fixes to unit tests >>> 1
173,"Add type checking to update method. This prevents setting `$this->_parts['update'][0]` with a value which can't be processed by _buildUpdatePart and causes an ""Array to string conversion"" error on [this line](https://github.com/cakephp/cakephp/blob/master/src/Database/QueryCompiler.php#L343)\r\n\r\nThis is possibly related to #9735 \r\n\r\nThanks Query update type check >>> 1"
174,"When running migrations, or running tests on bake plugin right now:\r\n```\r\n1) Bake\Test\TestCase\Shell\Task\FixtureTaskTest::testImportOptionsAlternateConnection\r\nTypeError: Argument 1 passed to Bake\Shell\Task\FixtureTask::_generateRecords() must be an instance of\r\nCake\Database\Schema\Table, instance of Cake\Database\Schema\TableSchema given, called in\r\nD:\...\Apps\sandbox.local\vendor\cakephp\bake\src\Shell\Task\FixtureTask.php on line 194\r\n\r\nD:\...\Apps\sandbox.local\vendor\cakephp\bake\src\Shell\Task\FixtureTask.php:322\r\nD:\...\Apps\sandbox.local\vendor\cakephp\bake\src\Shell\Task\FixtureTask.php:194\r\nD:\...\Apps\sandbox.local\vendor\cakephp\bake\tests\TestCase\Shell\Task\FixtureTaskTest.php:132\r\n```\r\n\r\nThis should resolve it. //cc @markstory  Provide BC typehinting for TableSchema. >>> 1"
175,I've moved the code that creates a new ```Mailer``` instance in the ```MailerAwareTrait``` to a static method. The trait keeps working the same.\r\n\r\nI think it's painful having to import the trait every time I need a ```Mailer``` instance in a different class.\r\n\r\nFixes #10071. Static method to create Mailer instances >>> 0
176,Adds the ability to manually setCrumbs. Examples of why this is useful was discussed in #9829.  Add setCrumbs to BreadcrumbHelper >>> 0
177,While doing some development work I noticed a few spelling issues and decided to try and fix as many as possible. \r\n\r\nI've tried to stay away from anything that affects functionality.  Fix numerous typos >>> 1
178,\r\nSee \t#9978 #9978 Entity getters and setters >>> 1
179,Fixes an issue where the name for connections was no longer set after the config getter/setting split.\r\n\r\n`$connection->configName()` would return an empty string. Also added a test case for this to ensure that it doesn't happen again. Connection manager config >>> 1
180,use default options within __invoke()\r\nRefs: #10094  fix 10094 >>> 0
181,HasMany->link is now using a single database transaction instead of multiple transactions.\r\nBelongsToMany->link already was using a single transaction.\r\n\r\nHasMany->unlink uses the updateAll function therefore I do not believe any changes are required.\r\n\r\nRef: #8196 Link HasMany associations using a single transaction >>> 1
182,Subject says it all nestedInput needed unsetting so it doesn't appear as an attribute >>> 1
183,https://api.cakephp.org/3.3/class-Cake.View.Helper.HtmlHelper.html#_meta\r\nhttps://api.cakephp.org/3.3/class-Cake.View.Helper.HtmlHelper.html#_media Improve HtmlHelper DocBlock formatting >>> 1
184,"During our PHP 7 upgrade I encountered this incorrect `@return` comment.\r\n\r\nFound using [etsy/phan](https://github.com/etsy/phan) ([see](https://github.com/cloudflare/docker-phan#getting-docker-phan)). There are probably more in `lib`, but I only scanned `app` (for now). Fix for incorrect @return phpdoccomment >>> 1"
185,"I was learning how the ORM in CakePHP works and found the ValueBinder class that had some behaviour I believe to be incorrect (please correct me if i'm wrong).\r\n\r\n$valueBinder->placeholder describes itself as taking a token and returning a SQL param based based on that token. The PHPDOC block also mentions that '?' should return '?' and ':value' should return ':value'.\r\n\r\nWhat happened \r\n-----------------\r\n$valueBinder->placeholder('param');\r\n> ':c0'\r\n\r\n$valueBinder->placeholder('?')\r\n> ':c1'\r\n\r\n$valueBinder->placeholder(':param')\r\n> ':c2'\r\n\r\nWhat I expected to happen:\r\n----------------------------\r\n$valueBinder->placeholder('param')\r\n> ':param0'\r\n\r\n$valueBinder->placeholder('?')\r\n> '?'\r\n\r\n$valueBinder->placeholder(':param')\r\n> ':param'\r\n\r\nAfter further investigation I found that around ~33 tests were written assuming this bug so I updated those tests.\r\n\r\nI found some small code improvements (removing unused variables, reduce logic duplication, etc) \r\n\r\nI also created unit tests for the class in question.\r\n\r\nI will comment the main change to logic using github code review tools.  ValueBinder->placeholder method fixes >>> 1"
186,"Adds a test for verifying that ResultSet objects can be cached with the file engine, and deserialized again with the same data.\r\n\r\nNew unit test is failing for me on PHP 7.1.1\r\n\r\nRelates to issue #10111  Adds test for caching result set with file engine >>> 1"
187,Update internal uses of deprecated methods with new methods. I've not updated `config()` on the `TableLocator` as `getConfig`/`setConfig()` are not part of that interface.\r\n\r\nRefs #10128\r\n 3.next Update deprecated config() calls. >>> 1
188,Allow FormHelper::input to pass options to ControlLabels (labels around Radio and MultiCheckbox)\r\n    Added `optionsLabel`\r\nAdded tests\r\nFixes any tests broken by this\r\n\r\nOnly slight functionality change is if class was set via the direct methods (FormHelper::multiCheckbox or FormHelper::radio) and the item is checked then 'selected' gets added. Previously it was skipped\r\n Fixes/options label class and attributes >>> 0
189,"Showing a logo on the top of a readme gives the else boring looking page a little coloured touch.\r\nAs we don't have to hide our awesome logo, I added it to the README :)\r\n\r\nBefore:\r\n\r\n![screen shot 2017-01-31 at 16 01 57](https://cloud.githubusercontent.com/assets/6617432/22470442/a0408ffe-e7cf-11e6-9f76-abb2cb101654.png)\r\n\r\nAfter:\r\n\r\n![screen shot 2017-01-31 at 16 02 11](https://cloud.githubusercontent.com/assets/6617432/22470443/a058c7cc-e7cf-11e6-961d-6ab784efa08b.png)\r\n\r\n(Github responded with some `504 (Gateway Timeout)`on the badges, so they are not displayed in the screenshots) Show the CakePHP logo on the top of the README >>> 0"
190,Fix #10115 - Implement the exception proposed in this issue. Throw exception indicating translator cache must be cleared >>> 1
191,See #10140 [RFC] Support for registryAlias in Association >>> 0
192,Refs #10144  Make it possible to define a custom routeClass for builder redirects >>> 0
193,I found a few more quickly grepping through the code. Fix a few deprecated method call sites. >>> 1
194,See a [comment](https://github.com/cakephp/cakephp/pull/10142#issuecomment-276918905) Throw an exception if association's target table has an incorrect class name. >>> 1
195,Completes addons mentioned in https://github.com/cakephp/cakephp/issues/9661 Add priority to Email. >>> 1
196,"This is WIP for implementing the requested `failedSaved` and `failedDelete` events in #9653\r\nIt turned out that this isn't as trivial as I thought, so I wanted to collect some feedback before I go on.\r\nI started with `failedSave`.\r\n\r\nOf course I'll add way more tests when I proceed. WIP: Implement failedSave Event >>> 0"
197,As discussed in #10158 and #9653. Add new saveOrFail and deleteOrFail method >>> 1
198,"With the release of phpunit version 6.0 it is not possible for us to test\r\nthe framework with versions older than PHP 7.\r\n\r\nThere are clever ways of solving this problem, but I think there are no interesting\r\nfeatures in verion 6 that we require for the time being. Pin phpunit to version 5.7 >>> 1"
199,"Following [comments](https://github.com/cakephp/cakephp/pull/10142).\r\n\r\nAssociation names are mutable, but that change isn't reflected in a collection nor the target table. Changing association name without manually reattaching it in a collection makes it unusable. `EagerLoader` [checks if these names match](https://github.com/cakephp/cakephp/blob/3.next/src/ORM/EagerLoader.php#L496) and throws an exception. Is this exception enough or should we disallow changing association name as it would probably break it?\r\n\r\nIn other words `AssociationCollection` key, `Association` name and target `Table` alias should be identical. Changing one of them would probably break things.\r\n\r\nMy way to improve this is to check if a new association name matches the target table alias **after** it has been resolved.\r\n\r\nThere's also an issue with `AssociationCollection` not reflecting association name change, but I'm not sure we can do anything about it without changing the way associations are collected. [RFC] Association name setter >>> 1"
200,"Fixes #9812 \r\n\r\nI didn't add support for `LOCK IN SHARE MODE`/`FOR SHARE`/`WITH (SERIALIZABLE)` and dynamic finder, to make things simple. After this PR has been merged, I'll try to update related 2.next documents. [2.next] Add support for having/lock options >>> 1"
201,"`HasMany::sort()` has been split, but `BelongsToMany::sort()` has not been split yet. People would  be confused.\r\n\r\nAdditionally, I have removed overridden combined setter/getter methods. Previously, `foreignKey()` returns `$this` incorrectly in some of them.\r\n\r\nAlso, I fixed `BelongsToMany::targetForeignKey()` returns `$this`.\r\n\r\nRelated to https://github.com/cakephp/cakephp/pull/9676 [3.5] Split BelongsToMany::sort() into getSort()/setSort() >>> 0"
202,nan 3.4 - Remove more deprecated methods usage >>> 1
203,"When using DebugKit SqlLog, I noticed that when saving hashed user password, log contained\r\n`UPDATE users SET password = 'y$uEFBAE...' WHERE id = 123` which is not in valid Bcrypt format.\r\n\r\nBut after checking database, password was saved correctly as `'$2a$10$uEFBAE...'`.\r\n\r\nI enabled logging to text file - still same result, so not fault of DebugKit.\r\n\r\nAfter inspecting code, problem is that QueryLogger interpolates bind parameters using `preg_replace`: but `preg_replace` replacement string uses `$1` or `\1` for backreferences.\r\n\r\n`$2` and `$10` in hash refers to invalid backrefences, so replaced with empty strings.\r\n\r\nAnother possible common string could be ""$0.12"" which would replace to `"":p1.12""` because `$0` refers to bind parameter matched by the whole pattern.\r\n\r\n---\r\n\r\nMy commit escapes `$` and `\` to `\$` and `\` to ignore them by preg_replace.\r\n\r\nAlso I replace `'` to double single-quote `''` and `\` to `\`  to be even more valid SQL string. This is why there is `\` to `\\\\` replacement (preg_replace and SQL escape).\r\n QueryLogger strips some characters from queries >>> 1"
204,nan Fix invalid return value hint >>> 1
205,"    This currently already works with CheckboxWidget\r\n\r\nCheckboxWidget already allows for passing attributes, this pull request brings RadioWidget and MultiCheckboxWidget in line. I also added a test method which checks the current attribute support of CheckboxWidget Allow attributes to be passed to RadioWidget & MultiCheckboxWidget >>> 1"
206,Optional is_uploaded_file() validation. Made the is_uploaded_file() check configurable >>> 0
207,"I have the following log config on app.php:\r\n```php\r\n\t'Log' => [\r\n\t\t'debug' => [\r\n\t\t\t'scopes' => false,\r\n\t\t\t'className' => 'DatabaseLog.Database'\r\n\t\t],\r\n\t\t'error' => [\r\n\t\t\t'scopes' => false,\r\n\t\t\t'className' => 'DatabaseLog.Database'\r\n\t\t],\r\n\t\t'404' => [\r\n\t\t\t'className' => 'DatabaseLog.Database',\r\n\t\t\t'type' => '404',\r\n\t\t\t'levels' => ['error'],\r\n\t\t\t'scopes' => ['404'],\r\n\t\t]\r\n```\r\nThose are all strings.\r\nSomewhere inside the core, though, some type joggling makes this a integer 404 then as key, breaking the config() nonsplit method.\r\nUsing this fix it works again as expected (as it checks for the actual array vs non array), and this is the same check used inside setConfig(), so it is then consistent, as well.\r\n\r\nNote: it used to work in 3.3 and before like this, that's why this is a regression bugfix. Fix up config() to accept any scalar value for getting as per setConfig() >>> 1"
208,"## New prototype implementation for cookies\r\n\r\nIt needs more tests and I'm not yet happy with the handling of the cookie data and encryption. But I would already like to get feedback on this to continue to develop it into a direction we all agree with.\r\n\r\nThe code is partially based on https://github.com/hansott/psr7-cookies/ but was changed to fit into our current API and the frameworks current php version. I've had a discussion with Mark Story if we'll directly use it or not. The final decision for the named reasons was to come up with our own implementation. I've also changed the implementation in some aspects, it's not a copy and paste job. I need some advice if we need to give credits and copyright and if yes, how to do it the right way.\r\n\r\nCookies can now be used from wherever the request object and response objects are available.\r\n\r\n## Overview of the new implementation\r\n\r\n* Uses the request and response objects\r\n* Uses a cookie object to built and modify a cookie\r\n* Generates cookie objects from the request object\r\n* Writes cookies to the response object\r\n* Uses getters and setters\r\n* No longer a component bound controller layer only solution (This change is required to get the cookie authenticator in the new authentication implementation working)\r\n* **TBD**: If possible a BC compatible cookie component that uses the new implementation\r\n\r\n## Related issues\r\n\r\n* https://github.com/cakephp/authentication/issues/26 New cookie implementation for the HTTP stack >>> 0"
209,Follows #10195\r\n\r\nI'm wondering what to do with deprecated `attach()` and `detach()`? Make EventManager methods chainable. >>> 1
210,Allow skipping counter cache. Add option to prevent counter cache from updating the counter cache >>> 1
211,"Since CakePHP 3.4 required php5.6 as minimal required version, it implicitly allows to only support PHPUnit 5+.\r\n\r\nAdding those couple of aliases will allow CakePHP users working on php7+ to pin to the latest version so I though that it could be a nice addition to 3.5.\r\n\r\nI'll provide a PR to the documentation/migration guide if those modifications are accepted. [3.4] Add Phpunit 6 compatibility  >>> 1"
212,This PR should bring the behavior of `Cake\Error\ExceptionRenderer` back to what it used to be before 56200bb.\r\n\r\nLinked issue: #10217 Return to original behavior in ExceptionRenderer >>> 1
213,"```\r\n@param array $limits This is array of options.\r\n@param array $default Default limit for option selecting. Default is Paginator limit.\r\n@param array $options Options for Select tag attributes like class, id or event\r\n\r\n@return string html output\r\n```\r\n\r\nIn Template, just use this code to get dropdown option\r\n\r\n```\r\n$this->Paginator->limitControl();\r\n```\r\n\r\nThis new function is useful if you need options for how many rows will show in Pagination. You can define $limits as array and also you can set $default selected value. \r\n\r\n<img width=""168"" alt=""show limit options"" src=""https://cloud.githubusercontent.com/assets/4234756/22939416/ce9d975e-f308-11e6-84a7-8f14c64aa01b.png"">\r\n\r\nHope it will help users to code less and not to depend on other plugin. \r\n\r\nThank you\r\n Dropdown array for Select limit >>> 0"
214,"While these are not values within the documented types, there exist use cases in CakeSession that necessitate these to be supported types.\r\n\r\nRefs #10196 Allow false/true to be read as keys in Hash::get(). >>> 1"
215,"- Added `Helper::getView()` to avoid having to use `$this->_View` in helpers. This was specially annoying as one had to remember that casing of `_View` is not consistent with other lower case names.\r\n- Added `Behavior::getTable()`. Is the method name fine or should be use something more generic like `getModel()`?\r\n\r\n`Component::getController()` already exits. View, table getter >>> 1"
216,Refs #10219 Fix issue where urlencoded URLs would not be matched >>> 1
217,The parent call in InflectedRoute was missing the `$method` parameter. This caused routes to not match correctly when routes have method conditions.\r\n\r\nRefs #10220 Fix InflectedRoute calling parent() incorrectly. >>> 1
218,Proposed change extend list of available encryption methods in CakeSocket with two new ones: TLSv1_1 and TLSv1_2. Moreover it added the possibility to choose custom TLS version by config option. 2.x Extend available TLS encrypt methods in CakeSocket >>> 0
219,Port changes from #10227 to 3.x Don't block when SMTP server quits. >>> 1
220,Resolves cakephp/docs#4715\r\n Fix lies in the return annotations >>> 1
221,"Previously, foreignKey() returns $this incorrectly.\r\nAlso, fixes BelongsToMany::targetForeignKey() returns $this. Remove overridden combined setter/getter methods >>> 1"
222,Ref: https://github.com/cakephp/cakephp/issues/4936#issuecomment-59808744\r\n\r\nWhere it is being used actually assumes that it can be handled like an array by casting it.\r\n```\r\n$id = (array)$this->_newId($primary) + $keys;\r\n```\r\n\r\nCould this cast be moved into the _newId() method to make clear that you can return array of primary key values?\r\n Fix docblock for PK generation and composite keys >>> 1
223,With the current implementation the default value is ignored when the EntityContext was created from an array. Return specified default value in EntityContext >>> 0
224,"Adds support to the entity context for recognizing `_joinData` properties and resolving the respective junction table classes. This makes reading errors, introspecting types, etc working with join table columns.\r\n\r\nWhile testing I noticed that the root fallback behavior in [**`EntityContext::_getTable()`**](https://github.com/cakephp/cakephp/blob/9fff233e7d38e0f30dd55377af637775f7f98104/src/View/Form/EntityContext.php#L483) doesn't actually falls back to the root node, but the last found / parent node.\r\n\r\nThe newly added `testTypeAssociatedJoinData` test seems to be the only one that fails when the fallback behavior is being changed to actually fall back to the root node, ie it seems that there is no other test actually covering that functionality, so I'm not 100% sure whether the current behavior is the actually intended behavior, and the description/name should be changed, or if it's maybe a bug, and it should really fall back to the root node? 3.next - Make entity context recognize junction tables >>> 1"
225,Using the `require-dev`section isn't enough for apps and therefore forbid tests to run on PHPUnit 5.7\r\nThis solution is probably not ideal but does solve the issue.\r\n\r\nThe other possibilities I can see are:\r\n - adding the aliases definition in the TestSuite files directly\r\n - reverting the support until we find something better \r\n Move PHPUnit aliases definition file to be loaded by apps by default >>> 1
226,This was accidentally dropped in 3.2 because there were no tests around the required keys of the beforeMarshal event.\r\n\r\nRefs #10247\r\n Restore 'association' option in beforeMarshal event. >>> 1
227,Readable diff: https://github.com/cakephp/cakephp/pull/10261/files?w=1 Fix memory leak in EagerLoader >>> 1
228,"Related to cakephp/docs#4734\r\n\r\nUpdated the docblocks here to refer to the correct method so that when replacing the deprecated method calls, functionality is preserved. Deprecated method docblock updates. >>> 1"
229,Closes #10256  getQuery() returns all when null is passed >>> 0
230,"Another attempt to support PHPUnit without forcing file loading for every request.\r\n\r\nI tested it with composer installation and .phar for PHPUnit 5.7 and 6 but I would appreciate if one can quickly check one of is app using this branch to confirm it does work properly before it (cc @dereuromark )\r\n\r\nI added a ""conflict"" definition in the `composer.json` to alert composer users.\r\n\r\nFinally, what fo you think of adding the following code inside the ifs:\r\n```\r\nif (version_compare(\PHPUnit_Runner_Version::id(), '5.7', '<')) {\r\n        trigger_error(sprintf('Your PHPUnit Version must be at least 5.7.0 to use CakePHP Testsuite, found %s', \PHPUnit_Runner_Version::id()), E_USER_ERROR);\r\n    }\r\n```\r\n\r\nI think it can help people to better understand what's going on than getting an standard error that could looks like \r\n```\r\n.../cakephp/src/TestSuite/Fixture/FixtureInjector.php on line 38\r\n````\r\n Support Phpunit 6 without impacting app runtime >>> 1"
231,Removing the public property is a regression caused by #9376 which breaks existing apps/plugins.\r\n\r\nRefs https://github.com/ADmad/CakePHP-HybridAuth/issues/94#issuecomment-281581244 Revert Event::$_result to Event::$result. >>> 1
232,Closes #10256  getQuery() returns all when null is passed >>> 1
233,closes #10085 \r\n\r\nAdds a new query method that resolves the problem of having to use an internal class `IdentifierExpression` in queries. Adds Query::identifier() method >>> 0
234,Closes #10119  Adds support for sort order in HasMany and BelongsToMany >>> 0
235,"This was not a correct fix, because the $message = str_replace('\n', ""\n"", json_encode($message)) do the folowing:\r\n\r\nJSON Encode replaces \n with \\n\r\nstr_replace replace \n with real line breaks, but this should not be real line breaks, because they appear in the html source code, javasource needs \n characters so the following line would be the correct fix:\r\n\r\n$message = str_replace('\n', 'n', json_encode($message));\r\nor\r\n$message = str_replace('\n', '\n', json_encode($message)); // I prefered this one, because it's reverse procedure, what json_encode does Make sure newline character is working in confirm messages for FormHelper >>> 1"
236,closes #9727\r\n\r\n- adds a new deleteEach method to the Table interface\r\n- adds beforeDeleteEach  and afterDeleteEach events\r\n\r\nmoved from: https://github.com/cakephp/cakephp/pull/9774 Adds Table::deleteEach() method >>> 0
237,Closes #10008 \r\n\r\nAdds a method that returns row count and closes the statement. Adds Query::executeAndClose() method >>> 1
238,Ref: https://github.com/cakephp/cakephp/issues/10288 3.4.2 BTM targetBindingKey proposal >>> 0
239,This implements #10290 \r\n\r\nIs sending an empty response body if no string was passed a good idea? Add withStringBody method >>> 1
240,Accept FILES data that has shuffled keys. Relying on key order in an associative array is fragile. I've also added tests for some of the more curious behaviors Validation::extension() has.\r\n\r\nRefs #10267 Allow out of order FILES data. >>> 1
241,Implements #10107  Add Validator::regex() >>> 1
242,"Dropdown array for Select limit\r\n\r\n@param array $limits This is array of options.\r\n@param int|null $default Default limit for option selecting. Default value is $this->param('perPage').\r\n@param array $options Options for Select tag attributes like class, id or event\r\n@return string html output.\r\n\r\nThis will help to show by data limit for per page without writing extra code, just add following code will give you below image output.\r\n\r\n`$this->Paginator->limitControl()`\r\n\r\n<img width=""109"" alt=""limit_control"" src=""https://cloud.githubusercontent.com/assets/4234756/23317055/1d6b9200-faf7-11e6-9b78-78c90c8f8971.png"">\r\n\r\nAlso override select tag attribute can be possible by passing $options.\r\n\r\nThank you Dropdown select limit (view limit) option for Paginator >>> 1"
243,"This is a bug fix ~~and feature enhancement~~ for dd()\r\n\r\n* CakePHP Version: 3.4.2\r\n\r\n* The `$showHtml` argument of the dd function was not used.\r\n* ~~I added the `$showFrom` argument because I expect the same behavior as the debug function except that the program terminates when I use the dd function.~~ \r\n* I couldn't invalidate `die()`, so I can not write a unit test. (But, `BasicsTest.php` is passed.) The $showHtml argument of the dd function was not used >>> 1"
244,PHPUnit 6 is a bit stricter against risky tests definition.\r\n 3.4 - Remove risky tests >>> 1
245,We went a bit overboard with traits in the ORM early on. This deprecates another trait from the Association classes. I've also updated the tests to contain more integration style tests and fewer mock dependent tests. I think the coverage should stay the same but we'll have higher confidence that the code actually works as real queries are used now. 3.next Update cascadeDelete to not use a trait >>> 1
246,"Describe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\nThe best way to propose a feature is to open an issue first and discuss your ideas there before implementing them.\r\n\r\nAlways follow the [contribution guidelines](https://github.com/cakephp/cakephp/blob/master/.github/CONTRIBUTING.md) when submitting a pull request. In particular, make sure existing tests still pass, and add tests for all new behavior. When fixing a bug, you may want to add a test to verify the fix.\r\n 增加可以设置文本高亮时的替换次数的功能 >>> 0"
247,nan add options of preg_replace limit >>> 0
248,"Added a test case of rendering an exception as json to ensure the returned url is correct.\r\n\r\nI encountered a situation in my application where my json errors were including the url inside the query component. Such as the following,\r\n\r\n```json\r\n    ""error"": {\r\n      ""message"": ""Not Found"",\r\n      ""url"": ""/api/v4/packages/home?/api/v4/packages/home&site_id=4&page=125""\r\n    }\r\n```\r\n\r\nI started by checking the `ServerRequest` to make sure that the request was being created properly, due to the `ExceptionRenderer` calling `ServerRequest::getRequestTarget()` to get it's url. However when I checked for a test case there wasn't one.\r\n\r\nI've created this test case, to check for this possible regression. However this test case passes, so my issue must be in my application, but worth contributing the test case anyway.\r\n\r\nHope it's helpful, even if it creates a minor cross over between ExceptionRenderer and ServerRequest in the tests. Adding a json exception test case >>> 1"
249,As it causes pain for users to install CakePHP and is only used in very few part of the framework where a warning will be thrown .\r\nRef https://github.com/cakephp/cakephp/issues/9697\r\n remove lib-ICU requirement >>> 1
250,"FormHelper loses some data when label of optgroup is numeric from output html elements.\r\n\r\n## Example case\r\n\r\n```php\r\n// controller\r\n$tags = $this->Tag->find('list', ['fields' => ['id', 'name', 'category_id']]);\r\n$this->set((compact('tags'));\r\n\r\n// .ctp\r\n$this->Form->input('tag_id');\r\n```\r\n\r\n## Example data in $tags\r\n\r\n```\r\n[\r\n    '1' => [\r\n        '1' => 'Foo',\r\n        '2' => 'Bar',\r\n    ],\r\n    '2' => [\r\n        '3' => 'Baz',\r\n    ],\r\n    '3' => [\r\n        '4' => '2Foo',\r\n        '5' => '2Bar',\r\n    ],\r\n];\r\n```\r\n\r\n`'4' => '2Foo'` or `'5' => '2Bar'` will be lost in this case. Fix FormHelper to get work fine with numeric label of optgroup >>> 1"
251,"This provides a better indication when trying to unload unloaded behaviors, instead of just returning `void`.\r\nI also added a case, when trying to unload a plugin behavior.\r\n\r\nDoing this directly inside the `ObjectRegistry` would have caused many breaking tests, as for example, `ObjectRegistry::set()` calls `ObjectRegistry::unload()`.\r\n\r\nWe can maybe expand this to other child classes of the `ObjectRegistry`, like `ComponentRegistry` and `HelperRegistry`.\r\n ObjectRegistry: Trigger an error if trying to unload an unknown object >>> 1"
252,"## New prototype implementation for cookies\r\n\r\nIt needs more tests and I'm not yet happy with the handling of the cookie data and encryption. But I would already like to get feedback on this to continue to develop it into a direction we all agree with.\r\n\r\nThe code is partially based on https://github.com/hansott/psr7-cookies/ but was changed to fit into our current API and the frameworks current php version. I've had a discussion with Mark Story if we'll directly use it or not. The final decision for the named reasons was to come up with our own implementation. I've also changed the implementation in some aspects, it's not a copy and paste job. I need some advice if we need to give credits and copyright and if yes, how to do it the right way.\r\n\r\nCookies can now be used from wherever the request object and response objects are available.\r\n\r\n## Overview of the new implementation\r\n\r\n* Uses the request and response objects\r\n* Uses a cookie object to built and modify a cookie\r\n* Generates cookie objects from the request object\r\n* Writes cookies to the response object\r\n* Uses getters and setters\r\n* No longer a component bound controller layer only solution (This change is required to get the cookie authenticator in the new authentication implementation working)\r\n* **TBD**: If possible a BC compatible cookie component that uses the new implementation\r\n\r\n## Related issues\r\n\r\n* https://github.com/cakephp/authentication/issues/26\r\n* https://github.com/cakephp/cakephp/pull/10203\r\n\r\n## History\r\n\r\nFormer PR from my fork https://github.com/cakephp/cakephp/pull/10203\r\n\r\nCreated a new PR based on the cake repo and not my fork so other people can contribute to the branch as well without being a member of my fork. New cookie implementation for the HTTP stack >>> 1"
253,"There are few cases when paginator uses ellipsis to skip 0 or 1 page.\r\n\r\nIt is more useful to display page link than skip one link with `...` as described in [""A good pager""](http://cweiske.de/tagebuch/pager.htm) \r\n\r\n **Main differences:** \r\n\r\n20 total pages, with first link 'F':\r\n```\r\n     5 => '1 2 3 4 *5 6 7 8 9 '\r\n// ellipsis skipped 0 pages\r\n-    6 => '<F ... 2 3 4 5 *6 7 8 9 10 '\r\n+    6 => '1 2 3 4 5 *6 7 8 9 10 '\r\n// ellipsis skipped 1 page\r\n-    7 => '<F ... 3 4 5 6 *7 8 9 10 11 '\r\n+    7 => '1 2 3 4 5 6 *7 8 9 10 11 '\r\n     8 => '<F ... 4 5 6 7 *8 9 10 11 12 '\r\n```\r\n\r\n20 total pages, with 2 first links:\r\n```\r\n     7 => '1 2 3 4 5 6 *7 8 9 10 11 '\r\n// ellipsis skipped 1 page\r\n-    8 => '1 2 ... 4 5 6 7 *8 9 10 11 12 '\r\n+    8 => '1 2 3 4 5 6 7 *8 9 10 11 12 '\r\n     9 => '1 2 ... 5 6 7 8 *9 10 11 12 13 '\r\n```\r\n\r\nSame with end of 'last'.\r\n\r\nI've added test cases using plain text links for testing paginator on series of pages.\r\n\r\nI also removed one duplicate test left after 'separator' option removal in 3af7ef6c985a32cc088ede93afd4e3e02797e828\r\n Paginator ellipsis should always replace more than one page >>> 1"
254,Resolves https://github.com/cakephp/cakephp/issues/10092\r\n 3.x - implemented Router::reverseToArray() >>> 1
255,`cake routes generate` should allow the usage of boolean parameters. This is necessary to test `_ssl` and a few other flag options.\r\n\r\nRelated to #10333 Fix parsing bool values out of route shell. >>> 1
256,"there are others usage in tests\r\nI'll come back as soon for edit it  change ""Cake\Network\Response"" to ""Cake\Http\Response"" >>> 1"
257,"This reverts commit 07f4779efef507c31291ca0f29222c1b06d626b8.\r\n\r\nAs of 2.4.3, [Cookie::delete('foo.bar')](https://book.cakephp.org/2.0/en/core-libraries/components/cookie.html#CookieComponent::delete) no longer works. Also, it's made impossible to set different expiration times for each individual entry in the same top level.\r\n```php\r\n$this->Cookie->write('Tracking.1', 'something', true, '+1 week');\r\n// After few days later.\r\n$this->Cookie->write('Tracking.2', 'something else', true, '+1 week');\r\n```\r\nThe second writing will update expiration time of the first tracking cookie. It is not an expected behavior, and it worked in earlier versions than 2.4.3.\r\n\r\nAlthough this is a bug, I would like to target to the 2.next branch, as some existing applications may be relying on the current behavior. [2.next] Revert ""Fix cookie component being inconsistent about writes."" >>> 0"
258,"- Help text wasn't rendered in `<option>` tag\r\n- Non-existent short name was rendered as '-' unintentionally\r\n- Related test cases had never worked\r\n\r\nNote that I didn't change boolean values to be rendered as '0'. Instead, I fixed the test case so that it matches empty strings.\r\n\r\nAfter this commit merged, I would like to port the same changes to the 2.x branch. Fix XML output of Cake\Console\HelpFormatter >>> 1"
259,"See #10323 and #7531\r\n\r\nThis tries to fix the following problem:\r\n\r\nYou add the `TranslationBehavior` like this:\r\n```\r\n$this->addBehavior('Translate', [\r\n  'fields' => ['title'],\r\n  'allowEmptyTranslations' => false\r\n]);\r\n```\r\n\r\nCreate a form, add some `controls` for different languages and save the request data.\r\nWhen this is saved, all empty translations will also be saved into the DB.\r\n\r\nNow comes the tricky part. If you now get the translations out of the `i18n` table, all the empty translations won't be selected (`allowEmptyTranslations`!).\r\nThis is caused by an applied condition in the `setupFieldAssociations()`method of the behavior (`$conditions[$name . '.content !='] = '';`).\r\n\r\nWhen you now try to update and save the former empty translations, you'll get an ` Integrity constraint violation` error. 💥\r\n\r\nMy approach is to unset all empty fields, so that they are not saved.\r\nI'm not happy with the implementation, but the `EntityTrait` is sometimes a little b*** 😉 Fix the allowEmptyTranslations presistence bug >>> 1"
260,"The `Cake\Network\Response` was moved to `Cake\Http\Response` few months ago.\r\n\r\nBut most references still refers to old name.\r\n\r\nIf controller action has `@return \Cake\Http\Response|void`, but if it uses code like `return this->response;`, PhpStorm displays warning ""Return value expected to be `'\Cake\Http\Response|void'`, `'\Cake\Http\Response'` returned"". I do not want to use old deprecated name in PhpDoc. Replace references of Cake\Network\Response to Cake\Http\Response >>> 0"
261,closed #10326 \r\n\r\nI've checked in verbose mode and the tests are not skipped anymore.\r\n\r\n(The skipped tests numbers went down from 87 to 78 on the php56 sqlite build) Add xcache to PHP 5.6 build >>> 1
262,"Per preliminary discussion in #10336 this PR:\r\n* adds new `tinyint` and `smallint` abstract schema types to support smaller storage requirements.\r\n* ~~adds an optional `storage` field to the schema abstraction so that various flavors of integers can be supported by specific datasources. This field provides a hint as to how many bytes are required to efficiently store the contents of a column. Backend can choose to provide and honor such hint.~~\r\n\r\nIn other words, this allows for a MySQL backend to keep its `smallint(6)`, `tinyint(4)`, or `tinyint(2)` intact instead of having them turn into `int(6)`, `int(4)`, or `int(2)`. This is particularly important when a table stores a million+ rows and a column can be stored as one byte (`tinyint`) instead of 4 (`int`).\r\n\r\nTests are provided. Add new `tinyint` and `smallint` abstract schema types to support smaller storage requirements >>> 1"
263,"Refs #10348\r\n\r\nI added the `$toBeginning` argument to `Connection::rollback()` in this pull request, as I couldn't break backward compatibility. One `rollback()` rolls back to the begining of transaction seems [intentional](https://github.com/cakephp/cakephp/blob/902c9910279eff909295e413d43fc6a5d2dbd7bf/tests/TestCase/Database/ConnectionTest.php#L475-L497). If it is arguable, I am willing to target 3.next branch.\r\n\r\nAlso, I added the `NestedTransactionRollbackException` class and the <code>Connection::wasNestedTransactionRoll<b>ed</b>back()</code> method. If this naming is not natural, please teach me.\r\n\r\n**Edit:** Added description Fix nested transactions may be rolled back or committed unexpectedly >>> 1"
264,This allows plugin maintainers to load validation providers during bootstrap therefore reducing the traction to install. Enhancement/default validation providers >>> 1
265,"The problem is that basic authentication credentials are encoded in ISO-8859-1, but if you use an other encoding f.e. UTF-8 and the Auth component compares a username or password with special chars it comes to errors.\r\n\r\nSo I added a check if there is set another valid encoding than ISO-8859-1 and if so, convert it to the actual encoding. Update BasicAuthenticate To Support Different Encodings Than ISO-8859-1 >>> 0"
266,"Interesting that there were no test cases for these methods, neither for the merge part nor without.\r\n\r\nResolves https://github.com/cakephp/cakephp/issues/10361 Fix EntityTrait and merging of properties that are not assoc arrays. >>> 1"
267,nan Bump phpstan to level 1 >>> 1
268,"Make error class of div in FormHelper::input configurable, so it is possible to generate html snippets compatible with Bootstrap 4. Make error class of div in FormHelper::input configurable >>> 1"
269,nan APCu is now already available on PHP 7.1 on travis VMs. >>> 1
270,This fixes #10354 for CakePHP 2.9\r\n\r\nCakeSchema would create duplicate primary keys for tables that already a primary key set on a non-conventional column (e.g. not `id`).\r\n\r\n fix duplicate primary keys for tables without models >>> 1
271,Pass through file name and line number from `PHP7ErrorException` up to `ExceptionRenderer`\r\n\r\nReplacement PR for https://github.com/cakephp/cakephp/pull/10399 3.x ExceptionRenderer should always report filename & line (if debug=true) (PHP 7) >>> 1
272,"Add `smallint` and `tinyint` types to the schema system. I'm targeting this for 3.5 as I don't think column types changing is acceptable in a bugfix release.\r\n\r\nPostgres doesn't support tinyint, so it is mapped to smallint.\r\n\r\nRefs #10347  3.next - Add small and tiny integer types >>> 1"
273,"- Made test suites compatible with PHPUnit 4.8\r\n- Fixed XML output of HelpFormatter (Backport from #10339. The related test cases don't pass with PHPUnit 4.8. I noticed this issue when I was working to make CakePHP 2.x compatible with PHPUnit 4.8 for my project. Then I checked 3.x branch and noticed that the related test cases had been fixed in a wrong way. That was why I reported #10339).\r\n- Added PHPUnit 4.8 to test matrix\r\n- ~~Added CakeFinalizer class. PHPUnit 4.8 marks test cases as 'risky', if the output buffering level is different before and after the test. CakeFinalizer allows us to adjust the output buffering level, when we want to test some function which changes the output buffering level and throws an exception.~~\r\n- Fixed several risky tests.\r\n    - Fixed CakeTestCase::run() to restore original output buffering level\r\n    - Fixed an undefined variable warning in missing_controller.ctp\r\n    - Fixed ViewTest (backport of 92bd86274b64c31ef6034caa74bb168be041b5e0)\r\n    - Fixed ThemeViewTest\r\n\r\nI have not changed `composer.json` as I think that using PHPUnit 4.8 is optional. So, for now, only a test is running with PHPUnit 4.8. However, It may be arguable. I am fine to change `composer.json` and `.travis.yml` to let all test cases run with PHPUnit 4.8.\r\n\r\nRefs: #10398 [2.next] Make test suites compatible with PHPUnit 4.8 >>> 1"
274,Add the generic/basic collection manipulation methods described in #10406. I'll do the other methods in separate pull requests once we've settled on names/approaches for them. 3.next - CookieCollection basic manipulation >>> 1
275,Fixed HTTP Basic Auth on FastCGI PHP - Applies to CakePHP 2.x (current 2.8.6)\n\nref. [http://stackoverflow.com/questions/16039263/cakephp-2-3-2-basicauthentication-not-working](http://stackoverflow.com/questions/16039263/cakephp-2-3-2-basicauthentication-not-working)\nref. [http://stackoverflow.com/questions/3663520/php-auth-user-not-set](http://stackoverflow.com/questions/3663520/php-auth-user-not-set)\nref. [https://github.com/symfony/symfony/issues/1813](https://github.com/symfony/symfony/issues/1813)\n fix HTTP Basic Auth on FastCGI PHP >>> 1
276,Currently `CookieCollection` will not return cookies without leading dot in a domain for subdomain requests.\r\n\r\nI had a real life example where cookies with domain `example.com` (no leading dot) would not be sent with requests to `subdomain.example.com`.\r\n\r\nI have found out that modern clients should ignore leading dot. Here is a fix for that.\r\n\r\nMore reading:\r\n- http://bayou.io/draft/cookie.domain.html\r\n- https://tools.ietf.org/html/rfc6265 Fix Http\Client\CookieCollection according to RFC6265. >>> 1
277,"This hard error makes using the `targetTable` option in associations really hard as it always triggers this condition. By reworking how we generate aliases and eagerloader aliases we can enable the `targetTable` option to behave as intended. The foreignKey will need to be set, when used with `targetTable` and a table that doesn't have a matching alias, but I think that is reasonable given how key generation uses the table's alias for other reasons.\r\n\r\nI've had to replace target aliases with the association names, so that the selected fields reflect the association's alias and not the table's original alias. I don't think this will cause problems with application code that relies on the target table's alias being used, as aliases always had to match between the association and table instance. One situation that could be problematic with the new code is if a user is using `$table->aliasField()` in query expressions for association finders. Because the table's alias and association alias no longer have be the same, mismatching aliases could end up in the same query.\r\n\r\nRefs #7125\r\nRefs #10410 Remove errors when aliases don't match >>> 1"
278,Attempt to reproduce the issue https://github.com/cakephp/cakephp/issues/10418.\r\n Wrong 0 list index >>> 0
279,"TranslateBehavior's method TranslationField always returned the alias for the i18n table for the field argument, be the default locale or not.\r\n\r\nThis fix compares before anything if the current locale is equals to the default locale for the table. If so, returns the default alias. If not, it behaves as always. TranslateBehavior's method TranslationField now returns standard table alias for default locale >>> 1"
280,"Make most of the cookie operations use immutable patterns. This makes cookies work in a similar way to the request & response. This is also important so that we can ensure that the request/response stay actually immutable.\r\n\r\nI've not attempted to solve the `expand()`, `flatten()`, and encryption related features as I want to do those separately. I feel that the expand/flatten methods should happen implicitly when complex values are manipulated, and that encryption should be moved into a standalone middleware with a whitelist of cookies it handles. 3.next - Make most cookie operations immutable >>> 1"
281,"We can transparently expand and flatten encoded data. This makes cookies easier to work with as the no longer have expanded state that developers have to manually check. While this implicit type changing is a departure from the immutable patterns, I think it is a reasonable trade off for more ergonomic objects.\r\n\r\nRefs #10406  3.next - Implicit cookie value expansion >>> 1"
282,"Add the methods that will allow the new CookieCollection to be used in Http\Client. This does not cover adding the backwards compatible aliases, those will be done separately.\r\n\r\nThere is a bunch of duplication with Http\Client\CookieCollection. But it is looking like that class can be entirely removed at the end of this refactoring.\r\n\r\nRefs #10406 3.next - Add Client specific methods to CookieCollection >>> 1"
283,"Add the methods that make Http\Cooke\CookieCollection compatible with Http\Client. Next, I'll use the new class in the client.\r\n\r\nI've also fixed a few issues around how cookies were being stored. Cookies need to be unique per name;domain;path tuple. Without this constraint expiring cookies becomes very challenging. Also it becomes impossible to update a cookie in the collection.\r\n\r\nRefs #10406  3.next - Add client compatibilty methods to CookieCollection >>> 1"
284,nan Getter/Setter/Clear for Query::contain() >>> 1
285,Update the Http\Client and Stream adapter to not use deprecated methods on CookieCollection. I've also fixed a small issue where cookie headers that were set in the request would be dropped.\r\n\r\nRefs #10406  3.next - Update Http\Client to use undeprecated cookie methods. >>> 1
286,Use the new CookieCollection objects in Client responses. This allows us to remove one copy of the Cookie parsing code we had.\r\n\r\nRefs #10406 3.next - Use CookieCollection in Client responses >>> 1
287,"While playing with Text::insert method with flattened entities I got to this use case where objects (Time) cannot be cast to (int) by asort here https://github.com/cakephp/cakephp/blob/3.4.0-RC1/src/Utility/Text.php#L198.\r\n\r\nRemoving this line makes all tests pass, so I guess we need additional unit tests to ensure we need to asort the data array. I'm leaving this PR open in case you could point me to either add additional test cases or it's fine to remove the asort?\r\n\r\nThanks,\r\n WIP Issue/text insert time >>> 1"
288,I've chosen to not use CookieCollection as the internal storage as  keeping the public property in-sync would be hard/impossible. This  approach also avoid eagerly allocating memory for the cookie collection and the cookie objects.\r\n\r\nRefs #10406 3.next - Expose CookieCollection in ServerRequest >>> 1
289,Refs #10374\r\n\r\nTests are coming Adding a security headers middleware >>> 1
290,"Basically copied the functionality and tests from `BelongsToMany` with slight modifications and additions. Also refactored the `saveAssociated` method a little bit, making it easier to test and to read.\r\n\r\nI wasn't sure about the rules regarding backwards compatibility for protected methods, if breaking changes were allowed, I would have changed `_unlinkAssociated()` to drop the `$entity` (unused) and `$target` arguments.\r\n\r\nRefs #9474 3.next - Add loose emptiness handling for `HasMany` associations. >>> 1"
291,"This feature allows one to be able to define a resource that may not have the same name as the controller. It is the same concept as the connect method\r\n\r\n      $routes->connect('/Gallery', ['controller' => 'Albums', 'action' => 'add']); Adding controller mapping in resources >>> 1"
292,Include the changes from #10232 and also fix the issue described in #10445 Fix CakeSocket not being able to connect to TLS1.2 only servers >>> 1
293,fix for #9963. This makes the use of SQL Server more flexible.\r\n additional DSN options for SQLServer  >>> 1
294,"Integrate the new Cookies and CookieCollection into the Response. All of the existing cookie related methods are backwards compatible, and `withCookie()` allows new style cookies as a parameter.\r\n\r\nI've had to add more compatibility shims into Cookie and relax the expires parameter typing to allow integers. I found this preferable to coercing the existing cookie data into a DateTime just so it can be converted back into an integer.\r\n\r\nRefs #10406 3.next - Integrate CookieCollection into Response >>> 1"
295,"Sadly there are 1200 deprecation warnings for `getMock`, but at least we can use PHP7 features now!\r\n\r\nCloses #9973 Make test suite compatible with PHPUnit 5. >>> 1"
296,Continues with #9978  Split QueryTrait::eagerLoaded() into getter/setter >>> 1
297,Continues with #9978  Split ConsoleOutput::outputAs() into getter/setter >>> 1
298,Continues #9978  Split ServerRequest::env() into getter/setter >>> 1
299,"After merging the changes from #10488 I started writing the documentation and noticed that most of the other RouteBuilder methods use 'path' and not 'controller' as an option. Given that changing the path was the original intent, I thought that might be a more consistent option name. Furthermore, the previous changes would result in awkward reverse routing scenarios where the routing parameter would not match the controller/model name, and instead match the resource name. e.g. `resources('BlogPosts', ['controller' => 'articles'])` would use `blog_post_id` as the routing parameter instead of `article_id` which does not match the conventions used elsewhere in the framework.\r\n\r\ncc @chrisShick \r\n 3.next - Rename controller option to path. >>> 1"
300,Continues with #9978  Split Shell::io() into getter/setter >>> 1
301,Continues with #9978  Split View::x() into getter/setter >>> 1
302,Continues with #9978  Split ServerRequest::session() into getter/setter >>> 1
303,"In order to deprecate the CookieComponent, we need to have an alternative solution. By having middleware that transparently encrypts/decrypts cookies we can deprecate the CookieComponent.\r\n\r\nThe previous use cases of using different encryption keys for different cookie names, can be afforded by applying the EncryptedCookieMiddleware multiple times.\r\n\r\n### TODO\r\n\r\n* [x] Complete API docs\r\n\r\n### Questions\r\n\r\n* Does the whitelist approach make sense?\r\n* Should there be a `*` mode to encrypt all cookies?\r\n* Are there other features of CookieComponent that are missing between the new Cookie objects and this middleware?\r\n\r\nRefs #10406  3.next - Add Cookie encryption middleware >>> 1"
304,Update code in TimestampBehavior for increase readbility. Update code in TimestampBehavior >>> 0
305,\r\nadd phpunit.xml.dist to .gitignore add phpunit.xml.dist to .gitignore >>> 0
306,add phpunit.xml.dist to .gitignore. Add phpunit.xml.dist to .gitignore >>> 0
307,Copy the implementation from 3.x as it works with -0.0 already.\r\n\r\nRefs #10521 Fix notBlank() to pass on -0.0 >>> 1
308,"In an effort to better understand CakePHP, I went through the entire core today and fixed any glaring issues that I came across. Most of the issues were missing annotations, however there were some minor code changes. I broke up my changes into separate commits for better clarity as well as to allow cherry-picking.\r\n\r\nFYI: I avoided the best I could adding missing annotations for deprecated code, wasn't sure it was absolutely necessary.\r\n\r\n~~P.S. I am still pushing commits, I am using this PR to help with automatic lint, etc. I will update this PR when I am done.~~\r\n\r\n~~EDIT: I am done with this PR now, I did it this way to avoid a ton of PR's. This way, you can pick and choose what changes you want to bring into the core. All changes have been ran through static code analysis and there should not be any problems, but you guys are the experts so I will leave it to you at this point. Thanks!~~\r\n\r\nUPDATE: I have split this PR into multiple PR's per request of core team. Please see below, referenced PR's. Added some missing annotations, cleaned up some code for best practice >>> 0"
309,"This is me splitting up a larger PR, please see #10532 for details. Minor updates (annotations and best-practices) >>> 1"
310,"This is me splitting up a larger PR, please see #10532 for details. Splitting some workflows for better control flow >>> 1"
311,"This is me splitting up a larger PR, please see #10532 for details. Prefer to use static:: method call invocation instead >>> 1"
312,"This is me splitting up a larger PR, please see #10532 for details. Use null comparison instead >>> 0"
313,"This is me splitting up a larger PR, please see #10532 for details. Extending some return types in annotation docblocks >>> 0"
314,"This is me splitting up a larger PR, please see #10532 for details. Removing default null property assignments. Class property default va… >>> 1"
315,"This is me splitting up a larger PR, please see #10532 for details. Changing unnecessary double-quotes to single-quotes >>> 1"
316,"This is me splitting up a larger PR, please see #10532 for details. Removing unnecessary parentheses >>> 1"
317,"This is me splitting up a larger PR, please see #10532 for details. Binary safe fopen() >>> 1"
318,"This is me splitting up a larger PR, please see #10532 for details. Using short syntax for applied operations >>> 1"
319,It's always tempting to commit `debug.log` and `error.log` when doing a PR.\r\n@dereuromark Can you add the `plugin` thing? Ignore debug.log and error.log >>> 1
320,Continues with #9978  Split ModelAwareTrait::modelType into getter/setter >>> 1
321,"Describe the big picture of your changes here to communicate to the maintainers why we should accept this pull request. If it fixes a bug or resolves a feature request, be sure to link to that issue.\r\n\r\nThe best way to propose a feature is to open an issue first and discuss your ideas there before implementing them.\r\n\r\nAlways follow the [contribution guidelines](https://github.com/cakephp/cakephp/blob/master/.github/CONTRIBUTING.md) when submitting a pull request. In particular, make sure existing tests still pass, and add tests for all new behavior. When fixing a bug, you may want to add a test to verify the fix.\r\n 2.x >>> 0"
322,nan extract controller name logic into another method >>> 1
323,"The root folder is already cluttered with noise files as it is.\r\nYou can't even see the badges anymore.\r\n\r\nI would like to propose moving the stan file at least inside tests (where it belongs as static analysis part of test CI), to clean up a bit the root.\r\nThere is already a https://github.com/cakephp/cakephp/tree/master/tests/PHPStan folder inside anyway.\r\nI did the same with my plugins.\r\n\r\nIt is nice that phpstan allows to customize/configure the path, I think we should leverage it here.\r\n\r\n//cc @ondrejmirtes Move phpstan file in tests >>> 0"
324,What the title says. Turning the CSRF component into a middleware >>> 1
325,https://github.com/cakephp/cakephp/issues/10235 Radio Labels indicate which button is selected. >>> 1
326,This should fix the issue reported by mamchenkov in https://github.com/cakephp/cakephp/issues/10115\r\n Fix translator package initialization from cache >>> 1
327,`Cake\Http\Client\Response` and `Cake\Client\Response` would call methods that wasn't a part of `CookieInterface`.\r\n\r\nRefs #10406 Update Response classes to accept custom implementations of CookieInterface. >>> 1
328,Continues with #9978 Split Debugger::outputAs() into getter/setter >>> 1
329,"Don't recommend methods that don't exist, it is confusing and not helpful.\r\n\r\nRefs #10565 Update deprecation suggestions >>> 1"
330,file Session.php has some duplicated code.for example : `$this->_hasSession() && !$this->started()` there is on 3 method.It can be on a method for doing a job. Add a mthod for 1 job and delete duplicate code >>> 0
331,Refs #9978 Split Registry/Locator methods into getters and setters. >>> 1
332,"This is more a feature than a bug, but the expectation sure is that the internal shell ""actions"" named ""commands"" work similar to the controller actions regarding their internal mapping.\r\n\r\nIn controllers it is `'action' => 'myCoolAction'` and does only inflect for the outside (via routing and dashed inflection).\r\nThe same expectation should be met on the CLI level when adding subcommands\r\n```php\r\n$parser->addSubcommand('initMyDb', [...]);\r\n```\r\n\r\nThose subcommands are basically the methods of this class just as the methods of a controller are its actions.\r\nWhereas from the outside (CLI) you can then either access it via underscored or CamelCase as usual.\r\n\r\nIn 4.0 we could probably make this a bit stricter then, allowing one internal format (methodName) only.\r\n\r\nPS: Is this safe to go into master or is 3.5 better, as people could (wrongly?) use the uninflected/raw camelCase format already in their apps. Allow camelBacked method names for subcommands. >>> 1"
333,Refs #9978 Split EventDispatcherTrait::eventManager() into getter/setter. >>> 1
334,Continues with #9978 Add Query::valueBinder() getter >>> 1
335,nan Adding a getter and setter for cache registry >>> 1
336,nan Getters and setters for I18n/I18n.php >>> 1
337,Plus remove unused variable. Correct and improve doc block of RouteCollection::match() >>> 1
338,nan Write Some Assertions for Validation Library >>> 1
339,"As discussed in #10578 this PR specifies a constant for the location of the Config directory. Userland code can easily move the dir, and just change the value of this constant. Add constant specifying the location of the Config dir >>> 0"
340,Add the methods that help developers register and configure scoped middleware for their applications. The next task will be to use these new features when dispatching requests.\r\n\r\nRefs #10308  3.next - Enable scoped middleware to be setup >>> 1
341,"Implements https://github.com/cakephp/cakephp/issues/10586\r\n\r\nA first stab at the issue.\r\nI would only recommend adding the single one ISO format `'Y-m-d\TH:i:s.uO'` which CakePHP generates on output in JSON files, so when doing the reverse, import, it can safely and directly convert back into the DB format.\r\n\r\nAny better idea on the subject? Allow extended ISO datetime format to be marshalled. >>> 0"
342,nan @link >>> 1
343,"When a named route doesn't match, the error message contains ""%s"" instead of the named route. That's my guess. Fix error message for named route matching >>> 0"
344,"Find and apply any scoped middleware from the RoutingMiddleware. With these changes, scoped middleware is now functional, but the proposed changes to `Application` are incomplete, and I'll be doing those next.\r\n\r\nI've intentionally not modified the `RoutingFilter` behavior as I feel that scoped middleware is a good reason to migrate to the new `Application` class and PSR7 stack which will become standard in future versions\r\n\r\nRefs #10308  3.next - Connect Middleware pipelines in RoutingMiddleware >>> 1"
345,This pull request will add `@method`'s for the Mail getters and setters. \r\n\r\nFor the deprecated `@method` there are currently no standard do define it as deprecated. \r\n\r\nphpDocumentor/phpDocumentor2#1604 Add Mail getter and setter method annotation >>> 1
346,"Resolves https://github.com/cakephp/cakephp/issues/8053\r\n\r\nIs this acceptable to be announced in migration notes in 3.5?\r\nShould we still add a feature flag of some sort to opt-in the object preserving extract?\r\n\r\nThe \ArrayAccess objects itself do not change, only the magic ones like Time() now don't get unnecessarily array transformed. Fix Hash::extract() to not array cast objects unnecessarily. >>> 1"
347,"So I've been encountering a strange issue today when trying to add a new user record. It continually fails with `id field is required` or similar validation errors. Even with no validation rules set, it still fails.\r\n\r\nAt first I thought the culprit might have been inverted logic in the `_checkPresence` method. Thought I'd try and write a test to abstract the problem from my project codebase.\r\n\r\nNeedless to say the testcase and other Validator tests pass just fine.\r\n\r\nJust thought I'd submit a test case in case it was any use. Added a test case for errors when validation is only for create >>> 1"
348,nan Fix _validatePost returns true when empty form is submitted >>> 1
349,Refs #10616 \r\n\r\nThis adds a new method on the IntegrationTestCase called `enableRememberFlashMessages()` that stores flash messages before rendering them and restores them to the session. It's opt-in as users may be relying on the messages being discarded during render.\r\n\r\nNaming ideas welcome 😄  I wanted to keep it similar to enabling the tokens. Proposal for #10616 >>> 1
350,nan Fix some errors reported by phpstan level 2. >>> 1
351,"A validation rule for `<input type=""color"">`. Add Validation::hexColor() >>> 1"
352,nan Update to codesniffer 3.0 >>> 1
353,`ConnectionInterface` does not provide a `driver()` method which `*Sql()` methods require.\r\n Change ConnectionInterface typehint to Connection in TableSchema. >>> 1
354,"I've added `return $this` for method chaining and implemented `__set()`, `__isset()`, `IteratorAggregate` and `Countable`. Enhancements to ObjectRegistry. >>> 1"
355,* Adding a filter callback\r\n* Adding a threshold to filter only slow queries Improving the QueryLogger >>> 0
356,context based messages should return the key when the translated value is ''. This mirrors the behavior of context-less messages. Returning '' was incorrectly added in #8932\r\n\r\nRefs #10440\r\n Empty message strings should return the key >>> 1
357,Added getters and setters for two classes. Adding getters and setters for logger objects >>> 1
358,"See #10632. As Table::alias was deprecated in 3.4, I added 3.4 to the interface as well. Is this correct? Add deprecation annotation to RepositoryInterface::alias >>> 1"
359,It's many time an inconvenience to have Schema Queries on DebugKit's SqlLog\r\n\r\nThis is a tiny fix to avoid logging those queries based on a App.logSchemaQueries Configure parameter. Avoid logging schema queries based on Configure parameter >>> 0
360,This finishes off the bulk of the work for #10308. I'm expanding the scope of the `Application` class to be a vessel for configuration/setup logic. This change also enables us to deprecate most of the static methods on `Router` (in a subsequent pull request) which will let us significantly reduce the static API surface area in CakePHP.\r\n\r\n## TODO\r\n\r\n* [x] Test these changes against an 3.2 style app to ensure backwards compatibility for route reloading.\r\n* [ ] Should `RouteBuilder` have an interface added? 3.next - Add application routes hook >>> 1
361,Redo of #10579 Add constant specifying the location of the Config dir >>> 1
362,Continues with #9978  Split TypedResultTrait::returnType into getter and setter >>> 1
363,"Refs https://github.com/cakephp/cakephp/issues/10651\r\n\r\nAdds a new signature for `contain()` that matches the signature of `matching()`:\r\n\r\n```php\r\n$query->contain('Articles', function ($q) {\r\n    return $q->where(['Articles.published' => true]);\r\n});\r\n``` Added second signature for contain that matches the signature for mat… >>> 1"
364,Continues with #9978  Split EntityTrait::invalid into getter and setter >>> 1
365,"Add constants that allow us to reduce the chances of error when referring to abstract schema types. I know I've made a few confusing typos when building schema reflection and having constants would have made work simpler.\r\n\r\nIf people think this is a good idea, then I can update the other drivers to use the constants as well. RFC - 3.next Abstract schema type constants >>> 1"
366,"So far \r\n```\r\n$result = $this->Bundles->find()->where(['name' => $bundle['name']])->first();\r\n```\r\nalways needed a manual inline doc block, as first() said ""mixed"".\r\nWith this it will be `EntityInterface|null`, as expected by Query(Trait).\r\nIt then also more clearly states that this could return null :)\r\n\r\nAlso fixed up a few other lies. Allow more IDE detectability. Fix invalid doc blocks. >>> 1"
367,"Since the sub packages can be used separately, they should be setup correctly.\r\n\r\nGave all composer.json files an overhaul. [3.x] Give the Composer JSONs some Love >>> 1"
368,"When an entity is used in multiple link operations, the joint record would be reused, resulting in incorrectly persisted state.\r\n\r\nI've also improve persistence related error messages a bit as I didn't grok them on first read.\r\n\r\nRefs #10665 Fix multiple link() operations not persisting correctly >>> 1"
369,"This fixes `enableRetainFlashMessages()` from overwriting the flash messages if no messages were retained in the first place, which fixes an issue (https://github.com/cakephp/cakephp/pull/10626#issuecomment-303332243) where early response returns had their messages overwritten.\r\n\r\nping @mirko-pagliai  Fixed overwriting existing flash messages if none were retained >>> 1"
370,See #10678 Fix issue 10678 >>> 1
371,"While testing a [MessagesDbParser](https://github.com/dereuromark/cakephp-translate/blob/master/src/I18n/MessagesDbLoader.php) I found out that the core silently skips certain context translations - if they have been defined without context already.\r\nSilently ignoring some of the translations is rather hard to detect.\r\n\r\nI found out that the following change always works, so no more silent drops. Fix PO parsing around context. >>> 1"
372,Implement the fluent setter features for Route described in #10689. I've opted to use `set` prefixes on the methods to better match the conventions we are headed towards with other parts of the framework and because `extensions()` is already deprecated in favour of `setExtensions()` and I did not want two method styles in the same class. 3.5 - Add fluent setters to Route >>> 1
373,"Following https://github.com/cakephp/cakephp/pull/10668#issuecomment-302897784\r\n\r\nThe idea here is to add an interface for `TableSchema` with commonly used methods. I'm wondering if we should add all methods to the interface or are there any specific ones?\r\n\r\nThere are some Connection related SQL methods. I'm wondering maybe these could be placed in another interface?\r\n\r\nAlso there are some legacy getter methods without the `get*` prefix. Should we add the ""correct"" ones to the interface for consistency (getFoo/setFoo) and deprecate the old ones?\r\n\r\nI've started with splitting another combined getter/setter for colum type and adding `hasColumn()` method. [RFC] Add TableSchemaInterface >>> 1"
374,Add polish locale add polish locale >>> 1
375,"While I don't think the SQL errors coming from EagerLoader re-use can be fixed without invasive changes to `EagerLoader`, we can treat the symptom of PaginatorComponent triggering this scenario by pre-emptively cloning the query.\r\n\r\nRefs #10697 Clone queries during pagination. >>> 1"
376,Add methods to RouteBuilder that make connecting routes for a single HTTP method simpler.\r\n\r\nRefs #10689 3.next - HTTP method route creation helpers >>> 1
377,The small and tiny integer types were not mapped in `Type`. This fixes errors that would be created when small/tiny integer fields were used as conditions in queries. Add small/tiny integer type mappings. >>> 1
378,Scope log wasn't in function _format() in BaseLog.php.Actually it just gives $context to that method.I move log message to scope if you declare a scope. Fix scope log format >>> 0
379,Cookie collections are immutable in 3.5 and the setter allows for modifying client cookies on runtime. HTTP Client cookies add/remove methods. >>> 1
380,See https://github.com/cakephp/cakephp/issues/10581 Adding methods to the entity to check for values >>> 0
381,"Hi folks,\r\n\r\nI've noted that classic array ""order"" aren't work in pagination (CakePHP 2.x):\r\n\r\n```php\r\n$this->Paginator->settings['Post'] = array(\r\n  'order' => array('Post.created DESC', 'Post.id ASC') // <--- this doesn't works\r\n);\r\n\r\n$items = $this->Paginator->paginate('Post');\r\n```\r\n\r\nIn my test only this kind of syntax are working:\r\n\r\n```php\r\n'order' => array('Post.created' => 'DESC', 'Post.id' => 'ASC') // this works\r\n'order' => 'Post.created DESC, Post.id ASC' // with string also works\r\n\r\n'order' => array('Post.created DESC', 'Post.id ASC') // <--- this doesn't works but with this PR will ;)\r\n```\r\n\r\nSo this pull request has a fix to support all kind of order by syntax, link find() model method does.\r\n\r\nThanks ;) Cake 2.x - Some fix into Paginator component for order / sort classic sintax >>> 1"
382,Refs #9978\r\n\r\nI've also added an interface with definitions for new methods. Could be useful to determine if an object supports validators. Split ValidatorAwareTrait::validator() into getter/setter. >>> 1
383,I've made the multibyte option an implicit setting. I feel this makes routing simpler to use as we can infer from the patterns used whether or not multiple patterns should be used. Having an additional method call required makes Route clunkier to use.\r\n\r\nRefs #10689 3.next - Add remaining fluent methods for routes. >>> 1
384,"Refs #6356, #7276. This should allow paginating in cells :)\r\n\r\nI'll update the docblocks once code changes are approved. Add standalone Paginator class. >>> 1"
385,Deprecate the static method and provide a route builder method that allows routes to be loaded inside scopes.\r\n\r\nRefs #10689 3.next - Add RouteBuilder::loadPlugin. >>> 1
386,Another combined get/set method.\r\n\r\nRefs #9978 Deprecate RouteCollection::extensions() >>> 1
387,"Missing or broken cache engines can cause applications to fail, and in my opinion, cache is *optional* and shouldn't break an app. Since engines are lazy loaded, it's difficult to predict that an engine will fail by catching the exception unless you build all engines during bootstrap, defeating the lazy loading.\r\n\r\nThis PR adds the ability to define fallback engines within cache config. An example might be falling back to the FileEngine if Redis fails to connect:\r\n\r\n```php\r\nCache::setConfig('tests', [\r\n    'engine' => 'Redis',\r\n    'host' => 'badhost',\r\n    'fallback' => 'tests_fallback'\r\n]);\r\nCache::setConfig('tests_fallback', [\r\n    'engine' => 'File',\r\n    'path' => TMP,\r\n    'prefix' => 'test_',\r\n]);\r\n```\r\n\r\nA developer could configure fallbacks all the way to the NullEngine in order to prevent their app from ever throwing an exception due to missing cache.\r\n\r\nI'd like to discuss potential pitfalls (especially regarding groups, as I do not have much experience with them) of this change. Also, hiding errors isn't a good thing, so ideas on logging those errors are welcome. [RFC] 3.next Cache fallback >>> 1"
388,"The current behaviour of CakePHP regarding the logging of unicode characters is inconsistent. Strings and arrays containing unicode characters will be logged as such, keeping the unicode characters readable. Entity properties and objects implementing JsonSerializable with unicode characters will be logged with all unicode characters escaped as \uXXXX, making them very hard to read and to search for.\r\n\r\nAs CakePHP is unicode aware in general, I'd like to suggest implementing a consistent logging of unicode characters over all types of parameters. Unicode characters should always be logged as such, keeping them readable and searchable.\r\n\r\nThis can be achieved by using `JSON_UNESCAPED_UNICODE` with `json_encode()`. This applies to `BaseLog::_format()` and `EntityTrait::__toString()` as those are the functions participating in logging message and objects. Consistent unicode logging >>> 0"
389,Fix type static string and make const and change name of var for make better code. Fix type static string and make const >>> 1
390,"The current behaviour of CakePHP regarding the logging of unicode characters is inconsistent. Strings and arrays containing unicode characters will be logged as such, keeping the unicode characters readable. Entity properties and objects implementing JsonSerializable with unicode characters will be logged with all unicode characters escaped as \uXXXX, making them very hard to read and to search for.\r\n\r\nAs CakePHP is unicode aware in general, I'd like to suggest implementing a consistent logging of unicode characters over all types of parameters. Unicode characters should always be logged as such, keeping them readable and searchable.\r\n\r\nThis can be achieved by using `JSON_UNESCAPED_UNICODE` with `json_encode()`. This applies to `BaseLog::_format()` and `EntityTrait::__toString()` as those are the functions participating in logging message and objects. Consistent unicode logging >>> 1"
391,Refs #10734\r\n Don't emit errors when clauses are undefined. >>> 1
392,"This is  part of #10716, and adds a very basic `CommandCollection` without the auto-discovery features. The class is very basic right now, but it is a start :smile: 3.next - Start adding CommandCollection >>> 1"
393,The 2.next build has been broken for a short while. 🤞 that this fixes it. Fix 2.next tests >>> 1
394,Make test method's request and fix some of problems in Request.php Make test method's request >>> 1
395,Resolves https://github.com/cakephp/cakephp/issues/9558\r\n left join support in TranslateBehavior >>> 1
396,"Build out the `autoDiscover()` method for `CommandCollection` this method will scan the core, app and all plugins for shell commands and append them into the collection. I've made the results of the scan include enough data that I can refactor the `CommandListShell` and `CompletionShell` in a subsequent pull request.\r\n\r\nRefs #10716 3.next - Command collection autoDiscover() >>> 1"
397,"By using `extract()`, I propose a modification because it causes poor readability as well as side effects (overwriting `$uri`, `$server`). Remove extract() from ServerRequestFactory::getBase() >>> 1"
398,Email addresses that contain both unicode and commas will not be correctly encoded by mime_encode_header if the comma precedes the unicode. In this scenario we have to quote the encoded address.\r\n\r\nRefs #10763 Fix encoding of addreses contain comma & unicode >>> 1
399,nan Use PSR7 response methods in IntegrationTestCaseTest >>> 1
400,"Related to #10778 it's a first implementation for your review & comments. I've tested it locally and found interesting improvements in performance and would be interested in routing use cases that could break the caching feature.\r\n\r\nThanks, add router scope cache >>> 0"
401,Fixes #10157  Add disableErrorHandlerMiddleware method >>> 1
402,"I noticed we are doing this quite often. Thought we could replace it with a convencience function. No big deal, though. Add a setAppNamespace() convenience function to replace literal calls >>> 1"
403,"Redis cluster will fail with Multiple Keys\r\n\r\nredis-cli : \r\n```\r\n> del kye1 kye2\r\n> (error) CROSSSLOT Keys in request don't hash to the same slot\r\n```\r\n\r\nExecution of the `RedisEngine::clear()` makes it true, but the key is not deleted Fix Correspond to RedisCluster from RedisEngine::clear() >>> 1"
404,Take the work done in #10770 and account for the complex options and non-empty disabled option sets.\r\n\r\nRefs #10770\r\n Fix disabled attribute calculation. >>> 1
405,I stumbled upon this while upgrading one of my apps to php 7.1 and strict types enabled.\r\n`ini_set()` expects second parameter to be a string. Fix ini_set calls to use strings as second parameter >>> 1
406,Continuing the work done in #10786 and adding tests. I've not modified the Memcached driver as memcached values will have a TTL set when the key is initially created.\r\n\r\nI am targeting `master` with these changes as it is reasonable to expect that counters would work consistently across Redis and Memcached implementations (which they didn't) and these changes are normalizing behavior across backends. Make Redis::increment/decrement() set a TTL >>> 1
407,We need to flip / into \ to correctly generate the namespace for fixtures in subdirectories.\r\n\r\nRefs #10795 Fix plugin fixtures in sub-directories not resolving. >>> 1
408,If fixed some issues while using `strict_types`. Most of them caused by passing `null` to internal php functions. Fix some issues that came up while using strict_types >>> 1
409,"This gets the new `CommandRunner` roughly in place. This class will eventually replace `ShellDispatcher` as the way shell commands are run. It integrates the `Application` with the console environment, and through a new hook method applications are able to selectively enable console commands, or rename commands.\r\n\r\nThis is by no means complete, but it will unblock work around `ConsoleIntegrationTest` and enable smaller pull requests for future work.\r\n\r\nRefs #10716 3.next - Start CommandRunner >>> 1"
410,This fixes #10223 as the option didn't work anymore for `hasMany` relations.\r\nI didn't created a new trait for `_extractFinder` and instead copied `Association::_extractFinder()`.\r\n\r\n@lorenzo Thank you for your help 😄  Fix usage of contain finder options for hasMany relations >>> 1
411,The IntegrationTestCase was not setting the correct server keys to set the URI state correctly for PSR7 methods.\r\n\r\nRefs #10800 Fix incorrect PSR7 request target. >>> 1
412,"This improves the performance of most collection methods sometimes\r\nby making it 100% faster compared to the previous implementation.\r\n\r\nThe improvement is not for free, with this change we are giving up\r\nthe lazyness feature (not iterate unless requested). The giving\r\nup of the feature only happens when the collection has been initialized\r\nwith an array, as oposed to being initialized with another iterator or\r\ngenerator; and this is the reson I consider this change a safe one.\r\n\r\nFor the curious, the improvement comes from the (sad) fact that calling\r\nfunctions in php is extremenly expensive, specially when using iterators\r\nsince each iteration will call at least 4 functions (valid, next, current, key).\r\n\r\nThis becomes even worse as `IteratorIterator` does not have any optimizations,\r\nso for each wrapped iterator, the number of functions is multiplied by 2\r\nfor each iteration.\r\n\r\nThe change proposed here will unwrap nested iterators as much as possible\r\nto avoid the function call explosion, and in some (safe) cases, will\r\niterator the collection immediately as an array before wrapping it again\r\nin an iterator.\r\n\r\nI was inspired by Haskell when implementing this, as the language is lazy\r\nby default, but the compiler optimizes the cases where code is safe to be\r\ncalled strictly. Thats is called strictness analysis. Improved collections performance in common use cases >>> 1"
413,"This is the first attempt at creating a new `ConsoleIntegrationTestCase` class that allows users to create simple integration tests against shells.\r\n\r\n```php\r\npublic function testRoutesShell()\r\n{\r\n    $this->exec('routes check /users/index');\r\n    $this->assertExitCode(Shell::CODE_SUCCESS);\r\n}\r\n```\r\n\r\nThe `exec()` method also allows an array of ""answers"" to be fed to an interactive shell:\r\n\r\n```php\r\npublic function testRoutesShell()\r\n{\r\n    $this->exec('my_plugin.ama', [\r\n        'yes', // answer ""yes"" to first input request\r\n        'cats', // answer ""cats"" to second\r\n    ]);\r\n}\r\n```\r\n\r\nThe current assertions are:\r\n\r\n- assertExitCode: assert that the shell exited with the expected code\r\n- assertOutputContains: assert that stdout contains expected string\r\n- assertErrorContains: assert that stderr contains expected string\r\n\r\nOpen questions:\r\n\r\n1. What other assertion methods, if any, would be helpful?\r\n2. <del>Parsing argv from a string is hard because of quoting options. The current solution execs a basic script that just captures what PHP actually receives in `$argv`, which is obviously slower than a regexp solution. Any regexp experts out there willing to help with parsing? `ConsoleIntegrationTestCaseTest::testCommandStringToArgs()` contains a good example of what we expect from a string. </del> 3.next RFC ConsoleIntegrationTestCase >>> 1"
414,By having aliases and standalone shells we avoid having 'command' logic in the command runner. It also means that application developers can replace the default --version behavior if they want to.\r\n\r\nPart of #10716  Add version shell and --version >>> 1
415,"My database version (mysqld  Ver 10.2.6-MariaDB for osx10.12 on x86_64 (Homebrew)) reports the default value current_timestamp with parenthesis (like so: ``` `updated_at` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp()```) which confuses the CakePHP mysql driver to think it is a string and that leads to the following error when running tests: \r\n```\r\n2017-06-27 11:51:16 Error: Fixture creation for ""ad_campaigns"" failed ""SQLSTATE[42000]: Syntax error or access violation: 1067 Invalid default value for 'updated_at'""\r\nPHP Warning:  Fixture creation for ""ad_campaigns"" failed ""SQLSTATE[42000]: Syntax error or access violation: 1067 Invalid default value for 'updated_at'"" in /Users/kurre/mcn/nativeflow/Vendor/cakephp/cakephp/lib/Cake/TestSuite/Fixture/CakeTestFixture.php on line 244\r\n```\r\nThis patch will allow for the parenthesis and that fixes my issue (#10829)\r\n\r\nI am unsure how to write a test for this as it seems to be database version specific issue. Fix error when default value is reported as CURRENT_TIMESTAMP() by mysql - Fixes #10829 >>> 1"
416,This shell is a replacement for CommandListShell. It uses the provided CommandCollection to list the commands instead of scanning the application. This will allow it to play nicer with standalone applications.\r\n\r\nI've not yet fixed the hardcoded `cake` in the help output. But I'll be doing that in my next pull request for the new console libraries.\r\n\r\nRefs #10716 3.next - Add Help shell >>> 1
417,Add root command propagation and an interface for shells to hint that they need the command collection in the new runner.\r\n\r\nPresently the `HelpShell` will not work for applications using `ShellDispatcher`. I'm thinking of putting that requirement into the error messages HelpShell creates. Add root command propagation and CommandCollectionInterface >>> 1
418,"When an association is cleaned out by a beforeMarshal hook, the entity property should not be marked as dirty as the related entity has no properties to persist.\r\n\r\nRefs #10658 Make one()/many() more consistent with empty associations >>> 1"
0,Please consider the following before submitting a pull request:\n\nGuidelines for contributing: https://github.com/chartjs/Chart.js/blob/master/CONTRIBUTING.md\n\nExample of changes on an interactive website such as the following:\n- http://jsbin.com/\n- http://jsfiddle.net/\n- http://codepen.io/pen/\n- Premade template: http://codepen.io/pen?template=JXVYzq\n  …ement.\n\nAlso no need to check .getContext twice.\n This is a more declarative way to check if the context is a jQuery el… >>> 0
1,"These updates add more structure to code styling. Tests will now fail if the code does not match the style set forth by these guidelines. The official standards were created based off of the most commonly used styles in this project already.\n\nHopefully this will make code easier to read over time and will keep all styling uniform throughout the project.\n\nEventually these rules should be extended to all code in the project (tests, etc.) rather than just the src files.\n\nI did my best to make sure that no functionality was changed through the adaptation of this styling throughout the project, but I might have caused a bug somewhere so please make sure that my changes will not induce any unexpected behavior.\n ESLint Updates >>> 1"
2,"Build two different conditions, weather normal or reverse scale. \nIn original this condition surpressed the drawing at index=0 which is the inner tick.\nThis leaded to surpressing the drawing of the outest gridLine if the scale is reversed.\nIn every case the inner gridline shouldnot to be drawn. which in normal scale is at index =0 and in reverse mode the last found index (me.ticks.length-1).\n Fix for outer tick (gridLIne) not shown in radar chart when scale is reverse #3251 >>> 0"
3,"- new `options` parameter to acquire a chart, allowing to configure canvas and wrapper attributes\n- charts are now automatically deleted after each tests (ignored if `persistent` option is true)\n- fix unit tests that was using a global instance but was supposed to test a local chart\n- make sure that all charts are locally acquired and released (no more global instance)\n- Jasmine custom matchers are automatically exposed to all tests\n Tests cleanup >>> 1"
4,"Copy/pasting [our discussion](https://github.com/chartjs/Chart.js/issues/3289#issuecomment-247062315) with @etimberg 👍 \n\n> there is no plugin call for after rendering fully finished (ie only after the last frame of animation). afterDraw will do after each frame\n> I would accept a PR adding it (could call it at the same place the onComplete animation callback is called)\n\nSo here it is @etimberg !\n feat(completedDraw): adds ""completedDraw"" plugin events. Fixes #3289 >>> 0"
5,Addresses the issue presented in #3309. Pie and Doughnut charts will now render correctly when the corresponding component is clicked on the legend.\n\nHere is an example **before** the changes: http://codepen.io/zachpanz88/pen/yaOzJV\nHere is an example **with** the changes: http://codepen.io/zachpanz88/pen/mAEbdz\n Fix error where pie/doughnut charts did not render correctly with multiple datasets >>> 1
6,"Hello everyone,\n\nJust added more informations to the custom tooltip callback argument.\n\nIn order to know which data points are matching, I added a new property `dataPoints` to the tooltip object passed to the custom tooltip.\n\nThis property contains an `array[CustomTooltipDataPoint]` describes as following:\n#### CustomTooltipDataPoint\n\n| Name | Type | Description |\n| --- | --- | --- |\n| index | Number | Matching point index. |\n| datasetIndex | Number | Matching dataset index. |\n| xLabel | String | Matching label on X axis. |\n| yLabel | String | Matching label on Y axis. |\n| pointX | Number | X position of matching point. |\n| pointY | Number | Y position of matching point. |\n\nI also added a test case covering this new property, and a description inside the documentation under the Advanced External Tooltips section.\n\nPlease let me know if anything,\nThanks\n Custom tooltip: add data points infos >>> 1"
7,Addresses #3125.\n\nSkips `Infinity` datapoints in `getRightValue` to fix graph creation glitches resulting from 'infinite' datapoints.\n Skip Infinity datapoints in getRightValue >>> 0
8,Fixes #3292 \n## After Fix\n\n![after](https://cloud.githubusercontent.com/assets/6757853/18608635/ef0ea0ae-7cbc-11e6-8b5b-77efb273f465.png)\n Fix the legend drawing when `labels.usePointStyle` is true >>> 1
9,"When there was no tooltip, the size was too small because we accidentally subtracted the title bottom margin.\n## After Fix\n\n![tooltip after fix](https://cloud.githubusercontent.com/assets/6757853/18608836/0a6a41dc-7cc2-11e6-8280-0bd6c8c21235.png)\n Compute correct tooltip size when there is no title present >>> 1"
10,"Ensure that the hidden iframe is stretched vertically in order to detect height changes. Remove the classlist check/call since it was incorrectly spelled (should be classList), but also useless since the iframe has just been generated. Also remove the callback check: addResizeListener should never be called w/o a valid callback.\n\nUnit tests are coming in another PR!\n Make charts vertically responsive (#3105) >>> 1"
11,Experimenting pull request to publish a new release and have most of the @chartjs/maintainers involved in the release process. We can iterate on action items and approve the new release once tasks are done. Merging that PR will (should) trigger the new release (see the [release process](/chartjs/Chart.js/blob/master/MAINTAINING.md#releasing-a-new-version)).\r\n\r\nVersion: `2.3.0-rc.1`\r\n\r\n**Checklist:**\r\n- [x] Draft the release notes\r\n- [x] Update package.json version\r\n- [x] Publish the documentation\r\n Version 2.3.0 Release Candidate 1 >>> 1
12,Fixes #2602 \n## Image Before Change\n\n![before](https://cloud.githubusercontent.com/assets/6757853/18371271/f9afea8a-7600-11e6-8120-d42bdb5ed8e1.png)\n## Image After Change\n\n![after](https://cloud.githubusercontent.com/assets/6757853/18371252/d2ad8578-7600-11e6-9963-d0d6ca9f50fe.png)\n Better number -> string callback for the radial linear scale >>> 1
13,Fixes only on typos found.\n Minor fixes (proposal) >>> 1
14,"CanvasRenderingContext2D.fillText() accepts a fourth parameter called maxWidth that sets the maximum width of the drawn text, enforced by scaling the entire line.\n\nThis commit uses the title element's layout dimensions to set maxWidth and avoid overflow outside of the canvas.\n\nExample before and after screenshots:\n![image](https://cloud.githubusercontent.com/assets/8778207/18801592/933d77ea-81db-11e6-8471-cd4947bf1f62.png)\n![image](https://cloud.githubusercontent.com/assets/8778207/18801633/de782c32-81db-11e6-8bfd-9a3f8c269cca.png)\n\nI expect the same logic could be applied to axis labels, but it's not a problem I've run into.\n Set maxWidth during title draw to avoid overflow >>> 1"
15,"is this useful to anyone? my organization was interested to see if we could customize tooltip stroke to match an internal style guide. i **kinda** got it working but it feels strange and i'm leaning towards just submitting a custom tooltip instead.\n\nif this is thought to be useful, though, i can add the documentation and clean up / add more tests.\n\ncheers. happy friday and thanks for the awesome lib.\n\nupdate: removed shadow option. worked well with main rectangle of tooltip but had lots of issues with the overlapping caret + shadow + background opacity. perhaps someone else can tackle that later.\n feat(core.tooltip): add border and shadow options >>> 0"
16,Should have been only unit tests but then I realized that aspect ratio for none responsive chart was totally broken. So I decided to clean up the whole canvas initialization process and I hope to not have broken too many things :)\n\nThis PR includes:\n- correctly handles aspect ratio on chart creation (see unit tests for the many cases)\n- properly restore initial canvas render size and overridden style on destroy\n- fix default `aspectRatio` for radar chart and associated samples\n- move most of the canvas initialization in the `core.controller.js`\n- new `test/core.controller.tests.js` currently testing AR and responsiveness (see screenshot below)\n- new command switch to run specific tests (`gulp unittest --inputs=test/core.controller.tests.js`)\n\nMore details in commit messages\n\n![image](https://cloud.githubusercontent.com/assets/3874900/18792987/12c9f0d8-81b9-11e6-877a-b9cb298c3828.png)\n Fix aspect ratio and add responsive unit tests >>> 1
17,Issue #3341 minor documentation fixes in scale docs\n Fix 2 minor documentation issues in the scale documentation. #3341 >>> 1
18,Implementation of this [plugins proposal](https://simonbrunel.gitbooks.io/chartjs-proposals/content/Plugins.html).\r\n\r\nFixed #3335 \r\n Add inline plugin support to charts. >>> 1
19,"Since fixing the legendCallback before, i've noticed that the legend produced by it looks nothing like the one produced on the canvas. So i've improved the styling to make it look as close to the canvas one as possible. The font of the generated Legend can now also be set in the chart's options i.e. Options: { display: false, legend: { fontFamily: ""insert font family here"" } }, this will stop the Canvas legend from displaying but still set the font for the generated legend. then the user must call generateLegend() and apply the generated html to the inner html of an element as per usual.\n legendCallback improvement >>> 0"
20,"Responsiveness is currently based on the use of an iframe, however this method causes performance issues and could be troublesome when used with ad blockers. So make sure that the user is still able to create a chart without iframe when responsive is false.\n\nReplaces #3333, #3351 and #3352\n Inject iframe for responsive charts only >>> 1"
21,Fixes #3059 \n\nShould merge #3325 first so that the tests are enabled and then I will update the tests in this branch\n Fixes HTML legend string for polar area charts to match doughnut charts. >>> 1
22,"In order to simulate real-time chart updates (i.e. horizontal animation), it's necessary to distinguish a removed or added value from a simple update. The dataset controller now hooks array methods that alter the data array length to synchronize metadata accordingly. Also remove the duplicate calls of updateBezierControlPoints() for line and radar charts.\n\n**Examples:** http://playground.abysscorp.org/chartjs/livecharts/\n**Plunker:** http://plnkr.co/Imxwl9OQJuaMepLNy6ly\n**Related issues:** #1997 #3126\n Better animation when adding or removing data >>> 1"
23,nan Update documentation of plugins to add Select2 >>> 0
24,"Significant improvements to the tooltip and hover modes\n## New Configuration\n\nIn addition to the `hover.mode` and `tooltips.mode` settings, `hover.intersect` and `tooltips.intersect` have been added. The intersect settings are used to configure certain modes so that when items are shown can be controlled\n## New Modes\n\n| Mode | Description |\n| --- | --- |\n| point | Returns all items under the point |\n| index | renamed `'label'` mode |\n| nearest | Returns the nearest item. Using in conjunction with `intersect` set to true works well for points that are hidden behind |\n| x | returns items that intersect the x coordinate |\n| y | returns items that intersect the y coordinate |\n### Point Mode\n\n![point mode](https://cloud.githubusercontent.com/assets/6757853/19017242/24180eee-8801-11e6-974e-e919db19d076.gif)\n### Nearest Mode, Intersect On\n\n![nearest with intersect](https://cloud.githubusercontent.com/assets/6757853/19017253/61a04588-8801-11e6-91c0-89f2ce9bbb1d.gif)\n### Nearest Mode, No Intersect\n\n![nearest no intersect](https://cloud.githubusercontent.com/assets/6757853/19017261/a2915668-8801-11e6-8b3b-40c019588729.gif)\n## Deprecated Modes\n\nThe `'single'` and `'x-axis'` modes are deprecated.\nSingle mode has been replaced by `'point'` which returns all of the items that intersect with the cursor\nThe X axis mode has been replaced by the `index` mode coupled with setting `intersect` to true.\n Improve Tooltip and Hover Interaction >>> 1"
25,Also adds a new option `displayColors` that allows turning off the color boxes\n\nFixes #3273 and #3407 \n Display tooltip color boxes for all tooltips >>> 1
26,When trying to run `gulp unittestWatch` I got an error for a missing watchify dep. This PR fixes it 👍 \n Added watchify dev dep >>> 1
27,"This PR closes #3409.\n\nIf you specify min, max and stepSize then it should generate the ticks using just these values. \n Ticks min, max and stepSize fix. Closes #3409 >>> 1"
28,See #3416 \n Refactor tooltip draw function to extract drawBackground method >>> 1
29,Fixes #3029 \n Fix bubble chart tooltip callback to use correct labels >>> 1
30,nan Replaces Unicode character with HTML entity >>> 1
31,I added a private helper to properly merge the colors and simplify the code. \n\nFixes #3442 \n## After Fix\n\n![fixed tooltip](https://cloud.githubusercontent.com/assets/6757853/19208892/5f23d3e8-8cce-11e6-9996-d644eaa31899.png)\n Properly merge colors for the label colors in the tooltip >>> 1
32,"Fixes the weird caret position hover issue described in #3061 \n\nI took the opportunity to refactor the private methods and make them correctly private. While I was in the alignment function, I properly used the tooltip alignment options if specified rather than using the auto options. \n Fix/3061 >>> 1"
33,Proposal for a new template aiming to reduce the number of incomplete or unwanted issues (such as questions or support requests). Any re-wording suggestions are more than welcome :)\n\nGenerated from: https://www.talater.com/open-source-templates\n Update the GitHub issue template >>> 1
34,Now we simply replace the property even if they are arrays since the array merges are weird.\n\nFixes #3075 \n No longer merge arrays during the config merge >>> 1
35,Created the concept of tooltip modes. User defined modes are supported by changing `Chart.Tooltip.modes` map.\n\nThe default mode is 'average'. We may consider changing this if we think the old behaviour is really a bug that no one wanted. Currently. this will not change existing charts.\n\n```\ntooltips: {\n  positionMode: 'average'\n}\n```\n\nAlso implemented is the 'nearest' mode which places the tooltip on the element closest to the event that triggered the tooltip. Implementing this required removing some of the optimizations from `ChartController#eventHandler`. This may not be acceptable so there may need to be some refactoring.\n\nThis addresses #2056\n\nCC @chartjs/maintainers \n Configurable Tooltip Position Modes >>> 1
36,Fixes some minor performance issues I came across while doing profiling. The most notable fixes are the changes to the bar controllers and how rulers are generated to prevent multiple creations of the ruler objects.\n Bar chart performance improvements >>> 1
37,Allows the user to customize how the line fills. \n\nA new `fillMode` property configures this. It defaults to `'zero'` which matches the current behaviour. It can also be set to `'top'` or `'bottom'`.\n## Top Mode\n\n![screen shot 2016-10-11 at 9 20 15 pm](https://cloud.githubusercontent.com/assets/6757853/19294415/6294387c-8ffa-11e6-850b-13e0615cf826.png)\n## Bottom Mode\n\n![screen shot 2016-10-11 at 9 19 26 pm](https://cloud.githubusercontent.com/assets/6757853/19294417/69200d06-8ffa-11e6-89f9-10e924e49fc6.png)\n\nFixes #3176 \n New fill modes for lines >>> 1
38,This only applies when `intersect: false` is in the tooltip options. @simonbrunel noticed this behaviour. I added a test to ensure that the horizontal distance is what applies. \n\nOne question ... what do we do for horizontal bar charts? In that case we want to use the y direction. @chartjs/maintainers thoughts on this?\n Make index mode only work with the horizontal distance to an element  >>> 1
39,"Add support for creating a chart from the canvas id and prevent exceptions, at construction time, when the given item doesn't provide a valid CanvasRenderingContext2D or when the getContext API is not accessible (e.g. undefined by add-ons to prevent fingerprinting). New jasmine matcher to verify chart validity.\n\nEnhances #2807\nReplaces #3303\n Enhance context acquisition on chart creation >>> 1"
40,Adds eslint rules for test files so that they follow the same standards as source files which helps to increase overall code style\n Extend eslint to test files >>> 1
41,"With this PR, you can now reset the chart to it's state before the initial animation takes place. Fixes #235 \n\n``` javascript\nchart.reset();\n```\n Add reset method to chart prototype >>> 1"
42,"You can now add padding inside the chart. This helps with charts that don't have displayed scales, etc. Padding can be specified for all sides of the chart (left, top, right, and bottom).\n## Configuration\n\nThis adds the same padding on all sides\n\n``` javascript\noptions: {\n  layout: {\n    padding: 10\n  }\n}\n```\n\nThis version allows different padding on each side\n\n``` javascript\noptions: {\n  layout: {\n    padding: {\n      // Any unspecified dimensions are assumed to be 0\n      left: 10,\n      top: 5\n    }\n  }\n}\n```\n\nFixes #3332, #3257 \nReplaces #3349\n Layout service now supports configurable padding >>> 1"
43,"Useful for custom tooltips and requested in #2944 and #2636 \n\nThe new properties are `caretX` and `caretY`. Together, these identify the point on the graph where the tip of the caret of the tooltip would be. Essentially this is the raw position of the tooltip before any alignment is considered.\n\nThe custom tooltip sample was updated as well to use the new properties.\n Add new properties for the caretX,caretY point of a tooltip.  >>> 1"
44,"I added a new helper, `samples/chartColors.js` that defines the colors that match the docs. I updated all of the samples to use these colours.\n\nI improved sample organization, with common samples grouped into folders. I added new samples that allow comparing different settings. For example, `samples/tooltips/interaction-modes.html` creates a number of charts that allow the user to compare the different interaction mode settings.\n Sample File Updates >>> 1"
45,"Adds a function, `itemFilter` to the tooltip config. If this function is specified, it is used to filter the items that show in the tooltip.\n\nI added a test to cover this case.\n\nFixes #1889 \n Add a way to filter items in the tooltip >>> 1"
46,Adding a single line to the docs to document autoSkipPadding.\n Fixes issue #3490: Tick documentation is missing autoSkipPadding >>> 1
47,I have a script which identifies potentially misspelled tokens. I manually select replacements. This request can be folded / reorganized if necessary.\n Spelling fixes >>> 1
48,"[Code Climate builds](https://codeclimate.com/github/chartjs/Chart.js/builds) are broken since the ESLint rules update (#3308) because CC (with no explicit channel config) is currently based on [ESLint version 1.10.3](https://docs.codeclimate.com/docs/eslint) which doesn't support some of the introduced rules (e.g. extended `no-console`, `func-names`, etc.)\n Bump ESLint to v3.x (gulp and Code Climate) >>> 1"
49,"When the iframe is attached to the DOM, its content is reloaded (invaliding the resize listener) so make sure to install the handler after the iframe is loaded. Optimize resize events by throttling resize process until the next animation frame. Rewrite the unit test ""waitForResize"" method, the previous one (timeout) was too weak and most tests was failing on FF.\n\nFixes #3521\n Fix iframe resize handler when re-attached to DOM >>> 1"
50,"When an event triggers an update while the bufferedUpdate state is true, we need to do that render with priority over any other renders that take place for animations and tooltips. \n\n@simonbrunel this fixes the animation issue you noticed.\n When a buffered update occurs, a normal render must be triggered >>> 1"
51,created fix for #3528. \n\nDisplay the correct tooltip Label when `datasets data[] array` contains null values.\n Display correct tooltip labels when data contains null values >>> 1
52,"When legend is disabled (i.e. `{options: {legend: false}}`), `me.legend` is null. Add the same test on `me.tooltip` even if the tooltip object is always created in case of `{options: {tooltips: false}}`. Fix the event handler when legend is disabled >>> 1"
53,The tooltip 'X' and 'Y' interaction modes now use the `intersect` setting. I added tests covering these modes and the use of the setting. I updated the interaction modes setting accordingly.\r\n\r\n X and Y interaction modes now use the intersect option >>> 1
54,"Thank you for creating chart.js, this fixes issue #3525.  bug #3525, added default borderWidth for stacked bars >>> 0"
55,"In many cases, the canvas render size is changed by the lib, causing the state stack to be discarded, meaning that we can't use `save()` and `restore()` to release the context with its initial state (i.e. before creating the chart). Since we don't need (want) to manually save / restore the context initial state, simply make sure to reset it to the default state to give a fresh context back to the user. That also means we don't need to revert the scale when the pixel device ratio is not 1.\r\n\r\nFixes #3561 Fix context state restoration on destroy >>> 1"
56,"Code changes, documentation update and sample for #2643.\r\n\r\nJSFiddle: https://jsfiddle.net/Lvj2qnrp/1/\r\n(same as samples/bar/bar-stacked-group.html) Group stacked bar charts (#2643) >>> 1"
57,"This PR adds a new legend option for filtering items out of the chart legend.\r\n\r\n```javascript\r\noptions: {\r\n  legend: {\r\n    labels: {\r\n      filter: function(legendItem, chartData) {\r\n        // return false to hide the label\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nResolves #3189  Add a function to filter items out of the legend >>> 1"
58,"The retinaScale helper now enforces the display size to the correct values because if no style has been set on the canvas, the render size is used as display size, making the chart bigger (or smaller) when deviceAspectRatio is different of 1.\r\n\r\nNew unit tests:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/3874900/20226743/9643dcf6-a849-11e6-97a0-4efd5237e63a.png)\r\n\r\nFixes #3575  Fix retina scale when display size is implicit >>> 1"
59,"Milestone: [2.4.0](/chartjs/Chart.js/milestone/10)\r\nMerged PRs: [47](/chartjs/Chart.js/pulls?q=is:pr+is:closed+milestone:""Version+2.4"")\r\n\r\n**Checklist:**\r\n- [x] Draft the release notes\r\n- [x] Update package.json version\r\n- [x] Publish the documentation (@nnnick)\r\n- [x] Fix #3561\r\n- [x] Fix #3575\r\n\r\nSee the [release process](/chartjs/Chart.js/blob/master/MAINTAINING.md#releasing-a-new-version) for details.\r\n\r\ncc @chartjs/maintainers \r\n Version 2.4.0 >>> 1"
60,Fixes #3570 remove unused cancel animation frame method >>> 1
61,"Hello! This adds a new point style, `rectRounded`, and updates the docs that mention the valid values of `pointStyle`.\n Add rectRounded point style >>> 1"
62,Fixes #3591 \r\n use correct option for setting tension on radar charts >>> 1
63,"The Display Formats anchor link for Scales was broken. To fix, I modified the header level of the display formats section to generate a link.\r\n Fixing broken ""Display Formats"" link >>> 0"
64,This fixes a link for `progress-bar.html`.\r\n Fix link in 01-Chart-Configuration.md >>> 1
65,Several plugins were recently renamed.\r\n\r\nAdds a new plugin: [https://github.com/compwright/chartjs-plugin-draggable](chartjs-plugin-draggable.js) Update plugin names >>> 1
66,labelOpts.padding is added to the y location when creating the first column. This fix also adds padding when a subsequent column is created in the legend.\r\n Fixed vertical alignment in legend text (#3387) >>> 1
67,The user can now call `updateConfig(partialConfig)` to update the config of a chart.\r\n\r\nThe following is currently supported\r\n* updating options\r\n* changing axis types\r\n* updating plugin options\r\n\r\nThe following is **not** currently supported\r\n* changing the chart type\r\n\r\nResolves #2060  Allow updating the config of a chart at runtime >>> 1
68,"A lot of cleanup of the radial scale. I moved a lot of functionality into private functions.\r\n\r\n## Issues Fixed\r\n* When the scale was user for the polar area graph, the scale didn't use the full available area because it tried to fit the point labels (used for radar chart) even though non existed. \r\n* The point labels can now have multiple lines of text. Resolves #3225 \r\n\r\n![screen shot 2016-11-19 at 11 02 36 pm](https://cloud.githubusercontent.com/assets/6757853/20460303/b7820bd6-aeac-11e6-8548-614449478eb8.png)\r\n\r\n## Testing Done\r\n* Unit tests run\r\n* Tested the polar area and radar charts manually\r\n\r\n## Changed Behaviour\r\n* the `getIndexAngle` method no longer subtracts 90 degrees since we added it back almost all of the time.  Improve radial scale >>> 1"
69,"As title states, create a fix for #3618.\r\n\r\n**summary of changes:**\r\n\r\n- Created a new helper method to map index into sliced array when min is defined\r\n- Added test for helper method\r\n- Added integration test to trigger element to ensure correct index Fixed tooltip labelling on Bar Chart when min is defined (#3618) >>> 1"
70,"This essentially fixes the issues of #2879 and #3354 but more testing is needed and this code needs some cleanup. I also don't think the current code works in all cases. When `offsetGridlines` is true, I think padding can be incorrectly calculated on the left and right sides\r\n\r\nWe should not merge this until #3508, #3553, and #3554 are merged start fixing rotated labels >>> 1"
71,"To correctly fix the issue, the default padding was changed from 0 to 10. This change caused all of the test changes since the width of a vertical scale was lowered by 10px.\r\n\r\n## After Fix\r\n![screen shot 2016-11-04 at 8 42 37 pm](https://cloud.githubusercontent.com/assets/6757853/20026330/5f3d45aa-a2cf-11e6-991a-23c9dca09f5e.png)\r\n\r\nFixes #3141  Properly use the ticks.padding option >>> 1"
72,Don't arbitrarily force the size to change.\r\n\r\nFixes #3412 \r\n\r\nWe should merge #3553 and #3508 first then I will update the tests because there will be merge conflicts. Labels can get bigger when the 2nd fit happens. >>> 1
73,Upgrading this dependency cuts gulp build times by ~2 seconds Upgraded dependency gulp-uglify to 2.0.0 >>> 1
74,"Fixed tooltip labelling on Bar chart when min option was set.\r\nIn addition, Fixed the tooltip label problem when 'xLabel' , 'yLabel' was set.\r\n\r\nResolves #3618\r\n\r\n\r\n## Before\r\n![issue3618-before](https://cloud.githubusercontent.com/assets/22541770/20648780/e8c0d788-b4ea-11e6-945d-d7c4411797a2.png)\r\nAll tooltip labels should be `May`.\r\nhttps://jsfiddle.net/KoyoSE/bvo2bc5f/1/\r\n\r\n## After\r\n![issue3618-after](https://cloud.githubusercontent.com/assets/22541770/20648781/f14498cc-b4ea-11e6-9aa8-7b0661547a2d.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/6f3hbLr1/2/\r\n\r\n## When added plugin\r\n![issue3618-plugin](https://cloud.githubusercontent.com/assets/22541770/20648782/f7331a06-b4ea-11e6-9524-9c0ff64a7010.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/6f3hbLr1/3/ Fix : Tooltip label for category scale. >>> 1"
75,"Fixed a mistake in calculating the width of the bar when setting the `min` `max` option.\r\nThe getRuler function performs complicated calculations, but as a result of checking it can be made more simple.\r\n\r\nAlso fixed the issue that `barPercentage` is disabled when stacked scale.\r\n\r\nResolves #3589\r\n\r\n\r\n## Testing\r\nI updated the unit test.\r\nPlease delete it if it is unnecessary.\r\n\r\n## Before\r\n![issue3689_li](https://cloud.githubusercontent.com/assets/22541770/20645632/0264b714-b49f-11e6-9ccc-a4bd62bb8647.jpg)\r\n\r\nhttps://jsfiddle.net/zf8gt2o6/\r\n\r\n\r\n## After\r\n![issue3689-fix_li](https://cloud.githubusercontent.com/assets/22541770/20645638/2bad5eb4-b49f-11e6-85e8-b2aae7ce241e.jpg)\r\n\r\nhttps://jsfiddle.net/0eaf0vj3/\r\n\r\n\r\n## When added plugin\r\n![issue3689-fix-plugin](https://cloud.githubusercontent.com/assets/22541770/20645642/36030fd0-b49f-11e6-81cb-aab14d18f483.png)\r\n\r\nhttps://jsfiddle.net/ww2dzrn5/ Fix miscalculation of Bar width. >>> 1"
76,"This changes prevents the `resize` method from notifying plugins if it is a silent resize. A silent resize occurs during startup and we do not want plugins to do anything here because the chart is not set up. In the demo linked below, the legend will always be present at the start regardless of the size. I think the correct solution is to have the plugin in this case use the `beforeInit` call to do this setup.\r\n\r\nAn alternative solution was to block `update` calls when the resize notification is sent from a silent resize. This still has the possibility that the plugin would try and access uninitialized data objects.\r\n\r\nAnother alternative solution would be to have the resize plugin notification also send the `silent` parameter. This forces plugins to handle this case, which is non ideal.\r\n\r\nThis allows us to provide a fix for #1933 \r\nDemo: http://jsfiddle.net/aLLzv3vL/3/ Do not notify plugins when a silent resize occurs >>> 1"
77,When two adjacent points in a line were at the same X value the monotone cubic interpolation would break because the differential slope would be `NaN`. This change adds logic to detect this and correctly set the `deltaK` property.\r\n\r\nResolves #3408\r\n\r\n## Testing\r\nI updated the unit test to test this case. I manually tested that the fiddle from the bug is solved including in cases where the points at the same X are at different Y values\r\n\r\n## Before\r\n![screen shot 2016-11-26 at 12 33 39 pm](https://cloud.githubusercontent.com/assets/6757853/20642107/9b2158de-b3d4-11e6-8bd2-3819ceabe1a2.png)\r\n\r\n## After\r\n![screen shot 2016-11-26 at 12 26 55 pm](https://cloud.githubusercontent.com/assets/6757853/20642093/571130ec-b3d4-11e6-82bb-b78f35b23f35.png)\r\n\r\n### Testing with 2nd point above first one but at the same X\r\n![screen shot 2016-11-26 at 12 27 10 pm](https://cloud.githubusercontent.com/assets/6757853/20642095/5bb4a192-b3d4-11e6-834f-9a564557b30e.png)\r\n\r\n### Testing with 2nd point below the second one but at the same X\r\n![screen shot 2016-11-26 at 12 27 22 pm](https://cloud.githubusercontent.com/assets/6757853/20642099/7421fc52-b3d4-11e6-9994-0eaafa753aef.png)\r\n\r\n Fix monotone cubic interpolation when two adjacent points are at the same X >>> 1
78,"I tried implementing clipping chart area.\r\nI would be pleased if you like it.\r\n\r\nResolves #3506, #3491, #2873\r\n\r\n## Bar and Line chart.\r\n### Before\r\n![non clip bar line](https://cloud.githubusercontent.com/assets/22541770/20744270/f40c6148-b715-11e6-85dc-8e3ba285b571.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/n7ben0fx/\r\n### After\r\n![clip bar line](https://cloud.githubusercontent.com/assets/22541770/20744275/fa63a8bc-b715-11e6-8439-3965ad51074a.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/98x7ny4b/\r\n(Note: jsfeddle takes time to execute.This is because I put the built chart.js in the html area.)\r\nLine chart: The point fades out when going out.\r\n\r\n\r\n## Radar and Scatter chart.\r\n### Before\r\n![non clip radar scatter](https://cloud.githubusercontent.com/assets/22541770/20744279/03825d12-b716-11e6-9b15-af68accc23ea.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/fwt4bxcc/\r\n (Do Radar chart have to consider the drawing of the points below min?)\r\n### After\r\n![clip radar scatter](https://cloud.githubusercontent.com/assets/22541770/20744286/0a75d284-b716-11e6-8238-bbd7b7ae3905.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/7v3ehcqt/\r\n(Note: jsfeddle takes time to execute.This is because I put the built chart.js in the html area.)\r\nRadar chart: It works the same before. Clipping is not done.\r\nThe Radar chart, the following error is displayed on the console.\r\n'Unable to get property 'xAxes' of undefined or null reference'\r\nThis is solved by removing #3620.\r\n\r\n## Other\r\n1. It may be better to display the tool tip position in the chart area.\r\n1. Drawing Bezier curve is slightly strange. Especially outside the chart area.\r\n\r\n\r\nchart.js version v2.4.0 Implement clipping >>> 1"
79,"I changed draw function for bar.\r\nThank you for your review.\r\n\r\n## Summary\r\n1. `Chart.elements.Rectangle.draw` function supports both horizontal and vertical bar.\r\n2. Corrected bar position at minus.\r\n3. Adjust bar size when `borderWidth` is set.\r\n4. Adjust bar size when `borderSkipped` is set.\r\n5. Adjust `borderWidth` with value near 0(base).\r\n6. Update unit test. \r\n\r\n## `borderWidth` = non, 5 ,20\r\n### before\r\n![befor](https://cloud.githubusercontent.com/assets/22541770/20912154/81f8b308-bba8-11e6-9440-c9610f7b6182.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/k8b3mrb4/\r\n### after\r\n![after](https://cloud.githubusercontent.com/assets/22541770/20912156/893fa6a8-bba8-11e6-9ac6-c60f8f205f74.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/ovf9w8dy/\r\n\r\n## `borderSkipped` = 'right'\r\n### before\r\n![before right](https://cloud.githubusercontent.com/assets/22541770/20912159/91acc4ce-bba8-11e6-8386-b13d1599594d.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/944rde4k/\r\n### after\r\n![after right](https://cloud.githubusercontent.com/assets/22541770/20912165/9626c446-bba8-11e6-8f97-4ad2d1c97a42.png)\r\n\r\nhttps://jsfiddle.net/KoyoSE/1L9jw9xz/\r\n Fix bar draw issue with `borderWidth`. >>> 1"
80,Fixes an infinite loop that occurred when generating logarithmic ticks.\r\nResolves #3381 \r\n\r\nAlso fixes a newly introduced (v2.5.0 dev) bug when the tick marks are not drawn but the labels are. The labels would be offset as if the tick marks were there which was incorrect.  Prevent infinite loop while generating logarithmic ticks >>> 1
81,"This PR fixes an error where if  `generationOptions.max` or `generationOptions.min` is calculated to an almost whole number (ie 9.999) then we display the number as 10 on the scale,  but calculating an evenly spaced  scale is not possible (eg stepSize is 2) as the check fails.  \r\n\r\nBy applying an almost whole check we are able to mitigate for FP arithmetic inconsistencies that would cause this behaviour\r\n \r\nPlease consider the following before submitting a pull request:\r\n\r\nGuidelines for contributing: https://github.com/chartjs/Chart.js/blob/master/CONTRIBUTING.md\r\n\r\nExample of changes on an interactive website such as the following:\r\n- http://jsbin.com/\r\n- http://jsfiddle.net/\r\n- http://codepen.io/pen/\r\n- Premade template: http://codepen.io/pen?template=JXVYzq\r\n Fix bug when calculating if steps fit into scale as a whole number th… >>> 1"
82,I fixed a simple mistake.\r\nShall I create jsfiddle?\r\n\r\nJust changed sie to size. Fix : The x axis label for position = 'top' is not displayed. >>> 1
83,"When a logarithmic scale is used for as the vertical axis of a stacked bar chart, the base value of 0 caused a problem. The solution is to add an item to the scale interface `getBaseValue` that gets the value in dataspace coordinates that corresponds to the pixel returned by `getBasePixel`. \r\n\r\nI added a test to cover this case.\r\n\r\nResolves #3585 \r\n fix stacked bars on logarithmic axes >>> 1"
84,### line-stacked-area.html\r\nChanged j-query code to javascript.\r\n### step-size.html\r\nFixed button not working.\r\n\r\nI checked the samples. \r\nIf need jsfiddle I will prepare.\r\nPlease do not hesitate to say.  Fix : samples (line-stacked-area.html & step-size.html) >>> 1
85,First revision of the implementation of https://simonbrunel.gitbooks.io/chartjs-proposals/content/Platform.html#event-management\r\n\r\nThe BrowserPlatform class is attached to each chart to handle browser specific logic. This can be replaced on a different platform. BrowserPlatform implements IPlatform (the general interface that all platform implementations must support). `Chart.Platform` is the constructor for the platform object that is attached to the chart instance.\r\n\r\nPlugins are notified about the event using the `onEvent` call. The legend plugin was converted to use onEvent instead of the older private `handleEvent` method.\r\nWrote test to check that plugins are notified about events Refactoring to put browser specific code in a new class >>> 1
86,"Instead of _only_ using data.labels, data.xLabels, or data.yLabels for category scales, also allow users to set labels on the scale config itself.\r\n\r\nExample config:\r\n\r\n            {\r\n              type: 'category',\r\n              position: 'right',\r\n              \r\n              labels: [""Different1"", ""Different2""]\r\n            }\r\n\r\nI attempted to update the documentation as specified in `CONTRIBUTING.md`, but I am not sure if it is understandable enough.\r\n\r\nI've also left the changed function as a one-liner. I'm not sure if it is still okay, or if it should be expanded due to length now.\r\n\r\nThis implements/fixes #3725\r\n\r\nExample functional codepen copy of the issue:\r\n\r\nhttp://codepen.io/albinodrought/pen/ObGVmY Allow category scales to use locally-set labels >>> 0"
87,The following problem was solved.\r\n1.Rotation of scale label when scale is right\r\n2.Scale position at rotation when scale is top.\r\n\r\n## Before\r\n![2016-12-26 15 _li](https://cloud.githubusercontent.com/assets/22541770/21480691/642ab900-cb9a-11e6-834e-c10b02231da1.jpg)\r\n[jsFiddle](https://jsfiddle.net/KoyoSE/smkfz241/)\r\n## After\r\n![2016-12-26 18](https://cloud.githubusercontent.com/assets/22541770/21480693/68595c34-cb9a-11e6-9666-7f145edc8559.png)\r\n[jsFiddle](https://jsfiddle.net/KoyoSE/cejnnLj9/)\r\n Fix : Scale label display at top and right. >>> 1
88,"Removed old doc files except for the comparison table. \r\n\r\nFixed #3547, #3490, #3118, #2043, #3274, #2362, #2325, #2386, #3299, #3237, #3549  while at it. Update the docs structure/content to use GitBook >>> 1"
89,Documentation points to non-existent header ticks configuration. Correct anchor link >>> 1
90,"please review when you get a chance!  Fixed HorizontalBar: stacked axis, displays NaN when all legends >>> 1"
91,"When using Chart.js with AngularJS, occasionally - what I believe is just changing scope causes an error to be thrown,\r\n\r\n```\r\nChart.js:4192 Uncaught TypeError: Cannot read property 'draw' of null\r\n    at Chart.Controller.<anonymous> (Chart.js:4192)\r\n    at Object.helpers.each (Chart.js:4808)\r\n    at Chart.Controller.draw (Chart.js:4189)\r\n    at ChartElement.animation.render (Chart.js:4154)\r\n    at Object.startDigest (Chart.js:3694)\r\n    at Chart.js:3667\r\n(anonymous) @ Chart.js:4192\r\nhelpers.each @ Chart.js:4808\r\ndraw @ Chart.js:4189\r\nanimation.render @ Chart.js:4154\r\nstartDigest @ Chart.js:3694\r\n(anonymous) @ Chart.js:3667\r\n2017-01-13 18:04:48.577 ```\r\n\r\nThis simple check stops those errors from being thrown. Sometimes controller is null and has no property 'draw' which throws … >>> 0"
92,"Improve performance of TimeScale.buildTicks.\r\n\r\nInstead of creating `me.scaleSizeInUnits` moments and throwing the vast majority away, only create what we plan to use. 10x speedup in my use case.\r\n\r\nFixes #3208. Only generate ticks we care about >>> 1"
93,"Move base platform definition and logic in `src/platform/platform.js` and simplify the browser -> Chart.js event mapping by listing only different naming then fallback to the native type.\r\n\r\nReplace `createEvent` by `add/removeEventListener` methods which dispatch Chart.js IEvent objects instead of native events. Move `add/removeResizeListener` implementation into the DOM platform which is now accessible via `platform.add/removeEventListener(chart, 'resize', listener)`.\r\n\r\nFinally, remove `bindEvent` and `unbindEvent` from the helpers since the implementation is specific to the chart controller (and should be private).\r\n\r\nIs related to #3718\r\n\r\n> CodeClimate complains about a new @TODO comment, but also for some mysterious duplicated code!\r\n Platform event API abstraction >>> 1"
94,"Fixes #3763  when the cutoutPercentage is 0, the inner radius should be 0 >>> 1"
95,"`labelPoints` controls the visibility of angle lines and point labels\r\n`gridLines.circular` toggles how the radial lines are drawn. When false, straight lines are drawn. When true, circular lines are drawn.\r\n\r\nAllows creating charts that look like:\r\n![circular radar](https://cloud.githubusercontent.com/assets/6757853/21831318/e9dace14-d770-11e6-8c99-93f257a143b0.png)\r\n\r\nResolves #3082 \r\n\r\n## API Change\r\nThis is technically a breaking API change since the old setting, `lineArc` is no longer used. I don't know if this will cause problems since I think this setting is only used internally and it is not well documented. split radial scale lineArc setting into two settings >>> 1"
96,"Resolves #3388 \r\n\r\nThe layout system now supports an `order` property which is used to align left and right boxes. A higher value is further away from the chart. This should not change the axis positions, and I did not observe any differences. It may be prudent to wait until v2.6 to include this.\r\n\r\n## After Fix\r\n![screen shot 2017-01-20 at 5 30 30 pm](https://cloud.githubusercontent.com/assets/6757853/22167718/35130fb4-df36-11e6-842d-a3d0f600c4e7.png)\r\n\r\n Layout service supports configuring box order >>> 1"
97,Ignore `.gitignore` (and more) from Bower packages.\r\n\r\nFixes #3801 Ignore .gitignore (and more) from Bower packages >>> 1
98,"With the changes to `update` to update configs, these steps are no longer needed as part of the chart initialization since `update` will take care of it. We still need to build scales though, since they are needed in `afterInit` and `update` currently cannot update axes. remove unnecessary extra init steps >>> 1"
99,"Make all `before` hooks cancellable (except `beforeInit`), meaning that if any plugin return explicitly `false`, the current action is not performed. Ensure that `init` hooks are called before `update` hooks and add associated calling order unit tests.  Deprecate `Chart.PluginBase` in favor of `IPlugin` (no more an inheritable class) and document plugin hooks (also rename `extension` by `hook`).\r\n\r\nDepends on #3818 Plugin hooks and jsdoc enhancements >>> 1"
100,The input labels&data is parsed and converted into moments in `determineDataLimits`; reuse these moments in `buildLabelDiffs` instead of duplicating the work. Parse labels once when building time scale >>> 1
101,Deprecates `Chart.Controller` (and the nested `chart` property) by gathering `chart` and `chart.chart` properties at the same level (`chart.chart` now being an alias to `chart`). `chartInstance` variables have been renamed to `chart` since there is no more distinction. A new test file has been added to keep track of deprecations (`test/global.deprecations.tests.js`).\r\n\r\nFixes #2481 Chart.Controller deprecation >>> 1
102,"Milestone: [2.5.0](/chartjs/Chart.js/milestone/11)\r\nMerged PRs: [50](/chartjs/Chart.js/pulls?q=is:pr+is:closed+milestone:""Version+2.5"")\r\n\r\n**Checklist:**\r\n- [x] Draft the release notes\r\n- [x] Update package.json version\r\n- [x] Publish the documentation (@nnnick)\r\n\r\nSee the [release process](/chartjs/Chart.js/blob/master/MAINTAINING.md#releasing-a-new-version) for details.\r\n\r\ncc @chartjs/maintainers  Version 2.5.0 >>> 1"
103,"Added a test to cover this case.\r\nResolves #3729  When the dataset label is not defined, the tooltip label should have no ':' >>> 1"
104,"As title states, added patch for Issue #3746 with test cases. Please review when you have a chance! @etimberg or @simonbrunel. \r\n\r\n Added Patch to update tooltip only when active element has changed >>> 1"
105,"This adds the possibility to draw a tooltip border, fixing the problems with the caret in this (closed) issue: https://github.com/chartjs/Chart.js/pull/3221\r\n\r\n<img width=""1394"" alt=""screen shot 2017-02-05 at 11 15 20 pm"" src=""https://cloud.githubusercontent.com/assets/5921389/22630357/fefad144-ebf8-11e6-8a04-c53056d305be.png"">\r\n Feature/tooltip border >>> 1"
106,"Hi,\r\n\r\nWe have screens in our application that have 20+ charts and on these pages we've noticed severe performance issues with chart.js initialization times. The whole page blocks for a noticeable amount of time. This issue has already been reported here: #2024.\r\n\r\nThis pull request fixes the issue by using the element-resize-detector library. We're using this library for other purposes in our app so it seemed like the easiest fix. Using this library we've noticed a 10-20x improvement in initialization time.\r\n\r\nHere are the jsfiddles for demonstrating this issue. Click the button a few times to render 20 charts.\r\n\r\nWithout this fix (2.4.0): http://jsfiddle.net/236rzj1h/1/\r\nWith this fix (master + fix): http://jsfiddle.net/qrzdLruu/\r\n\r\nThe tests are ok, but `waitForResize` probably could use some more work.\r\n\r\nI'd love to hear what you think about this.\r\n\r\nKind regards,\r\nAmir Use element-resize-detect for resize detection >>> 0"
107,Resolves #3881 \r\n\r\nTested with the Chart.Bands.js plugin. Fix issue with how Chart.PluginBase is defined >>> 1
108,"This PR contains 2 changes.\r\n\r\n1. Removed the references to Chart.Controller from the JSDoc comments\r\n2. When creating axes, we now define the default position for axes. If the position option of the axis does not match the actual location, we change the position. This prevents incorrectly formed charts and has been heavily requested.\r\n\r\nExample:\r\n```javascript\r\nlet options = {\r\n  scales: {\r\n    xAxes: [{\r\n      type: 'linear',\r\n      position: 'right' // will be changed to 'bottom' since 'right' is not an x axis\r\n    }],\r\n    yScales: [{\r\n      position: 'bottom' // will be changed to 'left' since 'bottom' is not a y axis position\r\n    }]\r\n  }\r\n};\r\n``` Axis default positions >>> 1"
109,"Chart data can now be entirely replaced using `chart.data = {...}` thanks to the new property setter (instead of using `chart.config.data = {}`). Also update the documentation, as suggested by @ldaguise and @kennethkalmer, with a note about versions prior 2.6.\r\n\r\nFixes #3891 #3785 Add chart data property setter and unit tests >>> 1"
110,With fix - https://jsfiddle.net/5xo8rp9f/3/\r\n\r\n(https://github.com/chartjs/Chart.js/issues/3849#issue-204514929 has jsfiddle before fix and before the bug) #3849 - Stack bars in z dimension >>> 1
111,Complete work started in https://github.com/chartjs/Chart.js/tree/time-scale-improvements (#3673) to convert `scale.time` to internally use unix timestamps instead of moments. Improves performance and reduces complexity.  Time scale improvements >>> 1
112,"This PR adds the following plugin hooks:\r\n- `beforeDatasetUpdate` (cancelable)\r\n- `afterDatasetUpdate`\r\n- `beforeDatasetDraw` (cancelable)\r\n- `afterDatasetDraw`\r\n\r\nIn order to take full advantage of the new plugin hooks called before and after a dataset is drawn, all drawing operations must happen on stable meta data, so make sure that transitions are performed before.\r\n\r\nRelates to #2380 Add new dataset update and draw plugin hooks >>> 1"
113,Fix the issue where built-in auto skip feature wasn't working properly as stated in the title. Please review.  Fixed built-in auto skip caused by previous commit (#3904) >>> 1
114,"Hi!\r\n\r\nI'm using Chart.js in my project and I also use server-side rendering. And if my page contains Chart.js rendering is failing (expectedly). I tried to wrap usage of Chart.js with something like:\r\n\r\n```javascript\r\nif (typeof window !== 'undefined') {\r\n // use Chart.js\r\n}\r\n```\r\n\r\nBut it still fails because Chart.js has been imported before and has tried to use `window` already.\r\n\r\nI came up with the fix in PR, please take a look.\r\n\r\nHere are some links to issues with the problem similar to mine:\r\n\r\n* https://github.com/gor181/react-chartjs-2/issues/43\r\n* https://github.com/reactjs/react-chartjs/issues/57\r\n\r\nP.S. Thanks for your great work! Avoid fails from serverside renderings >>> 1"
115,Adding a few additional tooltip callback events which I think are useful. Add support for new tooltip callback events >>> 0
116,Resolves #3929  Fix use of reserved keyword as a parameter name >>> 1
117,"Animation callbacks now receives `animationObject` directly with a reference on the associated chart (`animation.chart`), which deprecates `animation.animationObject` and `animation.chartInstance`. Also fix missing `onComplete` animation argument and update documentation about `onComplete` called with `animation: null` if animation is disabled.\r\n\r\nFixes #3781 #3911  Flatten animation object and fix callbacks >>> 1"
118,"If a value is set on the model after `pivot()` has been called, the view wasn't initialized and the animation started from 0. Now, `_start` and incomplete `_view` are initialized to the model value during the transition (no initial implicit transition).\r\n\r\nAlso remove exception handling when animating a string (color), which is faster when string are not valid colors (e.g. tooltip position). It requires to update `chartjs-color` to version 2.1.0. Handle incoming model values on element transition >>> 1"
119,Fixes #3036 Added a `maxBarThickness` setting for bar charts xAxis >>> 1
120,"Prevent attempt to remove the legend or title layout items if they haven't been created but also check if the item to remove is registered with the layout manager to avoid removing the wrong box `splice(-1, 1)`. Add ids to the legend and title plugins to allow to fully disable them:\r\n\r\n```javascript\r\noptions: {\r\n    plugins: {\r\n        legend: false,\r\n        title: false\r\n    }\r\n}\r\n```\r\n\r\nFixes #4140 Fix shorthand `legend: false` and `title: false` >>> 1"
121,"`karma.conf.ci.js` has been merged into `karma.conf.js` for local testing consistency: `gulp unittestWatch` has been replaced by `gulp unittest --watch` and thus use exactly the same config file. Upgrade to latest jasmine and karma packages and remove deprecated `gulp-karma` dependency (directly use `karma.Server` in gulp).\r\n\r\nSplit `test/mockContext.js` into smaller `test/jasmine.*` modules to make easier unit tests maintenance and finally, move all `*.test.js` files under the `test/specs` folder.\r\n Cleanup and upgrade unit tests environment >>> 1"
122,"Truncation up to caretSize pixels could happen if label text produced tooltip element with size width:\r\n* left side tooltip: `width < x` and `width > x - caretSize`\r\n* right side tooltip: `width < chartWidth - x` and `width > chartWidth - x - caretSize`\r\n\r\nDefault caretSize = 5, so with default configuration truncation up to 5 pixels could happen.\r\n\r\nCurrent problematic behaviour can be viewed in http://codepen.io/kaidohallik/pen/gmLQbN  \r\n'Blue exactly bad labelllll' is truncated.  \r\nAfter changing `caretSize: 10` to `caretSize: 0` no truncation, all is shown correctly.\r\n\r\nIt can fix someones problems in #1731  \r\nIt's improvement of #1840\r\n Avoid tooltip truncation in x axis if there is enough space >>> 0"
123,"The `fill` option now accepts the index of the target dataset (number) or a string starting by ""+"" or ""-"" followed by a number representing the dataset index relative to the current one (e.g. `fill: ""-2""` on dataset at index 3 will fill to dataset at index 1). It's also possible to ""propagate"" the filling to the target of an hidden dataset (`options.plugins.filler.propagate`). Fill boundaries `zero`, `top` and `bottom` have been deprecated and replaced by `origin`, `start` and `end`.\r\n\r\nImplementation has been moved out of the line element into a new plugin (`src/plugins/plugin.filler.js`) and does not rely anymore on the deprecated model `scaleTop`, `scaleBottom` and `scaleZero` values. Drawing Bézier splines has been refactored in the canvas helpers (note that `Chart.helpers.canvas` is now an alias of `Chart.canvasHelpers`).\r\n\r\nAdd 2 new examples (line and radar) and extend utils with a pseudo-random number generator that can be initialized with `srand`. That makes possible to design examples starting always with the same initial data.\r\n\r\nWill submit another PR with documentation when #3751 is merged (let's keep #3547 as a reminder).\r\n\r\nFixes #2380 #2945 \r\n\r\n[Demo](http://playground.abysscorp.org/chartjs/filler/samples/area/line-datasets.html) Add support to fill between datasets >>> 1"
124,Fix the `readUsedSize` regular expression to correctly parse (truncate) pixel decimal values.\r\n\r\nFixes #3860 Correctly handle decimal display size >>> 1
125,"Still resolves the original issue, #3585 while also fixing the issue identified for normal stacked charts when the Y axis has a user defined minimum value\r\n A better fix for stacked bar charts with log axes >>> 1"
126,* Add border dash options for zero line\r\n\r\n* Update Readme with zero line border dash config options\r\n Zero line dash options >>> 1
127,"Tooltips without any content (i.e. no `title`, `beforeBody`, `body`, `afterBody`, `footer`) should not be drawn onto canvas as empty tooltips.\r\n\r\nAdd check for tooltip content in `Core.Tooltip.draw`. Check for tooltip content before draw >>> 1"
128,Fix for #4038 \r\n\r\nAdd window scroll position to tooltip position calculation so they show up in the intended place instead of off-screen or otherwise detached from the graph. Update line-customTooltips.html (Re issue  #4038  ) >>> 1
129,Radar chart position is not center horizontally with v2.5.0.\r\n\r\nRight and left of `furthestLimits` would be switched wrongly on\r\nthis refactoring commit.\r\nhttps://github.com/chartjs/Chart.js/pull/3625/commits/e1606f88ed4805815038cba4fdcd6211d7490356\r\n\r\nI've confirmed radar chart is center with this fix. Fix radar chart position not center >>> 1
130,"Prepare examples in order to have them browsable online on the GitHub Pages. Samples still need to be reworked for a better style/layout and simpler code. @derekperkins maybe that fixes (or enhances) #2366, what do you think? \r\n\r\n[Demo](http://playground.abysscorp.org/chartjs/samples/samples/) Reorganize samples and list them in index.html >>> 1"
131,"Merge most of the `horizontalBar` controller into the `bar` one and fix stack groups and bar positioning when scales are stacked but also when a min and/or max tick values are explicitly defined. In addition to simplify the whole bar chart logic (which should make codeclimate happier), it decreases the minified size of about **3.7KB**.\r\n\r\n**Important:** this is a breaking change for derived controllers that rely on the following removed methods:\r\n\r\n```javascript\r\n// Vertical\r\ncalculateBarWidth() -> ruler.barSize;\r\ncalculateBarBase() -> return this.calculateBarValuePixels(datasetIndex, index).base;\r\ncalculateBarX() -> return this.calculateBarIndexPixels(datasetIndex, index, ruler).center;\r\ncalculateBarY() -> return this.calculateBarValuePixels(datasetIndex, index).head;\r\n\r\n// Horizontal\r\ncalculateBarHeight() -> return ruler.barSize;\r\ncalculateBarBase() -> return this.calculateBarValuePixels(datasetIndex, index).base;\r\ncalculateBarX() -> return this.calculateBarValuePixels(datasetIndex, index).head;\r\ncalculateBarY() -> return this.calculateBarIndexPixels(datasetIndex, index, ruler).base;\r\n```\r\n\r\n Fix and refactor bar controllers >>> 1"
132,"Added the axisColor option and its functionality to GridLines options, mentioned in issue #4041 \r\nAlso updated the axes documentation. Coloring an axis line separately from gridLines >>> 0"
133,"**Summary**\r\n\r\nThis PR (naively) resolves issue with bars alignment on time scale, by early returning. \r\nFixes https://github.com/chartjs/Chart.js/issues/2415. Don't shift timescale bars on barchart >>> 0"
134,"Example of changes:\r\n- separate instances (true, 'before', 'after'): http://codepen.io/ericnkatz/pen/ryvgdN\r\n- stacked dataset in 1 instance ('before', 'after'): http://codepen.io/ericnkatz/pen/bqKbRK\r\n\r\nSee issue: https://github.com/chartjs/Chart.js/issues/3162 Enhancement: adds step-after functionality, true defaults to step-before >>> 1"
135,Verifying the index before processing `getPixelForValue`\r\nSee #4060  Fixed misplaced data points on category scale >>> 1
136,"This PR is to add a plugin for 100% stacked bar chart.\r\n( from #2709 )\r\n\r\nA example on codepen: http://codepen.io/anon/pen/peKNNQ Add ""stacked100"" plugin link to the docs >>> 1"
137,"""showLines"" should be ""showLine"". See above Dataset Properties table, line 44 of file.\r\n\r\n#4072  Typo in Property Name >>> 0"
138,Coverage data are now generated by running `gulp unittest` with the `--coverage` argument: unit tests are now executed a single time on Travis. The gulp `coverage` task has been removed and `karma.coverage.conf.ci.js` merged into `karma.conf.ci.js`.\r\n\r\nUpdate documentation with gulp commands (and remove them from `README.md`) and remove unused `config.jshintrc` (oversight from #3256). Delete `thankyou.md` which has been merged into `README.md`. Add `gulp unittest --coverage` argument >>> 1
139,Move legend and title in the `plugins` folder Move legend and title in the plugins folder >>> 1
140,Previously this value was essentially hard coded to 2 because of a typo that read it from the positioner output. Based on #3599 we agreed to make this into a config setting.\r\n\r\nResolves #3599  Update the tooltip with a new `caretPadding` setting. >>> 1
141,Scale does not respect suggestedMin and suggestedMax when dataset contains no values.\r\n\r\njsfiddle: https://jsfiddle.net/o6g51gow/1/ Fixed calculation of scale min and max when dataset contains no values >>> 1
142,"#4092 \r\nAxes with undefined weight will have weight = 0 and will be drawn first, then will be drawn ordered axes by their weight. scale service - respect new weight scale option for axes ordering >>> 1"
143,Resolves #2068 and #3331 \r\n\r\nThis just adds back the live charts for each type. I didn't add the two column layout for the pie/doughnut charts. Not sure what the best or most sustainable way to do that is. Add live samples back to docs for each chart type >>> 1
144,Previously the user got a message about `type` being undefined.\r\nThis gives something that's easier to understand and debug.\r\n\r\nResolves #3873  Adds a better error message when the chart type is incorrect >>> 1
145,Similar to `minSize.height` calculation in same method.\r\n\r\nFixes hidden charts slowing/hanging the browser (#3921). Ensure scale width cannot be greater than maxWidth >>> 1
146,"This pull request is an extended version of pull request #4058, discussed directly with @etimberg .\r\n\r\n## Summary:\r\nCreated a separate plugin for drawing of gridLines and moved them outside of core.scale.js to allow better control over them and interaction between them, solving multiple problems regarding the gridLines. \r\nAlso added the border functionality to allow separate configuration of axisLine discussed in issue #4041 \r\n\r\n## Solved problems:\r\n- **Overlapping gridLines:** when having multiple axes on the same side of the graph, multiple gridLines were drawn on top of each other causing overlapping and merging of multiple colors with alphas.\r\n- **Inconsistent behaviour:** axisLine(the one near ticks) of non-adjacement axes to the chartArea could be either hidden, or visible with the color of gridLines depending on the scale.gridLines.drawBorder option. \r\nHowever the axisLine of an adjacement axis could never be completely hidden because of the overlapping of gridLines.\r\nAlso the axisLine color was the same as gridLines of that axis, but it has the opposite direction. For better explanation check #4041 \r\n- **Ignoring the offsetGridLines option:** when offsetGridLines option is enabled there is one less tick label than there should be tick marks and gridLines. Iterating through ticks array without respecting the option was resulting in one missing gridLine and tick mark\r\n\r\n## Added functionality:\r\n- **Borders:** there is an option to enable border on any axis resulting in the ability to set following border options: color, lineWidth, borderDash, borderDashOffset \r\n\r\n## \r\nI havent updated the documentation yet, but I want to start the pull request already as there is quite a lot to review and update the documentation while it is being reviewed.\r\nAlso please ignore the first two commits by Cizmarik as his pull request was already merged yesterday with those changes. Also all the merge commits as they were just the result of updating our fork, caused by us being new and a bit confused with this whole git thing :)\r\n Move drawing of gridLines into separate plugin and add border functionality >>> 0"
147,Uncaught TypeError: Cannot read property '_meta' of undefined\r\n    at Chart.getDatasetMeta (https://localhost/rmdb/res/libs.js:4351:16)\r\n    at Chart.updateHoverStyle (https://localhost/rmdb/res/libs.js:4489:11)\r\n    at Chart.handleEvent (https://localhost/rmdb/res/libs.js:4569:8)\r\n    at Chart.eventHandler (https://localhost/rmdb/res/libs.js:4509:21)\r\n    at listener (https://localhost/rmdb/res/libs.js:4444:21)\r\n    at HTMLCanvasElement.proxies.(anonymous function) (https://localhost/rmdb/res/libs.js:9916:5)\r\ngetDatasetMeta @ libs.js:4351\r\nupdateHoverStyle @ libs.js:4489\r\nhandleEvent @ libs.js:4569\r\neventHandler @ libs.js:4509\r\nlistener @ libs.js:4444\r\nproxies.(anonymous function) @ libs.js:9916\r\nlibs.js:4351 Uncaught TypeError: Cannot read property '_meta' of undefined\r\n    at Chart.getDatasetMeta (https://localhost/rmdb/res/libs.js:4351:16)\r\n    at Chart.updateHoverStyle (https://localhost/rmdb/res/libs.js:4489:11)\r\n    at Chart.handleEvent (https://localhost/rmdb/res/libs.js:4569:8)\r\n    at Chart.eventHandler (https://localhost/rmdb/res/libs.js:4509:21)\r\n    at listener (https://localhost/rmdb/res/libs.js:4444:21)\r\n    at HTMLCanvasElement.proxies.(anonymous function) (https://localhost/rmdb/res/libs.js:9916:5)\r\ngetDatasetMeta @ libs.js:4351\r\nupdateHoverStyle @ libs.js:4489\r\nhandleEvent @ libs.js:4569\r\neventHandler @ libs.js:4509\r\nlistener @ libs.js:4444\r\nproxies.(anonymous function) @ libs.js:9916\r\n\r\nEdit: Video of error (https://www.youtube.com/watch?v=twbyqVw8n4M&feature=youtu.be)\r\n\r\nPlease consider the following before submitting a pull request:\r\n\r\nGuidelines for contributing: https://github.com/chartjs/Chart.js/blob/master/CONTRIBUTING.md\r\n\r\nExample of changes on an interactive website such as the following:\r\n- http://jsbin.com/\r\n- http://jsfiddle.net/\r\n- http://codepen.io/pen/\r\n- Premade template: http://codepen.io/pen?template=JXVYzq Fixed an Uncaught TypeError >>> 0
148,"This new functionality will allow you to reverse the order of datasets in tooltips, similar to how the **reverse** parameter on legend allows you to reverse the order of datasets in the legend. Add the ability to reverse the order of tooltip items >>> 0"
149,Fixes https://github.com/chartjs/Chart.js/issues/4143 Combine the two contributing docs >>> 1
150,The link pointing to point-data time scaled graph was incorrectly referring to a different graph sample file. The commit points the link to the correct sample now. Fix broken link to point to correct sample file. >>> 1
151,nan Remove extraneous period >>> 1
152,This was requested by a client and I found a solution here http://stackoverflow.com/questions/39155400/legends-for-line-charts-in-chart-js and noticed there was no PR about it. Add the ability to draw a line as legend symbol >>> 0
153,`instanceof HTMLCanvasElement/CanvasRenderingContext2D` fails when the item is inside an iframe or when running in a protected environment. We could guess the types from their toString() value but let's keep things flexible and assume it's a sufficient condition if the item has a context2D which has item as `canvas`.\r\n\r\nFixes #3887 #4102 #4152 Fix failing instanceof when reading context >>> 1
154,Make sure to explain responsiveness limitations with `CANVAS` elements and how to correctly setup a responsive chart using a dedicated and relatively positioned div wrapper.\r\n\r\nEnhances #2958 Enhance the responsive documentation >>> 1
155,"Dear colleges!\r\n\r\nPlease, consider the pull request for [iss#3917](https://github.com/chartjs/Chart.js/issues/3917)\r\n\r\nThis is how the chart looks after fix:\r\n![iss3917-fix](https://cloud.githubusercontent.com/assets/19689271/23328791/1d7ee638-fadf-11e6-86dd-7452cfa3ac91.png)\r\n\r\n\r\n iss3917 >>> 0"
156,Resolves #3600  Make it clear that labels need to be specified when using a category axis on a line chart >>> 1
157,nan Reorganize extension docs >>> 1
158,Fixes https://github.com/chartjs/Chart.js/issues/4181\r\n Remove malformed comment >>> 1
159,I overlooked that two variables were being set at once while reading the code. I think this helps make it clearer Remove unnecessary variable >>> 1
160,"Legend and title layout items must be updated when changing chart options:\r\n\r\n```javascript\r\nvar chart = new Chart(ctx, {\r\n  options: {\r\n    legend: {\r\n      position: 'top'\r\n    }\r\n  }\r\n};\r\n\r\nchart.options.legend.position = 'left';\r\nchart.update(); // <-- broken layout\r\n``` Fix legend and title layout options update >>> 1"
161,"#4197 \r\nWhen updating the options, should merge with the default value and add ids before setting them, just like when initializing.\r\n\r\nAnd I think the function name should updateOptions since it really only touched the options. fix scale options update error >>> 0"
162,![image](https://cloud.githubusercontent.com/assets/11849763/25554507/bdc41a52-2cec-11e7-9078-32cc0e446d69.png)\r\n\r\nchange the textLabelColor for charts\r\n\r\nFixes #4191 Add tooltip textLabelColor callback >>> 1
163,![image](https://cloud.githubusercontent.com/assets/11849763/25347790/a7f4cae8-2939-11e7-9486-c9ee986f94ae.png)\r\n![image](https://cloud.githubusercontent.com/assets/11849763/25347809/bebb1ebc-2939-11e7-9a7a-960614e01ae1.png)\r\n adding values on the bars >>> 0
164,Please consider the following before submitting a pull request:\r\n\r\nGuidelines for contributing: https://github.com/chartjs/Chart.js/blob/master/CONTRIBUTING.md\r\n\r\nExample of changes on an interactive website such as the following:\r\n- http://jsbin.com/\r\n- http://jsfiddle.net/\r\n- http://codepen.io/pen/\r\n- Premade template: http://codepen.io/pen?template=JXVYzq Fix inconsistent aspect ratio >>> 1
165,nan Remove executable bit from js files >>> 1
166,I'd like to create a timeseries scale as described in https://github.com/chartjs/Chart.js/issues/4189. This change will allow the time scale and the timeseries scale to share a number of methods. Refactor time scale methods into a common location >>> 1
167,"The current implementation incorrectly dismisses values returned from tooltip callback methods if they are falsy rather than explicitly checking that they are missing. See the following example:\r\n\r\n```\r\n...\r\nlabel: (tooltipItem) => {\r\n  return 0; // Assume a more realistic situation resulting in the label being 0.\r\n},\r\n```\r\n\r\nWith this fix we will instead check for undefined or null values, and otherwise use the return value. Explicitly check if tooltip callback is undefined before ignoring value >>> 0"
168,"Starts fixing #3507 \r\n\r\n@simonbrunel @panzarino @tannerlinsley @derekperkins not entirely sure what this documentation should be, lets iterate on this and see. initial data update docs >>> 1"
169,Fixes #3547 Document the new filling modes and options >>> 1
170,Clarifies #2951 Fix RequireJS doc to use UMD file instead >>> 1
171,"Clarifies #4244 Make ""dedicated to the chart canvas"" a requirement >>> 1"
172,Added width + height arguments to ctx.drawImage when using an image as a pointStyle. \r\n\r\nPrevious functionality meant that images would be drawn at their source file size regardless of whether custom width or height properties were set. Fixed images being used as a pointStyle not rendering at custom dimensions >>> 1
173,"Add Travis CI task to deploy the docs, samples and dist files to chartjs.github.io for the `release` and `master` branches. A `latest` symbolic links is also created for each folder to the highest version (or `master` if any).\r\n\r\nDepends on chartjs/www.chartjs.org#1\r\nFixes #2068 #3993 #4122 #4171 Deploy to GitHub pages >>> 1"
174,"Currently, when dataset empty, we do not check whether min, max is null.\r\nhttps://github.com/chartjs/Chart.js/blob/master/src/scales/scale.linear.js#L102-L128\r\n\r\nSet it to default if null could prevent axis's ticks array being [0, NaN], which led to the issue #4216  fix #4216 when dataset empty, set scale min max to default if null >>> 0"
175,"Time axis ticks are aligned by senior units of determined base units (for example: if units determined as ""hour"", senior units will be ""day""). See https://github.com/chartjs/Chart.js/issues/4187\r\nCheck samples: \r\nsamples/scales/time/combo.html \r\nand \r\nsamples/scales/time/line-point-data.html Implemented aligment by senior unit in time axis. >>> 1"
176,Changed default formatting for time units.\r\nBold style added to senior unit tick labels. Time axis ticks formatting for base and senior unit >>> 1
177,"Hello,\r\n\r\nplease find below an implementation of feature #3293.\r\n\r\nYou are now able to do the following:\r\n\r\n    options: {\r\n        elements: {\r\n            rectangle: {\r\n                borderSkipped: [ 'left', 'right' ],\r\n           }\r\n        },\r\n       ...\r\n    }\r\n\r\nExample of implementation is visible here: https://bokysan.github.io/Chart.js/\r\n\r\n\r\n\r\nPlease note that the code also includes the initial ""undocumented"" support setting the `borderWidth` in the same manner, as suggested in the last comment:\r\n\r\n    options: {\r\n        elements: {\r\n            rectangle: {\r\n                borderWidth: [ 0, 2, 4, 6 ],\r\n           }\r\n        },\r\n       ...\r\n    }\r\n\r\nHowever not all user cases have been tested -- especially with the semi-transparent colours -- so I would leave it as undocumented for the time being.\r\n\r\nCheers,\r\nB Fix for #3293 >>> 0"
178,"I am using chartjs for an application where the charts are baked to PDF server-side, with the eventual goal of printing them. I think this is quite common. Even for people using chartjs on the client, they may want to print the charts direct from the browser.\r\n\r\nIn either case it can be useful to force the charts to render at a higher DPR than the default window.devicePixelRatio so they look good at higher printer DPIs.\r\n\r\nI found the existing code to deal with retina displays and I've extended it a bit to accept an override.\r\n\r\nI could not get the docs task to run, but I think I've conformed to the style of nearby docs. Please let me know if I need to change anything. Added 'devicePixelRatio' option to override the window's DPR setting >>> 1"
179,nan Upgrade dependencies >>> 1
180,"Milestone: [2.6.0](/chartjs/Chart.js/milestone/12)\r\nMerged PRs: [65](/chartjs/Chart.js/pulls?q=is:pr+is:closed+milestone:""Version+2.6"")\r\n\r\n**Checklist:**\r\n- [x] Draft the release notes (@etimberg)\r\n- [x] Update package.json version: #4237\r\n- [x] New docs, samples and website deploy scripts: #4256 chartjs/www.chartjs.org#1\r\n- [x] Change chartjs.org DNS to point on chartjs.github.io (@nnnick)\r\n\r\nSee the [release process](/chartjs/Chart.js/blob/master/MAINTAINING.md#releasing-a-new-version) for details.\r\n\r\ncc @chartjs/maintainers  Version 2.6.0 >>> 1"
181,nan Fix code climate badge and link >>> 1
182,Fix for #4279 and subsequently https://github.com/ashiguruma/patternomaly/issues/10 Add CanvasPattern support to tooltip >>> 0
183,nan Upgrade dependencies >>> 1
184,Signed-off-by: Suhaib Khan <suheb.work@gmail.com>\r\n\r\nFixes #3889 \r\nExample: https://codepen.io/anon/pen/EmEzKb\r\n Fixes labelOffset not working for vertical axes >>> 0
185,Add hard coded integer constants for *_SAFE_INTEGER which are not available on IE\r\n\r\nFixes #4294 Fix for issue #4294 >>> 1
186,The core controller was looking at the wrong object (options.hover) to find the function to be called on hover. The function is provided on the top level options object (options.onHover).\r\n\r\nFixes #4296 Fix onHover event not being triggered >>> 1
187,Resolves #4309  Provide a blank default global layout option >>> 1
188,The text is correctly centered in the box. I also did some minification cleanup in the legend plugin.\r\n\r\nResolves #4317  Ensure that disabled legend style is drawn in the center of the text  >>> 1
189,nan Use node 6 - the latest LTS version of node >>> 1
190,Resolves #4339  Make sure that the border width of the tooltip color box is always correct >>> 1
191,Resolves #4328 and makes the `round` setting match the docs. Fix round option for time scales >>> 1
192,[BUG] Violation warnings in verbose level chrome dev tools logging.\r\nI hope this closes https://github.com/chartjs/Chart.js/issues/4287\r\n\r\nI'm not sure whether this is the way to solve this issue. From https://developers.google.com/web/updates/2016/06/passive-event-listeners it may need to be in `passive` for a better scrolling in touch devices. Please comment and I'll change accordingly. \r\n\r\nNote: This's my first PR to a public repo. :smile:  Closes issue #4287 >>> 0
193,Fixes #4336 Fix filling between datasets of different lengths >>> 1
194,"See discussion in the issue for context and possible approaches.\r\n\r\nWhen invoking `update()` inside an event handler, such as onHover, `options.hover.animationDuration` was not being respected. Given that some use cases may require additional animation properties for the manual update call, this commit changes that method signature to accept a configuration object.\r\n\r\nThis object provides backwards compatibility with `duration` and `lazy` properties, and also introduces the `easing` property so that the event animation is different from the global one.\r\n\r\nIssue #4300 Support configurable update >>> 1"
195,"Adds support for multiple lines of text in the chart title. If the `text` property of the title options is an array, we render each item on it's own line. See attached screenshots for details of how it looks.\r\n\r\n![screen shot 2017-06-15 at 6 55 06 pm](https://user-images.githubusercontent.com/6757853/27205025-68ef553e-51fc-11e7-8e57-d068e6083510.png)\r\n\r\n![screen shot 2017-06-15 at 6 55 18 pm](https://user-images.githubusercontent.com/6757853/27205029-6ce97ab6-51fc-11e7-9d9f-226a71686cb0.png)\r\n\r\n![screen shot 2017-06-15 at 6 55 31 pm](https://user-images.githubusercontent.com/6757853/27205035-706c1018-51fc-11e7-9cc3-c2ff9909ac80.png)\r\n Multiple lines of text in the chart title >>> 1"
196,According to the [issue tracker of highcharts](https://github.com/highcharts/highcharts/issues/4405#issuecomment-125124004) they do not consider the code open source. It's available for reading but that's about it.\r\n\r\nThe code is provided under CC-BY-NC via github.\r\n\r\nSee also:\r\nhttps://shop.highsoft.com/faq#Non-Commercial-0 HighCharts is not open source >>> 1
197,"Timeseries scale added, see this issue for more info: https://github.com/chartjs/Chart.js/issues/4189\r\n\r\nsmall example here: https://codepen.io/ilyabelyaev/pen/BRvKPw Timeseries scale >>> 0"
198,"The text is centered within the line height, so setting the line height to a size greater than the font size moves it away from the axis edge.\r\n\r\nResolves #3562 Line height setting for scale titles >>> 1"
199,"Resolves #4398  When the legend label options are not defined, no error should occur >>> 1"
200,Also added tests\r\n\r\nresolves #4391  Ensure deprecated unitStepSize property of time scale is respected >>> 1
201,Elements were resizing incorrectly if they were resized while collapsed. Added a check to avoid this issue. fix/4397 >>> 1
202,Resolves #3992 ticks.padding option applies to both vertical and horizontal axes >>> 1
203,* Fixed arguments in IPlugin#before/afterDatasetUpdate description\r\n* Fixed arguments in IPlugin#before/afterDatasetDraw description Fix arguments in plugin interface description >>> 1
204,Add aria-hidden=true attribute to hidden iframe for resizing. This prevents screen readers in ItemMode from navigating to the hidden iframe Add aria-hidden=true attribute to hidden iframe for resizing >>> 1
205,- Up to date with the latest code\r\n- Added Example column Update Display Format table >>> 1
206,"Move some of the ""core"" and ""canvas"" utils in `helpers.core.js` and `helpers.canvas.js` and introduce the new `isNullOrUndef` and `isObject` helpers. Deprecate `indexOf` and rename `drawRoundedRectangle` to `roundedRect` which now creates a simple `rect` path if radius is 0. Write missing unit tests for the moved helpers.\r\n Cleanup and reorganize core and canvas helpers >>> 1"
207,Raise the cyclomatic complexity to 10 which seems to better match the project coding style and still reasonable (6 being quite low). Also move unit tests specific eslint rules in the cascaded `./test/.eslintrc` file (previously in `gulp.js`).\r\n Increase ESLint complexity and add config for tests >>> 1
208,"The `clone` method now accepts any type of input but also recursively perform a deep copy of the array items. Rewrite the `configMerge` and `scaleMerge` helpers which now rely on a new generic and customizable `merge` method, that one accepts a target object in which multiple sources are deep copied. Note that the target (first argument) is not cloned and will be modified after calling `merge(target, sources)`. Add a `mergeIf` helper which merge the source properties only if they do not exist in the target object.\r\n\r\n@chartjs/maintainers please review this PR carefully since it impacts the whole config process :) Rewrite the clone and merge helpers >>> 1"
209,"For consistency with `valueOrDefault`, `valueAtIndexOrDefault` now returns null if `value` (expected array) is null. Also get rid of the superfluous `get` prefix in `getValueOrDefault` and `getValueAtIndexOrDefault`.\r\n Change `valueAtIndexOrDefault` behavior >>> 1"
210,"Deprecate `addEvent` and `removeEvent`, and move implementation in `platform.dom.js`. Add 'options' feature detection to register event listeners as passive and prevent warning in Chrome.\r\n\r\nReplaces #4356\r\nFixes #4287 Fix non-passive event listener warning in Chrome >>> 1"
211,"Resolves #4372 \r\n\r\nNote: This is a potentially breaking change if we think that scatter defaults should show lines. In which case, I will update the two sample files Update scatter chart default to hide lines >>> 1"
212,"Resolves #4216 \r\n\r\n## Cause\r\nWhat happened in that bug was the following:\r\n1. All datasets hidden so x axis moves into default range\r\n2. X axis `ticks.min` property set which changes the default range\r\n3. The max of the x axis ended up being NaN hence why the axis drew incorrectly\r\n\r\n## Fix\r\nTo fix this, the range is checked after `ticks.min` and `ticks.max` are handled to ensure that the range is valid and that `min < max`. Fix range calculation when all datasets hidden and axis minimum set >>> 1"
213,"For example, when the radius is too high:\r\n![image](https://user-images.githubusercontent.com/3874900/27760215-44dcb220-5e42-11e7-9181-157c4b2709bb.png) ![image](https://user-images.githubusercontent.com/3874900/27760236-bcf18a9c-5e42-11e7-8fbe-2135d5aca41f.png)\r\n(left: master, right: PR) Clamp radius when drawing rounded rectangle >>> 1"
0,"LongGCDisruption simulates a Long GC by suspending all threads belonging to a node. That's fine, unless those threads hold shared locks that can prevent other nodes from running. Concretely the logging infrastructure, which is shared between the nodes, can cause some deadlocks. LongGCDisruption has protection for this, but it needs to be updated to point at log4j2 classes, introduced in #20235\n\nThis commit also fixes improper handling of retry logic in LongGCDisruption and adds a protection against deadlocking the test code which activates the disruption (and uses logging too! :)).\n\nOn top of that we have some new, evil and nasty tests.\n Fix LongGCDisruption to be aware of log4j2 >>> 1"
1,"During a networking partition, cluster states updates (like mapping changes or shard assignments)\nare committed if a majority of the masters node received the update correctly. This means that the current master has access to enough nodes in the cluster to continue to operate correctly. When the network partition heals, the isolated nodes catch up with the current state and get the changes they couldn't receive before. However, if a second partition happens while the cluster\nis still recovering from the previous one _and_ the old master is put in the minority side, it may be that a new master is elected which did not yet catch up. If that happens, cluster state updates can be lost.\n\nThis commit fixed 95% of this rare problem by adding the current cluster state version to `PingResponse` and use them when deciding which master to join (and thus casting the node's vote). \n\nNote: this doesn't fully mitigate the problem as a cluster state update which is issued concurrently with a network partition can be lost if the partition prevents the commit message (part of the two phased commit of cluster state updates) from reaching any single node in the majority side _and_ the partition does allow for the master to acknowledge the change. We are working on a more comprehensive fix but that requires considerate work  and is targeted at 6.0.\n\nPS this PR contains and depends on #20348 , which was required for long testing. That part doesn't need to be reviewed.\n Add current cluster state version to zen pings and use them in master election >>> 1"
2,The only repository we can be sure is safe to clean is `fs` so we clean\nany snapshots in those repositories after each test. Other repositories\nlike url and azure tend to throw exceptions rather than let us fetch\ntheir contents during the REST test. So we clean what we can....\n\nCloses #18159\n Clean up snapshots where possible after each REST test >>> 1
3,"Currently, we check if a node has the same set of custom metadata \nas the master before joining the cluster. This implies freshly installing \na plugin that has its custom metadata requires a full cluster restart.\n [DOC] Add note requiring full cluster restart for installing plugins with custom metadata >>> 1"
4,`GET /` now returns the clusterUUID as well as part of its output for monitoring purposes\n Add clusterUUID to RestMainAction output >>> 1
5,Adds an integration test for the file-based discovery plugin\nto test the plugin operates correctly and uses the hosts\nconfigured in `unicast_hosts.txt` with a real cluster\n\nCloses #20459\n File-based discovery plugin integration tests >>> 1
6,"TransportService is such a central part of the core server, replacing\nit's implementation is risky and can cause serious issues. This change removes the ability to\nplug in TransportService but allows registering a `TransportInterceptor` that enables\nplugins to intercept requests on both the sender and the receiver ends. This is a commonly used\nand overwritten functionality but encapsulates the custom code in a contained manner.\n Remove ability to plug-in TransportService >>> 1"
7,GET / now returns the clusterUUID as well as part of its output for monitoring purposes\n\nThis is the backport of #20503.\n Add clusterUUID to RestMainAction output >>> 1
8,We now have in 5.0.0 `ingest-attachment` plugin.\nWe can remove `mapper-attachments` plugin for 6.0.\n\nCloses #18837.\n Remove mapper attachments plugin >>> 1
9,If an index was created with pre 2.0 we should not treat it as supported\neven if all segments have been upgraded to a supported lucene version.\n\nCloses #20512\n Ensure elasticsearch doesn't start with unuspported indices >>> 1
10,"Currently, we silently accept malformed query where more\nthan one key is defined at the top-level for query object.\nIf all the keys have a valid query body, only the last query\nis executed, besides throwing off parsing for additional suggest,\naggregation or highlighting defined in the search request.\n\nThis commit throws a parsing exception when we encounter a query\nwith multiple keys.\n\ncloses #20500\n Fix silently accepting malformed queries >>> 1"
11,This pull request is a cleanup of two issues for plugins: \n- the plugin command specified in an error message for the remove\n  plugin command is incorrect\n- remove the ability to specify a custom path for plugins\n\nCloses #18588\n Plugins cleanup >>> 1
12,"This adds details about the `""version""` field to both the template and pipeline pages.\n [DOCS] Add ""version"" to template and pipeline docs >>> 1"
13,"A few of our unit tests generate a random search request body and run tests against it. The source can optionally contain ext elements under the ext sections, which can be parsed by plugins. With this commit we introduce a plugin so that the search requests tests don't use the one from `FetchSubPhasePluginIT` anymore. They rather generate multiple search ext elements. The plugin can parse and deal with all those. This extends the test coverage as we may have multiple elements with random names.\n\nTook the chance to introduce a common test base class for search requests, called `AbstractSearchTestCase`, given that the setup phase is the same for all three tests around search source. Then we can have the setup isolated to the base class and the subclasses relying on it.\n\nCloses #17685\n introduce test plugin to inject random search ext elements in search request tests >>> 1"
14,"Add missing readBytes in `ip` field deserialization\nAdd (de)serialization tests for all types\nThis change also removes the ability to set FieldStats.minValue or FieldStats.maxValue to null, this is not required anymore since the stats are built on fields with values only.\n\nFixes #20516\n Fix FieldStats deserialization of `ip` field >>> 1"
15,The `/_cat/nodes` document is just obsolete.\n\ncloses #20162\n [DOC] Update /_cat/nodes doc >>> 1
16,"This is just the 2.4.x backport for #19554.\n\nI had to add a core test dependency on `securemock` (the new test cause uses mockito), and upgrade it from 1.1 to 1.2; not sure this is OK/safe, so could someone who knows better please comment?\n Guard against negative result from FileStore.getUsableSpace when pick… >>> 1"
17,"This change removes all guice interaction from Transport, HttpServerTransport,\nHttpServer and TransportService. All these classes as well as their subclasses\nor extended version configured via plugins are now created by using plain old\nbloody java constructors. YAY!\n Unguice Transport and friends >>> 1"
18,"I was following the `Getting Started` chapter and found out, that I had a different response structure after successfully indexing a document. With this change I added `_shards` and `result` keys to the response.\n Edited response structure on indexing a document >>> 1"
19,"Most of our queries support a single field name, the field that gets queried. That is the key of the object which contains the query options. For such queries, in case multiple fields are presented, in most of the cases the query goes through and either the last or the first field only will be read and queried.\n\nThis PR changes the behaviour of all those parsers to throw exception in case multiple field names are provided.\n\nCloses #19547\n Query parsers to throw exception when multiple field names are provided >>> 1"
20,"Followup to #20515 where we added validation that after we parse a query within a query element, we should not get a field name. Truth is that the only token allowed at that point is END_OBJECT, as our DSL allows only one single query within the query object:\n\n```\n{\n  ""query"" : {\n    ""term"" : { ""field"" : ""value"" }\n  }\n}\n```\n\nWe can then check that after parsing of the query we have an end_object that closes the query itself (which we already do). Following that we can check that the query object is immediately closed, as there are no other tokens that can be present in that position.\n\nSee https://github.com/elastic/elasticsearch/pull/19791#discussion-diff-73647510 too\n\nRelates to #20515\n Throw error if query element doesn't end with END_OBJECT >>> 1"
21,"This tracks the snippets that probably should be converted to\n`// CONSOLE` or `// TESTRESPONSE` and fails the build if the list\nof files with such snippets doesn't match the list in `docs/build.gradle`.\nSetting the file looks like\n\n```\n/* List of files that have snippets that probably should be converted to\n * `// CONSOLE` and `// TESTRESPONSE` but have yet to be converted. Try and\n * only remove entries from this list. When it is empty we'll remove it\n * entirely and have a party! There will be cake and everything.... */\nbuildRestTests.expectedUnconvertedCandidates = [\n  'plugins/discovery-azure-classic.asciidoc',\n...\n  'reference/search/suggesters/completion-suggest.asciidoc',\n]\n```\n\nThis list is in `build.gradle` because we expect it to be fairly\ntemporary. In a few months we'll have converted all of the docs and won't\nned it any more.\n\nFrom now on if you add now docs that contain a snippet that shows an\ninteraction with elasticsearch you have three choices:\n1. Stick `// CONSOLE` on the interactions and `// TESTRESPONSE` on the\n   responses. The build (specifically (`gradle docs:check`) will test that\n   these interactions ""work"". If there isn't a `// TESTRESPONSE` snippet\n   then ""work"" just means ""Elasticsearch responds with a 200-level response\n   code and no `WARNING` headers. This is way better than nothing.\n2. Add `// NOTCONSOLE` if the snippet isn't actually interacting with\n   Elasticsearch. This should only be required for stuff like javascript\n   source code or `curl` against an external service like AWS or GCE. The\n   snippet will not get ""OPEN IN CONSOLE"" or ""COPY AS CURL"" buttons or be\n   tested.\n3. Add `// TEST[skip:reason]` under the snippet. This will just skip the\n   snippet in the test phase. This should really be reserved for snippets\n   where we can't test them because they require an external service that\n   we don't have at testing time.\n\nPlease, please, please, please don't add more things to the list. After\nall, it sais there'll be cake when we remove it entirely!\n\nRelates to #18160\n Fail build if new doc snippets aren't `// CONSOLE` >>> 1"
22,"Improves the documentation for the `cluster.routing.allocation.cluster_concurrent_rebalance` setting, clarifying in which shard allocation situations the rebalance limit takes effect.\n\nCloses #20529\n Improves the documentation for `cluster.routing.allocation.cluster_concurrent_rebalance` >>> 1"
23,"This PR introduces backward compatibility index tests to test the rolling upgrade process amongst Elasticsearch instances within the same major version.  The test executes in three phases.  In the first phase, we form a cluster of 2 ES instances on an old version.  In the second phase, we keep one of the nodes from the old cluster, kill the other node, but preserve its data directory and start an instance of the current version of ES using the same data directory as the killed instance.  In the third phase, we kill the other old version ES instance from the first phase and launch a new instance, using the same data directory as the killed instance.  Therefore, during phase 3, we have fully migrated and have all current versions of ES running.  In each phase, we run REST tests that index documents and search them, ensuring at each stage that the documents from the previous phase are still there.\n\nNote that because we haven't released a GA yet of 5.0, the tests currently don't start an old version cluster in the first phase.  Once GA is released, this will be changed to make the backward compatibility version 5.0, while the current version in the cluster will be 5.x.\n Backward compatability tests for rolling upgrades >>> 1"
24,This commit modifies the logger names within Elasticsearch to be the\nfully-qualified class name as opposed removing the org.elasticsearch\nprefix and dropping the class name. This change separates the root\nlogger from the Elasticsearch loggers (they were equated from the\nremoval of the org.elasticsearch prefix) and enables log levels to be\nset at the class level (instead of the package level).\n\nCloses #20326\n Complete Elasticsearch logger names >>> 1
25,"Since #19975 we are aggressively failing with AssertionError when we catch an ACE\ninside the InternalEngine. We treat everything that is neither a tragic even on\nthe IndexWriter or the Translog as a bug and throw an AssertionError. Yet, if the\nengine hits an IOException on refresh of some sort and the IW doesn't realize it since\nit's not fully under it's control we fail he engine but neither IW nor Translog are marked\nas failed by tragic event while they are already closed.\nThis change takes the `failedEngine` exception into account and if it's set we know\nthat the engine failed by some other even than a tragic one and can continue.\n\nThis change also uses the `ReferenceManager#RefreshListener` interface in the engine rather\nthan it's concrete implementation.\n\nRelates to #19975\n Take refresh IOExceptions into account when catching ACE in InternalEngine >>> 1"
26,"Currently all the reroute-like methods of `AllocationService` return a result object of type `RoutingAllocation.Result`. The result object contains the new `RoutingTable` and `MetaData` plus an indication whether those were changed. The caller is then responsible of updating a cluster state with these. These means that things can easily go wrong and one can take one of these but not the other causing inconsistencies. We already have a utility method on the `ClusterState` builder that does but no one forces you to do so. Also 99% of the callers do the same thing: i.e., check if the result was changed and if so update the very same cluster state that was passed to `AllocationService`.  This PR folds this pattern into `AllocationService` and changes almost all it's methods to return a new cluster state (potentially the original one).  This saves some 500 lines of code.\n\nThe one exception here is the reroute API which executes allocation commands and potentially returns an explanation as well (next to the routing table and metadata). That API now returns a `CommandsResult` object which encapsulate a cluster state and the explanation. \n Remove `RoutingAllocation.Result` >>> 1"
27,"Some objects like maps, iterables or arrays of objects can self-reference themselves. This is mostly due to a bug in code but the XContentBuilder should be able to detect such situations and throws an IllegalArgumentException instead of building objects over and over until a stackoverflow occurs.\n\ncloses #20540\ncloses #19475\n XContentBuilder: Avoid building self-referencing objects >>> 1"
28,"Currently the top level spec_id serves as a human-readable description of the ranking evaluation API call. Since there is only one id possible, it can be dropped to simplify the request.\n\nCloses #20438\n RankEval: Remove top level `spec_id` >>> 1"
29,It looks like [I lost it](https://github.com/elastic/elasticsearch/commit/1cc5ee7ad9be4412392b0f7e4de53c1c8b5fdd10#diff-ae608cc41abb24a78fb363cb8e415572L93) during lucene 4.0 migration. \n Named analyzer should close the analyzer that it wraps >>> 1
30,"Previously node attributes could be set via node.\* but this now requires\nusing node.attr.*. This commit fixes some leftover usages of the old\nway.\n\nThe command-line arguments for Elasticsearch must now be specified using\n-E. This commit fixes the usage of command-line arguments in the REST\nAPI spec README.\n\nRelates #17402, relates #18198,  closes #20542\n Node attributes and REST API spec README >>> 1"
31,"We can now run templates using `explain` and/or `profile` parameters.\nWhich is interesting when you have defined a complicated profile but want to debug it in an easier way than running the full query again.\n## Explain\n\nYou can use `explain` parameter when running a template:\n\n``` js\nGET /_search/template\n{\n  ""file"": ""my_template"",\n  ""params"": {\n    ""status"": [ ""pending"", ""published"" ]\n  },\n  ""explain"": true\n}\n```\n## Profiling\n\nYou can use `profile` parameter when running a template:\n\n``` js\nGET /_search/template\n{\n  ""file"": ""my_template"",\n  ""params"": {\n    ""status"": [ ""pending"", ""published"" ]\n  },\n  ""profile"": true\n}\n```\n Add profile and explain parameters to template API >>> 1"
32,"Adds a cat api endpoint: `/_cat/templates` and its more specific version, `/_cat/templates/{name}`.\n\nIt looks something like:\n\n```\n$ curl ""localhost:9200/_cat/templates?v""\nname                  template     order version\nsushi_california_roll *avocado*    1     1\npizza_hawaiian        *pineapples* 1\npizza_pepperoni       *pepperoni*  1\n```\n\nThe specified version (only allows \* globs) looks like:\n\n```\n$ curl ""localhost:9200/_cat/templates/pizza*""\nname            template     order version\npizza_hawaiian  *pineapples* 1\npizza_pepperoni *pepperoni*  1\n```\n\nPartially specified columns:\n\n```\n$ curl ""localhost:9200/_cat/templates/pizza*?v=true&h=name,template""\nname            template\npizza_hawaiian  *pineapples*\npizza_pepperoni *pepperoni*\n```\n\nThe help text:\n\n```\n$ curl ""localhost:9200/_cat/templates/pizza*?help""\nname     | n | template name\ntemplate | t | template pattern string\norder    | o | template application order number\nversion  | v | version\n```\n\nCloses #20467 \n Provides a cat api endpoint for templates. >>> 1"
33,This change adds tests for the snippets in the get and bulk API documentation.\n Add CONSOLE tests for snippets in get and bulk API docs >>> 1
34,This commit makes `ByteSizeUnit` implement `Writeable`.\n Make ByteSizeUnit implements Writeable >>> 1
35,The serial collector is not suitable for running with a server\napplication like Elasticsearch and can decimate performance and lead to\ncluster instability. This commit adds a bootstrap check to prevent usage\nof the serial collector when Elasticsearch is running in production\nmode.\n Add serial collector bootstrap check >>> 1
36,"This commit removes `ByteSizeValue`'s methods that are duplicated (ex: `mbFrac()` and `getMbFrac()`) in order to only keep the `getN` form.\n\nIt also renames `mb()` -> `getMb()`, `kb()` -> `getKB()` in order to be more coherent with the `ByteSizeUnit` method names.\n Remove duplicate methods in ByteSizeValue >>> 1"
37,"We were using maven snapshots during heavy development, but this should\nnot be something generally available (we should never release depending\non a snapshot version in maven). This change removes the snapshot repo.\nIf we ever need it temporarily for some reason, we can add it if/when\nit is necessary.\n\nrelates #20559\n Build: Remove maven central snapshots from repositories >>> 1"
38,"This change removes the old maven deploy that we have in parallel to\nmaven-publish, and makes maven-publish fully work with publishing to\nmaven local. Using `gradle publishToMavenLocal` should be used to\npublish to .m2.\n\nNote that there is an unfortunate hack that means for\nzip artifacts we must first create/publish a dummy pom file, and then\nfollow that with the real pom file. It would be nice to have the pom\nfile contains packaging=zip, but maven central then requires sources and\njavadocs. But our zips are really just attached artifacts, so we already\nset the packaging type to pom for our zip files. This change just works\naround a limitation of the underlying maven publishing library which\nsilently skips attached artifacts when the packaging type is set to pom.\n\nrelates #20164\ncloses #20375\n Build: Remove old maven deploy support >>> 1"
39,"IndexResponse#toString method outputs an error caused by the shards object needing to be wrapped into another object. It is fixed by calling a different variant of Strings.toString(XContent) which accepts a second boolean argument that makes sure that a new object is created before outputting ShardInfo. I didn't change ShardInfo#toString directly as whether it needs a new object or not very much depends on where it is printed out. IndexResponse seemed a specific case as the rest of the info were not json, hence the shards object was the first one, but it is usually not the case.\n fix IndexResponse#toString to print out shards info >>> 1"
40,This commit adds a new test `TribeIT#testClusterStateNodes()` to verify that the tribe node correctly reflects the nodes of the remote clusters it is connected to. It also changes the existing tests so that they use two `InternalTestCluster` clusters now.\n Improve TribeIT tests >>> 1
41,Adds some javadoc with more explanation on how to extend Plugin and\nwhy we have all these `@Deprecated public final` `onModule` methods.\n\nCloses #20564\n Better explain Plugin >>> 1
42,"With the unified release process across the elastic stack, download\nlinks for all products are changing. This change updates docs referring\nto the old download and packages urls.\n\nNote that this change also updates the plugin installation command as\nthe url for downloads is being changed to be consistent with that for\npackages (both plural).\n Convert old download links to unified release urls >>> 1"
43,"Today when CLI tools are executed, logging statements can intentionally\nor unintentionally be executed when logging is not configured. This\nleads to log messages that the status logger is not configured. This\ncommit reworks logging configuration for CLI tools so that logging is\nalways configured.\n Ensure logging is initialized in CLI tools >>> 1"
44,Gradle appears to have a bug in maven publishing which will not match the\nartifactId of a generated pom with the artifact id it puts in the file.\nThis adds back a copy hack from the original pom file name to the client\npom file name (which we had before #20403 inadvertently\nremoved it).\n Build: Add back hack for client copying client jar pom >>> 1
45,"so that we at least include total hits, otherwise also total hits would be equal to zero.\n\nPR for #20501\n If size / offset are out of bounds just do a plain count >>> 0"
46,This tests that the templates shipped with pre-5.0 versions of Logstash\nand Boats still work on an Elasticsearch 5.0+ node. We need to ensure\nthat ES can be upgraded prior to upgrading tools dependant on it.\n\nThis is by no means an exhaustive test of all template settings but it\nis a start on #17275\n Add rudimentary logstash and beats template BWC tests >>> 0
47,Long running searches now can be cancelled using standard task cancellation mechanism.\n Makes search action cancelable by task management API >>> 1
48,This commit removes/replaces some references to version 3.0.0.\n Remove reference to version 3.0.0 >>> 1
49,"Due to a typo, each exception is getting sent to recipients twice. This manifests itself by sporadic `Transport response handler not found of id [.....]` warnings in log files on nodes connected to 2.4.0+ nodes.\n\nThis bug only exists in 2.x branch.\n Don't send exception responses twice >>> 1"
50,`index.routing.allocation.initial_recovery` is used with index shrinking to make sure the new index's primary is assigned to the node that holds a copy of each of the source index shards. Sadly with the introduction of `RecoverySource` a regression was introduced that limits the allocation of replicas of the new index.\n `index.routing.allocation.initial_recovery` limits replica allocation >>> 1
51,"reindex-from-remote should ignore unknown fields so it is mostly\nfuture compatible. This makes it ignore unknown fields by adding an\noption to `ObjectParser` and `ConstructingObjectParser` that, if\nenabled, causes them to ignore unknown fields.\n\nCloses #20504\n Make reindex-from-remote ignore unknown fields >>> 1"
52,"In order to understand how well particular queries in a joint ranking evaluation request work we want to break down the overall metric into its components, each contributed by a particular query. This change adds a `partial_result` field to the response, under which we can summarize this kind of information. Each sub-section is keyed by the query-id and currently only contains the partial metric and the unknown_docs section for each query.\n\nRelates to #20363\n RankEval: Adding partial_results section to response >>> 1"
53,"Today we define a cluster wait condition to try to wait at least a\ncertain number of nodes when running integration tests. Alas, the wait\ncondition is incorrect because wait_for_nodes>=${numNodes} will be split\nby parameter parsing on the equals sign so the request looks like it has\na parameter named wait_for_nodes>. The fact that REST param parsing is\nlenient leads to this being undiscovered. This commit fixes this issue.\n\nRelates #14719\n Fix cluster wait condition >>> 1"
54,With the switch to Log4j 2 the logger usage checker was temporarily disabled. This PR adapts the checks to work with Log4j 2 and re-enables the Gradle checks. It also fixes the wrong logger usages that have sneaked into the code base.\n\nCloses #20243 \n Fix logger usage check for Log4j 2 >>> 1
55,"This marks the ""Prevent combinatorial explosion in aggregations from\ncausing OOM"" task as done in 5.0.0.\n\nRelates to #8081 and #19394\n [DOCS] Mark combinatorial explosion in aggs as 'done' >>> 1"
56,"The build currently depends on the presence of a Git remote named origin to determine the URL that is used in the generated POM file. As this is best-effort anyhow and only required by Maven Central, this commit allows the build to run even if a Git remote with the name ""origin"" is missing.\n Allow build to run even if there is no Git remote named origin >>> 1"
57,"If your native script needs to do some heavy computation on initialization,\nthe fact that we create a new one for every segment rather than for the whole\nindex could have a negative performance impact.\n Native scripts should be created once per index, not per segment. >>> 1"
58,"- Added a new argument to the smoke tester to also specify the release hash together with the elasticsearch commit hash, so that it works again\n- Adapted to new URLs\n- Added two missing plugins.\n- Downloaded gpg only once\n\nGets now called like this\n\n```\npython3 -B ./dev-tools/smoke_test_rc.py --version X.Y.Z --release-hash $RELEASE_COMMIT_HASH --hash $ES_COMMIT_HASH--plugins x-pack\n```\n Smoke tester: Adjust to latest changes >>> 1"
59,"When an active shadow replica is reinitialized during primary promotion, the recovery stats used by the allocation decider settings `cluster.routing.allocation.node_concurrent_recoveries` and `cluster.routing.allocation.node_concurrent_incoming_recoveries` are not correctly updated.\n\nFailing test: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.0+multijob-unix-compatibility/os=fedora/9/console\n Update incoming recoveries stats when shadow replica is reinitialized >>> 1"
60,"When testing tribe nodes in an integration test, we should pass the classpath\nplugins of the node down to the tribe client nodes. Without this the tribe client\nnodes could be prevented from communicating with the tribes.\n Pass classpath plugins to tribe nodes >>> 1"
61,"Today on Windows we attempt to set the GC options inside an if-block if\nthe environment variable ES_GC_OPTS is not set, as is the\ndefault. Unfortunately, Windows command prompt expands variables when\ncommands are parsed, not when they are run. For an if-block, the entire\ncommand is parsed before it is run. In this case, it means that\nES_GC_OPTS is """" for the life of the if-block, and so we are repeatedly\nsetting ES_GC_OPTS to """" and then some option that we intended to be\nappending. Thus, when the if-block exists, ES_GC_OPTS is only set to\n-XX:+UseCMSInitiatingOccupancyOnly. In particular, it means that the CMS\ncollector is never enabled and Windows users are sent to serial GC\npurgatory. This commit fixes this bug by enabling delayed expansion\naround the bad if-block.\n\nRelates #20558\n Fix Windows GC options >>> 1"
62,"@nik9000 found a bug related to String concatenation with the following query:\n\n```\ncurl -XPOST localhost:9200/_reindex?pretty&workers=5 -d{\n    ""source"": {\n      ""index"": ""source""\n    },\n    ""dest"": {\n      ""index"": ""source2""\n    },\n    ""script"": {\n      ""inline"": ""ctx._id += params.i + '.' + ctx._id"",\n      ""params"": {\n        ""i"": ""i""\n      }\n    }\n  }\n```\n\nThis PR fixes the bug which was related to an extra append being called since the assignment node was doing an erroneous check to see if the append was necessary.\n Fix String Concatenation Bug In Painless >>> 1"
63,"Surprise! You can use sliced scroll to easily parallelize reindex\nand friend. They support it because they use the same infrastructure\nas a regular search to parse the search request. While we would like\nto make an ""automatic"" option for parallelizing reindex, this manual\noption works right now and is pretty convenient!\n Document that sliced scroll works for reindex >>> 1"
64,- Remove the mention of using 0 to represent no throttle\n- Specify that the float must be greater than 0.\n\nSee #20625\n Update description for requests_per_second >>> 1
65,This is a issue in all 2.x releases that if we run into a FlushNotAllowedEngineException\non a replica (ie. a flush is already running) we fail the replica. We should just ignore this\nexception and not fail the shard.\n\nNote: this is against 2.x only. Master changed in #20597\nRelates to #20569\n Don't fail replica if FlushNotAllowedEngineException is thrown >>> 1
66,"Today we hold on to all possible tokenizers, tokenfilters etc. when we create\nan index service on a node. This was mainly done to allow the `_analyze` API to\ndirectly access all these primitive. We fixed this in #19827 and can now get rid of\nthe AnalysisService entirely and replace it with a simple map like class. This\nensures we don't create a gazillion long living objects that are entirely useless since\nthey are never used in most of the indices. Also those objects might consume a considerable\namount of memory since they might load stopwords or synonyms etc.\n\nCloses #19828\n Remove AnalysisService and reduce it to a simple name to analyzer mapping >>> 1"
67,"This PR removes the FailedRerouteAllocation and StartedRerouteAllocation\nclasses, as they were just wrappers for RerouteAllocation that stored\nstarted and failed shards, but these started and failed shards can\nbe passed in directly to the methods that needed them, removing the\nneed for this wrapper class and extra level of indirection.\n Removes FailedRerouteAllocation and StartedRerouteAllocation >>> 1"
68,"When initializing a new index routing table, we make a decision where the primary shards should be recovered from. This can be an empty folder for new indices, a set of specific allocation ids for old indices or a snapshot. We currently allow callers of `IndexRoutingTable.initializeEmpty` to supply the source but also set it automatically if null is given. Sadly the current logic is reusing the supplied parameter to store the result of the automatic decision. This is flawed if some of the decision should be _different_ between the different index shard (as the first decision that is maid sticks).\n\n This commit fixes this but also simplifies the API to always make an automatic decision.\n\nThis was discovered while working on #20637 which strengthens the testing infra and caused this to bubble up. I put it as a separate PR to make sure it is not lost as part of a bigger test only PR.\n IndexRoutingTable.initializeEmpty shouldn't override supplied primary RecoverySource >>> 1"
69,"Many of our unit tests instantiate an `AllocationService`, which requires having a `GatewayAllocator`. Today almost all of our test use a class called `NoopGatewayAllocator` which does nothing, effectively leaving all shard assignments to the balanced allocator. This is sad as it means we test a system that behaves differently than our production logic in very basic things. For example, a started primary that is lost will be assigned to a node that didn't use to have it.\n\nThis PR removes `NoopGatewayAllocator` in favor of a new `TestGatewayAllocator` that inherits the standard `GatewayAllocator` and overrides shard information fetching to return information based on historical assignments the allocator has done. The only exception is `BalanceConfigurationTests` which does test only the balancer and I opted to not have it work around the `GatewayAllocator` being in it's way.\n\nThe PR also fixes all resulting failures _plus_ one production failure for which I will open a dedicated PR (note - that fix is included here as well to make sure it works).\n Remove NoopGatewayAllocator in favor of a more realistic mock >>> 1"
70,"We're changing the URLs for the repos, and while the new URLs are in the install docs, we should be explicit and put it in the breaking changes so users don't assume they can just change the version number in the URL.\n Repo changes are breaking >>> 1"
71,Changes the API of GatewayAllocator#applyStartedShards and\nGatewayAllocator#applyFailedShards to take both a RoutingAllocation\nand a list of shards to apply. This allows better mock allocators\nto be created as being done in #20637.\n GatewayAllocator apply[Started|Failed]Shards method change >>> 1
72,Docs should be updated so the the YUM/APT urls point to the GA repository for 6.0 before release\n Update YUM/APT urls to point to the GA repository for 6.0 >>> 0
73,Docs should be updated so the the YUM/APT urls point to the GA repository for 5.0 before release\n Update YUM/APT urls to point to the GA repository for 5.0 >>> 0
74,"This PR adds a parameter to _cat apis that causes the rows returned by the api invocation to be sorted by a set of provided columns (header names).\n\nSorting on any object is supported, with .compareTo() being invoked for comparable objects, nulls being treated as the smallest object, and .toString() called on objects not implementing comparable before comparing them. Multiple header names can be provided, and they are applied to the sort in the order that they were provided.\n\nAn example invocation would be: `localhost:9200/_cat/templates?s=name,template`.\nAliases are also supported, so `localhost:9200/_cat/templates?s=n,t` would have an identical result to the first invocation. \n\nSorting is by default ascending, and can be made descending on a per-column basis by adding :desc to the header/header alias name. For example, `localhost:9200/_cat/templates?s=n:desc,t`.\n\nAlso included in this PR are compareTo methods for ByteSizeValue, SizeValue, and TimeValue, as these are the three 'values' that are special cased in RestTable.\n\nExamples:\n\nSimple example (sorting by name):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=name""\nname                  template     order version\npizza_hawaiian        *pineapples* 1     \npizza_pepperoni       *pepperoni*  1     \nsushi_california_roll *avocado*    1     1\n```\n\nAlias example (name sort with alias):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=n""\nname                  template     order version\npizza_hawaiian        *pineapples* 1     \npizza_pepperoni       *pepperoni*  1     \nsushi_california_roll *avocado*    1     1\n```\n\nReverse example (inverse name sort):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=name:desc""\nname                  template     order version\nsushi_california_roll *avocado*    1     1\npizza_pepperoni       *pepperoni*  1     \npizza_hawaiian        *pineapples* 1     \n```\n\nReverse alias example (inverse name sort with alias):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=n:desc""\nname                  template     order version\nsushi_california_roll *avocado*    1     1\npizza_pepperoni       *pepperoni*  1     \npizza_hawaiian        *pineapples* 1       \n```\n\nNull example (sort by version, some version values are missing):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=version""\nname                  template     order version\npizza_hawaiian        *pineapples* 1     \npizza_pepperoni       *pepperoni*  1     \nsushi_california_roll *avocado*    1     1\n```\n\nMultisort example (first sort by version, then by template):\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=version,template""\nname                  template     order version\npizza_pepperoni       *pepperoni*  1     \npizza_hawaiian        *pineapples* 1     \nsushi_california_roll *avocado*    1     1\n```\n\nMultisort alias example (version desc alias, then template)\n\n```\ncurl ""localhost:9200/_cat/templates?v&s=v:desc,t""\nname                  template     order version\nsushi_california_roll *avocado*    1     1\npizza_pepperoni       *pepperoni*  1     \npizza_hawaiian        *pineapples* 1      \n```\n\nSorting bytes example:\n\n```\ncurl ""localhost:9200/_cat/indices?v&h=health,status,index,docs.count,store.size&s=store.size""\nhealth status index    docs.count store.size\nyellow open   twix              0       795b\nyellow open   twizzler          2      3.3kb\nyellow open   tweeter          10      6.9kb\n```\n\nInverted sorting bytes example:\n\n```\ncurl ""localhost:9200/_cat/indices?v&h=health,status,index,docs.count,store.size&s=store.size:desc""\nhealth status index    docs.count store.size\nyellow open   tweeter          10      6.9kb\nyellow open   twizzler          2      3.3kb\nyellow open   twix              0       795b\n```\n\nSorting numbers example:\n\n```\ncurl ""localhost:9200/_cat/indices?v&h=health,status,index,docs.count,store.size&s=docs.count:desc""\nhealth status index    docs.count store.size\nyellow open   tweeter          10      6.9kb\nyellow open   twizzler          2      3.3kb\nyellow open   twix              0       795b\n```\n\nCloses #16975\n Adding built-in sorting capability to _cat apis. >>> 1"
75,In most places we model this as an enum e.g `wait_for_status`\n change health from string to enum in the cat indices rest test >>> 1
76,"We were swallowing the original exception when creating a client with bad credentials.\nSo even in `TRACE` log level, nothing useful were coming out of it.\nWith this commit, it now prints:\n\n```\n[2016-09-27 15:54:13,118][ERROR][cloud.azure.storage      ] [node_s0] can not create azure storage client: Storage Key is not a valid base64 encoded string.\n```\n\nCloses #20633.\n\nThis PR applies to 2.4 branch. When accepted, I'll also apply it to master / 5.x branch (and 5.0? cc @clintongormley)\n Fix logger when you can not create an azure storage client >>> 1"
77,"Today when getting setting via an API like the cluster settings API,\ncomplex settings are excluded (e.g.,\ndiscovery.zen.ping.unicast.hosts). This commit adds these settings to\nthe output of such APIs.\n Include complex settings in settings requests >>> 1"
78,"It currently returns something like:\n\n```\n""No feature for name [_siohgjoidfhjfihfg]""\n```\n\nWhich is not the most understandable message, this changes it to be a\nlittle more readable.\n\nResolves #10946\n Clean up confusing error message on unhandled endpoint >>> 1"
79,There was an issue with using fuzziness parameter in multi_match query that has been reported in #18710 and was fixed in Lucene 6.2 that is now used on master. In order to verify that fix and close the original issue this PR adds the test from that issue as an integration test.\n\nCloses #18710\n Add test for using fuzziness parameter in multi_match query >>> 1
80,This commit adds a usage warning when Elasticsearch is started with a\npre-release build.\n Add production warning for pre-release builds >>> 1
81,Quick fix to docs for version 2.4.\n Fix typo in delete-by-query.asciidoc >>> 1
82,this change adds a hard limit to `index.number_of_shard` that prevents\nindices from being created that have more than 1024 shards. This is still\na huge limit and can only be changed via settings a system property.\n Add a hard limit for `index.number_of_shard` >>> 1
83,"Today we allow system bootstrap checks to be ignored with a\nsetting. Yet, the system bootstrap checks are as vital to the health of\na production node as the non-system checks (e.g., the original bootstrap\ncheck, the file descriptor check, is critical for reducing the chances\nof data loss from being too low). This commit removes the ability to\nignore system bootstrap checks.\n Remove ignore system bootstrap checks >>> 1"
84,Follow up for #18790.\n\nWe need to be consistent with other discovery plugins and remove the support of GCE metadata `es_port`.\nIt means that we only discover elasticsearch instances running on port 9300.\n Remove es_port in GCE >>> 0
85,"We were swallowing the original exception when creating a client with bad credentials.\nSo even in `TRACE` log level, nothing useful were coming out of it.\nWith this commit, it now prints:\n\n```\n[2016-09-27 15:54:13,118][ERROR][cloud.azure.storage      ] [node_s0] can not create azure storage client: Storage Key is not a valid base64 encoded string.\n```\n\nCloses #20633.\n\nBackport of #20669 for master branch (6.0)\nWill be backported to 5.x branch at least.\n Fix logger when you can not create an azure storage client >>> 1"
86,"The invalid ingest configuration field name used to show itself,\neven when it was null, in error messages. Sometimes this does not make\nsense.\n\ne.g.\n`[null] Only one of [file], [id], or [inline] may be configure`\nvs.\n`Only one of [file], [id], or [inline] may be configure`\n\nThe above deals with three fields, therefore this no one property\nresponsible.\n no null values in ingest configuration error messages >>> 1"
87,Deprecate request parameters of _analyze API in 5.x\n\nAdd deprecation log\nAdd deprecation description in  the document\nAdd deprecation test in rest-api-test and unit test\n\nRelated #20246\n Deprecating request parameters of _analyze API in 5.x >>> 1
88, The Setting.timeValue() method uses TimeValue.toString() which can produce fractional time values. These fractional time values cannot be parsed again by the settings framework.\n\nThis commit fix a method that still use the .toString() method and replaces it with .getStringRep(). It also changes a second method so that it's not up to the caller to decide which stringify method to call.\n\ncloses #20662\n Fix Setting.timeValue() method >>> 1
89,"This makes geo-distance sorting use `LatLonDocValuesField.newDistanceSort`\nwhenever applicable, which should be faster that the current approach since it\ntracks a bounding box that documents need to be in in order to be competitive\ninstead of doing a costly distance computation all the time.\n\nCloses #20450\n Optimize geo-distance sorting. >>> 1"
90,Remove request params in _analyze API without index param\nChange rest-api-test using JSON\nChange docs using JSON\n\nCloses #20246\n Removing query-string parameters in `_analyze` API >>> 1
91,"Right now our unit tests in that area only simulate indexing single documents. As we go forward it should be easy to add other actions, like delete & bulk indexing. This commit extracts the common parts of the current indexing logic to a based class make it easier to extend.\n\nPS. This is needed for the seq no work\n ESIndexLevelReplicationTestCase: Make it easier to add new TRA-based actions >>> 1"
92,today it's not possible to use date math efficiently with the `_rollover`\nAPI. This change adds support for date-math in the target index as well as\nsupport for preserving the math logic when an existing index that was created with\na date math expression all subsequent indices are created with the same expression.\n\nthis has a pretty big impact on the search side since users can't efficiently use index patterns. This is crucial to get right from the beginning since users are used to search with date expressions. It also guarantees better performance and helps cluster stability if the number of indices can be reduced by simple expressions.\n Add date-math support to `_rollover` >>> 1
93,"Also added a test to check for a with a regular PDF,\ninstead of only an encrypted one with expected exception.\n Update ingest-attachment to latest libraries >>> 1"
94,"Exist requests are supposed to never throw an exception, but rather return `true` or `false` depending on whether some resource exists or not. Indices exists does that for indices and accepts wildcard expressions. The way the api works internally is by resolving indices like any other api would do and catching `IndexNotFoundException`: if an exception is thrown `false` is returned, otherwise `true`. That works only if `ignore_unavailable` and `allow_no_indices` indices options are both set to `false`, meaning that they are strict and any missing index or wildcard expression that resolves to no indices leads to an exception that can be thrown and cause `false` to be returned.\n\nUnfortunately the indices options have  been configurable up until now for the indices exists request, meaning that one can set `ignore_unavailable` or `allow_no_indices` to `true` and have the indices exist request return `true` for indices that really don't exist, which makes very little sense in the context of this api.\n\nFor instance `curl -iXHEAD localhost:9200/_all?allow_no_indices=true` returns `200 OK` against an empty cluster. `allow_no_indices` and `ignore_unavailable` affect badly what this api returns, hence I think they shouldn't be settable in this specific case.\n\nThis commit removes the `indicesOptions` setter from the `IndicesExistsRequest` and makes settable only `expandWildcardsOpen` and `expandWildcardsClosed`, hence a subset of the available indices options. This way we can guarantee more consistent behaviour of the indices exists api. `ignore_unavailable` will always be set to `false`, as well as `allow_no_indices`.\n Remove support for controversial `ignore_unavailable` and `allow_no_indices` from indices exists api >>> 1"
95,PR for #20563\n Upgrade geoip2 dependency >>> 1
96,"When a node get disconnected from the cluster and rejoins during a master election, it may be that the new master already has that node in it's cluster and will try to assign it shards. If the node hosts started primaries, the new shards will be initializing and will have the same allocation id as the allocation ids of the current started size. We currently do not recognize this currently. We should clean the current `IndexShard` instances and initialize new ones.\n\nThis also hardens test assertions in the same area.\n IndicesClusterStateService should clean local started when re-assigns an initializing shard with the same aid >>> 1"
97,"This change also fixes the version name for beta1, as it was never\nupdated from alpha6 in master.\n Build: Add 5.0.0-rc1 version >>> 1"
98,"Today when parsing a request, Elasticsearch silently ignores incorrect\n(including parameters with typos) or unused parameters. This is bad as\nit leads to requests having unintended behavior (e.g., if a user hits\nthe _analyze API and misspell the ""tokenizer"" then Elasticsearch will\njust use the standard analyzer, completely against intentions).\n\nThis commit removes lenient URL parameter parsing. The strategy is\nsimple: when a request is handled and a parameter is touched, we mark it\nas such. Before the request is actually executed, we check to ensure\nthat all parameters have been consumed. If there are remaining\nparameters yet to be consumed, we fail the request with a list of the\nunconsumed parameters. An exception has to be made for parameters that\nformat the response (as opposed to controlling the request); for this\ncase, handlers are able to provide a list of parameters that should be\nexcluded from tripping the unconsumed parameters check because those\nparameters will be used in formatting the response.\n\nAdditionally, some inconsistencies between the parameters in the code\nand in the docs are corrected.\n\nCloses #14719\n Remove lenient URL parameter parsing >>> 1"
99,"`CompositeIndicesRequest` should be implemented by all requests that are composed of multiple subrequests which relate to one or more indices. A composite request is\nexecuted by its own transport action class (e.g. `TransportMultiSearchAction` for `_msearch`), which goes through all the subrequests and delegates their execution to the appropriate transport action (e.g. `TransportSearchAction` for `_msearch`) for each single item. `IndicesAliasesRequest` is a particular request as it holds multiple items that implement `AliasesRequest`, but it shouldn't be considered a composite request, as it has no specific transport action for each of its items. Also, either all of its subitems fail or succeed.\n\nAlso clarified javadocs for `CompositeIndicesRequest`.\n IndicesAliasesRequest should not implement CompositeIndicesRequest >>> 1"
100,"We have new icons for elastic products with 5.0. This change updates the\nfavicon embedded in elasticsearch that users see when using the rest api\nthrough a browser.\n\nIt looks something like this:\n<img width=""132"" alt=""screen shot 2016-10-03 at 15 26 26"" src=""https://cloud.githubusercontent.com/assets/289412/19056396/d195abf8-897d-11e6-9290-ac678cfd88b2.png"">\n Update favicon >>> 1"
101,"`IndicesClusterStateService` and `IndicesStore` are responsible for synchronizing local shard state based on incoming cluster state updates. Actions that are taking are initializing shards, starting recoveries, or deleting shard and index data. On client or tribe nodes, which don't store any shard/index data, all of this is unnecessary, wasting unnecessary cycles to apply cluster state updates.\n Skip shard management code when updating cluster state on client/tribe nodes >>> 1"
102,"Today, the individual allocation deciders appear in random\norder when initialized in AllocationDeciders, which means\npotentially more performance intensive allocation deciders\ncould run before less expensive deciders. This adds to the\nexecution time when a less expensive decider could terminate\nthe decision making process early with a NO decision. This\ncommit orders the initialization of allocation deciders,\nbased on a general assessment of the big O runtime of each\ndecider, moving the likely more expensive deciders last.  \n\nThis manner of assessing the decider performance time is a \nbest guess and meant for a quick win in terms of performance\nbenefit.\n\nCloses #12815\n Process more expensive allocation deciders last >>> 1"
103,Elasticsearch 1.x used to implicitly round up upper bounds of queries when they\nwere inclusive so that eg. `[2016-09-18 TO 2016-09-20]` would actually run\n`[2016-09-18T00:00:00.000Z TO 2016-09-20T23:59:59.999Z]` and include dates like\n`2016-09-20T15:32:44`. This behaviour was lost in the cleanups of #8889.\n\nCloses #20579\n Make range queries round up upper bounds again. >>> 1
104,This also improves formatting a bit.\n Clarify some docs about geo-distance sorting. >>> 1
105,"Currently, bulk item requests can be any ActionRequest, this PR\nrestricts bulk item requests to DocumentRequest. This simplifies\nhandling failures during bulk requests. Additionally, a new enum\nis added to DocumentRequest to represent the intended operation\nto be performed by a document request (`create`, `index`, `update` \nand `delete`), which was previously represented with a mix of strings \nand index request operation type.\nNow, index request operation type reuses the new enum to specify \nwhether the request should `create` or `index` a document.\nRestricting bulk requests to DocumentRequest further simplifies\nexecution of shard-level bulk operations to use the same failure \nhandling for index, delete and update operations.\nThis PR also fixes a bug which executed delete operations twice for \nreplica copies while executing bulk requests.\n\nrelates https://github.com/elastic/elasticsearch/pull/19105\n Simplify bulk request execution  >>> 1"
106,On Windows the JDK uses `CreateFileW` which has a stupidly high\nlimit for the number of `Handle`s it can make - `16 * 1024 * 1024`.\nSo this isn't really a problem on Windows at all.\n\nCloses #20732\n File descriptors limit doesn't apply to Windows >>> 1
107,"Today it compiles when creating the aggregator, meaning that scripts will be\ncompiled as many times as there are buckets. Instead it should compile when\ncreating the factory so that scripts are compiled only once regardless of the\nnumber of buckets.\n The `top_hits` aggregation should compile scripts only once. >>> 1"
108,"This change proposes the removal of all non-tcp transport implementations. The\nmock transport can be used by default to run tests instead of local transport that has\nroughly the same performance compared to TCP or at least not noticeably slower.\n\nThis is a master only change, deprecation notice in 5.x will be committed as a\nseparate change.\n Remove LocalTransport in favor of MockTcpTransport >>> 1"
109,Before this change the processing of the ranges in the date range (and\nother range type) aggregations was done when the Aggregator was created.\nThis meant that the SearchContext did not know that now had been used in\na range until after the decision to cache was made.\n\nThis change moves the processing of the ranges to the aggregation builders\nso that the search context is made aware that now has been used before\nit decides if the request should be cached\n Fix date_range aggregation to not cache if now is used >>> 1
110,The previous commit d8b73e9 changed IndicesClusterStateService and IndicesStore to only run on master and data nodes. IndicesClusterStateService triggers acknowledgments of index deletes to the master. The current code that is waiting for the acknowledgments on the master is however expecting acknowledgements from tribe/client nodes as well. This commit reenables IndicesClusterStateService to run on tribe/client nodes to provide simple backward compatibility (but keeps IndicesStore disabled on tribe/client nodes). This is not an issue in the original patch on v5.x where acknowledgement of index deletes is done through the cluster state acking mechanism.\n Reenable shard management on IndicesClusterStateService >>> 1
111,Make getter for bulk shard requests items public rather than package private\n Make getter for bulk shard requests items visible >>> 1
112,"This commit changes the strict REST parameters message to say that\nunconsumed parameters are unrecognized rather than unused. Additionally,\nthe test is beefed up to include two unused parameters.\n\nRelates #20722\n Clarify wording for the strict REST params message >>> 1"
113,"This commit adds a did you mean feature to the strict REST params error\nmessage. This works by comparing any unconsumed parameters to all of the\nconsumer parameters, comparing the Levenstein distance between those\nparameters, and taking any consumed parameters that are close to an\nunconsumed parameter as candiates for the did you mean.\n\nRelates #20722\n Add did you mean to strict REST params >>> 1"
114,"Given that dnf doesn't do retries, installation of openjdk can sometimes be affected by checksum or network issues with mirrors offered by metalink.\n\nThis PR adds up to 5 retries for dnf based distros (Fedora for the moment).\nThe retry mechanism can be used by other distros too, by just setting the variable `install_command_retries` to any integer >0 in the corresponding section. At the moment this PR enables retries only for dnf based distros, as `install_command_retries` is set to `0` by default at the `provision()` level.\n\nThis PR has been extensively tested on Fedora 24: https://devops-ci.elastic.co/job/elastic+elasticsearch+master+fc24-retrytests/\n [vagrant] packaging tests: add parametrized retries for dnf install >>> 1"
115,"TRA currently resolves incoming requests to IndexShards in order to acquire operations locks on them. There is no need for all subclasses to have to go through the same IndicesService/IndexService song and dance. Also, doing it once means we don't need to worry about edge cases where the shard is removed while a TRA is in flight.\n TransportReplicationAction subclasses shouldn't have to resolve shards >>> 1"
116,"This commit adds the response params as candidates for the did you mean\nsuggestions for strict REST params handling.\n\nRelates #20722, relates #20747\n Add response params to REST params did you mean >>> 1"
117,We already override the name in plugin pom files to be that configured\nfor the plugin but we also need to explicitly set the artifactId.\n Build: Fix plugin poms to have correct artifact id >>> 1
118,"This commit improves the logic flow of BalancedShardsAllocator in\npreparation for separating out components of this class to be used\nin the cluster allocation explain APIs.  In particular, this commit:\n1. Adds a minimum value for the index/shard balance factor settings (0.0)\n2. Makes the Balancer data structures immutable and pre-calculated at\n   construction time.\n3. Removes difficult to follow labeled blocks / GOTOs\n4. Better logic for skipping over the same replica set when one of\n   the replicas received a NO decision\n5. Separates the decision making logic for a single shard from the logic\n   to iterate over all unassigned shards.\n BalancedShardAllocator code improvements >>> 1"
119,"This was an error-prone version type that allowed overriding previous\nversion semantics. It could cause primaries and replicas to be out of\nsync however, so it has been removed.\n\nThis is related to #20377, which removed the feature entirely. This\nallows operations to continue to use the `force` version type if the\nindex was created before 5.0, in the event a document using it exists in\na translog being replayed.\n Disallow VersionType.FORCE for 5.0+ indices >>> 0"
120,As the title says.\n\nThis closes (#20762).\n Remove all date 'now' methods from Painless >>> 1
121,"Adds support for `?slices=N` to reindex which automatically\r\nparallelizes the process using parallel scrolls on `_uid`. Performance\r\ntesting sees a 3x performance improvement for simple docs\r\non decent hardware, maybe 30% performance improvement\r\nfor more complex docs. Still compelling, especially because\r\nclusters should be able to get closer to the 3x than the 30%\r\nnumber.\r\n\r\nCloses #20624\r\n\r\n\r\n\r\nEdit: this used to say the below things but I've since changed it to make it reflect what I ended up implementing:\r\n\r\nAdds support for `?workers=N` to reindex which automatically\r\nparallelizes the process using parallel scrolls on `_uid`. Simple\r\nperformance testing sees a 3x performance improvement.\r\n Add automatic parallelization support to reindex and friends >>> 1"
122,"LongGCDisruption suspends and resumes node threads but respects several\n`unsafe` class name patterns where it's unsafe to suspend. For instance\nlog4j uses a global lock so we can't suspend a thread that is currently\ncalling into log4j. The same is true for the security manager, it's similar\nto log4j a shared resource between the test and the node that is _suspended_.\nThis change adds `java.lang.SecrityManager` to the unsafe patterns.\nThis prevents test framework deadlocking if a nodes thread is suspended\nwhile it's calling into the security manager that uses synchronized maps etc.\n Prevent thread suspension when inside SecurityManager >>> 1"
123,We already have some mechanism in place that prevents requests from being cached in the request cache if they use now(). Yet we don't have any streamlined way to assert that we are not accessing it later. We found some issues in #20645 that relate to stored scripts or scripts that are not pure functions. The immediate fix is to disable caching for these scripts. Unfortunately the script access wasn't streamlined in aggregations nor in query parsing / creation. This change adds a contained API that allows up to make cachabiltily decisions in a single place and causes requests to fail if they access scripts or `now()` after we cache the request.\n\nRelates to #20645\n Prevent requests that use scripts or now() from being cached >>> 1
124,"As the wise man @ywelsch said: currently when we batch cluster state update tasks by the same executor, we the first task un-queued from the pending task queue. That means that other tasks for the same executor are left in the queue. When those are dequeued, they will trigger another run for the same executor. This can give unfair precedence to future tasks of the same executor, even if they weren't batched in the first run. Take this queue for example (all with equal priority)\n\n```\n T1 (executor 1)\n T2 (executor 1)\n T3 (executor 2)\n T4 (executor 2)\n T5 (executor 1)\n T6 (executor 1)\n```\n\n If T1 & T2 are picked up first (when T5 & T6 are not yet queued), one would expect T3 & T4 to run second. However, since T2 is still in the queue, it will trigger execution of T5 & T6.\n\n The fix is easy - ignore processed tasks when extracting them from the queue.\n\nCloses #20768\n Improve scheduling fairness when batching cluster state changes with equal priority >>> 1"
125,Today `SearchContext` expose the _current_ context as a thread local which makes any kind of sane interface design very very hard. This PR removes the thread local entirely and instead passes the relevant context anywhere needed. This simplifies state management dramatically and will allow for a much leaner `SearchContext` interface down the road\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/19341 Remove SearchContext#current and all it's threadlocals >>> 1
126,"Currently, executing master level write operations against a tribe node leads to MasterNotDiscoveredException after a 1 minute timeout. This PR moves the setting ""action.master.force_local"" from TransportMasterNodeReadAction to its\nsuperclass TransportMasterNodeAction. Tribe nodes use this setting now to not only force local execution of metadata read operation, but also metadata write operation. The metadata write operations are then failed by making sure that the tribe always installs a metadata write block.\n\nThe class TransportMasterNodeReadAction becomes obsolete and has been deleted as part of the PR. When testing the PR, I noticed a tribe node test failing on a shard refresh. The reason is that refresh and flush operations check the metadata write block before executing. This makes little sense. Boaz suggested to remove the blocks for these 2 actions altogether, which is done here.\n\nSupersedes #15441\n Fail fast when executing master level write operations via a tribe node >>> 0"
127,"Mappings treat dots in field names as sub objects, for instance\n\n```\n{\n  ""a.b"": ""c""\n}\n```\n\ngenerates the same dynamic mappings as\n\n```\n{\n  ""a"": {\n    ""b"": ""c""\n  }\n}\n```\n\nSource filtering should be consistent with this behaviour so that an include\nlist containing `a` should include fields whose name is `a.b`.\n\nTo make this change easier, source filtering was refactored to use automata.\nThe ability to treat dots in field names as sub objects is provided by the\n`makeMatchDotsInFieldNames` method of `XContentMapValues`.\n\nCloses #20719\n Source filtering should treat dots in field names as sub objects. >>> 1"
128,Moves some test data used by those docs into the Elasticsearch\nrepository so we can use it when we test the docs during the build.\n\nRelates to #18160\n CONSOLEify remaining getting-started docs >>> 1
129,Relates to #20709\n Document date math use in the rollover API >>> 1
130,"Closes #20783\n Add a note that reindex does not set up mappings, etc >>> 1"
131,"This commit improves the shard decision container class in the following ways:\n1. Renames UnassignedShardDecision to ShardAllocationDecision, so that\n   the class can be used for general shard decisions, not just unassigned\n   shard decisions.\n2. Changes ShardAllocationDecision to have the final decision as a Type\n   instead of a Decision, because all the information needed from the final\n   decision is contained in `Type`.\n3. Uses cached instances of ShardAllocationDecision for NO and THROTTLE\n   decisions when no explanation is needed (which is the common case when\n   executing reroute's as opposed to using the explain API).  This saves the \n   deciders from a plethora of unnecessary object allocations in the common\n   case of non-explain mode.\n Shard Decision class improvements for Explain API >>> 1"
132,"UpdateHelper, MetaDataIndexUpgradeService, and some recovery\nstuff.\n\nMove a test from a single node test to a unit test.\n Deguice a few classes in IndicesModule >>> 1"
133,"The `StoreStatsCache` periodically sums up file size of all files it lists in the store directory.\n\nIt is expected that these files are sometimes deleted (and new files created) while it runs, since we don't prevent active indexing, so we catch and ignore FNFE and NSFE today:\n\n```\n    for (String file : files) {\n        try {\n            estimatedSize += directory.fileLength(file);\n        } catch (NoSuchFileException | FileNotFoundException e) {\n            // ignore, the file is not there no more\n        }\n    }\n```\n\nHowever, for some reason, on Windows it is also possible to sometimes hit `AccessDeniedException`.  E.g. see https://discuss.elastic.co/t/access-denied-in-accessing-the-elastic-search-index-files/39761/7 and https://github.com/elastic/elasticsearch/issues/17580 \n\nI dug a bit through the OpenJDK8 sources, and it eventually calls the `GetFileAttributesExW` Windows API, but I couldn't see in those docs (https://msdn.microsoft.com/en-us/library/windows/desktop/aa364946(v=vs.85).aspx) why access denied would occur.  So then I wrote a simple test:\n\n```\nimport java.nio.file.*;\nimport java.io.*;\n\npublic class Test {\n  public static void main(String[] args) throws IOException {\n\n    Thread thread = new Thread() {\n        @Override\n        public void run() {\n          while (true) {\n            for(int i=0;i<10;i++) {\n              try {\n                Files.size(Paths.get(""file"" + i));\n              } catch (NoSuchFileException | FileNotFoundException fnfe) {\n                // ok\n              } catch (IOException ioe) {\n                throw new RuntimeException(ioe);\n              }\n            }\n          }\n        }\n      };\n    thread.start();\n\n    while (true) {\n      for(int i=0;i<10;i++) {\n        OutputStream out = Files.newOutputStream(Paths.get(""file""+ i));\n        out.write(50);\n        out.close();\n      }\n      for(int i=0;i<10;i++) {\n        Files.delete(Paths.get(""file"" + i));\n      }\n    }\n  }\n}\n```\n\nMost of the time NSFE is hit, but sometimes `AccessDeniedException` does occur on a local NTFS file system.\n\nI think we should just catch and ignore `AccessDeniedException` as well.\n StoreStatsCache should also ignore AccessDeniedException when checking file size >>> 1"
134,"Previously, this doc was using a field called ""content"". This is\nconfusing, especially when the doc starts talking about the content of\nthe content field.  This change makes the field name ""comment"" which\nis less ambiguous and also changes some related field names in the doc\nto make a consistent example theme of editing docs around blog posts.\n [DOCS] Use a better name for fields in examples to avoid ambiguity >>> 1"
135,"Sequence number related data (maximum sequence number, local checkpoint,\nand global checkpoint) gets stored in Lucene on each commit. The logical\nplace to store this data is on each Lucene commit's user commit data\nstructure (see IndexWriter#setCommitData and the new version\nIndexWriter#setLiveCommitData). However, previously we did not store the\nmaximum sequence number in the commit data because the commit data got\ncopied over before the Lucene IndexWriter flushed the documents to segments\nin the commit.  This meant that between the time that the commit data was\nset on the IndexWriter and the time that the IndexWriter completes the commit,\ndocuments with higher sequence numbers could have entered the commit.\nHence, we would use FieldStats on the _seq_no field in the documents to get\nthe maximum sequence number value, but this suffers the drawback that if the\nlast sequence number in the commit corresponded to a delete document action,\nthat sequence number would not show up in FieldStats as there would be no\ncorresponding document in Lucene.\n\nIn Lucene 6.2, commit data was changed to take an Iterable interface, so\nthat the commit data can be calculated and retrieved _after_ all documents\nhave been flushed.  This commit changes max_seq_no so it is stored in the \ncommit data instead of being calculated from FieldStats, taking advantage of \nthe deferred calculation of the max_seq_no through passing an Iterable that \ndynamically sets the iterator data.\n\nRelates #10708 \n Sequence numbers commit data for Lucene uses Iterable interface >>> 1"
136,Previous to this change the `DateMathParser` accepted a `Callable<Long>` to use for accessing the now value. The implementations of this callable would fall back on `System.currentTimeMillis()` if there was no context object provided. This is no longer necessary for two reasons:\n- We should not fall back to `System.currentTimeMillis()` as a context should always be provided. This ensures consistency between shards for the now value in all cases\n- We should use a LongSupplier rather than requiring an implementation of Callable<Long>. This means that we can just pass in `context::noInMillis` for this parameter and not have not implement anything.\n Removes the now callable in the date math parser in favour of a LongSupplier >>> 1
137,since TransportAddress is now final we can simplify it's interface a bit\nand remove methods that are only used in tests or are plain delegates.\n Simplify TransportAddress >>> 1
138,This pull request adds notes to the breaking changes docs for #20722 and\n#20786.\n Add docs for strict REST params parsing >>> 1
139,The `QueryShardContext.failIfFrozen()` and `QueryShardContext.freezeContext()`\nmethods should be final so that overriding/bypassing the freezing of\n`QueryShardContext` is not possible. This is important so that we can\ntrust when the `QueryShardContext` says a request is cacheable.\n\nThis change also makes the methods that call `QueryShardContext.failIfFrozen()`\n`final` so they cannot be overridden to bypass setting the request as not\ncacheable.\n Makes freezing QueryShardContext safer by stopping overrides >>> 1
140,"The cache relies on the equals() method so we just need to make sure script\nqueries can never be equal, even to themselves in the case that a weight\nis used to produce a Scorer on the same segment multiple times.\n\nCloses #20763\n Do not cache script queries. >>> 1"
141,This commit upgrades the Log4j 2 dependency to version 2.7 and removes\nsome hacks that we had in place to work around bugs in Log4j 2 version\n2.6.2.\n\nCloses #20304\n Upgrade Log4j 2 to version 2.7 >>> 1
142,"We added a changed to disallow this in #19510, however, existing indices\nmay still be using this, so we should only disallow the setting on\nindices created after 5.0.\n\nResolves #20413\n Allow position_gap_increment for fields in indices created prior to 5.0 >>> 1"
143,"Instead of throwing a hard error, we should log about this and ignore\r\nthe setting.\r\n\r\nRelates to #20806\r\nRelates to #20413 Log about deprecated position_increment_gap setting on non-analyzed fields >>> 1"
144,Instead provide services where they are needed. The class worked\nwell as a temporary measure to easy removal of guice from the index\nlevel but now we can remove it entirely.\n\n-1 `@Inject` annotation\n Remove NodeServicesProvider >>> 1
145,"Today we throw an assertion error if we release an AbstractArray more than once.\nYet, it's recommended to implement close methods such that they can be invoked\nmore than once. Guaranteed single release calls are hard to implement and some\nsituations might not be tested causing for instance `CircuitBreaker` to operate on\ncorrupted memory stats.\n Prevent AbstractArrays from release bytes more than once >>> 1"
146,This change fixes the match_phrase_prefix query when a single term is queried on the _all field.\nIt builds a prefix query instead of an AllTermQuery which would not match any prefix.\n\nFixes #20470\n Fix match_phrase_prefix query with single term on _all field >>> 1
147,"Settings updates are important to be able to help and administer a cluster in distress. We shouldn't block it due to circuit breakers. An extreme example is where we are actually trying to increase and unreasonable low setting for the circuit breaker itself.\n\nSee https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/242/\n\nI've marked it as 5.0, but I'd be ok with 5.1 only, if people prefer.\n Cluster Settings Updates should not trigger circuit breakers. >>> 1"
148,Now that MatchNoDocsQuery has landed in Lucene we can safely replace our forked (org.elasticsearch.common.lucene.search.MatchNoDocsQuery) with the original Lucene query. \n\nrelates #18030\n Replace org.elasticsearch.common.lucene.search.MatchNoDocsQuery with its Lucene version (org.apache.lucene.search.MatchNoDocsQuery) >>> 1
149,SynonymQuery was ignored by the FastVectorHighlighter.\nThis change adds the support for SynonymQuery in the FVH.\nAlthough this change should be implemented in Lucene directly which is why https://issues.apache.org/jira/browse/LUCENE-7484 has been opened.\nIn the meantime this PR handles the issue on ES side and could be removed when LUCENE-7484 gets merged.\n\nFixes #20781\n Handle SynonymQuery extraction for the FastVectorHighlighter >>> 1
150,"This reverts commit 9411f18f27a023b28f28aaa6af904bc519cca8fa.\n\nRelates #18683, closes #20668\n Revert ""Display plugins versions"" >>> 1"
151,This commit adds a new Mustache function (codename: `url`) that can be used to URL encode strings.\n Mustache: Add {{#url}}{{/url}} function to URL encode strings >>> 1
152,`TcpTransport.ScheduledPing` doesn't handle rejected executions gracefully\nif the executor is shutting down. This change adds correct exception handling\nif we try to schedule another ping while the node is shutting down.\n\nWe had one test failure related to this [here](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=opensuse/85/console)\n Handle rejected pings on shutdown gracefully >>> 1
153,Fixed date math expression support in multi get requests.\nDate math index/alias expressions in `mget` will now be resolved to a concrete single index instead of failing the mget item with a `IndexNotFoundException`.\nNote `mget` uses the same `IndicesOptions` as `get`; which prevents the expression from resolving to multiple indices (i.e. wilcards).\nAdded integration test to verify multi index aliases do not fail the entire mget request.\n\nCloses #17957\n Fixed date math expression support in multi get requests. >>> 1
154,Add important note for supported Azure Storage account types\n\nCloses #20765\n Supported Azure Storage account types >>> 1
155,"I've been using this for a while without problems, and it seems to be helping on the gradle speed side without causing any problems.\n\n@rjernst thoughts on this?\n Enable incremental compilation in gradle >>> 1"
156,"The snapshot restore state tracks information about shards being restored from a snapshot in the cluster state. For example it records if a shard has been successfully restored or if restoring it was not possible due to a corruption of the snapshot. Recording these events is usually based on changes to the shard routing table, i.e., when a shard is started after a successful restore or failed after an unsuccessful one. As of now, there were two communication channels to transmit recovery failure / success to update the routing table and the restore state. This lead to issues where a shard was failed but the restore state was not updated due to connection issues between data and master node. In some rare situations, this lead to an issue where the restore state could not be properly cleaned up anymore by the master, making it impossible to start new restore operations. The following change updates routing table and restore state in the same cluster state update so that both always stay in sync. It also eliminates the extra communication channel for restore operations and uses the standard cluster state listener mechanism to update restore listener upon successful completion of a snapshot restore.\n\nCloses #19774\n Keep snapshot restore state and routing table in sync >>> 1"
157,some fix and improvement for common util class 'Strings'\n Common Utils Class Strings to improve >>> 0
158,This change adds a overloaded `XContentMapValues#filter` method that returns\na function enclosing the compiled automatons that can be reused across filter\ncalls. This for instance prevents compiling automatons over and over again when\nhits are filtered or in the SourceFieldMapper for each document.\n\nCloses #20839\n Ensure source filtering automatons are only compiled once >>> 1
159,"This option should not be recommended to anyone, and should never be\nused, upon chance of primary/replica divergence.\n\nRelates to #19769\n [DOCS] Remove documentation for `force` version-type >>> 1"
160,MultiGet should not fail entirely when one of the items of a multi get request refers to an alias that points to multiple indices.\n\ncloses #20845\n MultiGet should not fail entirely if alias resolves to many indices >>> 1
161,"In 2.x, the S3 repository accepted a `/` (forward slash) to start\nthe repositories.s3.base_path, and it used a different string splitting\nmethod that removed the forward slash from the base path, so there\nwere no issues.\n\nIn 5.x, we removed this custom string splitting method in favor of\nthe JDK's string splitting method, which preserved the leading `/`.\nThe AWS SDK does not like the leading `/` in the key path after the\nbucket name, and so it could not find any objects in the S3 repository.\n\nThis commit fixes the issue by removing the leading `/` if it exists\nand adding a deprecation notice that leading `/` will not be supported\nin the future in S3 repository's base_path.\n Fixes leading forward slash in S3 repository base_path >>> 1"
162,Shadow replicas can not be simply promoted to primary by updating boolean like normal shards. Instead the are reinitialized and shut down and rebuilt as primaries. Currently we also given them new allocation ids but that throws off the in-sync allocation ids management. This commit changes this behavior to keep the allocation id of the shard.\n\nCloses #20650\n Keep a shadow replicas' allocation id when it is promoted to primary >>> 1
163,"Replace ""we can see and total of ..."" by ""we can see a total of ...""\n [DOCS] Fix typo in ""Cluster Health"" part >>> 1"
164,Fixed writeable name from range to geo_distance\n Fixed writeable name from range to geo_distance >>> 1
165,Because I'm tired to see those empty javadoc comments every day.\n Remove empty javadoc >>> 1
166,"This adds back a User-Agent header, which we had before large plugin\nscript refactorings for 5.0. The value is now `elasticsearch-plugin`\nwhich is the name of the script.\n Plugins: Add back user agent when downloading plugins >>> 1"
167,"When refactoring DirectCandidateGeneratorBuilder recently, the ConstructingObjectParser that we have today was not available. Instead we used some workaround, but it is better to remove this now and use ConstructingObjectParser instead.\n Use ConstructingObjectParser for parsing DirectCandidateGenerator >>> 1"
168,"today we might release a bytes array more than once if the send listener\nthrows an exception but already has released the array. Yet, this is already fixed\nin the BytesArray class we use in production to ensure 3rd party users don't release\ntwice but our mocks still enforce it.\n\nsee this build for reference https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=debian/95/consoleFull\n Prevent double release in TcpTransport if send listener throws an exception >>> 1"
169,"As suggested by @nik9000 in https://github.com/elastic/elasticsearch/pull/20871#issuecomment-253010319, this PR adds a checkstyle check to ensure that Java files do not contain any empty Javadoc comment.\n\nThis also fix few remaining empty comments.\n Add checkstyle rule to forbid empty javadoc comments >>> 1"
170,"During a recent merge from master, we lost the bridge from IndicesClusterStateService to the GlobalCheckpointService of primary shards, notifying them of changes to the current set of active/initilizing shards. This commits add the bridge back (with unit tests). It also simplifies the GlobalCheckpoint tracking to use a simpler model (which makes use the fact that the global check point sync is done periodically).\n\nThe old integration CheckpointIT test is moved to IndexLevelReplicationTests. I also added simliar assertions to RelocationsIT, which surfaced a bug in the primary relocation logic and how it plays with global checkpoint updates. The test is currently await-fixed and will be fixed in a follow up issue.\n Simplify GlobalCheckpointService and properly hook it for cluster state updates >>> 1"
171,This changes the CacheBuilder methods that are used to set expiration times to accept a\nTimeValue instead of long. Accepting a long can lead to issues where the incorrect value is\npassed in as the time unit is not clearly identified. By using TimeValue the caller no longer\nneeds to worry about the time unit used by the cache or builder.\n Use TimveValue instead of long for CacheBuilder methods >>> 1
172,Before this change the `MultiMatchQuery` called the field types\n`termQuery()` with a null context. This is not correct so this change\nfixes this so the `MultiMatchQuery` now uses the `ShardQueryContext` it\nstores as a field.\n\nRelates to https://github.com/elastic/elasticsearch/pull/20796#pullrequestreview-3606305\n Fixes MultiMatchQuery so that it doesn't provide a null context >>> 1
173,"Currently the ""multi_match"" query accepts an array of strings as it's ""query"" parameter but actually performs the search using only the last value of the array. Specifying an array here should result in a parsing error instead.\n\nCloses #20785\n Stricter parsing of multi_match ""query"" parameter >>> 1"
174,"Sometimes it's useful / needed to use unreleased `Version` constants but we should not add those to the `Version.java` class for several reasons ie. BWC tests and assertions along those lines. Yet, it's not really obvious how to do that so I added some comments and a simple test for this.\n Explain how unreleased versions should be added to the codebase without adding it to Version.java >>> 1"
175,Both netty3 and netty4 http implementation printed the default\ntoString representation of PortRange if ports couldn't be bound.\nThis commit adds a better default toString method to PortRange and\nuses the string representation for the error message in the http\nimplementations.\n\ntoday it looks like this:\n\n`Caused by: org.elasticsearch.http.BindHttpException: Failed to bind to [org.elasticsearch.common.transport.PortsRange@1b10f60e]`\n Ensure port range is readable in the exception message >>> 1
176,"Change stored scripts/file scripts to take in a context, and ensure that context is available at compile-time as a precursor to this feature (#20426).\n\nThis is currently a WIP since it's a large change, and there's some way to go.  I wanted to get early feedback on this portion.\n\nThe steps for this change are the following:\n- [ ] Modify stored scripts API to allow for context, store lang, and make the namespace only based on id.  \n- [ ] Modify the file script API to allow for context, store lang, and make the namespace only based on id.\n- [ ] Update compilation by splitting the scripts into the 3 types of file, stored, and inline.  Ensure that context is always passed in upon compilation.\n Update Scripting API to Store Context for Scripts >>> 0"
177,This commit adds a note to the thread pool docs regarding the bound of\n32 on the number of processors used when sizing the thread pools.\n\nRelates #20828\n Add doc note regarding processors bound >>> 1
178,It is important that folks understand that snapshot/restore isn't\nfor archiving. It is appropriate for backup and disaster recovery\nbut not for archival over long periods of time because of version\nincompatibility.\n\nCloses #20866\n Docs: note about snapshot version compatibility >>> 1
179,"Today Elasticsearch limits the number of processors used in computing\nthread counts to 32. This was from a time when Elasticsearch created\nmore threads than it does now and users would run into out of memory\nerrors. It appears the real cause of these out of memory errors was not\nwell understood (it's often due to ulimit settings) and so users were\nleft hitting these out of memory errors on boxes with high core\ncounts. Today Elasticsearch creates less threads (but still a lot) and\nwe have a bootstrap check in place to ensure that the relevant ulimit is\nnot too low.\n\nThere are some caveats still to having too many concurrent indexing\nthreads as it can lead to too many little segments, and it's not a\nmagical go faster knob if indexing is already bottlenecked by disk, but\nthis limitation is artificial and surprising to users and so it should\nbe removed.\n\nCloses #20828\n Remove artificial default processors limit >>> 1"
180,"This change adds a `hits` section to the response part for each ranking evaluation query, containing a list of documents (index/type/id) and ratings (if the document was rated in the request). This section can be used to better understand the calculation of the ranking quality of this particular query, but it can also be used to identify the ""unknown"" (that is unrated) documents that were part of the seach hits, for example because a UI later wants to present those documents to the user to get a rating for them. If the user specifies a set of field names using a parameter called `summary_fields` in the request, those fields are also included as part of the response in addition to ""_index"", ""_type"", ""_id"".\n\nThere are two other changes to the internal structuring of the code that I'd like to discuss with this PR as well:\n-  Pull common operations into RankedListQualityMetric interface\n  \n  Currently each implementation of RankedListQualityMetric does some initial joining operation that links the input search hits with a rated document rating, if available. Also all metrics collect unknown docs and now also need to add the list of rated search hits to the partial query evaluation. This change centralizes this work in some new helper methods in RankedListQualityMetric.\n-  Remove unknown docs from EvalQueryQuality\n  \n  The unknown document section in the response for each query can be rendered using the rated hits that are now also part of the response by just filtering the documents without a rating. By removing the unknown docs section from the EvalQueryQuality we save work serializing this object over the wire. Should we choose to make the `hits` section optional later (via a verbose mode) we'd need to decide if we still always want to include the unknown docs for each request in the response. In that case we'd probably still need this structure.\n\nRelatest to #20364 \n RankEval: Add `hits` section to response for each query >>> 1"
181,"Solves #11579.\n\nAdded option to term suggest that returns the input term if it exists. Turned off by default so it doesn't break normal behaviour.\n\nThe example query given in #11579 would now be:\n\n```\n""Suggest"": {\n      ""term"": {\n        ""field"": ""word"",\n        ""suggest_mode"": ""popular"",\n        ""size"": 2,\n        ""prefix_len"": 1,\n        ""analyzer"": ""default"",\n        ""exact_matching"": true\n      },\n      ""text"": ""Software""\n    }\n```\n\nAnd the result:\n\n```\n""Suggest"": [\n      {\n         ""text"": ""software"",\n         ""offset"": 0,\n         ""length"": 8,\n         ""options"": [\n            {\n               ""text"": ""software"",\n               ""score"": 1,\n               ""freq"": 1\n            }\n         ]\n      }\n   ]\n```\n\nThe output now clearly indicates if the term is present. If it is, its score is always 1, representing an exact match, which differentiates this result from other suggestions.\n Add Exact Match option to Suggest >>> 0"
182,Remove Nullable notation\nAdd unit test\n\nCloses #20174\n IndexSettings should not be Null in Mapper.BuildContext >>> 1
183,nan Removed unnecessary assertion on boolean values >>> 1
184,`AbstractSearchAsyncAction` has only been tested in integration tests.\nThe infrastrucutre is rather critical and should be tested on a unittest\nlevel. This change takes the first step\n Make AbstractSearchAsyncAction more testable and add a basic test case >>> 1
185,"This PR updates the version of Checkstyle from current `5.9` to `7.1` and fix a bunch of new violations.\n\nA more recent version would allow us to use some new checkstyle modules (like [Javadocs](http://checkstyle.sourceforge.net/config_javadoc.html)). \n\n`RedundantModifier` is improved and now detects more violations that are fixed by this PR. The changes fall into three categories (thanks @nik9000 for the nice summary):\n- `public` methods on non-`public` classes don't make sense. Remove the modifier and let the method inherit the modifier.\n- try-with-resources variables should not be declared `final` because they are implicitly `final`. `final` adds no extra information for the reader. I guess that makes them names rather than variables....\n- `enum`s are alway `static` so there is no need to declare them `static`.\n- `final` doesn't do anything on anonymous classes because they can't be extended anyway. So we just remove the modifier.\n- Classes declared inside of interfaces are always `static` so we shouldn't also declare them as `static`.\n\nFor the record, most of the changes were automated using this small Bash script:\n\n```\n#!/bin/bash\ninput=""/tmp/violations.txt""\n\n# File is composed of lines like:\n# /tmp/elasticsearch/src/test/java/org/elasticsearch/action/Action.java:10:9:public\n# \nwhile IFS=':' read -ra ADDR; do\n    if [ ""${ADDR[3]}"" != """" ]; then\n        echo ""${ADDR[0]}"" | xargs sed -i ""${ADDR[1]}s/${ADDR[3]} //""\n    else \n        echo ""Skipping file ${ADDR[0]} line ${ADDR[1]} word ${ADDR[3]}""\n    fi\ndone < ""$input""\n```\n\nCheckstyle was not updated to the latest version 7.1.2 because it reports violations for `final` parameters in interface methods (checkstyle/checkstyle#3322) and we have a lot of these for good reason.\n\nI'm really sorry for the boringness of this kind of pull request.\n Update checkstyle and fix violations >>> 0"
186,"This changes the above mentioned SortBuilders to parsing via ObjectParser to make them more concise and use our new parser infrastructure more widely. \n Use ObjectParser in Score-, Field- and ScriptSortBuilder >>> 1"
187, This commit removes unused parameters from the Update-By-Query and Delete-By-Query REST specification files.\n Update Delete/Update-By-Query REST Specs >>> 1
188,"Today we don't parse alias filters on the coordinating node, we only forward\nthe alias patters to executing node and resolve it late. This has several problems\nlike requests that go through filtered aliases are never cached if they use date math,\nsince the parsing happens very late in the process even without rewriting. It also used\nto be processed on every shard while we can only do it once per index on the coordinating node.\nAnother nice side effect is that we are never prone to cluster-state updates that change an alias,\nall nodes will execute the exact same alias filter since they are process based on the same\ncluster state.\n Parse alias filters on the coordinating node >>> 1"
189,This PR references issue https://github.com/elastic/elasticsearch/issues/20920\n Deprecate EOL'ed Ubuntu 15.04 from Vagrantfile >>> 1
190,"The create request now requires that an ID be present.\nCurrently the clients hard code a create method, but\nwe should just add a create REST spec so this method\ncan be autogenerated.\n Add a REST spec for the create API >>> 1"
191,This commit removes the duplicated `timeout` parameter introduced in #20915\n Remove duplicate timeout parameter in Delete/Update-By-Query REST Specs >>> 1
192,"This PR changes the current REST API parser to make it fail and throw an exception when a REST specification file contains a duplicated parameters, or path, or method, or path part. This was suggested by @javanna in https://github.com/elastic/elasticsearch/pull/20934#pullrequestreview-4227796\n REST API parser should fail on duplicate params/paths/methods/parts >>> 1"
193,"<!--\nThank you for your interest in and contributing to Elasticsearch! There\nare a few simple things to check before submitting your pull request\nthat can help with the review process. You should delete these items\nfrom your submission, but they are here to help bring them to your\nattention.\n-->\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\n typo fix for index name >>> 0"
194,"There is currently a very confusing behavior in Elasticsearch for the\nfollowing:\n\nGiven the indices: `[test1, test2, -foo1, -foo2]`\n\n```\nDELETE /-foo*\n```\n\nWill cause the `test1` and `test2` indices to be deleted, when what is\nusually intended is to delete the `-foo1` and `-foo2` indices.\n\nPreviously we added a change in #20033 to disallow creating indices\nstarting with `-` or `+`, which will help with this situation. However,\nusers may have existing indices starting with these characters.\n\nThis changes the negation to only take effect in a wildcard (`*`) has\nbeen seen somewhere in the expression, so in order to delete `-foo1` and\n`-foo2` the following now works:\n\n```\nDELETE /-foo*\n```\n\nAs well as:\n\n```\nDELETE /-foo1,-foo2\n```\n\nso in order to actually delete everything except for the ""foo"" indices\n(ie, `test1` and `test2`) a user would now issue:\n\n```\nDELETE /*,--foo*\n```\n\nRelates to #19800\n Only negate index expression on all indices with preceding wildcard >>> 1"
195,Cleaning up a few remaining occurences of using junits ExpectedException rule in favor of using LuceneTestCase#expectThrows() which is more concise and versatile.\n Use Lucenes expectThrows() when testing exceptions >>> 1
196,"Currently, any write (e.g. `index`, `delete`) operation failure can be categorized as:\n- request failure (e.g. analysis, parsing error, version conflict) \n- transient operation failure (e.g. due to shard initializing, relocation)\n- environment failure (e.g. out of disk, corruption, lucene tragic event)\n\nThe main motivation of the PR is to handle these failure types appropriately for a \nwrite request. Each failure type needs to be handled differently:\n- request failure (being request specific) should be replicated and then failed\n- transient failure should be retried (eventually succeeding)\n- environment failure (persistent primary shard failure) should fail the request \n  immediately.\n\nCurrently, transient operation failures are retried in replication action but no distinction\nis made between request and environment failures, both fails write request immediately. \n\nIn this PR, we distinguish between request and environment failures for a write operation. \nIn case of environment failures, the exception is bubbled up failing the request and in case \nof request failures, the exception is captured and replication continues (we ignore performing \non replicas when such failures occur in primary). Transient operation failures are bubbled up \nto be retried by the replication operation, as before. \n\nNote: https://github.com/elastic/elasticsearch/issues/20109 simplifies bulk execution code, which should clean up error handling for shard bulk requests.\n Simplify write failure handling >>> 1"
197,"Today when logging an unknown or invalid setting, the log message does\nnot contain the source. This means that if we are archiving such a\nsetting, we do not specify where the setting is from (an index, and\nwhich index, or a persistent or transient cluster setting). This commit\nprovides such logging for the end user can better understand the\nconsequences of the unknown or invalid setting.\n\nCloses #20946\n Add precise logging on unknown or invalid settings >>> 1"
198,"Since 2.0, booleans have been represented as numeric fields (longs).\nHowever, in scripts, this is odd, since you expect doing a comparison\nagainst a boolean to work. While languages like groovy will auto convert\nbetween booleans and longs, painless does not.\n\nThis changes the doc values accessor for boolean fields in scripts to\nreturn Boolean objects instead of Long objects.\n\ncloses #20949\n Add support for booleans in scripts >>> 1"
199,"As part of b6537d719d840dc987a1fb2df4b48bf260e53371, a check for the `Warnings` headers has been added. Since the feature might be temporarily or permanently not supported by all clients (as is the case with eg. the `headers` feature), we should allow them to skip the tests.\n\nRelated: #20246, #20686\n/cc @johtani\n Added the `skip` configuration for `warnings` check in the ""Indices Analyze"" test >>> 1"
200,"This commit fixes an issue with the handling of the value ""close"" on the\nConnection header in the Netty 4 HTTP implementation. The issue was\nusing the wrong equals method to compare an AsciiString instance and a\nString instance (they could never be equal). This commit fixes this to\nuse the correct equals method to compare for content equality.\n\nCloses #20938\n Fix connection close header handling >>> 1"
201,"`LocalDiscovery` is a discovery implementation that uses static in memory maps to keep track of current live nodes. This is used extensively in our tests in order to speed up cluster formation (i.e., shortcut the 3 second ping period used by `ZenDiscovery` by default). This is sad as that mean that most of the test run using a different discovery semantics than what is used in production. Instead of replacing the entire discovery logic, we can use a similar approach to only shortcut the pinging components.\n\nNote: I will make two separate PRs to highlight a couple of changes that needed here as well:\n1) cluster state publishing shouldn't be subjected to circuit breaker.\n2) a better handling of having no min_master_nodes set. With the current implementation starting multiple nodes async and/or not waiting on the first node to elect it self are subject to race conditions.\n\nI'm tempted to put this into 5.x but am a bit hesitant due to the usage of `LocalDiscovery` in the tribe service. Please let me know if you have an opinion about this.\n Remove local discovery in favor of a simpler `MockZenPings` >>> 1"
202,Many tests currently use try-catch constructs to test for expected exceptions and usually do some assertions on the exception message. This can be achieved by using LuceneTestCase#expectThrows() in a more concise way. Also adding some try-with-resources around some streams that were not closed in tests before to remove those compile warnings. There are still more occurences of this pattern in the tests but this is a first step at replacing those.\n Use expectThrows() instead of try-catch blocks for testing expected exceptions >>> 1
203,This PR changes some permissions on files.\n Change permissions on config files >>> 1
204,"This change adds an option called `split_on_whitespace` which prevents the query parser to split free text part on whitespace prior to analysis. Instead the queryparser would parse around only real 'operators'. Default to true.\nFor instance the query `""foo bar""` would let the analyzer of the targeted field decide how the tokens should be splitted.\nSome options are missing in this change but I'd like to add them in a follow up PR in order to be able to simplify the backport in 5.x. The missing options (changes) are:\n- A `type` option which similarly to the `multi_match` query defines how the free text should be parsed when multi fields are defined.\n- Simple range query with additional tokens like "">100 50"" are broken when `split_on_whitespace` is set to false. It should be possible to preserve this syntax and make the parser aware of this special syntax even when `split_on_whitespace` is set to false.\n- Since all this options would make the `query_string_query` very similar to a match (multi_match) query we should be able to share the code that produce the final Lucene query.\n\nFixes #20841\n Expose splitOnWhitespace in `Query String Query` >>> 1"
205,The TypeQuery has an optimization when only one type is present in an index.\nThis optimization is used when a type query is explicitly used in the query (_type:t) but it is not leveraged when the type is in the URL (GET t/t/_search).\nThis change uses the TypeQuery when the type filter is built from the URL (GET t/t/_search).\nSince the number of types is unbounded in a query this change also preserves the current behavior which uses a TermsQuery when the number of types to query is greater than a threshold (16 by default).\n Optimize query with types filter in the URL (t/t/_search) >>> 1
206,The max score returned in the response of a query does not take rescorer into account.\nThis change updates the max_score when a rescorer is used in a query.\nFixes #20651\n Max score should be updated when a rescorer is used >>> 1
207,When using a top hits aggregation the rescorer are ignored.\nThis change applies the rescorer to the top hits of each bucket.\n\nFixes #19317\n Rescorer should be applied in the TopHits aggregation >>> 1
208,"- fixes a bug in the docs that mentions `lang` as optional\n- now `lang` defaults to ""painless""\n\nCloses #20943.\n make painless the default scripting language for ScriptProcessor >>> 1"
209,"Similar to the additional `ignore_missing` param that was added to many of the processors from: https://github.com/elastic/elasticsearch/issues/19995.\n\nWhen option is `true`, `null`-valued and non-existent fields will not result in a thrown exception.\n\nCloses #20840.\n add `ignore_missing` option to SplitProcessor >>> 1"
210,"Updating the circuit breaker settings (and other settings) should always be possible, even if the cluster is under stress. With #20827 we updated the cluster settings request to not trigger circuit breakers. However that change is not complete since the resulting cluster state can potentially not be published. This change makes sure cluster state publishing to not trigger circuit breakers as well.\n\nRelates to #20960 where this was discovered.\n ClusterState publishing shouldn't trigger circuit breakers >>> 1"
211,"Closes #20992 \n\nIf system crashes after creating the .es_temp_file but before deleting this file, next time the system will not be able to start because the Files.createFile(resolve) will throw an exception because of the left over temp file.\nSo We should first check if the temp file exists, if yes, delete it and create, delete it again; if not exist, then create and delete it.\n delete temp file if file exists >>> 0"
212,"fix a variable name for misspelling ""tracerLogExclude"" to ""tracelLogExclude""\n tracelLogExclude to tracerLogExclude >>> 1"
213,"This experimental setting enables relocation of shards that are being snapshotted, which can cause the shard allocation failures. This setting is undocumented and there is no good reason to ever set it in production.\n Remove `cluster.routing.allocation.snapshot.relocation_enabled` setting >>> 1"
214,The `fuzzy` query is deprecated from 5.0 on. Similar to IndicesQueryBuilder we should log a deprecation warning whenever this query is used.\n\nRelates to #15760\n Add deprecation logging message for 'fuzzy' query >>> 1
215,"This tests that the templates shipped with 5.0 versions of Logstash and\nBeats still work on an Elasticsearch 6.0+ node, so that we ensure that\nES can be upgraded prior to upgrading tools dependent on it.\n\nRelated to #20491\nResolves #17275\n Add rudimentary logstash and beats template BWC tests >>> 1"
216,"When running `gradle run`, a developer usually intends to get a running\ninstance as if they had run elasticsearch from the command line. This is\ndifferent than the isolated environment we use for integration testing\nplugins. This change switches the run task to use the zip distribution,\nso that all modules included in the normal distribution are included.\n Change `gradle run` to use zip distribution >>> 1"
217,"This allows you to whitelist `localhost:*` or `127.0.10.*:9200`.\nIt explicitly checks for patterns like `*` in the whitelist and\nrefuses to start if the whitelist would match everything. Beyond\nthat the user is on their own designing a secure whitelist.\n Add ""simple match"" support for reindex-from-remote whitelist >>> 1"
218,These scripts are no longer used now that we have the unified release\nprocess.\n Remove old release scripts >>> 1
219,Refactors the BalancedShardsAllocator to create a method that\nprovides an allocation decision for allocating a single\nunassigned shard or a single started shard that can no longer\nremain on its current node.  Having a separate method that\nprovides a detailed decision on the allocation of a single shard\nwill enable the cluster allocation explain API to directly\ninvoke these methods to provide allocation explanations.\n Separates decision making from decision application in BalancedShardsAllocator  >>> 1
220,"Closes #20992 \n\nMove the delete to `finally` block to ensure the file will be deleted when there is a left over temp file from a crash. \n .es_temp_file remains after system crash, causing it not to start again >>> 1"
221,"Relates to #20231\n\nCaveats:\n- This does not yet reflect exactly the request API w/ template as discussed in\n- One of the yaml tests this comes with does not yet pass (namely 30_template.yaml), currently I'm scratching my head whether this should even be there. If the answer is ""no"" I'll continue scratching my head how to best test this, would love to have some sort of integration test. Posting for early feedback.\n\nI tried writing a REST test to show the whole thing actually works. While this was helpful in finding some request parsing issues, the test currently doesn't work as intended as no specific mustache integration is on the classpath for the module. Also I feel like I'm replicating some of the request parsing logic we already have for template search requests. Would be great if @martijnvg could\nhave a look and provide some feedback.\n First step towards supporting templating in rank eval requests. >>> 1"
222,"This is made up of two parts, a revert of b4cc3cd so `force` version is still available for 5.x indices, as well as a commit to disallow it for all 6.0+ indices.\n\nOriginal commit message:\n\nThis was an error-prone version type that allowed overriding previous\nversion semantics. It could cause primaries and replicas to be out of\nsync however, so it has been removed.\n\nThis is related to #20377, which removed the feature entirely. This\nallows operations to continue to use the `force` version type if the\nindex was created before 6.0, in the event a document using it exists in\na translog being replayed.\n Disallow `VersionType.FORCE` versioning for 6.x indices >>> 1"
223,"<!--\nThank you for your interest in and contributing to Elasticsearch! There\nare a few simple things to check before submitting your pull request\nthat can help with the review process. You should delete these items\nfrom your submission, but they are here to help bring them to your\nattention.\n-->\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\n\nDocuments comment from Yannick Welsch in Forums:  https://discuss.elastic.co/t/500-error-synonym-file-cant-be-read/62345/3\n Update synonym-tokenfilter.asciidoc >>> 0"
224,Fixes two minor issues\n- An import from an already removed file was used.\n- The strict URL & parameter parsing hit one of the calls\n Testing: Fix smoke tester >>> 1
225,"Lucene 6.2 added support to index and query numeric ranges. This PR adds a new `RangeFieldMapper` for indexing numeric (int, long, float, double) and date ranges and creating appropriate range and term queries. The design is similar to `NumericFieldMapper` in that it uses a `RangeType` enumerator for implementing logic specific to each numeric type. The following new field types are supported by this mapper: `int_range`, `float_range`, `long_range`, `double_range`, `date_range`.\n\nLucene does not provide a DocValue field specific to RangeField types so the `RangeFieldMapper` implements a `CustomRangeDocValuesField` for handling doc value support.\n\nWhen executing a Range query over a Range field, the `RangeQueryBuilder` has been enhanced to accept a new `relation` parameter for defining the type of query as one of: `WITHIN`, `CONTAINS`, `INTERSECTS`. This provides support for finding all ranges that are related to a specific range in a desired way. As with other spatial queries, `DISJOINT` can be achieved as a `MUST_NOT` of an `INTERSECTS` query.\n\nUnit and Integration tests are provided.\n\nTODO:\n- add documentation for new field type \n\ncloses #20999 \n Add RangeFieldMapper for numeric and date range types >>> 1"
226,Based on the changes introduced in #20924 the `id` parameter is now required for `create` API. Required parameters are supposed to be checked client-side.\n [test] Required parameters are checked client-side >>> 1
227,"This commit removes an undocumented output parameter output_uuid from\nthe cluster stats API. Currently the parameter does not even work as it\nis not whitelisted as an output parameter. Since the cluster UUID is\navailable from the main action, and this parameter is not documented, we\nopt to just remove it.\n\nRelates #20722\n Remove output_uuid parameter from cluster stats >>> 1"
228,"This commit removes an undocumented output parameter node_info_format\nfrom the cluster stats and node stats APIs. Currently the parameter does\nnot even work as it is not whitelisted as an output parameter. Since\nthis parameter is not documented, we opt to just remove it.\n\nRelates #20722\n Remove node_info_format parameter from node stats >>> 1"
229,Previous to this change any request using a script sort in a top_hits\naggregation would fail because the compilation of the script happened\nafter the QueryShardContext was frozen (after we had worked out if the\nrequest is cachable).\n\nThis change moves the calling of build() on the SortBuilder to the\nTopHitsAggregationBuilder which means that the script in the script_sort\nwill be compiled before we decide whether to cache the request and freeze\nthe context.\n\nCloses #21022\n Fixes bug preventing script sort working on top_hits aggregation >>> 1
230,"When indices stats are requested via the node stats API, there is a\r\nlevel parameter to request stats at the index, node, or shards\r\nlevel. This parameter was not whitelisted when URL parsing was made\r\nstrict. This commit whitelists this parameter.\r\n\r\nAdditionally, there was some leniency in the parsing of this parameter\r\nthat has been removed.\r\n\r\nRelates #20722\r\n Whitelist node stats indices level parameter >>> 1"
231,"Currently test that check that equals() and hashCode() are working as expected for classes implementing them are quiet similar. This change moves common assertions in this method to a common utility class. In addition, another common utility function in most of these test classes that creates copies of input object by running them through a StreamOutput and reading them back in, is moved to ESTestCase so it can be shared across all these classes.\n\nRelates to #20629\n Consolidate code for equals/hashCode testing in central utility class >>> 1"
232,Removes the `publishAddress` parameter from the reindex-from-remote\nwhitelist checking because it isn't in use after #21004.\n Remove publishAddress from reindex whitelist >>> 1
233,"Now that the create api has its own spec, we can remove the special case in the yaml test client for it\n\nRelates to #20924\n [TEST] Remove create special case in yaml test client >>> 1"
234,"This change moves providing UnicastHostsProvider for zen discovery to be\npull based, adding a getter in DiscoveryPlugin. A new setting is added,\ndiscovery.zen.hosts_provider, to separate the discovery type from the\nhosts provider for zen when it is selected. Unfortunately existing\nplugins added ZenDiscovery with their own name in order to just provide\na hosts provider, so there are already many users setting the hosts\nprovider through discovery.type. This change also includes backcompat,\nfalling back to discovery.type when discovery.zen.hosts_provider is not\nset.\n Make UnicastHostsProvider extension pull based >>> 1"
235,"Here is what is happening without this fix when you try to connect to ec2 APIs:\n\n```\n[2016-10-20T12:41:49,925][DEBUG][c.a.a.AWSCredentialsProviderChain] Unable to load credentials from EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n[2016-10-20T12:41:49,926][DEBUG][c.a.a.AWSCredentialsProviderChain] Unable to load credentials from SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey)\n[2016-10-20T12:41:49,926][DEBUG][c.a.a.AWSCredentialsProviderChain] Unable to load credentials from com.amazonaws.auth.profile.ProfileCredentialsProvider@1ad14091: access denied (""java.io.FilePermission"" ""/home/ubuntu/.aws/credentials"" ""read"")\n[2016-10-20T12:41:49,927][DEBUG][c.a.i.EC2MetadataClient  ] Connecting to EC2 instance metadata service at URL: http://169.254.169.254/latest/meta-data/iam/security-credentials/\n[2016-10-20T12:41:49,951][DEBUG][c.a.i.EC2MetadataClient  ] Connecting to EC2 instance metadata service at URL: http://169.254.169.254/latest/meta-data/iam/security-credentials/discovery-tests\n[2016-10-20T12:41:49,965][DEBUG][c.a.a.AWSCredentialsProviderChain] Unable to load credentials from InstanceProfileCredentialsProvider: Unable to parse Json String.\n[2016-10-20T12:41:49,966][INFO ][o.e.d.e.AwsEc2UnicastHostsProvider] [dJfktmE] Exception while retrieving instance list from AWS API: Unable to load AWS credentials from any provider in the chain\n[2016-10-20T12:41:49,967][DEBUG][o.e.d.e.AwsEc2UnicastHostsProvider] [dJfktmE] Full exception:\ncom.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain\n    at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:131) ~[aws-java-sdk-core-1.10.69.jar:?]\n    at com.amazonaws.services.ec2.AmazonEC2Client.invoke(AmazonEC2Client.java:11117) ~[aws-java-sdk-ec2-1.10.69.jar:?]\n    at com.amazonaws.services.ec2.AmazonEC2Client.describeInstances(AmazonEC2Client.java:5403) ~[aws-java-sdk-ec2-1.10.69.jar:?]\n    at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.fetchDynamicNodes(AwsEc2UnicastHostsProvider.java:116) [discovery-ec2-5.0.0.jar:5.0.0]\n    at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:234) [discovery-ec2-5.0.0.jar:5.0.0]\n    at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:219) [discovery-ec2-5.0.0.jar:5.0.0]\n    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:54) [elasticsearch-5.0.0.jar:5.0.0]\n    at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.buildDynamicNodes(AwsEc2UnicastHostsProvider.java:102) [discovery-ec2-5.0.0.jar:5.0.0]\n    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:358) [elasticsearch-5.0.0.jar:5.0.0]\n    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$1.doRun(UnicastZenPing.java:272) [elasticsearch-5.0.0.jar:5.0.0]\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:504) [elasticsearch-5.0.0.jar:5.0.0]\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.0.0.jar:5.0.0]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]\n    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\n```\n\nFor whatever reason, it can not parse what is coming back from http://169.254.169.254/latest/meta-data/iam/security-credentials/discovery-tests.\n\nBut, if you wrap the code within an `AccessController.doPrivileged()` call, then it works perfectly.\n\nCloses #21039.\n Fix ec2 discovery when used with IAM profiles. >>> 1"
236,This commit cleans up the code handling load averages in OsProbe:\n- remove support for BSD; we do not support this OS\n- add Javadocs\n- strengthen assertions and testing\n- add debug logging for exceptional situation\n Cleanup load average handling >>> 1
237,As the title says.\n Remove more equivalents of the now method from the Painless whitelist. >>> 1
238,"Follow up for #21039.\n\nWe can revert the previous change and do that a bit smarter than it was.\n\nPatch tested successfully manually on ec2 with 2 nodes with a configuration like:\n\n``` yml\ndiscovery.type: ec2\nnetwork.host: [""_local_"", ""_site_"", ""_ec2_""]\ncloud.aws.region: us-west-2\n```\n Fix ec2 discovery when used with IAM profiles. >>> 1"
239,"ZenPing is the part of zen discovery which knows how to ping nodes.\nThere is only one alternative implementation, which is just for testing.\nThis change removes the ability to add custom zen pings, and instead\nhooks in the MockZenPing for tests through an overridden method in\nMockNode. This also folds in the ZenPingService (which was really just a\nsingle method) into ZenDiscovery, and removes the idea of having\nmultiple ZenPing instances. Finally, this was the last usage of the\nExtensionPoint classes, so that is also removed here.\n Remove pluggability of ZenPing >>> 1"
240,This commit upgrades the transport-netty4 module dependency from Netty\nversion 4.1.5 to version 4.1.6. This is a bug fix release of Netty.\n Upgrade to Netty 4.1.6 >>> 1
241,This commit adds basic cgroup CPU metrics to the node stats API.\n Add basic cgroup CPU metrics >>> 1
242,Closes #21006\n `ip_range` aggregation should accept null bounds. >>> 1
243,"This pr should cause unicode elements in the location header to be percent-encoded, instead of being left alone.\n\nFor the cases mentioned by  @weltenwort in #21016, they now return:\n\n```\ncurl -XPUT -v 'http://localhost:9200/someindex/sometype/%C3%A4' -d {}\n< HTTP/1.1 201 Created\n< Location: /someindex/sometype/%C3%A4\n< content-type: application/json; charset=UTF-8\n< content-length: 148\n< \n```\n\n```\ncurl -XPUT -v 'http://localhost:9200/someindex/sometype/%E2%9D%A4' -d '{}'\n< HTTP/1.1 201 Created\n< Location: /someindex/sometype/%E2%9D%A4\n< content-type: application/json; charset=UTF-8\n< content-length: 149\n< \n```\n\nThe above responses compare favorably with the responses from a checkout of current master:\n\n```\ncurl -XPUT -v 'http://localhost:9200/someindex/sometype/%C3%A4' -d {}\n< HTTP/1.1 201 Created\n< Location: /someindex/sometype/‰\n< content-type: application/json; charset=UTF-8\n< content-length: 147\n< \n```\n\n```\ncurl -XPUT -v 'http://localhost:9200/someindex/sometype/%E2%9D%A4' -d '{}'\n< HTTP/1.1 201 Created\n< Location: /someindex/sometype/?\n< content-type: application/json; charset=UTF-8\n< content-length: 149\n< \n```\n\nCloses #21016\n Adds percent-encoding for Location headers >>> 1"
244,Closes #18641\n Add support for `quote_field_suffix` to `simple_query_string`. >>> 1
245,Introduce skip validation param for index template API.\nUsers can skip validation logic if they set `validate` to `false`.\n\nAdd flag and docs\n\nRelated #20479\n Introduce skip validation flag for index template API >>> 0
246,Applying same patch we did in #21048 but for `repository-s3` plugin.\n Fix s3 repository when used with IAM profiles >>> 1
247,"Fixes NPE in SearchContext.toString() for user requests that contain scroll id but not scroll timeout.\n\nIt doesn't seem to do any harm to production code, but it was kind of annoying to see all these NPE error messages while debugging.\n Fix NPE in SearchContext.toString() >>> 1"
248,Support for this parameter was removed but the docs were not\nupdated. This commit removes this stale parameter from the docs.\n\nCloses #21066\n Remove timeout parameter from plugin script docs >>> 1
249,"When parsing nested queries or filters in IndexQueryParserService, the parser on the cached (or provided) QueryParseContext gets reset while parsing the inner element and never gets reset to the original parser that was present before the reset. In #21061 this cause strange NPEs while parsing InnerHits with highlighting where the highlighter itself containes an inner query. On 5.0 we already fixed this problem by cleaning up QueryParseContext internals. This PR captures the original parser present before the reset and restores it after the inner parsing is done.\n\nCloses #21061\n Fix NPE when parsing InnerHits with highlight query >>> 0"
250,"Versions before 2.0 needed to be told to return interesting fields\nlike `_parent`, `_routing`, and `_ttl`. And they come\nback inside a `fields` block which we need to parse.\n\nCloses #21044\n Fix reindex-from-remote for parent/child from <2.0 >>> 1"
251,"Since with `java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64`, the OpenJDK packaged for CentOS and OEL override the default value (`false`) for the JVM option `AssumeMP` and force it to `true` (see [this patch](https://git.centos.org/blob/rpms!!java-1.8.0-openjdk.git/ab03fcc7a277355a837dd4c8500f8f90201ea353/SOURCES!always_assumemp.patch))\n\nBecause it is forced to true by default for these packages, the following warning message is printed to the standard output when the Vagrant box has only 1 CPU:\n\n> OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N\n\nThis message will then fail the test introduced in #20422  where we check if no entries have been added to the journal after the service has been started.\n\nThis commit restore the default value for the `AssumeMP` option for CentOS and OracleServer.\n Add JVM option ""-XX:-AssumeMP"" in SystemD tests >>> 1"
252,Before this commit `curl -XHEAD localhost:9200?pretty` would return\n`Content-Length: 1` and a body which is fairly upsetting to standards\ncompliant tools. Now it'll return `Content-Length: 0` with an empty\nbody like every other `HEAD` request.\n\nRelates to #21075\n Make sure HEAD / has 0 Content-Length >>> 1
253,"Adds support for indexing into lists and arrays with negative indexes meaning ""counting from the back"". So for if `x = [""cat"", ""dog"", ""chicken""]` then `x[-1] == ""chicken""`.\n\nI put together a quick benchmark and it _looks_ like this has just about 0 performance impact when running array lookups in a tight loop. I'll need to check the cost of writes before we get this in but I wanted to open this so we could look at it. \n\nCloses #20870\n Painless negative offsets >>> 1"
254,"Applied (almost) the same rules we use to validate index names\nto new alias names. The only rule not applies it ""must be lowercase"".\nWe have tests that don't follow that rule and I except there are lots\nof examples of camelCase alias names in the wild. We can add that\nvalidation but I'm not sure it is worth it.\n\nCloses #20748\n Validate alias names the same as index names >>> 1"
255,"Today the request interceptor can't support async calls since the response\nof the async call would execute on a different thread ie. a client or listener\nthread. This means in-turn that the intercepted handler is not executed with the\nthread it was supposed to run and therefor can, if it's executing blocking\noperations, potentially deadlock an entire server.\n Pass executor name to request interceptor to support async intercept calls >>> 1"
256,"Previously, if a node left the cluster (for example, due to a long GC),\nduring a snapshot, the master node would mark the snapshot as failed, but\nthe node itself could continue snapshotting the data on its shards to the\nrepository.  If the node rejoins the cluster, the master may assign it to\nhold the replica shard (where it held the primary before getting kicked off\nthe cluster).  The initialization of the replica shard would repeatedly fail\nwith a ShardLockObtainFailedException until the snapshot thread finally\nfinishes and relinquishes the lock on the Store.\n\nThis commit resolves the situation by ensuring that when a shard is removed\nfrom a node (such as when a node rejoins the cluster and realizes it no longer\nholds the active shard copy), any snapshotting of the removed shards is aborted.\nIn the scenario above, when the node rejoins the cluster, it will see in the cluster \nstate that the node no longer holds the primary shard, so `IndicesClusterStateService`\nwill remove the shard, thereby causing any snapshots of that shard to be aborted.\n\nCloses #20876\n Abort snapshots on a node that leaves the cluster >>> 1"
257,"The cluster state on a node is updated either\n- by incoming cluster states that are received from the active master or\n- by the node itself when it notices that the master has gone.\n\nIn the second case, the node adds the NO_MASTER_BLOCK and removes the current master as active master from its cluster state. In one particular case, it would also update the list of nodes, removing the master node that just failed. In the future, we want a clear separation between actions that can be executed by a master publishing a cluster state and a node locally updating its cluster state when no active master is around.\n Only allow the master to update the list of nodes in the cluster state >>> 1"
258,"... regardless if you specify an index or an alias.\n\nIf an alias not equal than one index is specified, an IllegalArgumentException\nis thrown. An IndexNotFoundException is thrown if the specified alias or index\nname does not exist.\n\nThis is intended as a helper method in cases, the user usually knows that there\nshould be only one index hidden behind an alias (i.e. plugins).\n MetaData: Add method to retrieve concrete index meta data >>> 0"
259,"This doesn't make much sense to have at all, since a user can do a `GET`\nrequest without a version of they want to get it unconditionally.\n\nRelates to #20995\n Disallow `VersionType.FORCE` for GetRequest >>> 1"
260,"In #20995 we are making `force` versioning a hard failure for newly\ncreated indices, so in 5.0+ we need to deprecate it.\n Deprecate VersionType.FORCE >>> 1"
261,"Speaking with @s1monw it's not used, and specifying it results in `unrecognized parameter: [allow_no_indices]`.\n\nI'm not sure if there are other APIs where it has been removed?  Also applies to `5.x`\n Remove allow_no_indices from indices.upgrade >>> 1"
262,"On some systems, cgroups will be available but not configured. And in\nsome cases, cgroups will be configured, but not for the subsystems that\nwe are expecting (e.g., cpu and cpuacct). This commit strengthens the\nhandling of cgroup stats on such systems.\n\nRelates #21029\n Strengthen handling of unavailable cgroup stats >>> 1"
263,"This commit introduces a new execution mode for the `query_string` query, which\nis intended down the road to be a replacement for the current `_all` field.\n\nIt now does auto-field-expansion and auto-leniency when the following criteria\nare ALL met:\n- The `_all` field is disabled\n- No `default_field` has been set in the index settings\n- No `default_field` has been set in the request\n- No `fields` are specified in the request\n\nAdditionally, a user can force the ""all-like"" execution by setting the\n`all_fields` parameter to `true`.\n\nWhen executing in all field mode, the `query_string` query will look at all the\nfields in the mapping that are not metafields and can be searched, and\nautomatically expand the list of fields that are going to be queried.\n\nRelates to #19784\n Add ""all field"" execution mode to query_string query >>> 1"
264,nan Remove unused interface InitialStateDiscoveryListener >>> 1
265,The ScriptedHeuristic objects used in significant terms aggs were not thread safe when running local to the coordinating node. The new code spawns an object for each shard search execution rather than sharing a common ScriptedHeuristic instance which is not thread safe.\n\nCloses #18120\n Thread safety for scripted significance heuristics >>> 0
266,"Index templates now take an list for the template: field instead of a string.\r\n\r\nFor example:\r\n\r\n```\r\n{\r\n  ""index_patterns"":  [""c*"", ""d*""],\r\n}\r\n```\r\n\r\nInstead of the previous form\r\n\r\n```\r\n{\r\n  ""template"": ""c*""\r\n}\r\n```\r\n\r\nTemplate metadata from older versions of ES are still openable with this code, although I don't know how important that is since it's probably assumed that you can't just open a new elasticsearch release on an old one's data directory and expect it to work.\r\n\r\nCloses #20690\r\n Allows multiple patterns to be specified for index templates >>> 1"
267,"This commit fixes responses to HEAD requests so that the value of the\nContent-Length is correct per the HTTP spec. Namely, the value of this\nheader should be equal to the Content-Length if the request were not a\nHEAD request.\n\nThis commit also fixes a memory leak on HEAD requests to the main action\nthat arose from the bytes on a builder not being released due to them\nbeing dropped on the floor to ensure that the response to the main\naction did not have a body.\n\nRelates #21077\n Add correct Content-Length on HEAD requests >>> 1"
268,"There may be cases where the XContentBuilder is not used and therefore it never gets closed, which can cause a leak of bytes. This change moves the creation of the builder into a try with resources block.\n ensure the XContentBuilder is always closed in RestBuilderListener >>> 1"
269,"This commit introduces a single-shard balance step for deciding on\nrebalancing a single shard (without taking any other shards in the\ncluster into account).  This method will be used by the cluster\nallocation explain API to explain in detail the decision process for\nfinding a more optimal location for a started shard, if one exists.\n Balance step in BalancedShardsAllocator for a single shard >>> 1"
270,"The network disruption type ""network delay"" continues delaying existing requests even after the disruption has been cleared. This commit ensures that the requests get to execute right after the delay rule is cleared.\n\nIt also enables the `beforeIndexDeletion` checks in `DiscoveryWithServiceDisruptionsIT` except when the `NetworkUnresponsive` disruption type is set up (see also #21107).\n Stop delaying existing requests after network delay rule is cleared >>> 1"
271,This commit bumps the version number to 5.0.1.\n Bump version to 5.0.1 >>> 1
272,"As the title says this is the only change.  I'm going to attempt to clean up some of the scripting API, and this is the first in a series of smaller changes moving forward.  The lines of code appear much larger than the change itself simply because so many import statements changed.\n Refactor ScriptType to be a Top-Level Class >>> 1"
273,Today we return the old index name as the target / new index name.\nThis change passes the correct rollover index name to the response.\n Return target index name even if _rollover conditions are not met >>> 1
274,The mappings documentation was still listing the default similarity as TF/IDF rather than BM25. This change corrects that \n Corrects similarity default for 5.0 >>> 1
275,"This fixes our cluster formation task to run REST tests against a mixed version cluster.\nYet, due to some limitations in our test framework `indices.rollover` tests are currently\ndisabled for the BWC case since they select the current master as the merge node which\nhappens to be a BWC node and we can't relocate all shards to it since the primaries are on\na higher version node. This will be fixed in a followup.\n\nCloses #21142\n Fix bwc cluster formation in order to run BWC tests against a mixed version cluster >>> 1"
276,"Today we only use a single node to send requests to when we run REST tests.\nIn some cases we have more than one node (ie. in the BWC case) where we should\nsend requests to all nodes in a round-robin fashion. This change passes all\navailable node endpoints to the rest test.\n\nAdditionally, this change adds the setting of `discovery.zen.minimum_master_nodes`\nto the cluster formation forcing the nodes to wait for all other nodes until the cluster\nis formed. This allows for a more realistic master election and allows all master eligible\nnodes to become master while before always the first node in the cluster became the master.\n\nThis also adds logging to each test run to log the master nodes version and the minimum node\nversion in the cluster to help debugging BWC test failures.\n Use all available hosts in REST tests and allow for real master election >>> 1"
277,Setting `discovery.initial_state_timeout: 0s` to make `discovery.zen.minimum_master_nodes: N`\nwork reliably can cause issues in clusters that rely on state recovery once the cluster is available.\nThis change makes the use or `discovery.zen.minimum_master_nodes` optional for clusters where this behavior is desirable.\n Add a flag to use minimim_master_nodes on the integ test cluster >>> 1
278,Since Lucene 6.2. the UkrainianMorfologikAnalyzer is available through the lucene-analyzers-morfologik jar. This change exposes the analyzer to be used as an elasticsearch plugin.\n\nCloses #19433 \n Expose Lucenes Ukrainian analyzer >>> 1
279,nan Update resiliency page for the release of v5 >>> 1
280,5.0+'s deb and rpm packages aren't pushed to maven cnertral so\ninstead we have to download them from artifacts.elastic.org.\n Fix the package upgrade tests for 5.0.0 >>> 1
281,"Lucene 6.2 introduces the new `Analyzer.normalize` API, which allows to apply\nonly character-level normalization such as lowercasing or accent folding, which\nis exactly what is needed to process queries that operate on partial terms such\nas `prefix`, `wildcard` or `fuzzy` queries. As a consequence, the\n`lowercase_expanded_terms` option is not necessary anymore. Furthermore, the\n`locale` option was only needed in order to know how to perform the lowercasing,\nso this one can be removed as well.\n\nCloses #9978\n Remove `lowercase_expanded_terms` and `locale` from query-parser options. >>> 1"
282,Refactored ScriptType to clean up some of the variable and method names.  Added more documentation.  Deprecated the 'in' ParseField in favor of 'stored' to match the indexed scripts being replaced by stored scripts.\n Cleanup ScriptType >>> 1
283,This PR adds geo_point breaking changes to the migration docs.\n [DOC] Document geo_point breaking changes >>> 0
284,Closes #21134\n Reject legacy `index` values for `text` and `keyword`. >>> 0
285,Closes #21159\n Protect BytesStreamOutput against overflows of the current number of written bytes. >>> 1
286,Resolves #21046\n [DOCS] Document all date formats >>> 1
287,"Replication request may arrive at a replica before the replica's node has processed a required mapping update. In these cases the TransportReplicationAction will retry the request once a new cluster state arrives. Sadly that retry logic failed to call `ReplicationRequest#onRetry`, causing duplicates in the append only use case.\n\nThis PR fixes this and also the test which missed the check. I also added an assertion which would have helped finding the source of the duplicates.\n\nThis was discovered by https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=opensuse/174/\n\nThe test also surfaces an issue with mapping updates on the master (they are potentially performed on a live index :( ) but this will be fixed in another PR.\n\nRelates #20211\n Retrying replication requests on replica doesn't call `onRetry` >>> 1"
288,"Before publishing a cluster state the master connects to the nodes that are added in the cluster state. When publishing fails, it does not disconnect from these nodes, however, leaving NodeConnectionsService out of sync with the currently applied cluster state.\r\n\r\nBuild failure:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/473/consoleFull\r\n\r\nRelevant log lines:\r\n\r\n```\r\n1> [2016-10-30T14:01:21,434][DEBUG][o.e.d.z.ZenDiscovery     ] [node_t0] failed to publish cluster state version [9] (not enough nodes acknowledged, min master nodes [2])\r\n 1> [2016-10-30T14:01:21,434][WARN ][o.e.c.s.ClusterService   ] [node_t0] failing [zen-disco-node-join[{node_t1}{n9SEnXUhQhizsfKmupS59g}{QOn4dJWMQ--uztoIl5j37w}{127.0.0.1}{127.0.0.1:30301}]]: failed to commit cluster state version [9]\r\n```\r\n\r\nand\r\n\r\n```\r\njava.lang.AssertionError: node {node_t1}{n9SEnXUhQhizsfKmupS59g}{QOn4dJWMQ--uztoIl5j37w}{127.0.0.1}{127.0.0.1:30301} was added in event but already in internal nodes\r\n 2> \tat __randomizedtesting.SeedInfo.seed([B06683F6A37CB70F]:0)\r\n 2> \tat org.elasticsearch.cluster.NodeConnectionsService.connectToAddedNodes(NodeConnectionsService.java:84)\r\n 2> \tat org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:674)\r\n 2> \tat org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:897)\r\n```\r\n Disconnect from newly added nodes if cluster state publishing fails >>> 1"
289,"When installing the Windows service, certain settings like the minimum\r\nheap, maximum heap and thread stack size setting must be set. While\r\nthere is an error message making mention of this fact, the error message\r\nis not explicit exactly what setting needs to be set. This commit makes\r\nthese settings explicit.\r\n\r\nRelates #18317 Make explicit missing settings for Windows service >>> 1"
290,This is a topic that has triggered many questions recently so it would be good\nto have these recommendations documented.\n Add recommendations about getting consistent scores despite shards and replicas. >>> 1
291,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n\r\ncharachters -> characters Fix typo in docs >>> 1"
292,Change list here: https://lucene.apache.org/core/6_2_1/changes/Changes.html#v6.2.1.api_changes Upgrade to Lucene 6.2.1 >>> 1
293,Reindex-from-remote in 5.0.0 will fail to reindex parent-child docs\nfrom a 1.7 cluster. This adds a warning to the 5.0.0 docs. See #21070 and #21044.\n [DOCS] warn of using 5.0.0 reindex-remote for 1.7 parent-child docs >>> 1
294,"Today when installing Elasticsearch from an archive distribution (tar.gz\r\nor zip), an empty plugins folder is not included. This means that if you\r\ninstall Elasticsearch and immediately run elasticsearch-plugin list, you\r\nwill receive an error message about the plugins directory missing. While\r\nthe plugins directory would be created when starting Elasticsearch for\r\nthe first time, it would be better to just include an empty plugins\r\ndirectory in the archive distributions. This commit makes this the\r\ncase. Note that the package distributions already include an empty\r\nplugins folder.\r\n\r\nCloses #20342 Add empty plugins dir for archive distributions >>> 1"
295,"The `IndexService#newQueryShardContext()` method creates a QueryShardContext on\r\nshard `0`, with a `null` reader and that uses `System.currentTimeMillis()` to\r\nresolve `now`. This may hide bugs, since the shard id is sometimes used for\r\nquery parsing (it is used to salt random score generation in `function_score`),\r\npassing a `null` reader disables query rewriting and for some use-cases, it is\r\nsimply not ok to rely on the current timestamp (eg. percolation). So this pull\r\nrequest removes this method and instead requires that all call sites provide\r\nthese parameters explicitly. Require arguments for QueryShardContext creation. >>> 1"
296,"We throw this exception in some cases that the shard is closed, so we have to be consistent here. Otherwise we get logs like:\r\n\r\n```\r\n 1> [2016-10-30T21:06:22,529][WARN ][o.e.i.IndexService       ] [node_s_0] [test] failed to run task refresh - suppressing re-occurring exceptions unless the exception changes\r\n 1> org.elasticsearch.index.shard.IndexShardClosedException: CurrentState[CLOSED] operation only allowed when not closed\r\n 1> \tat org.elasticsearch.index.shard.IndexShard.verifyNotClosed(IndexShard.java:1147) ~[main/:?]\r\n 1> \tat org.elasticsearch.index.shard.IndexShard.verifyNotClosed(IndexShard.java:1141) ~[main/:?]\r\n``` IndexService#maybeRefresh should catch `IndexShardClosedException` >>> 1"
297,This commit adds the analysis-ukrainian plugin to the packaging tests.\r\n\r\nRelates #21176 Add analysis-ukrainian plugin to packaging tests >>> 1
298,This commit clarifies that production mode for the bootstrap checks is\r\nonly tripped if transport is bound to an external interface. Clarify production mode for bootstrap checks >>> 1
299,It mistakenly multiplies the quantile by the array `length` instead of `length-1`. Percentiles bucket fails for 100th percentile >>> 1
300,"It was 10mb and that was causing trouble when folks reindex-from-remoted\r\nwith large documents.\r\n\r\nWe also improve the error reporting so it tells folks to use a smaller\r\nbatch size if they hit a buffer size exception. Finally, adds some docs\r\nto reindex-from-remote mentioning the buffer and giving an example of\r\nlowering the size.\r\n\r\nCloses #21185 Bump reindex-from-remote's buffer to 200mb >>> 1"
301,"Previously Elasticsearch would only use the package name for logging\r\nlevels, truncating the package prefix and the class name. This meant\r\nthat logger names for Netty were just prefixed by netty3 and netty. We\r\nchanged this for Elasticsearch so that it's the fully-qualified class\r\nname now, but never corrected this for Netty. This commit fixes the\r\nlogger names for the Netty modules so that their levels are controlled\r\nby the fully-qualified class name.\r\n\r\nRelates #20457 Fix logger names for Netty >>> 1"
302,"When ES starts up we verify we can write to all data files by creating and deleting a temp file called `.es_temp_file`. If for some reason the file was successfully created but not successfully deleted, we still shut down correctly but subsequent start attempts will fail with a file already exists exception.\r\n\r\nThis PR makes sure to first clean any existing `.es_temp_file`\r\n\r\nSuperseeds #21007 Node should start up despite of a lingering `.es_temp_file` >>> 1"
303,Closes #21232  Provide error message when rest request path is null >>> 1
304,Currently there is no way to specify the pipeline id when using client().prepareDeletePipeline() method ClusterAdminClient.prepareDeletePipeline method should accept pipeline id to delete >>> 1
305,Stored scripts and ingest node configuration are important parts of the overall cluster state and should be included into a snapshot together with index templates and persistent settings if the includeGlobalState is set to true.\r\n\r\nCloses #21184\r\n\r\nNot sure if it qualifies as a bug or an enhancement and if it should be added to 5.0.1 or not.  Stored scripts and ingest node configurations should be included into a snapshot >>> 1
306,"Null safe dereferences make handling null or missing values shorter.\r\nCompare without:\r\n```\r\nif (ctx._source.missing != null && ctx._source.missing.foo != null) {\r\n  ctx._source.foo_length = ctx.source.missing.foo.length()\r\n}\r\n```\r\n\r\nTo with:\r\n```\r\nInteger length = ctx._source.missing?.foo?.length();\r\nif (length != null) {\r\n  ctx._source.foo_length = length\r\n}\r\n```\r\n\r\nCombining this with the as of yet unimplemented elvis operator allows\r\nfor very concise defaults for nulls:\r\n```\r\nctx._source.foo_length = ctx._source.missing?.foo?.length() ?: 0;\r\n```\r\n\r\nSince you have to start somewhere, we started with null safe dereferenes.\r\n\r\nAnyway, this is a feature borrowed from groovy. Groovy allows writing to\r\nnull values like:\r\n```\r\ndef v = null\r\nv?.field = 'cat'\r\n```\r\nAnd the writes are simply ignored. Painless doesn't support this at this\r\npoint because it'd be complex to implement and maybe not all that useful.\r\n\r\nThere is no runtime cost for this feature if it is not used. When it is\r\nused we implement it fairly efficiently, adding a jump rather than a\r\ntemporary variable.\r\n\r\nThis should also work fairly well with doc values. Implement reading from null safe dereferences >>> 1"
307,"Dependencies are currently marked as non-transitive in generated POM files by adding a wildcard (*) exclusion. This breaks compatibility with the dependency manager Apache Ivy as it incorrectly translates POMs with * excludes to Ivy XML with * excludes which results in the main artifact being excluded as well (see https://issues.apache.org/jira/browse/IVY-1531). To stay compatible with the current release of Ivy this commit uses explicit excludes for each transitive artifact instead to ensure that the main artifact is not excluded. This should be revisited when we upgrade Gradle to a higher version as the current one (2.13) as Gradle automatically translates non-transitive dependencies to * excludes in 2.14+.\r\n\r\nRelates to #21170\r\n\r\nI've tested this patch as follows:\r\n\r\n1) applied patch to 5.0 branch and published artifacts to local maven repository by running `gradle publishToMavenLocal`\r\n2) checked various dependency management systems how they compare between the officially published 5.0.0 and the generated 5.0.1-SNAPSHOT version with the applied patch.\r\n\r\nDependency management systems:\r\n\r\n**Gradle**\r\n\r\nBuild file:\r\n\r\n```\r\napply plugin: 'java'\r\n\r\nrepositories {\r\n\tmavenLocal()\r\n\tmavenCentral()\r\n}\r\n\r\ndependencies {\r\n\tcompile 'org.elasticsearch:elasticsearch:5.0.0'\r\n}\r\n```\r\n\r\nRun `gradle dependencies --configuration compile` which yields\r\n\r\n```\r\n\--- org.elasticsearch:elasticsearch:5.0.0\r\n     +--- org.apache.lucene:lucene-core:6.2.0\r\n     +--- org.apache.lucene:lucene-analyzers-common:6.2.0\r\n     +--- org.apache.lucene:lucene-backward-codecs:6.2.0\r\n     +--- org.apache.lucene:lucene-grouping:6.2.0\r\n     +--- org.apache.lucene:lucene-highlighter:6.2.0\r\n     +--- org.apache.lucene:lucene-join:6.2.0\r\n     +--- org.apache.lucene:lucene-memory:6.2.0\r\n     +--- org.apache.lucene:lucene-misc:6.2.0\r\n     +--- org.apache.lucene:lucene-queries:6.2.0\r\n     +--- org.apache.lucene:lucene-queryparser:6.2.0\r\n     +--- org.apache.lucene:lucene-sandbox:6.2.0\r\n     +--- org.apache.lucene:lucene-spatial:6.2.0\r\n     +--- org.apache.lucene:lucene-spatial-extras:6.2.0\r\n     +--- org.apache.lucene:lucene-spatial3d:6.2.0\r\n     +--- org.apache.lucene:lucene-suggest:6.2.0\r\n     +--- org.elasticsearch:securesm:1.1\r\n     +--- net.sf.jopt-simple:jopt-simple:5.0.2\r\n     +--- com.carrotsearch:hppc:0.7.1\r\n     +--- joda-time:joda-time:2.9.4\r\n     +--- org.joda:joda-convert:1.2\r\n     +--- org.yaml:snakeyaml:1.15\r\n     +--- com.fasterxml.jackson.core:jackson-core:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.8.1\r\n     +--- com.tdunning:t-digest:3.0\r\n     +--- org.hdrhistogram:HdrHistogram:2.1.6\r\n     \--- net.java.dev.jna:jna:4.2.2\r\n```\r\n\r\nReplace the 5.0.0 dependency by 5.0.1-SNAPSHOT in the build file and run same command again:\r\n\r\n```\r\n\--- org.elasticsearch:elasticsearch:5.0.1-SNAPSHOT\r\n     +--- org.apache.lucene:lucene-core:6.2.1\r\n     +--- org.apache.lucene:lucene-analyzers-common:6.2.1\r\n     +--- org.apache.lucene:lucene-backward-codecs:6.2.1\r\n     +--- org.apache.lucene:lucene-grouping:6.2.1\r\n     +--- org.apache.lucene:lucene-highlighter:6.2.1\r\n     +--- org.apache.lucene:lucene-join:6.2.1\r\n     +--- org.apache.lucene:lucene-memory:6.2.1\r\n     +--- org.apache.lucene:lucene-misc:6.2.1\r\n     +--- org.apache.lucene:lucene-queries:6.2.1\r\n     +--- org.apache.lucene:lucene-queryparser:6.2.1\r\n     +--- org.apache.lucene:lucene-sandbox:6.2.1\r\n     +--- org.apache.lucene:lucene-spatial:6.2.1\r\n     +--- org.apache.lucene:lucene-spatial-extras:6.2.1\r\n     +--- org.apache.lucene:lucene-spatial3d:6.2.1\r\n     +--- org.apache.lucene:lucene-suggest:6.2.1\r\n     +--- org.elasticsearch:securesm:1.1\r\n     +--- net.sf.jopt-simple:jopt-simple:5.0.2\r\n     +--- com.carrotsearch:hppc:0.7.1\r\n     +--- joda-time:joda-time:2.9.4\r\n     +--- org.joda:joda-convert:1.2\r\n     +--- org.yaml:snakeyaml:1.15\r\n     +--- com.fasterxml.jackson.core:jackson-core:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.8.1\r\n     +--- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.8.1\r\n     +--- com.tdunning:t-digest:3.0\r\n     +--- org.hdrhistogram:HdrHistogram:2.1.6\r\n     \--- net.java.dev.jna:jna:4.2.2\r\n```\r\n\r\nConclusion:\r\n\r\nGradle was handling the * excludes fine before and is still working as expected afterwards.\r\n\r\n\r\n**Ivy**\r\n\r\nCreate `ivysettings.xml` file with following contents:\r\n\r\n```\r\n<ivysettings>\r\n    <settings defaultResolver=""default""/>\r\n    <resolvers>\r\n        <chain name=""default"">\r\n            <filesystem name=""local-maven2"" m2compatible=""true"" checkmodified=""true"" changingPattern="".*SNAPSHOT"">      \r\n              <ivy pattern=""${user.home}/.m2/repository/[organisation]/[module]/[revision]/[module]-[revision].pom"" />\r\n              <artifact pattern=""${user.home}/.m2/repository/[organisation]/[module]/[revision]/[artifact]-[revision](-[classifier]).[ext]"" />\r\n            </filesystem>\r\n            <ibiblio name=""central"" m2compatible=""true""/>\r\n        </chain>\r\n    </resolvers>\r\n</ivysettings>\r\n```\r\n\r\nThen run\r\n`ivy -settings ivysettings.xml -confs default -dependency org.elasticsearch elasticsearch 5.0.0 -retrieve ""jars/[module]-[artifact](-[revision]).[ext]""`\r\n\r\nwhich yields\r\n\r\n```\r\n\t---------------------------------------------------------------------\r\n\t|                  |            modules            ||   artifacts   |\r\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\r\n\t---------------------------------------------------------------------\r\n\t|      default     |   29  |   29  |   29  |   0   ||   12  |   12  |\r\n\t---------------------------------------------------------------------\r\n```\r\n\r\nOnly 12 artifacts instead of the 29 that we expect.\r\n\r\n```\r\n$ ls jars\r\nHdrHistogram-HdrHistogram-2.1.6.jar   jackson-core-jackson-core-2.8.1.jar   joda-time-joda-time-2.9.4.jar         securesm-securesm-1.1.jar\r\nelasticsearch-elasticsearch-5.0.0.jar jna-jna-4.2.2.jar                     jopt-simple-jopt-simple-5.0.2.jar     snakeyaml-snakeyaml-1.15.jar\r\nhppc-hppc-0.7.1.jar                   joda-convert-joda-convert-1.2.jar     lucene-core-lucene-core-6.2.0.jar     t-digest-t-digest-3.0.jar\r\n```\r\n\r\nLet's repeat the same for 5.0.1-SNAPSHOT (but first `rm jars/*`):\r\n\r\n`ivy -settings ivysettings.xml -confs default -dependency org.elasticsearch elasticsearch 5.0.1-SNAPSHOT -retrieve ""jars/[module]-[artifact](-[revision]).[ext]""`\r\n\r\nwhich yields:\r\n\r\n```\r\n\t---------------------------------------------------------------------\r\n\t|                  |            modules            ||   artifacts   |\r\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\r\n\t---------------------------------------------------------------------\r\n\t|      default     |   29  |   16  |   16  |   0   ||   29  |   19  |\r\n\t---------------------------------------------------------------------\r\n```\r\n\r\nAll 29 artifacts are accounted for.\r\n\r\n```\r\n$ ls jars\r\nHdrHistogram-HdrHistogram-2.1.6.jar                         lucene-highlighter-lucene-highlighter-6.2.1.jar\r\nelasticsearch-elasticsearch-5.0.1-SNAPSHOT.jar              lucene-join-lucene-join-6.2.1.jar\r\nhppc-hppc-0.7.1.jar                                         lucene-memory-lucene-memory-6.2.1.jar\r\njackson-core-jackson-core-2.8.1.jar                         lucene-misc-lucene-misc-6.2.1.jar\r\njackson-dataformat-cbor-jackson-dataformat-cbor-2.8.1.jar   lucene-queries-lucene-queries-6.2.1.jar\r\njackson-dataformat-smile-jackson-dataformat-smile-2.8.1.jar lucene-queryparser-lucene-queryparser-6.2.1.jar\r\njackson-dataformat-yaml-jackson-dataformat-yaml-2.8.1.jar   lucene-sandbox-lucene-sandbox-6.2.1.jar\r\njna-jna-4.2.2.jar                                           lucene-spatial-extras-lucene-spatial-extras-6.2.1.jar\r\njoda-convert-joda-convert-1.2.jar                           lucene-spatial-lucene-spatial-6.2.1.jar\r\njoda-time-joda-time-2.9.4.jar                               lucene-spatial3d-lucene-spatial3d-6.2.1.jar\r\njopt-simple-jopt-simple-5.0.2.jar                           lucene-suggest-lucene-suggest-6.2.1.jar\r\nlucene-analyzers-common-lucene-analyzers-common-6.2.1.jar   securesm-securesm-1.1.jar\r\nlucene-backward-codecs-lucene-backward-codecs-6.2.1.jar     snakeyaml-snakeyaml-1.15.jar\r\nlucene-core-lucene-core-6.2.1.jar                           t-digest-t-digest-3.0.jar\r\nlucene-grouping-lucene-grouping-6.2.1.jar\r\n```\r\n\r\n**SBT** (version 0.13.12)\r\n\r\nLet's create a simple build file (`build.sbt`):\r\n\r\n```\r\nname := ""test""\r\n\r\nresolvers += Resolver.mavenLocal\r\n\r\nlibraryDependencies ++= Seq(\r\n   ""org.elasticsearch"" % ""elasticsearch"" % ""5.0.0""\r\n)\r\n\r\n```\r\n\r\nand run `sbt 'show runtime:fullClasspath'`\r\n\r\n```\r\nList(Attributed(/Users/ywelsch/dev/sbt-example/target/scala-2.10/classes), Attributed(/Users/ywelsch/.sbt/boot/scala-2.10.6/lib/scala-library.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.elasticsearch/elasticsearch/jars/elasticsearch-5.0.0.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-core/jars/lucene-core-6.2.0.jar), Attributed(/Users/ywelsch/.m2/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar), Attributed(/Users/ywelsch/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar), Attributed(/Users/ywelsch/.m2/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar), Attributed(/Users/ywelsch/.m2/repository/joda-time/joda-time/2.9.4/joda-time-2.9.4.jar), Attributed(/Users/ywelsch/.m2/repository/org/joda/joda-convert/1.2/joda-convert-1.2.jar), Attributed(/Users/ywelsch/.m2/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar), Attributed(/Users/ywelsch/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar), Attributed(/Users/ywelsch/.m2/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar), Attributed(/Users/ywelsch/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar), Attributed(/Users/ywelsch/.m2/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar))\r\n```\r\n\r\nSame as for Ivy, only 12 artifacts instead of the 29 that we expect.\r\n\r\nReplace the 5.0.0 dependency by 5.0.1-SNAPSHOT in the build file and run same command again:\r\n\r\n```\r\nList(Attributed(/Users/ywelsch/dev/sbt-example/target/scala-2.10/classes), Attributed(/Users/ywelsch/.sbt/boot/scala-2.10.6/lib/scala-library.jar), Attributed(/Users/ywelsch/.m2/repository/org/elasticsearch/elasticsearch/5.0.1-SNAPSHOT/elasticsearch-5.0.1-SNAPSHOT.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-core/jars/lucene-core-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-analyzers-common/jars/lucene-analyzers-common-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-backward-codecs/jars/lucene-backward-codecs-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-grouping/jars/lucene-grouping-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-highlighter/jars/lucene-highlighter-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-join/jars/lucene-join-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-memory/jars/lucene-memory-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-misc/jars/lucene-misc-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-queries/jars/lucene-queries-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-queryparser/jars/lucene-queryparser-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-sandbox/jars/lucene-sandbox-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-spatial/jars/lucene-spatial-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-spatial-extras/jars/lucene-spatial-extras-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-spatial3d/jars/lucene-spatial3d-6.2.1.jar), Attributed(/Users/ywelsch/.ivy2/cache/org.apache.lucene/lucene-suggest/jars/lucene-suggest-6.2.1.jar), Attributed(/Users/ywelsch/.m2/repository/org/elasticsearch/securesm/1.1/securesm-1.1.jar), Attributed(/Users/ywelsch/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar), Attributed(/Users/ywelsch/.m2/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar), Attributed(/Users/ywelsch/.m2/repository/joda-time/joda-time/2.9.4/joda-time-2.9.4.jar), Attributed(/Users/ywelsch/.m2/repository/org/joda/joda-convert/1.2/joda-convert-1.2.jar), Attributed(/Users/ywelsch/.m2/repository/org/yaml/snakeyaml/1.15/snakeyaml-1.15.jar), Attributed(/Users/ywelsch/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.8.1/jackson-core-2.8.1.jar), Attributed(/Users/ywelsch/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.8.1/jackson-dataformat-smile-2.8.1.jar), Attributed(/Users/ywelsch/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.8.1/jackson-dataformat-yaml-2.8.1.jar), Attributed(/Users/ywelsch/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.1/jackson-dataformat-cbor-2.8.1.jar), Attributed(/Users/ywelsch/.m2/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar), Attributed(/Users/ywelsch/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar), Attributed(/Users/ywelsch/.m2/repository/net/java/dev/jna/jna/4.2.2/jna-4.2.2.jar))\r\n```\r\n\r\nAll 29 artifacts are there. Generate POM files with non-wildcard excludes >>> 1"
308,Today these two are considered mutual exclusive but they are not in\r\npractice. For instance a mixed version cluster might not return a\r\ngiven warning depending on which node we talk to but on the other hand\r\nsome runners might not even support warnings at all so the test might be\r\nskipped either by version or by feature. Allow skip test by version OR feature >>> 1
309,This commit adds the version constant for 5.0.0. Add version for 5.0.0 >>> 1
310,The default similarity can no longer be set in the configuration file\r\n(you will get an error on startup). Update the docs with the method\r\nthat works. Add docs with up to date instructions on updating default similarity >>> 1
311,The current XContent output is much harder to read than the prettyPrint format. This commit folds prettyPrint into toString and removes it. Change ClusterState and PendingClusterTasksResponse's toString() to their prettyPrint format >>> 1
312,Partially resolves #19757 \r\n\r\n Documentation updates for scroll API size parameter >>> 1
313,"Since we have the ingest node type, there is either `IngestActionFilter` or `IngestProxyActionFilter` registered, depending on whether the node is an ingest node or not. The special case that shortcuts the execution in case there are no filters is never exercised, hence we are probably better off removing it as it is not needed and never tested. Remove special case in case no action filters are registered >>> 1"
314,"This commit adds the sequence number global checkpoint to translog\r\ncheckpoints, and removes them from Lucene commits. Add global checkpoint to translog checkpoints >>> 1"
315,"The pending cluster state queue is used to hold incoming cluster states from the master. Currently the elected master doesn't publish to itself as thus the queue is not used. Sometimes, depending on the timing of disruptions, a pending cluster state can be put on the queue (but not committed) but another master before being isolated. If this happens concurrently with a master election the elected master can have a pending cluster state in its queue. This is not really a problem but it does confuse our assertions during tests as we check to see everything was processed correctly.\r\n\r\nThis commit takes a temporary step to clear (and fail) any pending cluster state on the master after it has successfully published a CS. Most notably this will happen when the master publishes the cluster state indicating it has just become the master.\r\n\r\nLong term we are working to change the publishing mechanism to make the master use the pending queue just like other nodes, which will make this a non issue.\r\n\r\nSee https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.x+java9-periodic/509 for example. Publishing a cluster state should clear the pending states queue >>> 1"
316,"Checks on static test state are run by an `@After` method in `ESTestCase`. Suite-scoped tests in `ESIntegTestCase` only shut down in an `@AfterClass` method, which executes after the `@After` method in ESTestCase. The suite-scoped cluster can thus still execute actions that will violate the checks in `@After` without those being caught. A subsequent test executing within the same JVM will fail these checks however when `@After` gets called for that test.\r\n\r\nThis commit adds an explicit call to check the static test state after the suite-scoped cluster has been shut down. [TEST] Check static test state after suite scoped cluster is shut down >>> 1"
317,"today the `_shrink` tests do relocate all shards to a single node in\r\nthe cluster. Yet, that is not always possible since the only node we can\r\nsafely identify in the cluster is the master and if the master is a BWC\r\nnode in such a cluster we won't be able to relocate shards that have a\r\nprimary on the newer version nodes since allocation deciders forbit this.\r\n\r\nThis change restricts allocation for that index when the index is created\r\nto restrict allocation to the master that guarantees that all primaries\r\nare on the same node which is sufficient for the `_shrink` API to run. Fix `_shrink` test to work under a mixed version cluster >>> 1"
318,"PR does 2 things:\r\n- Updates some of the verbiage for 5.x, e.g. changing Marvel plugin to X-Pack and updating the license information there\r\n- Drops any plugins that obviously haven't been updated since before Elasticsearch 2.0.  I didn't dig in too much and left plugins that looked to be updated at least by 2.3/2.4, since they may be updating to 5.0 soon Updates plugin documentation for 5.0 >>> 1"
319,We have some settings like `index.number_of_shards` and `index.number_of_replicas`\r\nthat should never be archived if they are invalid. If such a setting is invalid\r\nthe node should not start-up instead. This change throws an IllegalStateException\r\nif such a setting can't be parsed.\r\n\r\nCloses #21114 Never archive mandatory settings >>> 0
320,We plan to deprecate `_suggest` during 5.0 so it isn't worth fixing\r\nit to support the `_source` parameter for `_source` filtering. But we\r\nshould fix the docs so they are accurate.\r\n\r\nSince this removes the last non-`// CONSOLE` line in\r\n`completion-suggest.asciidoc` this also removes it from the list of\r\nfiles that have non-`// CONSOLE` docs.\r\n\r\nCloses #20482 Make it clear _suggest doesn't support source filtering >>> 1
321,"Today when running gradle clean\r\n:distribution:(integ-test-zip|tar|zip):assemble, the created archive\r\ndistribution will be missing the empty plugins directory. This is\r\nbecause the empty plugins folder created in the build folder for the\r\ncopy spec task is created during configuration and then is later wiped\r\naway by the clean task. This commit addresses this issue, by pushing\r\ncreation of the directory out of the configuration phase. Fix distribution build ordering issue >>> 1"
322,"Since we now validate all consumed request parameter, users can't specify\r\n`_cat/nodes?full_id=true|false` anymore since this parameter is consumed late.\r\nThis commit adds a test for this parameter and consumes it before request is processed.\r\n\r\nCloses #21266 Consume `full_id` request parameter early >>> 1"
323,The usage information for `elasticsearch-plugin` is quiet verbose and makes the actual error message that is shown when trying to remove a non-existing plugin hard to spot. This changes the error code to not trigger printing the usage information.\r\n\r\nCloses #21250 Removing plugin that isn't installed shouldn't trigger usage information >>> 0
324,"When installing a plugin when the plugins directory does not exist, the\r\ninstall plugin command outputs a line saying that it is creating this\r\ndirectory. The packaging tests for the archive distributions accounted\r\nfor this including an assertion that this line was output. The packages\r\nhave since been updated to include an empty plugins folder, so this line\r\nwill no longer be output. This commit removes this stale assertion from\r\nthe packaging tests.\r\n\r\nRelates #21204 Remove stale install plugins assertion >>> 1"
325,These tests had a single method due to the fact that es didn't support resetting settings when they were first written. They can now be rewritten to have separate methods and an after method that resets the setting that is left behind. [TEST] Update destructive operations and disable close IT tests >>> 1
326,A first step moving away from the current parsing to use the generalized ObjectParser and ConstructingObjectParser. This PR start by making use of it in MatchAllQueryBuilder and IdsQueryBuilder. Using ObjectParser in MatchAllQueryBuilder and IdsQueryBuilder >>> 1
327,"Currently the request cache adds a `CacheEntity` object. It looks quite innocent\r\nbut in practice it has a reference to a lambda that knows how to create a value.\r\nThe issue is that this lambda has implicit references to the SearchContext\r\nobject, which can be quite heavy since it holds a structured representation of\r\naggregations for instance.\r\n\r\nThis pull request splits the `CacheEntity` object from the object that generates\r\ncache values. Fix the request cache keys to not hold references to the SearchContext. >>> 1"
328,"@rjernst I have started migrating the old native script example plugin but instead of moving boring is_primary function I have tried to move something more realistic and a bit more complicated.  It turned out to be trickier than I expected. So, before I spend too much time on it and other example function, could you take a look at this first function to make sure that this is what you meant in #19966.\n Add a native script service example >>> 0"
329,"Currently the default S3 buffer size is 100MB, which can be a lot for small\r\nheaps. This pull request updates the default to be 100MB for heaps that are\r\ngreater than 2GB and 5% of the heap size otherwise. Make the default S3 buffer size depend on the available memory. >>> 1"
330,"With #21099 we removed support for the ignored allow_no_indices parameter in indices upgrade API. Truth is that ignore_unavailable and expand_wildcards were also ignored, in indices upgrade as well as upgrade status API. Those parameters are though supported internally and settable through java API, hence they should be all supported on the REST layer too. Read indices options in indices upgrade API >>> 1"
331,"Previously node.max_local_storage_nodes defaulted to fifty, and this\r\npermitted users to start multiple instances of Elasticsearch sharing the\r\nsame data folder. This can be dangerous, and usually it does not make\r\nsense to run more than one instance of Elasticsearch on a single\r\nserver. Because of this, we had a note in the important settings docs\r\nadvising users to set this setting to one. However, we have since\r\nchanged the default value of this setting to one so this advise is no\r\nlonger needed.\r\n\r\nRelates #19964, relates #20029 Remove node.max_local_storage_nodes from setup doc >>> 1"
332,Moves the `_flush` in the `_cat/indices` snippets testing framework\nto the very first test. We need to flush super early because index\nsize is cached for a few seconds so we really need to read a\nconsistent size on the first read so we can sort by it properly.\n\nCloses #21062\n Move flush in _cat/indices docs tests >>> 1
333,`FilterAggregationBuilder` today misses to rewrite queries which causes failures\r\nif a query that uses a client for instance to lookup terms since it must be rewritten first.\r\nThis change also ensures that if a client is used from the rewrite context we mark the query as\r\nnon-cacheable.\r\n\r\nCloses #21301 Rewrite Queries/Filter in FilterAggregationBuilder and ensure client usage marks query as non-cachable >>> 1
334,"When processing a mapping updates, the master current creates an `IndexService` and uses its mapper service to do the hard work. However, if the master is also a data node and it already has an instance of `IndexService`, we currently reuse the the `MapperService` of that instance. Sadly, since mapping updates are change the in memory objects, this means that a mapping change that can rejected later on during cluster state publishing will leave a side effect on the index in question, bypassing the cluster state safety mechanism.\r\n\r\nThis commit removes this optimization and replaces the `IndexService` creation with a direct creation of a `MapperService`. \r\n\r\nAlso, this fixes an issue multiple from multiple shards for the same field caused unneeded cluster state publishing as the current code always created a new cluster state.\r\n\r\nThis were discovered while researching #21189 \r\n Uncommitted mapping updates should not efect existing indices >>> 1"
335,"Our test infrastructure checks after running each test that there are no more in-flight requests on the shard level. Whenever the check fails, we only know\r\nthat there were in-flight requests but don't know what requests were causing this issue.\r\n\r\nSupersedes #21307 Add information about in-flight requests when checking IndexShard operation counter >>> 1"
336,"The Windows `service.bat` script was renamed to `elasticsearch-service.bat` in ES 5.0+. [DOCS] ""service"" should be ""elasticsearch-service"" >>> 1"
337,"ShardInfo#toString resorts to calling ShardInfo#toXContent via\r\nStrings#toString. However, the resulting XContent object will not start\r\nwith an object, and this is a violation of the generator state\r\nmachine. This commit fixes this issue by invoking the override of\r\nStrings#toString that will wrap the resulting XContent in an\r\nobject. Without this fix, ShardInfo#toString will instead produce ""Error\r\nbuilding toString out of XContent:\r\ncom.fasterxml.jackson.core.JsonGenerationException: Can not write a\r\nfield name, expecting a value"" Fix ShardInfo#toString >>> 1"
338,As the title says. Fix example in documentation for Painless using _source >>> 1
339,Backport of #20836 to 5.x.\n\nCommit b47ff92 implements the BWC layer for 5.0 / 5.x clusters.\n\nI have marked this PR as stalled as I'm unable to run the BWC tests.\n Keep snapshot restore state and routing table in sync (5.x backport) >>> 1
340,Today we still have a leftover from older percolators where Lucene\r\nquery instances where created ahead of time and rewritten later.\r\nThis `LateParsingQuery` was resolving `now()` when it's really used which we\r\ndon't need anymore. As a side-effect this failed to execute some highlighting\r\nqueries when they get rewritten since at that point `now` access it not permitted\r\nanymore to prevent bugs when queries get cached.\r\n\r\nCloses #21295 Remove LateParsingQuery to prevent timestamp access after context is frozen >>> 1
341,"As the title says.  \r\n\r\nFirst of all, this change looks huge, but it's not.  The only real changes have been made to the Script class itself.  All other changes have been made (mostly through Intellij refactoring) to a change in the Script constructor API to try to clean it up a bit.  Unfortunately, this affects a lot of classes, but the only change will be the arguments being passed in.\r\n\r\nTwo other changes to be aware of:\r\n1. Instead of having XContentType be a variable, made the change to an options Map instead for compiler options.  This makes way more sense given the way options can be passed in at compile time.  This opens up the ability to have user-defined compiler options potentially added in the future.  XContentType simply becomes one of the options.\r\n2. Enforces that all member variables of the script are non-null.  Before null and default value were synonymous for the same thing which could lead to confusion.  There is no reason to not enforce the default values at least, as it can clean up ambiguities passed into the engines where they have to check for null and apply default values themselves. Clean up Script >>> 0"
342,Today we validate the target index name late and therefore don't fail for instance\r\nif the target index already exists and `dry_run=true` was specified. This change\r\nvalidates the index name before we early terminate if dry_run is set.\r\n\r\nCloses #21149 Validate the `_rollover` target index name early to also fail if dry_run=true >>> 1
343,"Today, when listing thread pools via the cat thread pool API, thread\npools are listed in a column-delimited format. This is unfriendly to\ncommand-line tools, and inconsistent with other cat APIs. Instead,\nthread pools should be listed in a row-delimited format.\n\nAdditionally, the cat thread pool API is limited to a fixed list of\nthread pools that excludes certain built-in thread pools as well as all\ncustom thread pools. These thread pools should be available via the cat\nthread pool API.\n\nThis commit improves the cat thread pool API by listing all thread pools\n(built-in or custom), and by listing them in a row-delimited\nformat. Finally, for each node, the output thread pools are sorted by\nthread pool name.\n\nCloses #19590\n Improve cat thread pool API >>> 1"
344,"This adds the necessary `AuthCache` needed to support preemptive authentication. By adding every host to the cache, the automatically added `RequestAuthCache` interceptor will add credentials on the first pass rather than waiting to do it after _each_ anonymous request is rejected (thus always sending everything twice when basic auth is required).\r\n\r\nIt's easy to test with/without this using a nodejs proxy. `package.json`:\r\n\r\n```js\r\n{\r\n  ""name"": ""proxy-listener"",\r\n  ""version"": ""1.0.0"",\r\n  ""description"": ""Proxy Listener will print METHOD URL [AUTHORIZATION CONTENT-LENGTH]"",\r\n  ""main"": ""proxy.js"",\r\n  ""dependencies"": {\r\n    ""http"": ""^0.0.0"",\r\n    ""http-proxy"": ""^1.15.2""\r\n  },\r\n  ""devDependencies"": {},\r\n  ""scripts"": {\r\n    ""test"": ""echo \""Error: no test specified\"" && exit 1""\r\n  },\r\n  ""author"": """",\r\n  ""license"": ""ISC""\r\n}\r\n```\r\n\r\nUsing npm, install necessary dependencies\r\n\r\n```sh\r\nnpm install\r\n```\r\n\r\nThen run the proxy to intercept comms:\r\n\r\n```sh\r\nnode proxy.js\r\n```\r\n\r\nproxy.js:\r\n\r\n```js\r\nvar http = require('http'),\r\n    httpProxy = require('http-proxy');\r\n    \r\n// Create a new instance of HttProxy to use in your server \r\nvar proxy = httpProxy.createProxyServer({});\r\n \r\n// Create a regular http server and proxy its handler \r\nhttp.createServer(function (req, res) {\r\n  console.log(req.method + ' ' + req.url + ' [' + req.headers['authorization'] + ', ' + req.headers['content-length'] + ']');\r\n\r\n  proxy.web(req, res, { target: 'http://localhost:9250' });\r\n}).listen(9550);\r\n\r\nconsole.log('Listening at 9550');\r\n```\r\n\r\n/cc @javanna  Support Preemptive Authentication with RestClient >>> 1"
345,"Today if you start Elasticsearch with the status logger configured to\r\nthe warn level, or use a transport client with the default status logger\r\nlevel, you will see warn messages about deprecation loggers being\r\ncreated with different message factories and that formatting might be\r\nbroken. This happens because the deprecation logger is constructed using\r\nthe message factory from its parent, an artifact leftover from the first\r\nLog4j 2 implementation that used a custom message factory. When that\r\ncustom message factory was removed, this constructor invocation should\r\nhave been changed to not explicitly use the message factory from the\r\nparent. This commit fixes this invocation. However, we also had some\r\nstatus checking to all tests to ensure that there are no warn status log\r\nmessages that might indicate a configuration problem with Log4j 2. These\r\nassertions blow up badly without the fix for the deprecation logger\r\nconstruction, and also caught a misconfiguration in one of the logging\r\ntests. Assert status logger does not warn on Log4j usage >>> 1"
346,"Today when writing an unbounded queue_size in the cat thread pool API,\r\nwe write null. This commit modifies this so that the output is -1 so\r\nthat the output is always present, and always a numeric value.\r\n\r\nCloses #21187 Write -1 on unbounded queue in cat thread pool >>> 1"
347,"On ubuntu 14.04, which uses upstart, where as our debian package uses\r\nsysvinit, there is no stdout/stderr message printed when starting up,\r\nbecause the start-stop-daemon swallows it.\r\n\r\nAs Elasticsearch is started to daemonize, we can remove the background\r\nflag from the start-stop-daemon and thus see, if the system does not have\r\nenough memory for starting up - something that happens often on VMs, since\r\nElasticsearch 5.0 uses 2gb by default instead of one.\r\n\r\nRelates #21300\r\nRelates #12716\r\n\r\nI'd appreciate few more hints of what could go wrong with this approach that I may be missing. Debian: configure start-stop-daemon to not go into background >>> 1"
348,"Before, when requesting to get the snapshots in a repository, if `_all` was\r\nspecified for the set of snapshots to get, any in-progress snapshots would\r\nbe returned twice.  This commit fixes the issue ensuring that each snapshot,\r\nwhether in-progress or completed, is only returned once when making a call to\r\nget snapshots (GET /_snapshot/{repository_name}/_all).\r\n\r\nCloses #21335 Fixes get snapshot duplicates when asking for _all >>> 1"
349,"This PR changes the current `:qa:vagrant` build file and transforms it into a Gradle plugin in order to reuse it in other projects.\r\n    \r\nMost of the code from the build.gradle file has been moved into the `VagrantTestPlugin` class. To avoid duplicated VMs when running vagrant tests, the Gradle plugin sets the following environment variables before running vagrant commands:\r\n  - VAGRANT_CWD:  absolute path to the folder that contains the Vagrantfile\r\n  - VAGRANT_PROJECT_DIR: absolute path to the Gradle project that use the `VagrantTestPlugin`\r\n    \r\nThe `VAGRANT_PROJECT_DIR` is used to share project folders and files with the vagrant VM. \r\n\r\nThese folders and files are exported when running the task `gradle vagrantSetUp` which:\r\n  - collects all project archives dependencies and copies them into `${project.buildDir}/bats/archives`\r\n  - copy all project bats testing files from 'src/test/resources/packaging/tests' into `${project.buildDir}/bats/tests`\r\n  - copy all project bats utils files from 'src/test/resources/packaging/utils' into `${project.buildDir}/bats/utils`\r\n    \r\nOther projects can also possible to inherit and grab the archives/tests/utils files from project dependencies using the plugin configuration:\r\n\r\n```\r\napply plugin: 'elasticsearch.vagrant'\r\n\r\nesvagrant {\r\n        inheritTestUtils true|false\r\n        inheritTestArchives true|false\r\n        inheritTests true|false\r\n}\r\n   \r\ndependencies {\r\n        // Inherit Bats test utils from :qa:vagrant project\r\n        bats project(path: ':qa:vagrant', configuration: 'bats')\r\n}\r\n\r\n```    \r\n\r\nThe folders `${project.buildDir}/bats/archives`, `${project.buildDir}/bats/tests` and `${project.buildDir}/bats/utils` are then exported to the vagrant VMs and mapped to the BATS_ARCHIVES, BATS_TESTS and BATS_UTILS environment variables.\r\n    \r\nThe following Gradle tasks have also be renamed to have a vagrant prefix:\r\n\r\n  * gradle vagrantSetUp\r\n  This task copies all the necessary files to the project build directory (was `prepareTestRoot`)\r\n    \r\n* gradle vagrantSmokeTest\r\nThis task starts the VMs and echoes a ""Hello world"" within each VM (was: `smokeTest`)\r\n\r\nUsing the same Vagrantfile for multiple Gradle projects makes a bit harder to use the usual `vagrant up box` and `vagrant ssh box`command (they need to be executed in the right folder and with the right vagrant environment variables for other project than elasticsearch) but I think it is a good move forward to allow other projects to be tested on the exact same VMs. Add Vagrant Gradle plugin >>> 1"
350,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n\r\n Update store.asciidoc >>> 1"
351,This removes the exception thrown when serializing a reindex request\r\nto a node < 5.1.0 and instead silently falls back to a single slice.\r\nThis is required to be compatible with `assertVersionSerializeable`.\r\n\r\nRelates to #20767 Remove exception for sliced reindex on mixed version >>> 0
352,This PR adds the `routing` query string param to the `mget` rest spec. The routing query string param is supported by mget but was missing from the rest spec >>> 1
353,"This commit introduces a new execution mode for the\r\n`simple_query_string` query, which is intended down the road to be a\r\nreplacement for the current _all field.\r\n\r\nIt now does auto-field-expansion and auto-leniency when the following criteria\r\nare ALL met:\r\n\r\n    The _all field is disabled\r\n    No default_field has been set in the index settings\r\n    No fields are specified in the request\r\n\r\nAdditionally, a user can force the ""all-like"" execution by setting the\r\nall_fields parameter to true.\r\n\r\nWhen executing in all field mode, the `simple_query_string` query will\r\nlook at all the fields in the mapping that are not metafields and can be\r\nsearched, and automatically expand the list of fields that are going to\r\nbe queried.\r\n\r\nRelates to #20925, which is the `query_string` version of this work.\r\nThis is basically the same behavior, but for the `simple_query_string`\r\nquery.\r\n\r\nRelates to #19784 Add ""all fields"" execution mode to simple_query_string query >>> 1"
354,"This adds a better extension point for custom cluster state parts by only allowing adding custom cluster state parts via the ClusterPlugin interface.\r\n\r\nThis is a WIP and would be great to get feedback.\r\n\r\n* By piggybacking on the named writeable infrastructure we can serialize custom cluster state parts for the internal protocol. This seems to workout nice as the serialization code doesn't need to know about custom cluster metadata implementations.\r\n* For the xcontent serialization there is no such thing as named writaebles, so I passed down a registry holding prototypes for the custom cluster state parts on all the places we need to do de-serialization. We can maybe consider adding something like named writable on top of XContentParser? However passing down the registry doesn't seem to be too bad.\r\n* Should custom index metadata be removed? It isn't used in any place. I think it should, but in a follow up change.\r\n* This change needs tests.\r\n\r\nPR for #20888  Better extension point for custom cluster state parts >>> 0"
355,"We currently often use ensureGreen or ensureYellow to check whether the cluster is in a good state again after shutting down a node:\r\n```\r\nstopRandomNode(...);\r\nensureGreen() / ensureYellow();\r\n```\r\n\r\nWith the change in #21092, however, it can happen that if the node that is stopped is the master node, another node will become master and publish a cluster state where it is master but where the node that was stopped hasn't been removed yet from the cluster state. It will only publish a second state thereafter where the old master is removed. If the ensureGreen/ensureYellow is timed just right, it will get to execute before the second cluster state update removing the old master and the condition ensureGreen / ensureYellow might not hold at that point anymore. Make ensureGreen and ensureYellow wait for cluster size consistency >>> 1"
356,This commit ensure that that VirtualBox is available in version 5.1+ in the system before running packaging tests. It also check for Vagrant version is now greater than 1.8.6.\r\n Add VirtualBox version check >>> 1
357,"The `ClusterState` class currently has a mutable volatile field ""status"" that is only used by the `ClusterStateObserver` to differentiate between a cluster state that is being applied or one that has already been applied. This PR removes the field from cluster state, making it a truly immutable class. This information is stored instead by `ClusterService`, which is the only place that should update this field (`PublishClusterStateAction` was also updating it, but that information was never used anywhere). A new class is introduced (`ClusterServiceState`) which emcompasses the current cluster state as well as the current status, which is only used by the `ClusterStateObserver` mechanism. Remove mutable status field from cluster state >>> 1"
358,"Turns out that `AbstractAsyncResponseConsumer` from apache async http client is stateful and cannot be reused across multiple requests. The failover mechanism was mistakenly reusing that same instance, which can be provided by users, across retries in case nodes are down or return 5xx errors. The downside is that we have to change the signature of two public methods, as `HttpAsyncResponseConsumer` cannot be provided directly anymore, rather its factory needs to be provided which is going to be used to create one instance of the consumer per request attempt.\r\n\r\nUp until now we tested our `RestClient` against multiple nodes only in a mock environment, where we don't really send http requests. In that scenario we can verify that retries etc. work properly but the interaction with the http client library in a real scenario is different and can catch other problems. With this commit we also add an integration test that sends requests to multiple hosts, and some of them may also get stopped meanwhile. The specific test for `pathPrefix` was also removed as pathPrefix is now randomly applied by default, hence implicitly tested. Moved also a small test method that checked the validity of the path argument to the unit test `RestClientSingleHostTests`. Rest client: don't reuse the same HttpAsyncResponseConsumer across multiple retries >>> 1"
359,Adds docs for UnicastHostsDiscovery and removal of ZenPing and\r\nMasterElectService. Docs: Add breaking changes docs for zen plugin changes >>> 1
360,This commit removes checks for buggy JDK 7 JVMs from the JVM check as\r\nElasticsearch does not support JDK 7.\r\n\r\nRelates #7580 Remove JDK 7 checks from JVMCheck >>> 1
361,This commit migrates the G1GC JVM check to a bootstrap check.\r\n\r\nRelates #16737 Migrate G1GC JVM check to bootstrap check >>> 1
362,This commit adds an assertion to ensure that we do not introduce blocking calls in code\r\nthat is called in a ClusterStateListener or another part of the cluster state update process. assert blocking calls are not made on the cluster state update thread >>> 1
363,"This new setting was added with deguicing unicast hosts providers, but\r\nthe docs were not updated. Docs: Update discovery plugins example configuration with discovery.zen.hosts_provider >>> 1"
364,"At one point in the past when moving out the rest tests from core to\r\ntheir own subproject, we had multiple test classes which evenly split up\r\nthe tests to run. However, we simplified this and went back to a single\r\ntest runner to have better reproduceability in tests. This change\r\nremoves the remnants of that multiplexing support. Test: Remove multi process support from rest test runner >>> 1"
365,"All plugins currently have their own licenses dir for the\r\ndependencyLicenses task, but core disables this and has the check inside\r\ndistribution. This may have been better for maven, but for\r\ngradle it makes more sense to just use the dependencyLicenses task that\r\nautomatically exists inside :core, and remove the hacked up version that\r\nis inside distribution. Move licenses for core jar to core directory >>> 1"
366,"This change simply makes the level of the ant timestamp for waiting on\r\nthe integ test cluster echo at the info level instead of warn (the\r\ndefault) so that it is only output when running with gradle --info, or\r\nwhen the wait condition fails. Test: Make ant log for wait condition quieter by default >>> 1"
367,"While test driving the current state of the api we realized that specifying a cut-off ""size"" in e.g. the Precision@N metric and issuing a search request with a different size leads to rather counter-intuitive results. This change removes the ""size"" parameters from precison and dcg metric, leaving a ""max_acceptable_rank"" for the reciprocal rank metric. The actual window size of the results that are evaluated should be controlled by setting the ""size"" parameter in each search request itself or in the template. We might also consider an option to set the maximum number of document to be retrieved on the top-level request and then set it on each individual sub-query. Rank Eval: Remove size from metrics >>> 1"
368,Before this change Eclipse (4.6.1) would show compile errors in `ClusterRerouteRequestTests` for the elements in `RANDOM_COMMAND_GENERATORS`. This seems to be because the eclipse compiler does not recognised that the elements in the list are all `Supplier`s of bjects that are subclasses of `AllocationCommand`.\r\n\r\nThis change fixes the problem by adding a generics hint to the `Arrays.toList()` call. Fixes compile eeror in Eclipse in ClusterRerouteRequestTests >>> 1
369,Currently the task cancellation command returns as soon as the top-level parent child is marked as cancelled. This create race conditions in tests where child tasks on other nodes may continue to run for some time after the main task is cancelled. This commit fixes this situation making task cancellation command to wait until it got propagated to all nodes that have child tasks.\r\n\r\nCloses #21126 Task cancellation command should wait for all child nodes to receive cancellation request before returning >>> 1
370,"This change primarily moves registering custom Discovery implementations\r\nto the pull based DiscoveryPlugin interface. It also keeps the cloud\r\nbased discovery plugins re-registering ZenDiscovery under their own name\r\nin order to maintain backwards compatibility. However,\r\ndiscovery.zen.hosts_provider is changed here to no longer fallback to\r\ndiscovery.type. Instead, each plugin which previously relied on the\r\nvalue of discovery.type now sets the hosts_provider to itself if\r\ndiscovery.type is set to itself, along with a deprecation warning. Plugins: Convert custom discovery to pull based plugin >>> 1"
371,This gives us a handy place to tell users that they can't make a certain request in a mixed version cluster. It fixes reindex to use that mechanism.\r\n\r\nReplaces #21350\r\nRelates to #20767 Ignore IllegalArgumentException with assertVersionSerializable >>> 1
372,"This commit ensures that we always restore the thread's original context after execution of a context preserving runnable. We always wrap runnables in a wrapper that restores the context at the time it was submitted to the execute method. The ContextPreservingAbstractRunnable would restore the calling context in the doRun method and then in a try with resources block would restore the thread's original context. However, the onFailure and onAfter method of a AbstractRunnable could modify the thread context and this modified thread context would continue on as the thread's context after it was returned to the pool and potentially used for a different purpose. Restore thread's original context before returning to the ThreadPool >>> 1"
373,"There is an issue in the Grok Processor, where `trace_match: true` does not inject the `_ingest._grok_match_index` into the ingest-document when there is just one pattern provided. This is due to an optimization in the regex construction. This commit adds a check for when this is the case, and injects a static index value of `""0""`, since there is only one pattern matched (at the first index into the patterns).\r\n\r\nTo make this clearer, more documentation was added to the grok-processor docs.\r\n\r\nFixes #21371. fix trace_match behavior for when there is only one grok pattern >>> 1"
374,There were still a couple test use cases and examples that were using\r\nonModule. This change cleans those cases up. Tests: Remove a couple test uses of onModule >>> 1
375,"Currently, `executeIndexRequestOnPrimary` and `executeDeleteRequestOnPrimary`\r\nmethods used to prepare and execute write operations, modifies the provided\r\nrequest, updating the version and versionType. This commit makes it the\r\ncallers responsibility to update request version and versionType and avoids\r\nmutating the provided request in the execute methods. Ensure write operation execution does not have side-effects >>> 1"
376,"Each node checks on every cluster state update if there are shards that it can possibly delete from its disk. It decides this by doing a file-system lookup for each shard id that is fully allocated in the cluster. With lots of shards, this amounts to lots of `Files.exists()` checks, considerably slowing down cluster state updates. This commit adds a caching layer so that the `Files.exists()` checks can be skipped if not needed.\r\n\r\n\r\nI've done some simple benchmarking to validate the approach using the following test method:\r\n\r\n```\r\npublic void testSimple() {\r\n    String node1 = internalCluster().startNode();\r\n    String node2 = internalCluster().startNode();\r\n    ensureStableCluster(2);\r\n\r\n    ClusterState state = internalCluster().getInstance(ClusterService.class, node1).state();\r\n    ClusterStateChanges cluster = new ClusterStateChanges();\r\n    for (int i = 0; i < 1000; i++) {\r\n        String name = ""index_"" + i;\r\n        Settings.Builder settingsBuilder = Settings.builder()\r\n            .put(SETTING_NUMBER_OF_SHARDS, 1)\r\n            .put(SETTING_NUMBER_OF_REPLICAS, 0)\r\n            .put(""index.routing.allocation.exclude._name"", node1);\r\n        CreateIndexRequest request = new CreateIndexRequest(name, settingsBuilder.build()).waitForActiveShards(ActiveShardCount.NONE);\r\n\r\n        state = cluster.createIndex(state, request);\r\n    }\r\n\r\n    state = cluster.applyStartedShards(state, state.routingTable().shardsWithState(ShardRoutingState.INITIALIZING));\r\n\r\n    ClusterState state1 = cluster.closeIndices(state, new CloseIndexRequest(""index_0""));\r\n    ClusterState state2 = cluster.openIndices(state, new OpenIndexRequest(""index_0""));\r\n\r\n    ClusterChangedEvent changedEvent = new ClusterChangedEvent(""simulated change 1"", state1, state2);\r\n\r\n    IndicesStore indicesStore = internalCluster().getInstance(IndicesStore.class, node1);\r\n\r\n    logger.info(""first iteration"");\r\n    indicesStore.clusterChanged(changedEvent);\r\n    logger.info(""second iteration"");\r\n    indicesStore.clusterChanged(changedEvent);\r\n    logger.info(""finished"");\r\n}\r\n```\r\n\r\nOutput on my Laptop (with SSD):\r\n\r\n\r\nWITHOUT PATCH\r\n```\r\n[2016-11-10T02:30:15,456][INFO ][o.e.i.s.IndicesStoreIntegrationIT] first iteration\r\n[2016-11-10T02:30:15,834][INFO ][o.e.i.s.IndicesStoreIntegrationIT] second iteration\r\n[2016-11-10T02:30:16,015][INFO ][o.e.i.s.IndicesStoreIntegrationIT] finished\r\n```\r\n\r\nWITH PATCH APPLIED\r\n```\r\n[2016-11-10T13:29:07,053][INFO ][o.e.i.s.IndicesStoreIntegrationIT] first iteration\r\n[2016-11-10T13:29:07,337][INFO ][o.e.i.s.IndicesStoreIntegrationIT] second iteration\r\n[2016-11-10T13:29:07,340][INFO ][o.e.i.s.IndicesStoreIntegrationIT] finished\r\n``` Cache successful shard deletion checks >>> 1"
377,`sourceRef` always throw `NullPointerException` when `source` is `null`\r\n\r\nCloses #19279 Null checked for source when calling sourceRef >>> 1
378,"The method used to be called isSourceEmpty, and was renamed to hasSource, but the return value never changed. Updated tests and users accordingly.\r\n\r\nCloses #21419 Fix InternalSearchHit#hasSource to return the proper boolean value >>> 1"
379,"The environment variable ES_JVM_OPTIONS allows end-users to specify a\r\ncustom location for the jvm.options file. Unfortunately, this\r\nenvironment variable is not exported from the SysV init scripts. This\r\ncommit addresses this issue, and includes a test that ES_JVM_OPTIONS and\r\nES_JAVA_OPTS work for the SysV init packages.\r\n\r\nCloses #21255 Export ES_JVM_OPTIONS for SysV init >>> 1"
380,"Whoops, these should always work and link to the specific release\r\nversion. Fix URLs in elasticsearch.yml linking to correct documentation >>> 1"
381,"\r\nGiven that the default is now 1, the comment in the config file was outdated. Also considering that the default value is production ready, we shouldn't list it among the values that need attention when going to production.\r\n\r\nRelates to #19964 Remove max_local_storage_nodes from elasticsearch.yml >>> 1"
382,This is the final release. Upgrade to lucene-6.3.0. >>> 1
383,The dependency was carried all the way to InternalAggregation but it was never used. Remove unused ClusterService dependency from SearchPhaseController >>> 1
384,"This PR ensures pending `index-*` blobs are deleted when snapshotting.  The\r\n`index-*` blobs are generational files that maintain the snapshots\r\nin the repository.  To write these atomically, we first write a\r\n`pending-index-*` blob, then move it to `index-*`, which also deletes\r\n`pending-index-*` in case its not a file-system level move (e.g.\r\nS3 repositories) .  For example, to write the 5th generation of the\r\nindex blob for the repository, we would first write the bytes to\r\n`pending-index-5` and then move `pending-index-5` to `index-5`.  It is\r\npossible that we fail after writing `pending-index-5`, but before\r\nmoving it to `index-5` or deleting `pending-index-5`.  In this case,\r\nwe will have a dangling `pending-index-5` blob laying around.  Since\r\nsnapshot number 5 would have failed, the next snapshot assumes a generation\r\nnumber of 5, so it tries to write to `index-5`, which first tries to\r\nwrite to `pending-index-5` before moving the blob to `index-5`.  Since\r\n`pending-index-5` is leftover from the previous failure, the snapshot\r\nfails as it cannot overwrite this blob.\r\n\r\nThis commit solves the problem by first, adding a UUID to the\r\n`pending-index-*` blobs, and secondly, strengthen the logic around\r\nfailure to write the `index-*` generational blob to ensure pending\r\nfiles are deleted on cleanup.\r\n\r\nCloses #21462 Ensures cleanup of temporary index-* generational blobs during snapshotting >>> 1"
385,"#20960 removed `LocalDiscovery` and we now use `ZenDiscovery` in all our tests. To keep cluster forming fast, we are using a `MockZenPing` implementation which uses static maps to return instant results making master election fast. Currently, we don't set `minimum_master_nodes` causing the occasional split brain when starting multiple nodes concurrently and their pinging is so fast that it misses the fact that one of the node has elected it self master. To solve this, `InternalTestCluster` is modified to behave like a true cluster and manage and set `minimum_master_nodes` correctly with every change to the number of nodes.\r\n\r\nTests that want to manage the settings themselves can opt out using a new `autoMinMasterNodes` parameter to the `ClusterScope` annotation. \r\n\r\nHaving `min_master_nodes` set means the started node may need to wait for other nodes to be started as well. To combat this, we set `discovery.initial_state_timeout` to `0` and wait for the cluster to form once all node have been started.\r\n\r\nWith this change in place, I think we can/should do a couple of follow ups:\r\n1) Remove async node starts. Now we can just start the nodes one by one (using `initial_state_timeout=0`) and wait for them to join at the end. This will simplify things.\r\n2) ~~`MockZenPing` makes pinging fast. If a node waits on other nodes to be started, it can busy-ping generating lots of logs. I have some ideas of how to tackle that but I hope that removing async  multi node starting (meaning we can reduce the time a node waits by first creating all of them and then starting them)  will be enough to not require this.~~. This is addressed by this PR as well. Adapt InternalTestCluster to auto adjust `minimum_master_nodes` >>> 1"
386,ShardActiveResponseHandler doesn't need to hold to ab entire cluster state since it only needs to know the cluster state version. It seems that on overloaded systems where nodes are unresponsive holding onto a lot of different cluster states can make the situation worse.\r\n\r\nCloses #21394\r\n\r\nNot sure how far back should we go in propagating this fix. ShardActiveResponseHandler shouldn't hold to an entire cluster state >>> 1
387,"Today when handling responses from nodes in TransportNodesAction, if a\r\nnode timeouts or some other failure occurs and the action is not\r\naccumulating exceptions, we log a confusing message:\r\n\r\n```\r\norg.elasticsearch.action.admin.cluster.stats.TransportClusterStatsAction] ignoring unexpected response [null] of type [null], expected [ClusterStatsNodeResponse] or [FailedNodeException]\r\n```\r\n\r\nMoreover, the original exception is completely lost. Since this log\r\nmessage is confusing and unhelpful, we can drop it. Instead, we hold\r\nonto the exception and log it at the warn level before dropping it from\r\nthe response. Remove unhelpful log message from TransportNodesAction >>> 1"
388,"Today when a message is not fully read on a response, we log (among\r\nother details) the handler name. Unfortunately, if the handler is a\r\nwrapper, all that we see is\r\n\r\n```\r\norg.elasticsearch.transport.TransportService$ContextRestoreResponseHandler@7446ba18\r\n```\r\n\r\ncompletely losing the offending handler. This commit adds an override\r\nfor TransportService$ContextRestoreResponseHandler#toString so that the\r\nunderlying offender can be discovered. Fix handler name on message not fully read >>> 1"
389,"This documents the `aws_availability_zone` node attribute as part of the `discovery-ec2` plugin. Also fixes outdated usage of ""cloud aws"". [DOCS] Show EC2's auto attribute >>> 1"
390,Closes #21331 Add documentation for Logger with Transport Client >>> 1
391,"The netty transports are doing the same, so this just aligns the mock tcp\r\ntransport. MockTcpTransport: Close resources regardless of being able to send data >>> 0"
392,"[TEST] Makes the snapshot throttling test go much faster.  Before,\r\nthe snapshot throttling test would throttle at a rate of 0.5 kb per\r\nsecond, even though it would snapshot/restore about 25 kb of data.\r\nThis commit increases the throttling rate to 10kb per second, so\r\nwe still test the throttling mechanism while speeding up the test from\r\ntaking 30 plus seconds down to approximately 3 seconds or less. Make snapshot throttling test go much faster >>> 1"
393,This change adds a mock discovery (which internally uses the existing\r\nmock zenping). Having the mock the test framework selects be a discovery\r\ngreatly simplifies discovery setup (no more weird callback to a Node\r\nmethod). Tests: Add TestZenDiscovery and replace uses of MockZenPing with it >>> 1
394,This applies the same stance as 5.1 and 6.0 branches that we can  just use System.currentTimeMillis() when returning the value for expiration in the fetch phase instead of getting he value of now from the frozen context which is now not possible.\r\n\r\nCloses #21457 Fixes cachability problems with fetching TTL values when searching >>> 1
395,This adds a first page of reference documentation to the current state of the\r\nrank evaluation API.\r\n\r\nCloses to #21402\r\n\r\n@cbuescher Would you mind taking a look? Reference documentation for rank evaluation API >>> 1
396,"Similar to ResourceNotFoundException, a generic ResourceAlreadyExistsException will reduce the need to introduce an additional exception for each type of resource that can have that error.\r\n\r\nAt the moment, there is:\r\n  - IndexAlreadyExistsException\r\n  - IndexTemplateAlreadyExistsException\r\n  - IndexShardAlreadyExistsException\r\n\r\nThis commit changes those to inherit ResourceAlreadyExistsException\r\nas removing them would cause backwards compatibility problems. Replace IndexAlreadyExistsException with ResourceAlreadyExistsException >>> 1"
397,This PR contains documentation for the Elasticsearch Docker image.\r\n\r\nIt is based on the README.md on https://github.com/elastic/elasticsearch-docker\r\n Documentation for Docker Image >>> 1
398,Adds an assertion that checks that the same shard with same id is not added to same node. Previously we would just silently ignore the second shard being added. Add assertion that checks that the same shard with same id is not added to same node >>> 1
399,This adds support to painless for decimal constants with trailing `d` or\r\n`D` to make it compatible with Java. It already supported integer\r\nconstants with a trailing `d` or `D` but this adds tests for it.\r\n\r\nCloses #21116 Support decimal constants with trailing [dD] in painless >>> 1
400,"In painless we prefer explicit types over implicit ones whereas\r\ngroovy is the other way around. Take this groovy code:\r\n\r\n```\r\n> 86400000.class\r\njava.lang.Integer\r\n> 864000000000.class\r\njava.lang.Long\r\n```\r\n\r\nPainless accepts `86400000` just fine because that is a valid `int`\r\nin the jvm. It rejects `864000000000` as an invlid `int` constant\r\nbecause, in painless as in java, `long` constants always end in `L`\r\nor `l`.\r\n\r\nTo ease the transition from groovy to painless, this changes the\r\ncompilation error returned from these invalid constants from:\r\n\r\n```\r\nInvalid int constant [864000000000].\r\n```\r\n\r\nto\r\n\r\n```\r\nInvalid int constant [864000000000]. If you want a long constant then change it to [864000000000L].\r\n```\r\n\r\nInspired by #21313 In painless suggest a long constant if int won't do >>> 1"
401,"Implements a [null coalescing operator](https://en.wikipedia.org/wiki/Null_coalescing_operator) in painless that looks like `?:`. This form was chosen to emulate Groovy's `?:` operator. It is different in that it only coalesces `null` values, instead of Groovy's `?:` operator which coalesces all [falsy](http://groovy-lang.org/semantics.html#Groovy-Truth) values. I believe that makes it the same as Kotlin's `?:` operator. In other languages this operator looks like `??` (C#) and `COALESCE` ([SQL](https://www.postgresql.org/docs/9.1/static/functions-conditional.html#FUNCTIONS-COALESCE-NVL-IFNULL)) `:-` (bash).\r\n\r\nThis operator is lazy, meaning the right hand side is only evaluated at all if the left hand side is null. Implement the ?: operator in painless >>> 1"
402,This commit removes the shard ID from doc write response; this was\r\nuseful for debugging but its time has passed.\r\n\r\nRelates #10708 Remove shard ID from doc write response >>> 1
403,"The clear scroll api currently allows to provide a scroll by specifying it either as part of the url (it is effectively the resource that gets deleted) or within the request body. The current api uses the DELETE method though, and we have decided to remove support for providing the request body with any DELETE endpoint in the future. In order to get to this for the next major version, we introduce the  new endpoint `POST /_search/clear_scroll` which replaces the current clear_scroll api and uses POST instead of DELETE. It allows to provide the `scroll_id` as a url parameter, which is though deprecated (will output a deprecation warning when used) in favour of providing it as part of the request body.\r\n\r\n The `DELETE /_search/scroll/` is deprecated, hence it will output a deprecation warning whenever used. The DELETE endpoints will be removed in 6.0, as well as the support for providing the scroll_id as a url parameter against the POST endpoint.\r\n\r\nRelates to #8217\r\nRelates to #21453 Add POST /_search/clear_scroll endpoint and deprecate delete scroll endpoint >>> 0"
404,"Our docs claim that we set vm.max_map_count automatically. This is not\r\nquite the case. The story is that on SysV init we set vm.max_map_count\r\neach time the service starts, which is good. On systemd, we create a\r\nsysctl.d conf file that sets vm.map_max_count, but this is only\r\nmeaningful if the system is rebooted after package install. This commit\r\nmodifies the post-install script so that we run systemd-sysctl so that\r\nthe vm.max_map_count change occurs after package install without a\r\nreboot. Set vm.max_map_count on systemd package install >>> 1"
405,"We have an assertion in the engine regarding the initial state of a\r\nsequence number before an indexing operation. This assertion is too\r\nloose, it catches operations during recovery from old indices where\r\nsequence numbers do not even exist. This commit tightens these\r\nassertions to not catch such operations and enables us to reenable some\r\ntests.\r\n\r\nRelates #10708 Tighten sequence number assertion >>> 1"
406,"It was possible that in IndexService#closeShard(), an attempt\r\nwould be made to close the store where the store was not yet\r\ninitialized, throwing a NullPointerException.  This commit\r\nensures that we do not call close on a store that has not\r\nyet been initialized.\r\n\r\nThis was already fixed as part of #21084 which went into 6.0\r\nand 5.x (see https://github.com/elastic/elasticsearch/pull/21084/files#diff-00f21a995130f7df8879749075287053R414), so this is a backport of this \r\nparticular NPE issue to 5.0.\r\n Fixes potential NullPointerException on shard closing >>> 1"
407,"When a cluster update task executes, there can be log messages after the\r\nupdate task has finished processing and the new cluster state becomes\r\nvisible. The visibility of the cluster state allows the test thread in\r\nUpdateSettingsIT#testUpdateAutoThrottleSettings and\r\nUpdateSettingsiT#testUpdateMergeMaxThreadCount to proceed. The test\r\nthread will remove and stop a mock appender setup at the beginning of\r\nthe test. The log messages in the cluster state update task that occur\r\nafter processing has finished can race with the removal of the\r\nappender. Log4j will grab a reference to the appenders when processing\r\nthese log messages, and this races with the removal and stopping of the\r\nappenders. If Log4j grabs a reference to the appenders before the mock\r\nappender has been removed, and the test thread subsequently removes and\r\nstops the appender before Log4j has appended the log message, Log4j will\r\nget angry that we are appending to a stopped appender, causing the test\r\nto fail. This commit addresses this race by waiting for the cluster\r\nstate update task to have finished processing before freeing the test\r\nthread to make its assertions and finally remove and stop the\r\nappender. Yes, this is a hack.\r\n\r\nCloses #21461 Hack around cluster service and logging race >>> 1"
408,Otherwise an empty string get added as _parent field.\r\n\r\nPR for #21503 Skip adding a parent field to nested documents. >>> 1
409,With ES 5.0 we do not include Jackson\r\nDatabind anymore with ES core. This commit\r\nupdates our docs to state that users need\r\nto add this artifact now in their projects. [docs] clients need to add jackson-databind >>> 1
410,NodeBuilder has been removed and this section rather adds confusion how\r\nto run tests since we have a test-framework for this that should be used.\r\n\r\nRelates to #21359 Remove outdated section about NodeBuilder with local transport >>> 1
411,"Integrate @mikemccand's patch from [LUCENE-6664](https://issues.apache.org/jira/browse/LUCENE-6664) into elasticsearch and add support for handling a graph token stream in match/multi-match queries.  \r\n\r\nThis PR still needs some work, ie. lots more tests, fix some TODO's, etc.  I wanted to get some feedback and see if you would even consider merging this before I spend much more time on it.  I do have it as a plugin as well but it would be much nicer in core since the plugin needs to fork Match and MultiMatch queries.\r\n Synonym Graph Support (LUCENE-6664) >>> 1"
412,"Both exception can be replaced with java built-in exception, IAE and ISE respectively.\r\nThis should be backported partially to 5.x which the transport layer code should be preserved.\r\n\r\nRelates to #21494 Remove `IndexTemplateAlreadyExistsException` and `IndexShardAlreadyExistsException` >>> 1"
413,This commit enables real BWC testing against a 5.1 snapshot. All\r\nREST tests plus rolling upgrade test now run against a mixed version\r\ncross major version cluster. Enable 5.x to 6.x BWC tests >>> 1
414,"This class had been added to address a bug in PointValues, which has been fixed\r\nsince then. Remove XPointValues. >>> 1"
415,Backport of #21468 for 2.4 Update Joda Time to version 2.9.5 on branch 2.4 >>> 1
416,This change fixes the range query so that an exception is always thrown if the range query uses epoch time together with a time zone. Since epoch time is always UTC it should not be used with a time zone.\r\n\r\nCloses #21501 Fixes date range query using epoch with timezone >>> 1
417,"Today when parsing a stats request, Elasticsearch silently ignores\r\nincorrect metrics. This commit removes lenient parsing of stats requests\r\nfor the nodes stats and indices stats APIs.\r\n\r\nRelates #20722, relates #21410 Remove lenient stats parsing >>> 1"
418,"Due to various issues (https://github.com/elastic/elasticsearch/issues/20618) reported by the field on setting index.mapper.dynamic at the node level in the yml, let's not actively promote users to set this in the yml and only mention setting it at the index level (which is what should be done starting in 5.0 anyway). Do not actively promote setting index.mapper.dynamic at the node level >>> 1"
419,"Today when a node starts, we create dynamic socket permissions based on\r\nthe configured HTTP ports and transport ports. If no ports are\r\nconfigured, we use the default port ranges. When a tribe node starts, a\r\ntribe node creates an internal node client for connecting to each remote\r\ncluster. If neither an explicit HTTP port nor transport ports were\r\nspecified, the default port ranges are large enough for the tribe node\r\nand its internal node clients. If an explicit HTTP port or transport\r\nport was specified for the tribe node, then socket permissions for those\r\nports will be created, but not for the internal node clients. Whether\r\nthe internal node clients have explicit ports specified, or attempt to\r\nbind within the default range, socket permissions for these will not\r\nhave been created and the internal node clients will hit a permissions\r\nissue when attempting to bind. This commit addresses this issue by also\r\naccounting for tribe nodes when creating the dynamic socket\r\npermissions. Additionally, we add our first real integration test for\r\ntribe nodes.\r\n\r\nCloses #16392 \r\nCloses #21122 Add socket permissions for tribe nodes >>> 1"
420,closes #21368 Remove generics from ActionRequest >>> 1
421,"This changes only the query parsing behavior to be strict when searching on\r\nboolean values. We continue to accept the variety of values during index time,\r\nbut searches will only be parsed using `""true""` or `""false""`.\r\n\r\nResolves #21545\r\n Be strict when parsing values searching for booleans >>> 1"
422,nan Adjust bootstrap sequence >>> 1
423,"When using TimeUnitRounding with a DAY_OF_MONTH unit, failing tests in #20833 uncovered an issue when the DST shift happenes just one hour after midnight local time and sets back the clock to midnight, leading to an overlap. Previously this would lead to two different rounding values, depending on whether a date before or after the transition was rounded. This change detects this special case and correct for it by using the previous rounding date for both cases.\r\n\r\nIn this particular case (tz: ""Atlantic/Azores"", dates around the DST transition on e.g. 2000-10-29T01:00:00.000Z, unit: DAY_OF_MONTH) we currently get the overlapping part during the DST transition as a separate rounding interval as illustrated in this table:\r\n\r\n| date                                      | round(date)                          | nextRoundingValue(date)      |\r\n| --------------------------------|---------------------------------|---------------------------------|\r\n| 2000-10-28T22:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-28T23:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-29T00:00:00.000Z | **2000-10-29T00:00:00.000Z** | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T00:00:00.000-01:00 | **2000-10-29T00:00:00.000-01:00** | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T01:00:00.000-01:00 | 2000-10-29T00:00:00.000-01:00 | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T02:00:00.000-01:00 | 2000-10-29T00:00:00.000-01:00 | 2000-10-30T00:00:00.000-01:00 |\r\n\r\nAccording to https://www.timeanddate.com/time/change/portugal/horta?year=2000 the DST change happenes on Oct-29th at 1am local time, turning back the clock one hour (to offset -01:00). Currently the dates between ""2000-10-29T00:00:00.000Z"" and ""2000-10-29T01:00:00.000Z"" all round down to ""2000-10-29T00:00:00.000Z"" (before the transition) and the dates after but before next midnight round down to ""2000-10-29T00:00:00.000-01:00"". For a day rounding we would prefer a 25h hour bucket for ""2000-10-29"". \r\n\r\nWith this fix, the situation above changes to \r\n\r\n| date                                      | round(date)                          | nextRoundingValue(date)      |\r\n| --------------------------------|---------------------------------|---------------------------------|\r\n| 2000-10-28T22:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-28T23:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-29T00:00:00.000Z | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T00:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T01:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T02:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n\r\nSo now everything on ""2000-10-29"" gets rounded down to ""2000-10-29T00:00:00.000Z"".\r\n\r\nCloses #20833 Fix time zone rounding edge case for DST overlaps >>> 1"
424,"This failure is due to the fact that we sort on store size, which is cached. So\r\nit might happen that the store size that is taken into account is not the right\r\none, which makes the indices sorted in the wrong order. This changes the doc\r\nexample to sort on the number of docs instead.\r\n\r\nCloses #21062 Fix recurring doc test failures with the cat API. >>> 1"
425,"We run an assert on an potentially closed thread context. this should\r\nnot bubble up the `IllegalStateException`.\r\n\r\n[here](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/660/console) is a failure that was triggered by this:\r\n\r\n```\r\n  2> نوف 15, 2016 8:37:00 ص com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\r\n  2> WARNING: Uncaught exception in thread: Thread[elasticsearch[[unicast_connect]][T#3],5,TGRP-ZenDiscoveryUnitTests]\r\n  2> java.lang.IllegalStateException: threadcontext is already closed\r\n  2> \tat __randomizedtesting.SeedInfo.seed([232DDB1DEF56B2F2]:0)\r\n  2> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextThreadLocal.ensureOpen(ThreadContext.java:421)\r\n  2> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextThreadLocal.get(ThreadContext.java:414)\r\n  2> \tat org.elasticsearch.common.util.concurrent.ThreadContext.isDefaultContext(ThreadContext.java:253)\r\n  2> \tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.afterExecute(EsThreadPoolExecutor.java:115)\r\n  2> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1150)\r\n  2> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n  2> \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n Don't fail in `afterExecute` if context is already closed >>> 1"
426," In #21348 the command executed to run the packaging tests has been changed to ""sudo -E bats ..."", forcing all environment variables from the vagrant user to be passed to the `sudo` command. This breaks a test on opensuse-13 (the one where it checks that elasticsearch cannot be started when `java` is not found) because all the PATH from the user is passed to the sudo command.\r\n    \r\nThis commit restores the previous behavior while allowing only necessary testing environment variables to be passed using a /etc/sudoers.d file. [Tests] Do not pass all env vars to sudo >>> 1"
427,"There have been reports that the query cache did not manage to speed up search\r\nrequests when the query includes a large number of different sub queries since\r\na single request may manage to exhaust the whole history (256 queries) while\r\nthe query cache only starts caching queries once they appear multiple times in\r\nthe history (#16031). On the other hand, increasing the size of the query cache\r\nis a bit controversial (#20116) so this pull request proposes a different\r\napproach that consists of never caching term queries, and not adding them to the\r\nhistory of queries either. The reasoning is that these queries should be fast\r\nanyway, regardless of caching, so taking them out of the equation should not\r\ncause any slow down. On the other hand, the fact that they are not added to the\r\ncache history anymore means that other queries have greater chances of being\r\ncached. Do not cache term queries. >>> 1"
428,All plugin extension points have been converted to pull based\r\ninterfaces. This change removes the infrastructure for the black-magic\r\nonModule methods. Plugins: Remove support for onModule >>> 1
429,"We log deprecation events at ""WARN"", so setting it to `info` means the events\r\nare still logged. It must be set to `error` in order to disable the logging. Fix incorrect instructions for disabling deprecation logging >>> 1"
430,"Relates to #21555, this is the deprecation logging for what was removed. Add deprecation logging for lenient boolean queries >>> 1"
431,Store throttling has been disabled by default since Lucene added automatic\r\nthrottling of merge operations based on the indexing rate. Remove store throttling. >>> 1
432,"Today when parsing a stats request, Elasticsearch silently ignores\r\nincorrect metrics. This commit removes lenient parsing of stats requests\r\nfor the nodes stats and indices stats APIs.\r\n\r\nThis pull request is a backport of #21417 to 5.x; notable changes in the\r\nbackport include f497c7da502234125e5e77dc6b43542f24dd9d93 which adds a\r\nnote to the migration docs and 322009f57c5bffef5d7b24fa4f8d8bc1076afe48\r\nwhich adds a backwards compatibility layer for percolate stats.\r\n\r\nRelates #20722, relates #21410, relates #21417 Remove lenient stats parsing 5.x >>> 1"
433,"Similar to https://github.com/elastic/elasticsearch/pull/21551, removing reference to setting dynamic mapper at the node level. Updated dynamic mapper section >>> 1"
434,"A previous commit added strict level parsing for the node stats API, but\r\nthat commit missed adding the same for the indices stats API. This\r\ncommit rectifies this miss.\r\n\r\nNote that a garbage level parameter produces an empty stats response,\r\nthere is nothing gained by allowing this.\r\n\r\nRelates #21024 Strict level parsing for indices stats >>> 1"
435,Fixes an issue where the cluster service does not remove an update task from its internal data structures that are used for batching cluster state updates.\r\n\r\nRelates #21568 Remove cluster update task when task times out >>> 1
436,nan Add proper toString() method to UpdateTask >>> 1
437,Added a warning note that clarifies bucket sizes diverging from the intended `interval` size when using a time zone that has DST changes.\r\n\r\nCloses #18805 Docs: Clarify date_histogram bucket sizes for DST time zones >>> 1
438,We kept `netty_3` as a fallback in the 5.x series but now that master\r\nis 6.0 we don't need this or in other words all issues coming up with\r\nnetty 4 will be blockers for 6.0. Remove `modules/transport_netty_3` in favor of `netty_4` >>> 1
439,"Our default pattern layout truncates log messages. This is to avoid\r\nblowing disk space from excessively log messages, which can happen if a\r\nmessage contains a mapping or an large query. Yet, we trunacte from the\r\nbeginning which is probably where the most germane information is. This\r\ncommit modifies the default pattern layout to trunacte from the end.\r\n\r\nCloses #21602 Truncate log messages from the end >>> 1"
440,"I'm not sure what the bug is, but ecj doesn't like this expression\r\nunless the type is set explicitly. Fix compilation in Eclipse >>> 1"
441,This commit exposes the executor service interface from thread\r\npool. This will enable some high-level concurrency primitives that will\r\nmake some code cleaner and simpler.\r\n\r\nCloses #19678\r\n Expose executor service interface from thread pool >>> 1
442,"Currently, when any underlying cluster has custom metadata\r\n(via plugin), tribe node does not store custom meta data in its\r\ncluster state. This is because the tribe node has no idea how to\r\nselect the appropriate custom metadata from one or many custom\r\nmetadata (corresponding to the number of underlying clusters).\r\n\r\nThis change adds an interface that custom metadata implementations\r\ncan extend to add support for merging mulitple custom metadata of\r\nthe same type for storing in the tribe state.\r\n\r\nRelates to #20544\r\nSupersedes #20791\r\nCloses #9372 Add support for merging custom meta data in tribe node >>> 1"
443,`lenient` option is documented for `match` query but not for `multi_match` query. Add documentation for lenient in multimatch >>> 1
444,"Groovy was deprecated in 5.0. This change removes it, along with the\r\nlegacy default language infrastructure in scripting. Remove groovy scripting language >>> 1"
445,This PR will go to master branch.\r\nOnce merged I will either:\r\n\r\n* Push the changes as is to 5.x branch\r\n* Also update to Tika 1.14 for mapper-attachment plugin which is still living in 5.x series but deprecated.\r\n\r\nI think I should do the later and update both plugins. What do you think?\r\n\r\nCloses #20390. Update to Tika 1.14 >>> 1
446,This change fixes the match_phrase_prefix on fields that define a boost in their mapping.\r\n\r\nFixes #21613 Fix match_phrase_prefix on boosted fields >>> 1
447,"This change integrates the Lucene highlighter called ""unified"" in the list of supported highlighters for ES.\r\nThis highlighter has multiple modes:\r\n  * plain: a mode that analyzes the plain text directly\r\n  * postings: a mode that uses the postings offsets to perform the highlight\r\n  * fvh: a mode that uses the term vectors to perform the highlighting\r\n\r\nSince this is a ""unified"" highlighter here is the complete list of highlighting features supported or not by this integration:\r\n\r\n- [x] works on fields with `term_vectors`, `offsets` or simply`stored`(or extracted from _source).\r\n- [x] `force_source` \r\n- [x] `pre_tags` and `post_tags`\r\nMultiple tags are not allowed\r\n- [x] `encoder` (html and default)\r\n- [x] `number_of_fragments`\r\n- [x] `no_match_size`\r\n- [x] `require_field_match`\r\nRequires http://issues.apache.org/jira/browse/LUCENE-7575. Note that LUCENE-7575 is more than just require_field_match since it can select which fields to highlight\r\n- [ ] `matched_fields` \r\n- [x] `fragment_size ` \r\nRequires http://issues.apache.org/jira/browse/LUCENE-7565\r\n- [ ] `boundary_chars`\r\n- [ ] `boundary_max_scan`\r\n- [x] Queries with rewrite that needs an index reader are ignored (MultiPhrasePrefixQuery, CommonTermsQuery, AllFieldQuery, ...).\r\n- [ ] Collapse contiguous highlights: `<b>a</b><b>b</b><b>c</b>` => `<b>abc</b>`\r\n\r\nFixes #21376 Integrate UnifiedHighlighter >>> 1"
448,Filters terms into arbitrary sized partitions so that multiple requests can be done without trying to compute everything in one request.\r\n\r\nFirst draft for review of approach - needs tests/docs etc\r\n\r\nCloses #21487 Support for partitioning set of terms  >>> 0
449,It will be remoned in 6.0. Add deprecation logging for users that explicitly opt in for the `default` fs type. >>> 1
450,It will be removed in 6.0.\r\n Add deprecation logging for the case that store throttling is used. >>> 1
451,This is a backport PR for #21494 \r\n\r\n Backport: Replace IndexAlreadyExistsException with ResourceAlreadyExistsException >>> 1
452,"It used to be a hybrid store between `niofs` and `mmapfs`, which we removed when\r\nwe switched to `fs` by default (which is `mmapfs` on 64-bits systems). Remove the `default` store type. >>> 1"
453,"The overflows were happening in two places, the parsing of the template that\r\nimplicitly truncates the `order` when its value does not fall into the `integer`\r\nrange, and the comparator that sorts templates in ascending order, since it\r\nreturns `order2-order1`, which might overflow.\r\n\r\nCloses #21622 Fix integer overflows when dealing with templates. >>> 1"
454,"Today we eagerly resolve unicast hosts. This means that if DNS changes,\r\nwe will never find the host at the new address. Moreover, a single host\r\nfailng to resolve causes startup to abort. This commit introduces lazy\r\nresolution of unicast hosts. If a DNS entry changes, there is an\r\nopportunity for the host to be discovered. Note that under the Java\r\nsecurity manager, there is a default positive cache of infinity for\r\nresolved hosts; this means that if a user does want to operate in an\r\nenvironment where DNS can change, they must adjust\r\nnetworkaddress.cache.ttl in their security policy. And if a host fails\r\nto resolve, we warn log the hostname but continue pinging other\r\nconfigured hosts.\r\n\r\nWhen doing DNS resolutions for unicast hostnames, we wait until the DNS\r\nlookups timeout. This appears to be forty-five seconds on modern JVMs,\r\nand it is not configurable. If we do these serially, the cluster can be\r\nblocked during ping for a lengthy period of time. This commit introduces\r\ndoing the DNS lookups in parallel on the generic thread pool, and adds a\r\nuser-configurable timeout for these lookups.\r\n\r\nCloses #14441, closes #16412\r\n Lazy resolve unicast hosts >>> 1"
455,As the title says.  See the tests for examples.\r\n\r\nRelates (#21479)\r\n\r\nAs a bonus this also fixes a bug related to certain reserved variable names not working in a lambda.\r\n\r\nRelates (#20869)\r\n\r\n@s1monw I think I'm under the 1.5 hours you said I had :) Fix Lambdas in Painless to be Able to Use Top-Level Variables Such as params and doc >>> 1
456,Build was failing on Windows because the only test had an assumeFalse Fix Windows test failure >>> 1
457,The highlighter converts stored keyword fields using toString().\r\nSince the keyword fields are stored as utf8 bytes the conversion is broken.\r\nThis change uses BytesRef.utf8toString() to convert the field value in a valid string.\r\n\r\nFixes #21636 Fix highlighting on a stored keyword field >>> 1
458,nan Remove transport-netty3-client >>> 1
459,\r\n Add a recommendation against large documents to the docs. >>> 1
460,This commit removes a stale mention of the minimum master nodes boostrap\r\ncheck from the docs.\r\n\r\nRelates #20082\r\n Remove stale mention of minimum master nodes check >>> 1
461,The bucket script aggregation uses `InternalDateRange.createBucket` to clone the buckets and add its results. Previous to this change it was using `InternalDateRange.Bucket.getFrom()` and `InternalDateRange.Bucket.getTo()` to get the values of `from` and `to` from the existing (prototype) bucket. The problem is that these methods do not return the raw `from` and `to` values and instead return either `null` or a `DateTime` object. This caused `NullPointerException` to be thrown if either from or to was unbounded.\r\n\r\nThis change allows the `createBucket` method to access the raw double values for `from` and `to` so that the bucket can be created with the exact values from the prototype.\r\n\r\nCloses #21579 Fixes InternalDateRange.createBucket when from/to is null >>> 1
462,"This commit moves several allocation decider related inner classes\r\ninto their own top-level class, in order to use more easily in\r\nthe allocation explain API.  This commit also renames some of those\r\ndecision related classes to more suitable names.\r\n\r\nThis is simply a cosmetic change - no functionality changes with this\r\ncommit whatsoever.\r\n\r\nTo summarize the changes:\r\n1. `ShardAllocationDecision` renamed to `AllocateUnassignedDecision`\r\n2. `RelocationDecision` moved to a top-level class\r\n3. `MoveDecision` moved to a top-level class\r\n4. `RebalanceDecision` moved to a top-level class\r\n5. `ShardAllocationDecisionTests` renamed to `AllocateUnassignedDecisionTests`\r\n6. `NodeRebalanceResult` moved to a top-level class\r\n7. `ShardAllocationDecision#WeightedDecision` moved to a top-level class and renamed to `NodeAllocationResult`. Makes allocator decision classes top-level classes >>> 1"
463,AbstractScopedSettings has the ability to only apply updates/deletes\r\nto dynamic settings. The flag is currently not respected when a setting\r\nis reset/deleted which causes static node settings to be reset if a non-dynamic\r\nkey is reset via `null` value.\r\n\r\nCloses #21593 Don't reset non-dynamic settings unless explicitly requested >>> 1
464,This PR back ports #21591 (Update to Tika 1.14) in 5.x branch for (5.1) and also applies the same update to the mapper attachments plugin.\r\n\r\nNote that it also applies #20710 (update some Tika deps) in 5.x branch to the mapper attachments plugin. Update to Tika 1.14 >>> 1
465,"By default, it is recommended to start bulk with a size of 10-15MB, and increase it gradually to get the right size for the environment. The example shows originally 1GB, which can lead to some users to just copy-paste the code snippet and start with excessively big sizes. Update BulkProcessor size in the example >>> 1"
466,"PR #19416 added a safety mechanism to shard state fetching to only access the store when the shard lock can be acquired. This can lead to the following situation however where a shard has not fully shut down yet while the shard fetching is going on, resulting in a `ShardLockObtainFailedException`. `PrimaryShardAllocator` that decides where to allocate primary shards sees this exception and treats the shard as unusable. If this is the only shard copy in the cluster, the cluster stays red and a new shard fetching cycle will not be triggered as shard state fetching treats exceptions while opening the store as permanent failures.\r\n\r\nThis PR makes it so that `PrimaryShardAllocator` treats the locked shard as a possible allocation target (although with the least priority). Allow master to assign primary shard to node that has shard store locked during shard state fetching >>> 1"
467,Plugins are closed if they implement java.io.Closeable but this is not\r\nclear from the plugin interface. This commit clarifies this by declaring\r\nthat Plugins implement java.io.Closeable and adding an empty\r\nimplementation to the base Plugin class. Clarify that plugins can be closed >>> 1
468,"This shows an example of how to install a plugin on Windows, which is not as obvious at I would have expected. [DOCS] Plugin Installation for Windows >>> 1"
469,This PR splits the main method in `ClusterService` into smaller chunks so that it's easier to understand and simpler to modify in subsequent PRs. Split main ClusterService method into smaller chunks >>> 1
470,"If the node name is explicitly set it's not derived from the node ID\r\nmeaning that it doesn't immediately appear in the logs. While it can be\r\ntracked down in other places, it would be easier for info purposes if it\r\njust showed up explicitly. This commit adds the node ID to the logs,\r\nwhether or not the node name is set. Log node ID on startup >>> 1"
471,"Removes the need to pass through `null` for sequences like\r\n`a?.foo() ?: 0`. This both eliminates a null check and makes\r\nit possible to support primitive return types for null safe\r\ndereferences as long as they are followed by an elvis operator.\r\n\r\nI don't imagine we'll see a large performance bump from this but\r\nthe emitted bytecode is shorter and supporting primitives is a\r\nplus, however rare they are. Optimize null safe deref into elvis >>> 0"
472,Today we call `writeByte` up to 3x per character in each string written via\r\n`StreamOutput#writeString` this can have quite some overhead when strings\r\nare long or many strings are written. This change adds a local buffer to\r\nconvert chars to bytes into the local buffer. Converted bytes are then\r\nwritten via `writeBytes` instead reducing the overhead of this operation.\r\n\r\nCloses #21660 Use a buffer to do character to byte conversion in StreamOutput#writeString >>> 1
473,nan Set execute permissions for native plugin programs >>> 1
474,"`ClusterStateObserver`  is a utility class which simplifies interacting with the cluster state in cases where an action takes a decision based on the current cluster state but may want to wait for a new state and retry upon failure. The `ClusterStateObserver` implements its functionality by keeping a reference to the last cluster state that it observed. When a new `ClusterStateObserver` is created, it samples a cluster state from the cluster service which is subsequently used for change detection. If actions take a long time to process, however, the cluster observer can reference very old cluster states. Due to cluster observers being created very frequently and cluster states being potentially large the referenced cluster states can waste a lot of heap space. A specific example where this can make a node go out of memory is given in point 2 of #21568:\r\n> the action listener in TransportMasterNodeAction.AsyncSingleAction has a ClusterStateObserver to coordinate the retry mechanism if the action on the master node fails due to the node not being master anymore. The ClusterStateObserver in AsyncSingleAction keeps a reference to the full cluster state when the action was initiated. If the pending tasks queue grows quite large and has older items in it lots of cluster states can possibly be referenced.\r\n\r\nThis PR builds on the observation that the cluster state referenced by the `ClusterStateObserver` is not really needed. All that's needed is some form of identity to distinguish between two states. Let ClusterStateObserver only hold onto state that's needed for change detection >>> 1"
475,"- Remove minNodeVersion and corresponding public `getSmallestVersion` getter method from DiscoveryNodes\r\n- Remove unused DiscoveryNode#removeDeadMembers public method\r\n- Remove unused DiscoveryNodes.Delta constructor\r\n- Adjust visibility of DiscoveryNodes.Delta constructor: it can be private as it gets called by DiscoveryNodes#delta method, which is supposed to be the only way to create a Delta Remove unused bits from DiscoveryNodes >>> 1"
476,"The `lookupPrototype` method is not used anywhere. Seems like we rather use its `lookupProrotypeSafe` variant (which also throws exception if the prototype is not found). This commit makes the safer variant the default one, by renaming it to  `lookupPrototype` and removes the previous ""unsafe"" variant. Rename ClusterState#lookupPrototypeSafe to `lookupPrototype` and remove ""unsafe"" unused variant >>> 1"
477,"`TransportSearchAction` optimizes the `search_type` in certain cases, when for instance we are searching against a single shard, or when there is only a suggest section in the request. That optimization is wrapped in a try catch, and when an exception happens we log it and ignore it. This may be a leftover from the past though, as no exception is expected to be thrown in that code block, hence if there is any exception we are probably better off bubbling it up rather than ignoring it. remove pointless catch exception in TransportSearchAction >>> 1"
478,"The `type` parameter has always been accepted by the search_shards api, probably to make the api and its urls the same as search. Truth is that the type never had any effect, it's been ignored from day one while accepting it may make users think that we actually do something with it.\r\n\r\nThis commit removes support for the type parameter from the REST layer and the Java API. Backwards compatibility is maintained on the transport layer though.\r\n\r\nThe new added serialization test also uncovered a bug in the java API where the `ClusterSearchShardsRequest` could be created with no arguments, but the indices were required to be not null otherwise the request couldn't be serialized as `writeTo` would throw NPE. Fixed by setting a default value (empty array) for indices.\r\n\r\nNote that we could backport this to 5.x and make it non breaking on the REST layer, by maintaining the endpoint that accepts the type but log a deprecation warning for it. Then the version checks on the transport layer would need to be updated in this PR too. Remove ignored type parameter in search_shards api >>> 1"
479,Today it's not possible to add exceptions to the serialization layer\r\nwithout breaking BWC. This commit adds the ability to specify the Version\r\nan exception was added that allows to fall back not NotSerializableExceptionWrapper\r\nif the exception is not present in the streams version.\r\n\r\nRelates to #21656 Add BWC layer for Exceptions >>> 1
480,Today we read a vint from the stream to allocate the size of an array up-front\r\nbefore we start reading the values. This can be dangerous if for instance we read\r\nfrom a corrupted stream or if some manipulated bytes are send for instance from\r\nan attacker or a fuzzer. In most of the cases we can apply some best effort and\r\nvalidate the array size to be _sane_ by ensuring we can at read at least N bytes\r\nwhere N is the expected size of the array. Add a StreamInput#readArraySize method that ensures sane array sizes >>> 1
481,Closes https://github.com/elastic/elasticsearch/issues/21490 Add docs for the batch mode of plugin installation >>> 1
482,try to refacotry code according to this: https://github.com/elastic/elasticsearch/issues/21641#issuecomment-261881443\r\n refactory ShardsLimitAllocationDecider.java.  >>> 0
483,"This is a followup to #21689 where we removed a misplaced try catch for IndexMissingException and IndexClosedException which was related to #9047 (at least for the index closed case). The code block within the change was moved as part of #20890, which made the catch redundant. It was somehow used before (e.g. in 5.0) but it doesn't seem that this catch had any effect. Added tests to verify that. In fact a specific catch added to the search api only would defeat the purpose of having common indices options that work throughout all our APIs.\r\n\r\nRelates to #21689 Add indices options tests to search api REST tests >>> 1"
484,"This commit enhances the allocator decision result objects (namely,\r\n`AllocateUnassignedDecision`, `MoveDecision`, and `RebalanceDecision`)\r\nto enable them to be used directly by the cluster allocation explain API.  In\r\nparticular, this commit does the following:\r\n\r\n 1. Adds serialization and `toXContent` methods to the response objects,\r\n    which will form the explain API responses.\r\n 2. Moves the calculation of the final explanation to the response\r\n    object itself, removing it from the responsibility of the\r\n    allocators.\r\n 3. Adds shard store information to the `NodeAllocationResult`, so that\r\n    store information is available for each node, when explaining a\r\n    shard allocation by the `PrimaryShardAllocator` or the\r\n    `ReplicaShardAllocator`.\r\n 4. Removes `RebalanceDecision` in favor of using `MoveDecision` for both\r\n     moving and rebalancing shards.\r\n 5. Removes `NodeRebalanceResult` in favor of using `NodeAllocationResult`.\r\n 6. Changes the notion of weight ranking to be relative to the current node, \r\n     instead of an absolute weight that doesn't convey any added value to the \r\n     API user and can be confusing.\r\n 7. Introduces a new enum `AllocationDecision` to convey the decision type, \r\n     which enables conveying unassigned, moving, and rebalancing scenarios \r\n     with more detail as opposed to just `Decision.Type` and `AllocationStatus`. Prepares allocator decision objects for use with the allocation explain API >>> 1"
485,"As part of #20925 and #21341 we added an ""all-fields"" mode to the\r\n`query_string` and `simple_query_string`. This would expand the query to\r\nall fields and automatically set `lenient` to true.\r\n\r\nHowever, we should still allow a user to override the `lenient` flag to\r\nwhichever value they desire, should they add it in the request. This\r\ncommit does that. Allow overriding all-field leniency when `lenient` option is specified >>> 1"
486,Today there is no way to get notified if a node is disconnected. Client code\r\nmust poll the TranportClient constantly to detect that a node is not connected\r\nanymore in order to react and add new nodes or notify altering etc. For instance\r\nif a hostname  gets resolved to an IP but that host is disconnected clients want\r\nto reconnect by resolving the hostname again which is a common situation in cloud\r\nenvironments.\r\n\r\nCloses #21424 Add a HostFailureListener to notify client code if a node got disconnected >>> 1
487,I kept getting lost when debugging Painless's code generation because the AST nodes all had java's default `toString` implementation. So I implemented `toString` on all of them. I implemented it such that node's `toString` is valid painless that'd generate the node in the first place because I thought that'd be the easiest thing to read while debugging. Implement toString on painless's node classes >>> 1
488,"When Elasticsearch starts, we go through some initialization before we\r\ninstall a security manager. Yet, the JVM makes internal policy decisions\r\non the basis of whether or not a security manager is present. This\r\ncommit installs a security manager immediately on startup so that the\r\nJVM always thinks a security manager is present when making such policy\r\ndecisions. Install a security manager on startup >>> 1"
489,"When a fatal error is thrown on the network layer, such an error never\r\nmakes its way to the uncaught exception handler. This prevents the node\r\nfrom being torn down if an out of memory error or other fatal error is\r\nthrown while handling HTTP or transport traffic. This commit adds logic\r\nto ensure that such errors bubble their way up to the uncaught exception\r\nhandler, even though Netty tries really hard to swallow everything.\r\n\r\nRelates #19272\r\n Die with dignity on the network layer >>> 1"
490,"When a fatal error tragically closes an index writer, such an error\r\nnever makes its way to the uncaught exception handler. This prevents the\r\nnode from being torn down if an out of memory error or other fatal error\r\nis thrown in the Lucene layer. This commit ensures that such events\r\nbubble their way up to the uncaught exception handler.\r\n\r\nRelates #19272 Die with dignity on the Lucene layer >>> 1"
491,They were deprecated in 5.0. We are concentrating on making\nPainless awesome rather than supporting every language possible.\n\nCloses #20698\n Remove lang-python and lang-javascript >>> 1
492,You can use `Debug.explain(someObject)` in painless to throw an\r\n`Error` that can't be caught by painless code and contains an\r\nobject's class. This is useful because painless's sandbox doesn't\r\nallow you to call `someObject.getClass()`.\r\n\r\nCloses #20263 Add Debug.explain to painless >>> 1
493,This commit adds the ability to support running with plugins in tests that make use of\r\nbackwards compatibility nodes. This can be used to test rolling upgrades with plugins\r\nto ensure they do not cause issues during a rolling upgrade of elasticsearch. Build: add the ability to support plugins in BWC tests >>> 1
494,This commit refactors the handling of bad default permissions that come\r\nfrom the system security policy.\r\n\r\nRelates #14704 Refactor handling for bad default permissions >>> 1
495,"The search shards api returns info about which shards are going to be hit by executing a search with provided parameters: indices, routing, preference. Indices can also be aliases, which can also hold filters. The output includes an array of shards and a summary of all the nodes the shards are allocated on. This commit adds a new indices section to the search shards output that includes one entry per index, where each index can be associated with an optional filter in case the index was hit through a filtered alias.\r\n\r\nThis is relevant since we have moved parsing of alias filters to the coordinating node.\r\n\r\nRelates to #20916 Add indices and filter information to search shards api output >>> 1"
496,"The `type` parameter has always been accepted by the search_shards api, probably to make the api and its urls the same as search. Truth is that the type never had any effect, it's been ignored from day one while accepting it may make users think that we actually do something with it.\r\n\r\nThis commit deprecate support for the type parameter from the REST layer and removes it from the Java API. Backwards compatibility is maintained on the transport layer though.\r\n\r\nThe new added serialization test also uncovered a bug in the java API where the `ClusterSearchShardsRequest` could be created with no arguments, but the indices were required to be not null otherwise the request couldn't be serialized as `writeTo` would throw NPE. Fixed by setting a default value (empty array) for indices.\r\n\r\nRelates to #21688 Deprecate ignored type parameter in search_shards api >>> 1"
497,"This commit clarifies the contract of Cache#computeIfAbsent so that an exception that occurs during the execution of the loader is thrown to all callers. Prior to this commit, the first caller would get the ExecutionException and other callers that called during the load execution would get null, which is confusing. Rethrow ExecutionException from the loader to concurrent callers of Cache#computeIfAbsent >>> 1"
498,Since we added ability to cancel searches it would be nice to see which searches we are actually cancelling. Add search task descriptions >>> 1
499,"Changes the default socket and connection timeouts for the rest\r\nclient from 10 seconds to the more generous 30 seconds.\r\n\r\nDefaults reindex-from-remote to those timeouts and make the\r\ntimeouts configurable like so:\r\n```\r\nPOST _reindex\r\n{\r\n  ""source"": {\r\n    ""remote"": {\r\n      ""host"": ""http://otherhost:9200"",\r\n      ""socket_timeout"": ""1m"",\r\n      ""connect_timeout"": ""10s""\r\n    },\r\n    ""index"": ""source"",\r\n    ""query"": {\r\n      ""match"": {\r\n        ""test"": ""data""\r\n      }\r\n    }\r\n  },\r\n  ""dest"": {\r\n    ""index"": ""dest""\r\n  }\r\n}\r\n```\r\n\r\nCloses #21707 Timeout improvements for rest client and reindex >>> 1"
500,nan Add Gradle coordinates for the REST client >>> 1
501,"Resolves #14469\r\n\r\nIndexed docs with binary doc value type as mentioned in the issue: https://gist.github.com/umeshdangat/43b10532dc52c5d92c1a2d7aaf8c0d8c\r\n\r\nPainless script to access binary type as script values:\r\nhttps://gist.github.com/umeshdangat/d236b8f70ddfda739df8b01c672413b5\r\n\r\nResult: No more unsupported operations exception: https://gist.github.com/umeshdangat/c137c5d426bc23c5ff02b087d3a3684c\r\n\r\nJava Plugin which reads the binary field via ScriptDocValues\r\nhttps://gist.github.com/umeshdangat/882caf25fdfd6090855a1252f210067c\r\n\r\nI decided to add `BytesRefs` to ScriptDocValues because it seems more appropriate to return a `BytesRef` and then use it to extract byte[] like so:\r\n_byte[] arr = ((ScriptDocValues.BytesRefs) doc().get(""qa_data"")).get(0).bytes;_\r\n Support binary field type in script values >>> 1"
502,"This commit refactors the handling of bind permissions, which is in need\r\nof a little cleanup. For example, in its current state, the code for\r\nhandling permissions for transport profiles is split across two\r\nmethods. This commit refactors this code hopefully making it easier to\r\nwork with in future changes. This change is mostly mechanical, no\r\nfunctionality is changed.\r\n Refactor handling of bind permissions >>> 1"
503,"The removeTransportAddress method of TransportClient removes the address\r\nfrom the list of nodes that the client pings to sniff for nodes.\r\nHowever, it does not remove it from the list of existing connected\r\nnodes. This means removing a node is not possible, as long as that node\r\nis still up.\r\n\r\nThis change removes the node from the connected nodes list before\r\ntriggering sampling (ie sniffing).  While the fix is simple, testing was\r\nnot because there were no existing tests for sniffing. This change also\r\nmodifies the mocks used by transport client unit tests in order to allow\r\nmocking sniffing. Transport client: Fix remove address to actually work >>> 1"
504,"ShardSearchRequest was previously taking in the whole ShardRouting as a constructor argument while it only needs the ShardsId, changed that to carry over only the needed bits. ShardSearchRequest to take ShardId constructor argument rather than the whole ShardRouting >>> 1"
505,"This commit bumps the version to 5.2.0, and adds unreleased version\r\nconstants for version 5.1.0. Bump version to 5.2.0 >>> 1"
506,The packaging tests must be adapted since 'lang-groovy' has been removed from master.\r\n\r\n [TEST] Update packaging tests after lang-groovy removal >>> 1
507,"Sets the size of the current document as it will be rendered as a JSON document.\r\n\r\nExample for size processor:\r\n\r\n```js\r\n{\r\n  ""size"": { \r\n    ""target"": ""my_object.my_size"",\r\n  }\r\n}\r\n```\r\n\r\nFirst step for #21520.\r\n\r\nThe goal is to mark as deprecated `mapper-size` plugin in 5.1 and remove it for 6.0 (master branch) which will come as part as another PR. Add size ingest processor >>> 0"
508,NOTE: The result of `?.` and `?:` can't be assigned to primitives. So\r\n`int[] someArray = null; int l = someArray?.length` and\r\n`int s = params.size ?: 100` don't work. Do\r\n`def someArray = null; def l = someArray?.length` and\r\n`def s = params.size ?: 100` instead.\r\n\r\nRelates to #21748 Docs and tests for painless lack of boxing for ?: and ?. >>> 1
509,"Today when handling unreleased versions for backwards compatilibity\r\nsupport, we scatted version constants across the code base and add some\r\nasserts to support removing these constants when the version in question\r\nis actually released. This commit improves this situation, enabling us\r\nto just add a single unreleased version constant that can be renamed\r\nwhen the version is actually released. This should make maintenance of\r\nthese versions simpler. Improve handling of unreleased versions >>> 1"
510,"The index uuid is unique across multiple clusters, while the index name is not. Using the index uuid to look up filters in the alias filters map is better and will be needed for multi cluster search. Use index uuid as key in the alias filter map rather than the index name >>> 1"
511,"- ClusterSearchShardsGroup to return ShardId rather than the int shard id and index separately. ShardId also holds the Index object which holds the index uuid that is useful to retrieve in some cases\r\n- Make ClusterSearchShardsResponse empty constructor public, otherwise the response cannot be created and read from the stream input unless done from the same package\r\n- adjust visibility of ClusterSearchShards members: some could be private or package private rather than public\r\n Cluster search shards improvements: expose ShardId, adjust visibility of some members >>> 1"
512,"This PR replaces the `ShardRouting` argument in `AbstractSearchAsyncAction#onFirstPhaseResult` with the more contained `String nodeId` argument, as that is the only info retrieved from it. The `ShardId` can also be read directly from the `ShardIterator` that's already provided as an argument, plus there is no need to create new instances of `ShardId` by providing the index and the int shard_id separately, the whole `ShardId` should be passed around when possible.\r\n\r\nThe `SearchShardTarget` constructor that takes shard_id and index separately stays only for testing purposes. Don't carry ShardRouting around when not needed in AbstractSearchAsyncAction >>> 1"
513,"This commit makes sure that there is only one instance of the two services rather than one per transport action that uses it.\r\n\r\nAlso, we bind them to a specific instance in guice so those instances will be provided to transport actions that need them. Otherwise those two objects would get created within a constructor that is called by guice. That may cause problems for instance when throwing an exception from such constructors as guice tries all over again to re-initialize objects and fills up logs with stacktraces. Move SearchTransportService and SearchPhaseController creation outside of TransportSearchAction constructor >>> 1"
514,There is a bug in Painless that causes a VerifyError when scripts using the === operator were comparing a def type against a primitive type since the primitive type wasn't being appropriately boxed. Fix a VerifyError bug in Painless >>> 1
515,"When a file script fails to compile, rather than logging the\r\nexception that caused the failure this logs the xcontent of\r\nthat exception. This is both shorter and has the script stack\r\nwhich is useful for figuring out why the compilation failed.\r\n\r\nStill logs the entire stacktrace at debug level just in case\r\nyou need it.\r\n\r\nRelates to #21733 Log ScriptException's xcontent if file script compilation fails >>> 1"
516,"If a bug occurs in painless compilation (not from a user, but from the\r\npainless infrastructure), a VerifyError may be thrown when compiling the\r\nbroken generated class. This commit wraps VerifyErrors in\r\nScriptException so that useful information is returned to the user,\r\nwhich can be passed on to the ES team for analysis. Wrap VerifyError in ScriptException >>> 1"
517,"This commit improves the decision explanation messages,\r\nparticularly for NO decisions, in the various AllocationDecider\r\nimplementations by including the setting(s) in the explanation\r\nmessage that led to the decision.\r\n\r\nThis commit also returns a THROTTLE decision instead of a NO\r\ndecision when the concurrent rebalances limit has been reached\r\nin `ConcurrentRebalanceAllocationDecider` and when a snapshot is\r\nin progress in `SnapshotInProgressAllocationDecider`, because it \r\nmore accurately reflects a temporary throttling that will turn into a \r\nYES decision once the number of concurrent rebalances lessens or \r\nonce snapshotting completes, as opposed to a more permanent NO \r\ndecision (e.g. due to filtering). Improves allocation decider decision explanation messages >>> 1"
518,This change fixes the cross_fields type of the multi_match query when synonyms are involved.\r\nSince 2.x the Lucene query parser creates SynonymQuery for words that appear at the same position.\r\nFor simple term query the CrossFieldsQueryBuilder expands the term to all requested fields and creates a BlendedTermQuery.\r\nThis change adds the same mechanism for SynonymQuery which otherwise are not expanded to all requested fields.\r\nAs a side note I wonder if we should not replace the BlendedTermQuery with the SynonymQuery. They have the same purpose and behave similarly.\r\n\r\nFixes #21633 Fix cross_fields type on multi_match query with synonyms >>> 1
519,"It seems `sort_mode` is deprecated, hence the change in the parameters' explanation. Related question [here](http://stackoverflow.com/questions/40785477/elasticsearch-geo-distance-sorting) clarification on geo distance sorting >>> 1"
520,"also added ""whereas"" to show the contrast a bit clearer. typo fix (it self -> itself) >>> 1"
521,"Currently, the `terms` query is just syntactic sugar for a `bool` query when\r\nused in a query context. This change proposes to always generate the same query\r\nin query and filter contexts, which is less confusing. The `terms` query should always map to a Lucene `TermsQuery`. >>> 1"
522,"With #21738 we added an indices section to the search shards api, that will return the concrete indices hit by the request, and eventually the corresponding alias filter.\r\n\r\nThe java API returns the `AliasFilter` object, which holds the filter itself and an array of aliases that pointed to the index in the original request. The REST layer doesn't print out the aliases array though. This commit adds the aliases array as well and tests for this. Search shards to print out aliases array together with alias filter >>> 1"
523,"Group and Affix settings generate a bogus diff that turns the actual\r\ndiff into a string containing a json structure for instance:\r\n\r\n```\r\n""action"" : {\r\n  ""search"" : {\r\n    ""remote"" : {\r\n      """" : ""{\""my_remote_cluster\"":\""[::1]:60378\""}""\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwhich make reading the setting impossible. This happens for instance\r\nif a group or affix setting is rendered via `_cluster/settings?include_defaults=true`\r\nThis change fixes the issue as well as several minor issues with affix settings that\r\nwhere not accepted as valid setting today. Fix settings diff generation for affix and group settings >>> 1"
524,Today if a comma-separated list is passed to `action.auto_create_index`\r\nleading and trailing whitespaces are not trimmed but since the values are\r\nindex expressions whitespaces should be removed for convenience.\r\n\r\nCloses #21449 Handle spaces in `action.auto_create_index` gracefully >>> 1
525,"The disruption type `LongGCDisruption` simulates GCs on a node by suspending all the threads of that node. If the suspended threads are in a code section with shared JVM locks, however, it can prevent the other nodes from doing their thing. The class `LongGCDisruption` has a list of class names for which we know that this can occur. Whenever a test using the GC disruption type fails in mysterious ways, it becomes a long guessing game to find the offending class. This PR adds code to\r\n`LongGCDisruption` to automatically detect these situations, fail the test early and report the offending class and all relevant context. Detect nodes being blocked by GC-disrupted node >>> 1"
526,"REST tests use the default OOTB low/high disk watermarks of 85%/90%, which can make some tests fail if run on a machine with a fuller disk. This PR changes the watermarks in the same way as in IntegTestCase so that they're essentially ignored. Disable disk watermarks on REST tests >>> 1"
527,"The Transport#connectToNodeLight concepts is confusing and not very flexible.\r\nneither really testable on a unittest level. This commit cleans up the code used\r\nto connect to nodes and simplifies transport implementations to share more code.\r\nThis also allows to connect to nodes with custom profiles if needed, for instance\r\nfuture improvements can be added to connect to/from nodes that are non-data nodes without\r\ndedicated bulks and recovery connections. Remove connectToNodeLight and replace it with a connection profile >>> 1"
528,Closes #6468 Log failure to connect to node at info instead of debug >>> 1
529,\r\nCloses #8427 Document using round-robin DNS for discovery >>> 1
530,This PR is trying to fix: https://github.com/elastic/elasticsearch/issues/21739 by Option 1.\r\n\r\n@bleskes Could you review this? avoid repeat connections in pings every round >>> 0
531,"In the past we ran yaml tests against an internal cluster, which would get restarted after each test failure, hence the client objects needed to eventually be refreshed before each test. That is why we had the initClient method to re-initialize the YamlTestClient in the execution context. We ended up though re-initializing the client unconditionally, which is not needed.\r\n\r\nAlso, `ESRestTestCase` recreates the `RestClient` against the external cluster before each test, which is not needed given that nothing changes in the external cluster.\r\n\r\nThis commit removes the `initClient` method from the yaml tests execution context. The YamlTestClient can be eagerly created before the first yaml test runs and then re-used in subsequent tests. Also api calls to check for nodes versions etc. are moved out of YamlTestClient to `ESClientYamlSuiteTestCase`. Also the RestClient is now initialized in `ESRestTestCase` before the first test runs, and kept around afterwards as a static member. \r\n\r\nBasically each subclass of `EsRestTestCase` will have its own `RestClient` instance, but the client will be shared across the different tests within the same class. The yaml test suite is just a special suite, composed of 600+ tests that are loaded from files, which will share the same client instance.\r\n\r\nThis change should speed tests up as well, as we don't recreate the `RestClient` before each single test, and we don't call `_cat/nodes` either before each single test. [TEST] Don't reinitialize YamlTestClient and RestClient before each single test >>> 1"
532,Removes an unused static variable and an unused instance variable. Minor clean-ups in MockBigArrays >>> 1
533,"Some tests on the 5.x branch have used only the `version` declaration for a skip,\r\nnot the `features` version, causing runners which have not yet\r\nimplemented the feature to fail.\r\n\r\nRelated: f4f62a1 Add missing skip declarations for the ""warnings"" feature into the YAML tests >>> 1"
534,"In making changes for the 5.0 version of snapshots, a bug was\r\nintroduced where if an index-N file could not be found for an\r\nindividual shard, the backup was to iterate over all `snap-*.dat`\r\nfiles in the shard folder to know which snapshots contain that\r\nshard's data, but in 5.0, reading the `snap-*.dat` files as backup\r\nwas incorrectly passing in the blob name for the `snap-*.dat` file,\r\nthereby failing to load all index files for a given snapshot\r\nwhen the index-N file is missing.  This condition should be rare\r\nas there is no reason an index-N file should be absent (unless\r\nit was deleted or there was corruption reading the file), but\r\nnevertheless, this situation can be encountered and this commit\r\nfixes the bug by reading the correct `snap-*.dat` blob name in the\r\nshard data folder. Fixes shard level snapshot metadata loading when index-N file is missing >>> 1"
535,"This change adds non-searchable fields to the FieldStats response. These fields do not have min/max informations but they can be aggregatable. Fields that are only stored in _source (store:no, index:no, doc_values:no) will still be missing since they do not have any useful information to show. Indices and clients must be at least on V_5_2_0 to see this change.\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/21952 Include unindexed field in FieldStats response >>> 1"
536,This is a cleanup of the fix pushed in https://github.com/elastic/elasticsearch/pull/20400.\r\nFiltersFunctionScoreQuery sub query should be extracted in CustomQueryScorer.extract (and not in CustomQueryScorer.extractUnknownQuery).\r\nThis does not fix any bug in this branch (it's just a cleanup) but the intent is first to clean up and then to backport in 2.x where there is a real bug.\r\nThe bug is in 2.x only because the backport of https://github.com/elastic/elasticsearch/pull/20400 in 2.x mistakenly renamed the FiltersFunctionScoreQuery to FunctionScoreQuery.\r\nThis leads to incorrect highlighting on FiltersFunctionScoreQuery in 2.x. Fix FiltersFunctionScoreQuery highlighting >>> 1
537,"In some cases, such as the creation of DiscoveryNode instances for unicast ping requests, the host information was not being populated properly and instead the address string was being used. Additionally, when serializing a DiscoveryNode and in turn a transport address, the host was not being set on the InetAddress when deserializing the object, so even if the address was created from a hostname, the address in the deserialized instance had no knowledge of the hostname that was originally used. DiscoveryNode and TransportAddress should preserve host information >>> 1"
538,\r\n\r\n [DOCS] add source filtering example to reindex docs >>> 1
539,"The gradle build currenlty allows extra plugins to be hooked into the\r\nelasticsearch build by placing under an x-plugins directory as a sibling\r\nof the elasticsearch checkout. This change converts this directory to\r\none called elasticsearch-extra. The subdirectories of\r\nelasticsearch-extra become first level projects in gradle (while before\r\nthey would have been subprojects of `:x-plugins`. Additionally, this\r\nallows major version checkouts to be associated with extra plugins. For\r\nexample, you could have elasticsearch checked out as\r\n`elasticsearch-5.x`, and a sibling `elasticsearch-5.x-extra` directory. Build: Remove hardcoded reference to x-plugins in build >>> 1"
540,"When you send a wrong type for a sub field of an attachments field, it raises a `NullPointerException`.\r\n\r\n```\r\nDELETE test\r\nPUT test/_mapping\r\n{\r\n    ""person"": {\r\n        ""properties"": {\r\n            ""file"": {\r\n                ""type"": ""attachment"",\r\n                ""fields"": {\r\n                    ""content"": {\r\n                        ""type"": ""nonexistingtype""\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis fix now sends back to the user a comprehensive error message:\r\n\r\n```json\r\n{\r\n   ""error"":{\r\n      ""root_cause"":[\r\n         {\r\n            ""type"":""mapper_parsing_exception"",\r\n            ""reason"":""Type [nonexistingtype] is not supported. Check your [content] field.""\r\n         }\r\n      ],\r\n      ""type"":""mapper_parsing_exception"",\r\n      ""reason"":""Failed to parse mapping [doc]: Type [nonexistingtype] is not supported. Check your [content] field."",\r\n      ""caused_by"":{\r\n         ""type"":""mapper_parsing_exception"",\r\n         ""reason"":""Type [nonexistingtype] is not supported. Check your [content] field.""\r\n      }\r\n   },\r\n   ""status"":400\r\n}\r\n``` NPE is raised when defining a non existing type within attachments type >>> 1"
541,"Today we can easily join a cluster that holds an index we don't support since\r\nwe currently allow rolling upgrades from 5.x to 6.x.  Along the same lines we don't check if we can support an index based on the nodes in the cluster when we `open`, `restore` or `metadata-upgrade` and index. This commit adds\r\nadditional safety that fails cluster state validation, open, restore and /or upgrade if there is an open index with an incompatible index version created in the cluster.\r\n\r\nRealtes to #21670 Add validation for supported index version on node join, restore, upgrade & open index >>> 1"
542,"This change allows specifying alias/wildcard expression in indices_boost.\r\nAnd added another format for specifying indices_boost. It accepts array of index name and boost pair.\r\nIf an index is included in multiple aliases/wildcard expressions, the first match will be used.\r\n\r\nCloses #4756 Resolve index names in indices_boost >>> 1"
543,"Since the removal of local discovery of #https://github.com/elastic/elasticsearch/pull/20960 we rely on minimum master nodes to be set in our test cluster. The settings is automatically managed by the cluster (by default) but current management doesn't work with concurrent single node async starting. On the other hand, with `MockZenPing` and the `discovery.initial_state_timeout` set to `0s` node starting and joining is very fast making async starting an unneeded complexity. Test that still need async starting could, in theory, still do so themselves via background threads. Remove `InternalTestCluster.startNode(s)Async` >>> 1"
544,"This changes the trace level logging to warn, and adds the needed number to the message as well.\n\nMy fear is that it may get noisy, but this is an issue that you want to be noisy.\n\nCloses #8362\n Warn on not enough masters during election >>> 1"
545,Timeouts are global today across all connections this commit allows to specify\r\na connection timeout per node such that depending on the context connections can\r\nbe established with different timeouts.\r\n\r\nRelates to #19719 Add a connect timeout to the ConnectionProfile to allow per node connect timeouts >>> 1
546,"Fuzzy query is deprecated since 5.0.0, it can now be removed.\r\n\r\nCloses #15760 Remove deprecated fuzzy query >>> 0"
547,"The upgrade is needed to enable index sorting on multi-valued fields (all fields are multi-valued in ES).\r\nNote for reviewers: I tried to split the commits in order to fix one problem at a time so better to check the diffs commit by commit (there are only 4 and the last 2 are real fixes).\r\n\r\n`gradle check` passes on my machine (with x-plugins), let's see what the CI says and I'll also rerun the checks locally.\r\n \r\n Upgrade to lucene-6.4.0-snapshot-ec38570 >>> 1"
548,"When users send large `terms` query to Elasticsearch, every value is stored in\r\nan object. This change does not reduce the amount of created objects, but makes\r\nsure these objects die young by optimizing the list storage in case all values\r\nare either non-null instances of Long objects or BytesRef objects, which seems\r\nto help the JVM significantly. Reduce memory pressure when sending large terms queries. >>> 1"
549,Inline scripts defined in Ingest Pipelines are not compiled at creation time and compile-time errors lazily occur upon initial execution of the pipeline.\r\n\r\nWIP because it lacks tests.\r\n\r\nFixes #21842. compile ScriptProcessor inline scripts when creating ingest pipelines >>> 1
550,"Given that `SearchTemplateRequest` effectively delegates to search when a search is being executed, it should implement the `CompositeIndicesRequest` interface. The `subrequests` method should return a single search request. When a search is not going to be executed, because we are in simulate mode, there are no inner requests, and there are no corresponding indices to that request either.\r\n\r\nCloses #21747 SearchTemplateRequest to implement CompositeIndicesRequest >>> 1"
551,This commit adds version 5.0.3 and the BWC indices for version 5.0.2. Add version 5.0.3 >>> 1
552,"Fix for #21878\r\n\r\nAdds `include_segment_file_sizes` to acceptable parameters in the `RestIndicesStatsAction` REST handler.\r\n\r\nThanks! Add support for ""include_segment_file_sizes"" in indices stats REST handler >>> 1"
553,"We currently treat every node equally when we establish connections to a node.\r\nYet, if we are not master eligible or can't hold any data there is no point in creating\r\na dedicated connection for sending the cluster state or running remote recoveries respectively.\r\nThis change ensures that in these cases STATE and RECOVERY connections are shared with REGular\r\nconnections since they are unnecessary. Reduce number of connections per node depending on the nodes role >>> 1"
554,re-pull request and try to refacotry code according to this: #21641 (comment) refactory ShardsLimitAllocationDecider.java >>> 1
555,The indices query is deprecated since 5.0.0 (#17710). It can now be removed in master (future 6.0 version). Remove indices query >>> 1
556,Some of the methods have been removed or deprecated.\r\n\r\nAlso related to #21825. Update Java documentation for 5.0 >>> 1
557,"The restore snapshot API does allow to select indices that should be restored the same way the create snapshot does. Unfortunately the `org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest` does not implement the `org.elasticsearch.action.IndicesRequest.Replaceable` interface like `org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest` does, although all needed methods are already present.\r\n\r\nCloses #21491 Implement IndicesRequest.Replaceable in RestoreSnapshotRequest >>> 0"
558,This method is not used anywhere and is not needed for the time being. Remove subrequests method from CompositeIndicesRequest >>> 1
559,This commit removes the version constant for 5.1.0 (due to an\r\ninadvertent release) and adds the version constant for 5.1.1. Add version 5.1.1 >>> 1
560,For some fields we have a specialized implementation of a TermQuery that is specific for the field.\r\nWhen these kind of fields are used in a wildcard query or a span term query it fails with an exception because they don't recognize the specialized form.\r\nThe impacted fields are [_all] and [_type] and the impacted queries are [span_term] and [wilcard].\r\nThis change handles these forms and correctly extracts the term inside them for further use.\r\n\r\nFixes #21882 Handle specialized term queries in MappedFieldType.extractTerm(Query) >>> 1
561,"Today when sending responses to discovery pings, we unconditionally\r\nreply. Instead, this commit modifies the response handler to not reply\r\nwhen the cluster names do not match.\r\n\r\nThis addresses a race condition identified after reducing the timeout in\r\nUnicastZenPingTests#testSimplePings. In particular, we send pings in the\r\nfollowing way:\r\n - if not connected to the node, connect to the node and after\r\n   successful handshake, send a ping\r\n - if connected to the node, send a ping\r\n\r\nWhen the ping timeout is set low, a subsequent batch of pings can race\r\nagainst a connect/disconnect cycle from a prior batch of pings. In\r\nparticular, consider the following scenario:\r\n - node A from cluster X\r\n - node B from cluster Y\r\n - pings are initiated from node A with node B in the hosts list\r\n - node A will try to connect and handshake with B\r\n - the connection will succeed, and the handshake will eventually fail due to mismatched cluster names\r\n - on a short timeout, a second batch of pings will fire, and on this\r\n   batch node A will see that it is still connected to node B; thus, it\r\n   will immediately fire a ping to node B and node B will dutifully\r\n   respond\r\n\r\nRelates #21874 Do not reply to pings from another cluster >>> 1"
562,Related to #21768 Add descriptions to create snapshot and restore snapshot tasks. >>> 1
563,"Related to #21768 Add proper descriptions to reindex, update-by-query and delete-by-query tasks. >>> 1"
564,"During package install on systemd-based systems, we try to set\r\nvm.max_map_count. On some systems (e.g., containers), users do not have\r\nthe ability to tune these parameters from within the container. This\r\ncommit provides an option for these users to skip setting such kernel\r\nparameters.\r\n\r\nCloses #21877 Add option to skip kernel parameters on install >>> 1"
565,Implementing a few more of the access methods in ClientSearchHits/ClientSearchHit which are implementations of the common SearchHit(s) interfaces in core.\r\n\r\nThis is a PR against a feature branch. HLClient: First additions to search interface >>> 1
566,"RestFilters are a complex way of allowing plugins to add extra code\r\nbefore rest actions are executed. This change removes rest filters, and\r\nreplaces with a wrapper which a single plugin may provide. Plugins: Replace Rest filters with RestHandler wrapper >>> 1"
567,"IndexRequestBuilder (and IndexRequest) has some setters for String-Object pairs that are redundant given that a generic `setSource(Object...)` method exists, so we can potentially remove it. I might not be aware of other reasons why these methods are there, maybe they are to make some data binding work for some client? I though its worth to discuss if we can remove them.\r\n Remove redundant source setters from IndexRequestBuilder >>> 1"
568,"If you ask for the term vectors of an artificial document with\r\nterm_statistics=true, but a shard does not have any terms of the doc's\r\nfield(s), it returns the doc's term vectors values as the shard-level\r\nterm statistics. This commit fixes that to return 0 for ttf and also\r\nfield-level aggregated statistics.\r\n\r\nThis closes #21906 Return correct term statistics when a field is not found in a shard >>> 1"
569,"This adds a new `analyzer` property to `keyword` fields that pre-processes the\r\nfield value prior to indexing, but without altering the `_source`. Note that\r\nonly the normalization components that work on a per-character basis are\r\napplied, so for instance stemming filters will be ignored while html stripping,\r\nlowercasing or ascii folding will be applied.\r\n\r\nCloses #18064\r\n\r\nAnother alternative would be to accept a list of char/token filters. I like that it is\r\nmore explicit but I initially started following that path and I hit issues due to the\r\nfact that these classes are hard to use outside of the context of an analyzer.\r\n\r\nI marked the feature as experimental in case we want to gather feedback before\r\nsettling on the way to expose this feature. Add the ability to set a normalizer on keyword fields. >>> 1"
570,"This commit fixes the handling of spaces in Windows paths. The current\r\nmechanism works fine in a path that contains a single space, but fails\r\non a path that contains multiple spaces. With this commit, that is no\r\nlonger the case.\r\n\r\nRelates #20809, relates #21525 Fix handling of spaces in Windows paths >>> 1"
571,"When there are no indexes, get mapping has a series of special cases.\r\nTwo of those expect the response object already started, and the other\r\ntwo respond with an exception. Those two cases (types passed in but no\r\nindexes and vice versa) would fail in their error response generation\r\nbecause it did not expect an object to already be started in the json\r\ngenerator. This change moves the object start to where it is needed for\r\nthe empty responses.\r\n\r\ncloses #21916 Mappings: Fix get mapping when no indexes exist to not fail in response generation >>> 1"
572,"The warnings received via response headers get printed out in a single line:\r\n\r\n```\r\nWARNING: request [DELETE http://localhost:9200/index/type/_api] returned 3 warnings:[this is warning number 0],[this is warning number 1],[this is warning number 2]\r\n``` Warn log deprecation warnings received from server >>> 1"
573,"This is a pretty straight forward backport of #21830 to 5.x but it changes some behavior in BWC. \r\nIn 5.x we tested BWC against 2.0.0-RC and it's betas but in master we don't allow this anymore, we only guarantee BWC to GA releases. We also do this for 5.0, 5.0.0-RC1 is not in the list of indices we support. Note: with this PR we actively reject these indices when we see on index / snapshot etc. when we upgrade...\r\n\r\nTo discuss this change I opened this PR just make everybody aware of this change and with the question if we should do it that way. Backport #21830 to 5.x >>> 1"
574,"The goal here is to add the same support for a timeout as in the search API as per #11310\r\n\r\nOne note:\r\n\r\nAdding a timeout like so to the list of requests works already without code changes:\r\n\r\n    {""index"" : ""test"" }\r\n    {""query"" : {""match_all"" : {}}, ""from"" : 0, ""size"" : 10, ""timeout"": ""1000m""}\r\n\r\n\r\n Add support for supplying timeout to _msearch via headers. >>> 0"
575,This PR includes a bunch of changes discussed after the initial commit of the high level client feature branch. Rest Client: Clean up search objects >>> 0
576,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\nThis PR is used to upgrade ```Comparator``` with lambda of java 8 which will make the code much shorter and cleaner. Happy to hear your comments. Thanks.\r\n\r\n Replace comparator with java8 way >>> 0"
577,"In #21828, serialization of the host string was added to preserve this information when\r\na TransportAddress gets serialized. However, there is still a case where this did not always\r\nwork. In UnicastZenPings, DiscoveryNode instances are created for the ping hosts with the\r\nminimum compatibility version, which is currently less than the version required to preserve\r\nthe host information. This means that when a node is received from a PingResponse that the\r\nhost information is no longer set correctly on the InetSocketAddress contained in the\r\nDiscoveryNode.\r\n\r\nThis commit adds a workaround for this situation by allowing the host string to be passed\r\ninto the TransportAddress constructor that takes a StreamInput and using that as the host\r\nfor the InetAddress that is created during deserialization. Do not lose host information when pinging >>> 1"
578,"Today if system call filters fail to install on startup, we log a\r\nmessage but otherwise march on. This might leave users without system\r\ncall filters installed not knowing that they have implicitly accepted\r\nthe additional risk. We should not be lenient like this, instead clearly\r\ninforming the user that they have to either fix their configuration or\r\naccept the risk of not having system call filters installed. This commit\r\nadds a bootstrap check that if system call filters are enabled, they\r\nmust successfully install. Add system call filter bootstrap check >>> 1"
579,This fixes reindex and friend's retries to keep the context. Keep context during reindex's retries >>> 1
580,"Today when starting a new engine, we read the global checkpoint from the\r\ntranslog only if we are opening an existing translog. This commit\r\nclarifies this situation by distinguishing the three cases of engine\r\ncreation in the constructor leading to clearer code.\r\n\r\nRelates #21254 Clarify global checkpoint recovery >>> 1"
581,Added a migration note for stored scripts related to using the parameter 'stored' in place of 'id' during scripting queries. Add Migration Note for Stored Scripts >>> 0
582,"I was under the impression that Integer caching in Java would only go from the range -128 through 127, but this appears to not be the case.  I have removed the results check for the tests in question, and now just run the code to ensure it compiles appropriately which was the original issue anyway.\r\n\r\nRelates #21801 Test fix for def equals in Painless >>> 1"
583,"The RecoveryFailedException's output prints the source and\r\ntarget nodes for the recovery.  However, sometimes there is\r\nno source node for the recovery, only a target node (such as\r\nwhen recovering a primary shard from disk).  In this case,\r\nwe don't want to display the source node.  This commit fixes\r\nthis by displaying ""Recovery failed to target node.."" instead\r\nof ""Recovery failed from null to target node"" which is what the\r\noutput currently displays. Don't output null source node in RecoveryFailedException >>> 1"
584,Fix for #19078 Add setting to set read timeout for EC2 discovery and S3 repository plugins >>> 1
585,Also add `ap-south` region linked to `ap-south-1`.\r\nFix for #21881 \r\n Add us-east-2 AWS region >>> 1
586,I compiled elasticsearch with error-prone\n(https://github.com/google/error-prone) and it reported\n13 errors. This commit fixes those errors.\n\nThe errors were related to the following:\n- http://errorprone.info/bugpattern/SuppressWarningsDeprecated\n- http://errorprone.info/bugpattern/StaticAccessedFromInstance\n- http://errorprone.info/bugpattern/CheckReturnValue\n- http://errorprone.info/bugpattern/TryFailThrowable\n Fix errors reported by error-prone >>> 0
587,"Relates to #21777\r\n    \r\nSearch templates for rank_eval endpoint so far only worked when sent through\r\nREST end point\r\n    \r\nHowever we also allow templates to be set through a Java API call to\r\n""setTemplate"" on that same spec. This doesn't go through template execution so\r\nfails further down the line.\r\n    \r\nTo make this work, moved template execution further down, probably to\r\nTransportRankEvalAction.\r\n\r\n@cbuescher would be nice if you could have a look Move rank-eval template compilation down to TransportRankEvalAction >>> 1"
588,"As it's good to get this correction included in our docs as soon as possible, I am creating this PR in lieu of https://github.com/elastic/elasticsearch/pull/21808 untill @vshank77 has had a chance to sign the CLA with the same email address as that used in his commit.\r\n\r\nOriginally reported by Shankar Vasudevan (@vshank77). [docs] Add missing RUN command from custom docker config >>> 1"
589,Action filters currently have the ability to filter both the request and\r\nresponse. But the response side was not actually used. This change\r\nremoves support for filtering responses with action filters. Plugins: Remove response action filters >>> 1
590,Throw an exception when specifying `include_in_all` on multi-fields\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/21710 Sub-fields should not accept `include_in_all` parameter >>> 1
591,close #21600 support numeric bounds with decimal parts for long/integer/short/byte datatypes >>> 1
592,This ports the fix of https://issues.apache.org/jira/browse/LUCENE-7536 to\r\nElasticsearch's ASCIIFoldingTokenFilterFactory. AsciiFoldingFilter's multi-term component should never preserve the original token. >>> 1
593,This PR makes two changes to how the in-sync allocations set is updated:\r\n- the set is only trimmed when it grows. This prevents trimming too eagerly when the number of replicas was decreased while shards were unassigned.\r\n- the allocation id of an active primary that failed is only removed from the in-sync set if another replica gets promoted to primary. This prevents the situation where the only available shard copy in the cluster gets removed the in-sync set.\r\n\r\nRelates to #21719 Trim in-sync allocations set only when it grows >>> 1
594,"This adds the `_primary_term` field internally to the mappings. This field is\r\npopulated with the current shard's primary term.\r\n\r\nIt is intended to be used for collision resolution when two document copies have\r\nthe same sequence id, therefore, doc_values for the field are stored but the\r\nfiled itself is not indexed.\r\n\r\nThis also fixes the `_seq_no` field so that doc_values are retrievable (they\r\nwere previously stored but irretrievable) and changes the `stats` implementation\r\nto more efficiently use the points API to retrieve the min/max instead of\r\niterating on each doc_value value. Additionally, even though we intend to be\r\nable to search on the field, it was previously not searchable. This commit makes\r\nit searchable.\r\n\r\nThere is no user-visible `_primary_term` field. Instead, the fields are\r\nupdated by calling:\r\n\r\n```java\r\nindex.parsedDoc().updateSeqID(seqNum, primaryTerm);\r\n```\r\n\r\nThis includes example methods in `Versions` and `Engine` for retrieving the\r\nsequence id values from the index (see `Engine.getSequenceID`) that are only\r\nused in unit tests. These will be extended/replaced by actual implementations\r\nonce we make use of sequence numbers as a conflict resolution measure.\r\n\r\nRelates to #10708\r\nSupercedes #21480\r\n\r\nP.S. As a side effect of this commit, `SlowCompositeReaderWrapper` cannot be\r\nused for documents that contain `_seq_no` because it is a Point value and SCRW\r\ncannot wrap documents with points, so the tests have been updated to loop\r\nthrough the `LeafReaderContext`s now instead. Add internal _primary_term doc values field, fix _seq_no indexing >>> 1"
595,"If you make a mistake and specify a mapping like:\r\n```\r\n{\r\n  ""parent"": {\r\n    ""properties"": {}\r\n  },\r\n  ""child"": {\r\n    ""_parent"": ""parent"",\r\n    ""properties"": {}\r\n  }\r\n}\r\n```\r\n\r\nthen the error message you get back amounts to\r\n`Failed to parse mapping for [child]: can't cast a String to a Map`.\r\nSince it doens't tell you *which* string can't be cast to a map you\r\nhave to dig through the stack trace to figure out what to fix. This\r\nreplaces the error message with:\r\n```\r\nFailed to parse mapping [child]: [_parent] must be an object containing [type]\r\n```\r\nso you can tell that the problem is with the `parent` field. Better error message when _parent isn't an object >>> 1"
596,"The `RestHighLevelClient` should accept an instance of the low-level client externally created, and managed. The high level client shouldn't create nor close the underlying low level client, this way it also doesn't need to implement `Closeable`. RestHighLevelClient to not implement Closeable >>> 1"
597,`_update_by_query` supports specifying the `pipeline` to process the\r\ndocuments as a url parameter but `_reindex` doesn't. It doesn't because\r\neverything about the `_reindex` request that has to do with writing\r\nthe documents is grouped under the `dest` object in the request body.\r\nThis changes the response parameter from\r\n`request [_reindex] contains unrecognized parameter: [pipeline]` to\r\n`_reindex doesn't support [pipeline] as a query parmaeter. Specify it in the [dest] object instead.` Reindex: Better error message for pipeline in wrong place >>> 1
598,"This adds several missing tests around serialising components of the rank evaluation requests (and fixes issues in the code uncovered by these tests, commits that introduce fixes are grouped with the tests that uncovered the underlying issues).\r\n\r\nOnly class left aside for a separate PR is RankEvalSpec itself.\r\n\r\n@cbuescher would be great if you could have a look Serialisation and validation checks for rank evaluation request components >>> 1"
599,"Introduces `XContentParser#namedObject` which works a little like `StreamInput#readNamedWriteable`: on startup components register parsers under names and a superclass. At runtime we look up the parser and call it to parse the object.\r\n    \r\nRight now the parsers take a `context` object they use to help with the parsing but I hope to be able to eliminate the need for this context as most what it is used for at this point is to move around parser registries which should be replaced by this method eventually. I make no effort to do so in this PR because it is big enough already. This is meant to the a start down a road that allows us to remove classes like `QueryParseContext`, `AggregatorParsers`, `IndicesQueriesRegistry`, and `ParseFieldRegistry`.\r\n\r\nThe goal here is to reduce the amount of plumbing required to allow parsing pluggable things. With this you don't have to pass registries all over the place. Instead you must pass a super registry to fewer places and use it to wrap the reader. This is the same tradeoff that we use for NamedWriteable and it allows much, much simpler binary serialization. We *think* we want that same thing for xcontent serialization.\r\n\r\nThe only parsing actually converted to this method is parsing `ScoreFunction`s inside of `FunctionScoreQuery`. I chose this because it is relatively self contained.\r\n\r\n Introduce XContentParser#namedObject >>> 1"
600,"Changes the build to recognize `NORELEASE` as well as `NOCOMMIT` to\r\nmean the same thing as `norelease` and `nocommit` respectively. This\r\nis useful because people have been using them that way but haven't\r\nrealized that only the lowercase versions worked.\r\n\r\nThis also explicitly forbids silly things like `NoReLeAsE` and\r\n`noCOMMIT`, failing the build and telling you to spell them properly. Recognize `NORELEASE` as the same as `norelease` >>> 1"
601,If you write a yaml test with a `warnings` section in a `do` block\r\nthat doesn't also have a corresponding `skip` section for `warnings`\r\nthen client test runners that don't support `warnings` will fail.\r\nThis causes the elasticsearch build to fail so we catch these errors\r\nearlier.\r\n\r\nRelated to #21811 Don't allow yaml tests with `warnings` that don't skip `warnings` >>> 1
602,"In 5.0, the search slow log switched to the multi-line format with no option to get back to the origin single-line format that was used prior to 5.0 by default. This commit changes the default back to the single-line format and restores an option of switching to the multi-line format if needed.\r\n\r\nCloses #21711 \r\n\r\nNot sure if this should be treated as bug or as an enhancement and therefore if we should document it in breaking changes or not (if this is a bug, we are fixing it, if this is an enhancement, we are breaking it) and if this is a bug, should we push it to 5.1.0, 5.1.1 or 5.2.0 is good enough. Restores the original default format of search slow log >>> 1"
603,"Ingest was originally setup as a plugin, and in order to hook into the\r\nindex and bulk actions, action filters were used. However, ingest was\r\nlater moved into core, but the action filters were never removed. This\r\nchange moves the execution of ingest into the index and bulk actions. Ingest: Moved ingest invocation into index/bulk actions >>> 1"
604,"Performance testing by @danielmitterdorfer revealed single\r\nindex/delete operations have similar performance (indexing\r\nthroughput) to equivalent single item bulk request.\r\nThis PR reduces the code paths to executing single write\r\noperations, by reusing the logic in (shard) bulk action for\r\nexecuting single operation as a single-item bulk request. Make index and delete operation execute as a single bulk item >>> 1"
605,"We had tests for the regular factories, but not for the pre-built ones, that\r\nship by default without requiring users to define them in the analysis settings. Pre-built analysis factories do not implement MultiTermAware correctly. >>> 1"
606,"When a primary shard fails, ES promotes a replica to be the new primary,\r\nhowever, if there are two replicas to choose from, one on a higher node version\r\nand one on a lower node version, promoting the higher node version could be\r\nproblematic because replicating requests could then be in a format the older\r\nversion replica cannot read.\r\n\r\nThis changes the `activeReplica` method in `RoutingNodes` to return the replica\r\non the node with the lowest ES version. This method is used to select a replica\r\nfor promotion.\r\n\r\nResolves #22002 Promote replica on lowest version of Elasticsearch when primary fails >>> 0"
607,This PR adds a new `GeoPoint` class to `FieldStats` for computing field stats over `geo_point` field types. Integration tests are updated to include testing geo_point types.\r\n\r\nCloses #20707 Add geo_point to FieldStats >>> 0
608,This is a start to addressing #21887. This removes:\r\n* pre 2.0 snapshot format support\r\n* automatic units addition to cluster settings\r\n* bwc check for delete by query in pre 2.0 indexes Remove 2.0 prerelease version constants >>> 1
609,"Failing an initializing primary when shadow replicas are enabled for the index can leave the primary unassigned with replicas being active. Instead, a replica should be promoted to primary, which is fixed by this commit.\r\n\r\nTest failure:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.x+multijob-unix-compatibility/os=ubuntu/324/console Promote shadow replica to primary when initializing primary fails >>> 1"
610,This adds a fromXContent method and unit test to the HighlightField class so we can parse it as part of a serch response. This is part of the preparation for parsing search responses on the client side. Add fromXContent to HighlightField >>> 1
611,\r\nSee #22024 Add a deprecation notice to shadow replicas >>> 1
612,Closes #22005 IP range masks exclude the maximum address of the range. >>> 1
613,"Before, it was possible that the SameShardAllocationDecider would allow\r\nforce allocation of an unassigned primary to the same node on which an\r\nactive replica is assigned.  This could only happen with shadow replica\r\nindices, because when a shadow replica primary fails, the replica gets\r\npromoted to primary but in the INITIALIZED state, not in the STARTED\r\nstate (because the engine has specific reinitialization that must take\r\nplace in the case of shadow replicas).  Therefore, if the now promoted\r\nprimary that is initializing fails also, the primary will be in the\r\nunassigned state, because replica to primary promotion only happens when\r\nthe failed shard was in the started state.  The now unassigned primary\r\nshard will go through the allocation deciders, where the\r\nSameShardsAllocationDecider would return a NO decision, but would still\r\npermit force allocation on the primary if all deciders returned NO.\r\n\r\nThis commit implements canForceAllocatePrimary on the\r\nSameShardAllocationDecider, which ensures that a primary cannot be\r\nforce allocated to the same node on which an active replica already\r\nexists.\r\n\r\nRelates #22021  Cannot force allocate primary to a node where the shard already exists >>> 1"
614,"Indexing requests with operation type `create` currently auto-convert external versioning to internal versioning and silently ignore the version number instead of failing with an error message:\r\n\r\n```\r\n$ curl -XPOST ""http://localhost:9200/index/type/1/_create?version=42&version_type=external"" -d '\r\n> {""data"":""test data""}\r\n> '\r\n{""_index"":""index"",""_type"":""type"",""_id"":""1"",""_version"":1,""result"":""created"",""_shards"":{""total"":2,""successful"":1,""failed"":0},""created"":true}\r\n```\r\n\r\nThis PR tries to fix the validation logic in the least invasive way. The first commit shows an earlier attempt to fix this that I believe to be cleaner but will potentially break more usages of IndexRequest (among others it breaks RestIndexAction that sets by default `MATCH_ANY` if no version can be found). Reject external versioning and explicit version numbers on create >>> 1"
615,"To get #22003 in cleanly we need to centralize as much `XContentParser` creation as possible into `RestRequest`. That'll mean we have to plumb the `NamedXContentRegistry` into fewer places.\r\n\r\nThis removes `RestAction.hasBody`, `RestAction.guessBodyContentType`, and `RestActions.getRestContent`, moving callers over to `RestRequest.hasContentOrSourceParam`, `RestRequest.contentOrSourceParam`, and `RestRequest.contentOrSourceParamParser` and `RestRequest.withContentOrSourceParamParserOrNull`. The idea is to use `withContentOrSourceParamParserOrNull` if you need to handle requests without any sort of body content and to use `contentOrSourceParamParser` otherwise.\r\n\r\nI believe the vast majority of this PR to be purely mechanical but I know I've made the following behavioral change (I'll add more if I think of more):\r\n* If you make a request to an endpoint that requires a request body and has cut over to the new APIs instead of getting `Failed to derive xcontent` you'll get `Body required`.\r\n* Template parsing is now non-strict by default. This is important because we need to be able to deprecate things without requests failing. Begin centralizing XContentParser creation into RestRequest >>> 1"
616,"Relates to #21260, but implemented in a slightly more lenient way to allow for templated and non-templated rated requests to co-exist in one rank-eval call.\r\n\r\nFail early in case no metric is supplied, no rated requests are supplied or the\r\nsearch source builder is missing but no template is supplied neither.\r\n\r\nWould be great if @cbuescher could have a brief look. Add checks to RankEvalSpec to safe guard against missing parameters. >>> 1"
617,nan Clarify requirement for re-indexing from ES 1.x >>> 1
618,Today we connect and publish the nodes connection before we execute a\r\nhandshake with the node we connect to. In the case of connecting to a node\r\nthat won't pass the handshake this connection is already `published` and other\r\ncode paths can use it. This commit detaches the connection and the publish of the\r\nconnection such that `TransportService` can do a handshake before actually connect\r\nand publish the connection. Detach handshake from connect to node >>> 1
619,"This is an attempt to start moving aggs parsing to `ObjectParser`. There is\r\nstill A LOT to do, but ObjectParser is way better than the way aggregations\r\nparsing works today. For instance in most cases, we reject numbers that are\r\nprovided as strings, which we are supposed to accept since some client languages\r\n(looking at you Perl) cannot make sure to use the appropriate types.\r\n\r\nRelates to #22009 Start using `ObjectParser` for aggs. >>> 1"
620,Today we still leak some knowledge about how to parse the es wire protocol\r\ninto the actual implementation (mock / netty4). This commit moves all\r\nthe remaining logic into the TcpTransport to make the protocol logic\r\nentirely transparent. Move all protocol parsing logic into TcpTransport >>> 0
621,This also adds another level of protection against using the default locale.\r\nRelates to https://discuss.elastic.co/t/mapping-for-12h-date-format/68433/3. Document the `locale` option of the `date` field. >>> 1
622,Discovered while working on #22048.\r\n\r\nI'm labelling this as a non-issue since it is fixing a feature that was nor released. Partition-based include-exclude does not implement equals/hashcode/serialization correctly. >>> 1
623,This commit simplifies the node update logic so that nodes are never removed from the cluster state when the cluster state is not published. Don't update nodes list when stepping down as master >>> 1
624,This commit bumps the version to 5.1.2. Bump version to 5.1.2 >>> 1
625,This adds a fromXContent method and unit test to InternalNestedIdentity so we can parse it as part of a search response. This is part of the preparation for parsing search responses on the client side. Add fromXContent to InternalNestedIdentity >>> 1
626,"Follow on from #21440.\r\n\r\nThis just adds stored binary fields to our back compat indices and ensures we can still load these fields.\r\n\r\nThis change is just for master, so only regenerates 5.0.{0,1,2} back compat indices; when I backport to 5.x and 2.x I'll regenerate the older versions too.  I'll also add a compressed binary field to the 1.x indices, and I confirmed this would have caught #21440.\r\n Add stored binary fields to static backwards compatibility indices tests >>> 1"
627,Related to #21768 Add descriptions to bulk tasks >>> 1
628,"This allow to test if a version can be upgraded via a package repository.\r\nThe idea is to ensure that a bats test runs, that installs an old version first,\r\nthen runs apt-get/yum/dnf/zypper to upgrade, restarts elasticsearch and checks\r\nif the expected version is running.\r\n\r\nA sample call on the console can look like this\r\n\r\n```\r\ngradle ':qa:vagrant:vagrantRepositoryTest' -Pvagrant.boxes=all -Prepo.version=X.Y.Z -Prepo.url=https://whatever.elastic.co/A.B.C-my-repo/packages/5.x\r\n```\r\n\r\nThe above call installs the current stable version of ES, then adds the repository, upgrades the elasticsearch package,\r\nstarts elasticsearch and checks if the ES version matches the expected one given. Also an index is created and checked\r\nif it can be read.\r\n\r\nThe parameters above are basically passed to vagrant by creating a file, that in turn gets read by the bats test, which only runs if those files exist.\r\n\r\nI took a look at the vagrant/groovy test infra for the first time in months, so there might be much more efficient ways to do this. Open for any hints. Smoke testing: Allow to test upgrades against repository via bats >>> 0"
629,This commit adds safety against an NPE if HTTP stats are requested but\r\nHTTP is disabled on a node.\r\n\r\nCloses #22058 Avoid NPE in NodeService#stats if HTTP is disabled >>> 1
630,"If you try to close the rest client inside one of its callbacks then\r\nit blocks itself. The thread pool switches the status to one that\r\nrequests a shutdown and then waits for the pool to shutdown. When\r\nanother thread attempts to honor the shutdown request it waits\r\nfor all the threads in the pool to finish what they are working on.\r\nThus thread a is waiting on thread b while thread b is waiting\r\non thread a. It isn't quite that simple, but it is close.\r\n\r\nRelates to #22027 Don't close rest client from its callback >>> 1"
631,This commit cleans up a bit the current Gradle file of the highlevel-rest module and adds a hard check to avoid adding more dependencies. Clean up highlevel-rest gradle file >>> 1
632,"In order to start clusters with min master nodes set without setting `discovery.initial_state_timeout`, #21846 has changed the way we start nodes. Instead to the previous serial start up, we now always start the nodes in an async fashion (internally). This means that starting a cluster is unsafe without `min_master_nodes` being set. We should therefore make it mandatory. \r\n Enforce min master nodes in test cluster >>> 1"
633,"URLBlobContainer can in certain situations throw a FileNotFoundException. To fulfill the contract of the readBlob method it should throw a NoSuchFileException instead when the given blob cannot be found. All other BlobContainer implementations do this correctly, see e.g. FsBlobContainer: https://github.com/elastic/elasticsearch/blob/7560101ec73331acb39a042bde6130e59e4bb630/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java#L119-L120\r\n\r\nThe second commit of this PR fixes a BWC issue. PR #22004 removed BWC for pre-5.0 snapshots in 6.0, which also meant removing the capability to load pre-5.0 snapshots. In 6.0, such snapshots are now invisible and must be treated by the BWC tests in that way.\r\n\r\nFirst commit also goes back to 5.x, second commit only goes to 6.0. URLRepository should throw NoSuchFileException to correctly adhere to readBlob contract >>> 1"
634,"A shard that is locally marked as relocated (but relocation target shard has not been activated yet by the master) can still receive index operations, which in return can lead to flushing. If this happens, flushing goes into an endless retry loop and logs warnings until the shard is closed.\r\n\r\nThis PR also allows force_merge and upgrade operations to run on shards that are marked as relocated.\r\n\r\nRelates to #22043 Allow flush/force_merge/upgrade on shard marked as relocated >>> 1"
635,"* Send a non supported document to an ingest pipeline using `ingest-attachment`\r\n* If Tika is not able to parse the document because of a missing class (we are not importing all jars needed by Tika), Tika throws a Throwable which is not catch.\r\n\r\nThis commit removes extracting embedded content from Office XML docs.\r\n\r\nSo elasticsearch is not killed anymore when you run a command like:\r\n\r\n```\r\nGET _ingest/pipeline/_simulate\r\n{\r\n  ""pipeline"" : {\r\n    ""processors"" : [\r\n      {\r\n        ""attachment"" : {\r\n          ""field"" : ""file""\r\n        }\r\n      }\r\n    ]  \r\n  },\r\n  ""docs"" : [\r\n    {\r\n      ""_source"" : {\r\n        ""file"" : ""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""  \r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe good news is that it does not kill the node anymore and allows to extract the text which is in the Office document.\r\n\r\nCloses #22077. Remove support for Visio and potm files >>> 1"
636,"This adds fromXContent method and unit test for sort values that are part of\r\nInternalSearchHit. In order to centralize serialisation and xContent parsing and\r\nrendering code, move all relevant parts to a new class which can be unit tested\r\nmuch better in isolation.This is part of the preparation for parsing search\r\nresponses on the client side. Factor out sort values from InternalSearchHit >>> 1"
637,"With this commit we enable the Jackson feature 'STRICT_DUPLICATE_DETECTION'\r\nby default. This ensures that JSON keys are always unique. While this has\r\na performance impact, benchmarking has indicated that the typical drop in\r\nindexing throughput is around 1 - 2%.\r\n\r\nAs a last resort, we allow users to still disable strict duplicate checks\r\nby setting `-Des.json.strict_duplicate_detection=false` which is\r\nintentionally undocumented.\r\n\r\nCloses #19614 Enable strict duplicate checks for JSON content >>> 1"
638,Improves the error message returned when looking up a task that\r\nbelongs to a node that is no longer part of the cluster. The new\r\nerror message tells the user that the node isn't part of the cluster.\r\nThis is useful because if you start a task and the node goes down\r\nthere isn't a record of the task at all. This hints to the user that\r\nthe task might have died with the node.\r\n\r\nRelates to #22027 Improve the error message if task and node isn't found >>> 1
639,"While I was fixing a documentation issue (#22007), I looked at the code and discovered that we actually never read what the user entered as a `readonly` parameter when he creates an azure repository.\r\n\r\nSo if someone sends:\r\n\r\n```\r\nPUT _snapshot/my_backup4\r\n{\r\n    ""type"": ""azure"",\r\n    ""settings"": {\r\n        ""account"": ""my_account2"",\r\n        ""location_mode"": ""primary_only"",\r\n        ""readonly"": true\r\n    }\r\n}\r\n```\r\n\r\nThe repository is not actually defined as `readonly`.\r\n\r\nIt's caused by the fact we are always overwriting `readonly`setting based on `location_mode`.\r\nIf a user sets it to `primary_only`, `readonly` is forced to `false`.\r\nIf a user sets it to `primary_then_secondary`, `readonly` is forced to `false`.\r\nIf a user sets it to `secondary_only`, `readonly` is forced to `false`.\r\n\r\nNote that with this change, a user can force a `secondary_only` repository to `readonly: false` which will lead him to an error later on when we check the repository as per definition in Azure, a secondary repository is not writable.\r\nAnother option could have been to detect this mismatch and throw an exception in that case. Note sure it is worth writing more code though.\r\n\r\nCloses #22053. readonly on azure repository must be taken into account >>> 1"
640,"Moved field values `toXContent` logic to `GetField` (from `GetResult`), which outputs its own fields, and can also parse them now. Also added `fromXContent` to `GetResult` and `GetResponse`.\r\n\r\n The start object and end object for the get response output have been moved to `GetResult#toXContent`, from the corresponding rest action. This makes it possible to have `toXContent` and `fromXContent` completely symmetric, as parsing requires looping till an end object is found which is weird when the corresponding `toXContent` doesn't print that object out. Add fromXContent method to GetResponse >>> 1"
641,We shouldn't output an empty ext object if no ext sections have been set to the SearchSourceBuilder.\r\n\r\nCloses #20969 Don't output empty ext object in SearchSourceBuilder#toXContent >>> 1
642,"When we decided to deprecate and remove fuzzy query in #15760, we didn't realize we would take away the possibility for users to use a fuzzy query as part of a span query, which is not possible using match query (designed to be the replacement for fuzzy query as it supports fuzziness). This means we have to go back and un-deprecate fuzzy query, which will not be removed anymore.\r\n\r\nCloses #15760 Un-deprecate fuzzy query >>> 1"
643,"Moves the last of the ""easy"" parser construction into\r\n`RestRequest`, this time with a new method\r\n`RestRequest#contentParser`. The rest of the production\r\ncode that builds `XContentParser` isn't ""easy"" because it is\r\nexposed in the Transport Client API (a Builder) object. Consolidate the last easy parser construction >>> 1"
644,"When using dynamic templates, ES will now throw an exception if a\r\n`match_mapping_type` is used that doesn't correspond to an actual type.\r\n\r\nRelates to #17285\r\n\r\nI was reminded of this from a recent post on discuss.elastic.co where a user had used ""text"" instead of ""string"". Throw an exception on unrecognized ""match_mapping_type"" >>> 1"
645,This commit adds a plugin that allows to specify\r\nany number of clusters to be formed and used with\r\nthe existing `RestTestPlugin`. The plugin takes care\r\nof creating tasks for forming additional clusters and\r\ndependancies between test task and cluster formation tasks. Add plugin for integration test with multiple clusters >>> 0
646,"Our query DSL supports empty queries (`{}`), which have a different meaning depending on the query that holds it, either ignored, match_all or match_none. We deprecated the support for empty queries in 5.0, where we log a deprecation warning wherever they are used.\r\n\r\nThe way we supported it once we moved query parsing to the coordinating node was having an `Optional<QueryBuilder>` return type in all of our parse methods (called `fromXContent`). See #17624. The central place for this was `QueryParseContext#parseInnerQueryBuilder`. We can now remove all the optional return types and simply throw an exception whenever an empty query is found while parsing. Remove support for empty queries >>> 1"
647,"`ClusterService` is responsible of updating the cluster state on every node (as a response to an API call on the master and when non-masters receive a new state from the master). When a new cluster state is processed, it is made visible via the `ClusterService#state` method and is sent to series of listeners. Those listeners come in two flavours - one is to change the state of the node in response to the new cluster state (call these cluster state appliers), the other is to start a secondary process. Examples for the later include an indexing operation waiting for a shard to be started or a master node action waiting for a master to be elected. \r\n\r\nThe fact that we expose the state before applying it means that samplers of the cluster state had to worry about two things - working based on a stale CS and working based on a future, i.e., ""being applied"" CS. The `ClusterStateStatus` was used to allow distinguishing between the two. Working with a stale cluster state is not avoidable. How this PR changes things to make sure consumers don't need to worry about future CS, removing the need for the status and simplifying the waiting logic.\r\n\r\nThis change does come with a price as ""cluster state appliers"" can't sample the cluster state from `ClusterService` whenever they want as the cluster state isn't exposed yet. However, recent clean ups made this is situation easier and this PR takes the last steps to remove such sampling. This also helps clarify the ""information flow"" and helps component separation (and thus potential unit testing). It also adds an assertion that will trigger if the cluster state is sampled by such listeners. \r\n\r\nNote that there are still many ""appliers"" that could be made a simpler, unrestricted ""listener"" but this can be done in smaller bits in the future. The commit also makes it clear what the `appliers` and what the `listeners` are by using dedicated interfaces.\r\n\r\nAlso, since I had to change the listener types I went ahead and changed the data structure for temporary/timeout listeners (used for the observer) so addition and removal won't be an O(n) operation ClusterService should expose ""applied"" cluster states (i.e., remove ClusterStateStatus) >>> 1"
648,Plugins also have the need to provide better OOTB experience by configuring\r\ndefaults unless the plugin is used in _production_ mode. This change exposes\r\nthe bootstrap check infrastructure as part of the plugin API to allow plugins\r\nto specify / install their own bootstrap checks if necessary. Allow plugins to install bootstrap checks >>> 1
649,"Added unit tests for testing fromXContent/toXContent implementations of IncludeExclude class. Tests cover ""partition"" mode plus the regex and exact-value modes of use.\r\nAdded a REST test for partitioned terms agg.\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/22102 Tests - added tests for Terms agg partitioning >>> 0"
650,"This commit enables CLI commands to be closeable and installs a runtime\r\nshutdown hook to ensure that if the JVM shuts down (as opposed to\r\naborting) the close method is called.\r\n\r\nIt is not enough to wrap uses of commands in main methods in\r\ntry-with-resources blocks as these will not run if, say, the virtual\r\nmachine is terminated in response to SIGINT, or system shutdown event.\r\n\r\nCloses #22111 Add shutdown hook for closing CLI commands >>> 1"
651,"Today we rely on the version that the API user passes in together with the `DiscoveryNode`. This commit introduces a low level handshake where nodes exchange their version to be used with the transport protocol that is executed every time a connection to a node is established. This, on the one hand allows to change the wire protocol based on the version we are talking to even without a full cluster restart. Today we would need to carry on a BWC layer across major versions but with a handshake we can rely on the fact that the latest version of the previous minor executes a handshake and uses the latest protocol version across all communication with the N+1 version nodes.\r\n\r\nThis change is yet fully backwards compatible, a followup PR will remove the BWC in 6.0 once this has been back-ported to the 5.x branch Introduce a low level protocol handshake >>> 1"
652,"This class is just a wrapper around `SearchContext`, so let's use\r\n`SearchContext` directly. The change is mechanical, except the\r\n`ValuesSourceConfig` class, where I moved the logic to get a `ValuesSource`\r\ngiven a config. Remove `AggregationContext`. >>> 1"
653,Starts to centralize creation of the `XContentParser` in\r\n`protected final` methods on `ESTestCase`. The idea is to enable\r\nadding `NamedXContentRegistry` relatively easily by giving tests\r\na single place they can override to define the\r\n`NamedXContentRegistry`. Since `NamedXContentRegistry` doesn't\r\nexist yet neither does the override point.\r\n\r\nThis doesn't attempt to migrate all the tests to calling the\r\nnew methods to build the parsers. I wanted to make this so we\r\ncould review the concept and then I'll merge a followup to\r\nmigrate the tests. Start to centralize creation of XContentParser in tests >>> 1
654,"Grok was originally ignoring potential matches to named-capture groups\r\nlarger than one. For example, If you had two patterns containing the\r\nsame named field, but only the second pattern matched, it would fail to\r\npick this up.\r\n\r\nThis PR fixes this by exploring all potential places where a\r\nnamed-capture was used and chooses the first one that matched.\r\n\r\nFixes #22117. Fixes GrokProcessor's ignorance of named-captures with same name. >>> 1"
655,The creation of the `ValuesSource` used to pass `DateTimeZone.UTC` as a time\r\nzone all the time in case of empty fields in spite of the fact that all doc\r\nvalue formats but the date one reject this parameter.\r\n\r\nThis commit centralizes the creation of the `ValuesSource` and adds unit tests\r\nto it.\r\n\r\nCloses #22009 Fix `missing` on aggs on `boolean` fields. >>> 1
656,"In 5.0, we optimize for suggest-only search requests meaning\nusing suggest API via _suggest endpoint has the same performance\nimplication as using _search endpoint with only suggest.\n\nThis commit deprecates _suggest endpoint in favour of using suggest API\nvia _search endpoint. Now the response from _suggest endpoint is identical\nto response of a suggest-only search request.\n Deprecate _suggest endpoint >>> 0"
657,"This is the first step towards removing `ParseFieldMatcher`. The only reason to have this class is strict parsing, which is only used in our test infra. Because it depends on a setting (although this setting was never settable by users), we were moved all of the `ParseField` checks to `ParseFieldMatcher` when it was introduced (see #11859), which was quite an intrusive change in our parsing code. We now have a deprecation logger, and we return warning headers whenever some deprecated feature is used throughout the execution of a request. The functionality of strict parsing can either be moved to clients, or re-implemented without the need for `ParseFieldMatcher`, by rather leveraging our current infrastructure based on `ThreadContext` and `DeprecationLogger`. Another disadvantage of `ParseFieldMatcher` is that it's completely decoupled from the deprecation logger, hence it became hard to keep the two in sync. Also, we ended up creating many copies of `ParseFieldMatcher` in our codebase that cause confusion.\r\n\r\nThis PR deprecates `ParseFieldMatcher` and makes it not useful as it removes support for strict parsing. All tests that used to rely on strict parsing are now moved to checking warning headers, introduced first with #20993 in query tests. Even more, we can now move the warnings check from `AbstractQueryTestCase` to `ESTestCase` and test more closely deprecation warnings, as well as easily catch which tests use deprecated features. This can only be leveraged in unit tests or single node test (where the only node runs in the same jvm as the test).\r\n\r\nOnce this change goes in, we can go and step by step remove all the usages of `ParseFieldMatcher`, which will be a mechanical change (a big one), and finally remove the class.\r\n\r\nRelates to #19552\r\n Replace strict parsing mode with response headers assertions >>> 1"
658," `ElasticsearchException` is used in various response objects like `IndexResponse`, `DeleteResponse` or `BulkItemResponse.Failure`. It would be helpful for the High Level Rest Client to be able to parse these exceptions back as objects.\r\n    \r\nThis commit adds the `fromXContent()` method to the `ElasticsearchException` object. This method does not return the original (wrapped or unwrapped) exception but always returns a `ElasticsearchException` that serves as a simple POJO for all types of exceptions. The parsed ElasticsearchException's message will be composed of the original exception type (ex: `illegal_argument_exception`) concatenated with the original reason to help users/clients to known and handle the error. The exception's cause tree is preserved within `ElasticsearchException` instances.\r\n Add parsing method for ElasticsearchException >>> 1"
659,"This change disables the `_all` meta field by default.\r\n\r\nNow that we have the ""all-fields"" method of query execution, we can save both\r\nindexing time and disk space by disabling it.\r\n\r\n`_all` can no longer be configured for indices created after 6.0.\r\n\r\nRelates to #20925 and #21341\r\nResolves #19784 Disable _all by default, disallow configuring _all on 6.0+ indices >>> 1"
660,"Currently `docvalues_fields` return the values of the fields as they are stored\r\nin doc values. I don't like that it exposes implementation details, but there\r\nare also user-facing issues like the fact it cannot work with binary fields.\r\nThis change will also make it easier for users to reindex if they do not store\r\nthe source, since `docvalues_fields` will return data is such a format that it\r\ncan be put in an indexing request with the same mappings.\r\n\r\nThe hard part of the change is backward compatibility, since it is breaking.\r\nThe approach taken here is that 5.x will keep exposing the internal\r\nrepresentation, with a special format name called `use_field_format` which\r\nwill format the field depending on how it is mapped. This will become the\r\ndefault in 6.0, and this hardcoded format name will be removed in 7.0 to ease\r\nthe transition from 5.x to 6.x. Format doc values fields. >>> 0"
661,"PR #22049 changed the node update logic to never remove nodes from the cluster state when the cluster state is not published. This led to an issue electing a master (#22120) based on nodes with same transport address (but different node id) as previous nodes. The joining nodes should take precedence over conflicting ones as these nodes will also be the ones that have to commit the cluster state. Note that this only applies to the action of becoming master. If a master is established and a node joins an existing master, it will still be rejected if there is another node with same transport address. Prefer joining node with conflicting transport address when becoming master >>> 1"
662,"This commit fixes a for loop that reverses the order of shard stats\r\ncoming off the wire, and is really hard to read anyway (with the\r\npost-increment in the loop initializer). Fix for loop that is bad and should feel bad >>> 1"
663,Continue consolidating `XContentParser` construction in tests as a prerequisite to #22003 Continue consolidating `XContentParser` construction in tests >>> 1
664,"The snapshot APIs prevent more than one snapshot from being taken in the\r\nsystem at any given time, but nothing prevents concurrent snapshot\r\ndeletions from occuring.   If two snapshot deletion requests are\r\nexecuting simultaneously, it is possible that both deletion requests\r\nread the same index-N file, then the first deletion request will remove\r\nthe snapshot from the list found in index-N and write a new index-{N+1}\r\nindex file with the snapshot removed.  The second deletion request will\r\nremove another snapshot from the list found in index-N, but since it is\r\noperating based on the data it read from index-N, it does not know that\r\na different snapshot was removed simulatenously and the index file\r\nupdated to index-{N+1}.  Therefore, the second deletion request takes\r\nthe original snapshot list found in index-N and removes the snapshot it\r\nis supposed to delete, and writes a new index-{N+2} file, but the new\r\nindex-{N+2} file (which is now the active index file for the repository)\r\nstill contains the snapshot that was supposed to be deleted from the\r\nfirst snapshot deletion request.\r\n\r\nThis leads to various issues, including the fact that the deletion\r\noperation also deletes all associated snapshot blobs in the repository.\r\nWhen the repository tries to load its snapshot data, it sees that the\r\ndeleted snapshot from the first deletion request is still there, so\r\ntries to load its metadata, but it fails because the blob is not found.\r\n\r\nThis commit fixes the issue by ensuring that repository data is only\r\nwritten to a new index-{N+1} if the repository data was loaded from the\r\nindex-{N} file.  If it was loaded from any other index file (e.g.\r\nindex-{N-1}), then we assume we are operating on stale repository data\r\nand throw an exception.  This still leaves a small window where two\r\nsimultaneous deletions may determine at the same time that the next\r\nindex generation is {N+1}, so both operations try to write the new index\r\nblob to index-{N+1}.  However, repository implementations will throw an\r\nIOException if they attempt to write to a blob that already exists, so\r\nthe operation will fail in that case too, preventing simultaneous\r\ndeletes.  Note that this solution also enables detecting concurrent \r\nmodifications of the index file across multiple clusters that share the \r\nsame repository.\r\n\r\nThis solution does not account for the situation where a snapshot deletion\r\nmay be in progress when a snapshot is started, potentially causing a deletion\r\nto delete data that the snapshot requires.  The window for this is small however,\r\nand this situation will be better solved in #19957. Prevents unsafe concurrent snapshot deletions >>> 0"
665,Since #22094 has been back-ported to 5.2 we can remove all BWC layers from master since all supported version will handle handshake requests.\r\n\r\nRelates to #22094 Remove TCP handshake BWC from master >>> 1
666,"Today we write 0x00 or 0x01 for false or true when serializing a boolean\r\n(and 0x02 for null when serializing an optional boolean) but we\r\ndeserialize any non-zero byte to true (except when deserializing an\r\noptional boolean in which case we deserialize 0x02 to null, 0x01 to\r\ntrue, and any other non-zero byte to false). This too easily allows\r\ncorruption into the stream. Instead, we should mark the stream as\r\ncorrupted and stop deserializing. This catches when we try to\r\ndeserialize something as a boolean that is not a boolean.\r\n Avoid corruption when deserializing booleans >>> 1"
667,For minDocCount=0 the numeric terms aggregator should also check the includes/excludes when buckets with empty count are added to the result.\r\nThis change fixes this bug and adds a test for it.\r\n\r\nFixes #22140 Fix numeric terms aggregations with includes/excludes and minDocCount=0 >>> 1
668,"For 6.0/5.2/5.1/5.0:\r\n\r\n* We are now using painless as our language so I made sure that scripts can be run actually\r\n\r\nFor 6.0/5.2/5.1:\r\n\r\n* From 5.1 we changed the order of `Script` class ctor.\r\n* From 5.1, we can't use anymore `ScriptService.ScriptType` but `ScriptType`.\r\n Update Java API docs for 5.1/5.0 >>> 1"
669,"Merging mappings ensures that fields are used consistently across mapping types. Disabling norms for a specific field in one mapping type for example also disables norms for the same field in other mapping types of that index. The logic that ensures this while merging mappings currently always creates a fresh document mapper for all existing mapping types, even if no change occurred. Creating such a fresh document mapper does not come for free though as it involves recompressing the source. Making a mapping change to one type of an index with 100 types will thus re-serialize and recompress all 100 types, independent of any changes made to those types. Only update DocumentMapper if field type changes >>> 1"
670,This commit adds the equals() and hashCode() methods to ReplicationResponse.ShardInfo and ReplicationResponse.ShardInfo.Failure classes. Add equals() and hashCode() methods to ReplicationResponse.ShardInfo >>> 1
671,This change fixes the cloning of the FunctionScoreQueryBuilder when the inner query or functions are rewritten.\r\n\r\nFixes #22138 Fix boost_mode propagation when the function score query builder is rewritten >>> 1
672,Noted that backslashes in paths require escaping in yml Noted that backslashes in paths require escaping >>> 0
673,Low level handshake code doesn't handle situations gracefully if the connection\r\nis concurrently closed or reset by peer. This commit adds the relevant code to\r\nfail the handshake if the connection is closed. Handle connection close / reset events gracefully during handshake >>> 1
674,"In 5.0, `_search` endpoint has been optimized for suggest\nonly search requests. Users should move away from\nusing the `_suggest` endpoint, as it may be deprecated and\nremoved in the future. \n Deprecate _suggest endpoint in favour of _search >>> 1"
675,There is a small typo in the convert processor code example. Small typo fix in the docs. >>> 1
676,"With recent changes to our parsing code we have drastically reduced the places where we auto-detect the content type from the input. The usage of these methods spread in our codebase for no reason, given that in most of the cases we know the content type upfront and we don't need any auto-detection mechanism. Deprecating these methods is a way to try and make sure that these methods are carefully used, and hopefully not introduced in newly written code.\r\n\r\nWe have yet to fix the REST layer to read the Content-Type header, which is the long term solution, but for now we just want to make sure that the usage of these methods doesn't spread any further.\r\n\r\nRelates to #19388 Deprecate XContentType auto detection methods in XContentFactory >>> 1"
677,"This change renames and changes the behavior of SettingCommand to have\r\nits primary method take in a fully initialized Environment for\r\nelasticsearch instead of just a map of settings. All of the subclasses\r\nof SettingCommand already did this at some point, so this just removes\r\nduplication. Internal: Refactor SettingCommand into EnvironmentAwareCommand >>> 1"
678,This commit adds unit tests for the `toXContent()` methods of the inner classes  `ReplicationResponse.ShardInfo` and `ReplicationResponse.ShardInfo.Failure`. Add unit tests for toXContent methods in ReplicationResponse >>> 1
679,"Sequence BWC logic consists of two elements:\r\n\r\n1) Wire level BWC using stream versions.\r\n2) A changed to the global checkpoint maintenance semantics.\r\n\r\nFor the sequence number infra to work with a mixed version clusters, we have to consider situation where the primary is on an old node and replicas are on new ones (i.e., the replicas will receive operations without seq#) and also the reverse (i.e., the primary sends operations to a replica but the replica can't process the seq# and respond with local checkpoint). An new primary with an old replica is a rare because we do not allow a replica to recover from a new primary. However, it can occur if the old primary failed and a new replica was promoted or during primary relocation where the source primary is treated as a replica until the master starts the target.\r\n\r\n1) Old Primary & New Replica - this case is easy as is taken care of by the wire level BWC. All incoming requests will have their seq# set to `UNASSIGNED_SEQ_NO`, which doesn't confuse the local checkpoint logic (keeping it at `NO_OPS_PERFORMED`) \r\n2) New Primary & Old replica - this one is trickier as the global checkpoint service currently takes all in sync replicas into consideration for the global checkpoint calculation. In order to deal with old replicas, we change the semantics to say all *new node* in sync replicas. That means the replicas on old nodes don't count for the global checkpointing. In this state the seq# infra is not fully operational (you can't search on it, because copies may miss it) but it is maintained on shards that can support it. The old replicas will have to go through a file based recovery at some point and will get the seq# information at that point. There is still an edge case where a new primary fails and an old replica takes over. I'lll discuss this one with @ywelsch as I prefer to avoid it completely.\r\n\r\nThis PR also re-enables the BWC tests which were disabled. As such it had to fix any BWC issue that had crept in. Most notably an issue with the removal of the `timestamp` field in #21670, which I marked with nocommit. The problem with the removal is that old nodes still [expect a non-null field value](https://github.com/elastic/elasticsearch/blob/5.x/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java#L195) . I considered serializing the current time stamp but that causes indexing requests to different shards to be potentially different. The two other options I saw was to always send a crazy but valid value (i.e., 0) or add the logic to 5.x to deal with `null` properly. I currently opted for the first one. I'll reach out to @jpountz to discuss this one further.\r\n\r\nLast - I added some debugging tools like more sane node names and forcing replication request to implement a `toString` Add BWC layer to seq no infra and enable BWC tests >>> 1"
680,"The JSON processor has an optional field called ""target_field"".\r\nIf you don't specify target_field then target_field becomes what you specified as ""field"".\r\nThere isn't anyway to add the fields to the root of a document. By\r\nsetting `add_to_root`, now serialized fields will be inserted into the\r\ntop-level fields of the ingest document.\r\n\r\nCloses #21898. Enables the ability to inject serialized json fields into root of document >>> 1"
681,"If the response comes back with a content type with a `null`\r\ncharset we were blindly using it, causing `NullPointerException`s.\r\n\r\nCloses #22190 Don't use null charset in RequestLogger >>> 1"
682,"Sends the `error_trace` parameter with all requests sent by the\r\nyaml test framework, including the doc snippet tests. This can be\r\noverridden by settings `error_trace: false`. While this drift's\r\ncore's handling of the yaml tests from the client's slightly this\r\nshould only be a problem if we have a test that tests Elasticsearch's\r\ndefault value for `error_trace` which we don't. And the extra data\r\nwhen debugging a failure is very welcome.\r\n\r\nThis also escapes `\n` and `\t` in the `Stash dump on failure` so\r\nthe `stack_trace` is more readable.\r\n\r\nAlso fixes `RestUpdateSettingsAction` to not think of the `error_trace`\r\nparameter as a setting. Send error_trace by default when testing >>> 1"
683,"Some expert users like UnicastZenPing today establishes real connections to nodes during it's ping\r\nphase that can be used by other parts of the system. Yet, this is potentially dangerous\r\nand undesirable unless the nodes have been fully verified and should be connected to in the\r\ncase of a cluster state update or if we join a newly elected master. For usecases like this, this change adds the infrastructure to manually handle connections that are not _publicly_  available on the node ie. should not be managed by Transport/TransportSerivce Add infrastructure to manage network connections outside of Transport/TransportService >>> 1"
684,"This PR removes all leniency in the conversion of Strings to booleans: ""true"" is converted to the boolean value `true`, ""false"" is converted to the boolean value `false`. Everything else raises an error.\r\n\r\nThis applies to:\r\n\r\n* All settings (index settings, cluster settings)\r\n* Boolean values in the REST API\r\n* Documents\r\n\r\nExceptions are:\r\n\r\n* For indices created before Elasticsearch 6, we still apply the old coercion logic (but we add deprecation logging).\r\n* We allow to omit the value ""true"" for request parameters in the REST API, i.e. `?pretty` is interpreted as `?pretty=true`. \r\n\r\nNoteworthy / to discuss:\r\n\r\n* Bucket term aggregations on boolean fields continue to use `1` and `0` for the `key`, and the strings `""true""` and `""false""` for the `key_as_string` (I think that's fine).\r\n* Existing index templates can be read after an upgrade *but* creating a new index based on an index template with invalid boolean values will fail (see below for a discussion of the tradeoffs).\r\n\r\nExample scenario (the `lowercase` field of `my_custom_pattern_analyzer` is a boolean field):\r\n\r\n1. Create the following index template on Elasticsearch 5.x:\r\n\r\n```json\r\nDELETE /_template/default_template\r\n\r\nPUT /_template/default_template\r\n{\r\n  ""template"": ""*"",\r\n    ""settings"": {\r\n        ""analysis"": {\r\n            ""analyzer"": {\r\n                ""my_custom_pattern_analyzer"": {\r\n                    ""type"": ""pattern"",\r\n                    ""lowercase"": 0\r\n                }\r\n            }\r\n        }    \r\n    }  \r\n}\r\n```\r\n\r\n2. Upgrade to Elasticsearch 6\r\n\r\n3. Create the following index:\r\n\r\n```\r\nDELETE /test\r\n\r\nPUT /test\r\n```\r\n\r\nResult: Creating the index will fail, because the `lowercase` setting is invalid for a 6.x index.\r\n\r\nAnalysis: When a new index is created, `MetaDataCreateIndexService` will load all matching index templates, merge them settings and apply request settings overrides. By the time the index is created we have no clue where the affected setting came from and before that we don't have enough information to know a value's type.\r\n\r\nDiscussion:\r\n\r\nI can think of the following options to address this:\r\n\r\n1. Keep the code as it is but highlight this specific behavior in the migration docs (and the migration plugin?) to ensure that users are aware and can take appropriate action before the upgrade.\r\n2. Be lenient when we find a matching index template (I'd rather avoid this).\r\n3. Add additional metadata to index templates to store the Elasticsearch version when an index template has been created. Also reimplement the merging logic so we know a setting's origin.\r\n\r\nTo me, option 1 seems the only feasible one (and the doc update is contained in the PR) but I want to put that up for discussion. Make boolean conversion strict >>> 1"
685,As per https://github.com/elastic/elasticsearch/pull/15372 `indices.recovery.concurrent_streams` was replaced by `cluster.routing.allocation.node_concurrent_recoveries`\r\n\r\nCloses #22001 [DOCS]  Remove indices.recovery.concurrent_streams from breaking changes >>> 1
686,nan Don't print empty indices_boost >>> 1
687,"This PR makes mapping updates atomic when multiple types in an index are updated. Mappings for an index are now applied in a single atomic operation, which also allows to optimize some of the cross-type updates and checks. Atomic mapping updates across types >>> 1"
688,Some of our stats serialization code duplicates complicated serialization logic\r\nor could use existing building blocks from StreamOutput/Input. This commit\r\ncleans up some of the serialization code. Cleanup random stats serialization code >>> 1
689,"This commit touches addresses issues related to recovery and sequence numbers:\r\n - A sequence number can be assigned and a Lucene commit created with a\r\n   maximum sequence number at least as large as that sequence number,\r\n   yet the operation corresponding to that sequence number can be\r\n   missing from both the Lucene commit and the translog. This means that\r\n   upon recovery the local checkpoint will be stuck at or below this\r\n   missing sequence number. To address this, we force the local\r\n   checkpoint to the maximum sequence number in the Lucene commit when\r\n   opening the engine. Note that there can still be gaps in the history\r\n   in the translog but we do not address those here.\r\n - The global checkpoint is transferred to the target shard at the end\r\n   of peer recovery.\r\n - Additionally, we reenable the relocation integration tests.\r\n\r\nLastly, this work uncovered some bugs in the assignment of sequence\r\nnumbers on replica operations:\r\n - setting the sequence number on replica write requests was missing,\r\n   very likely introduced as a result of resolving merge conflicts\r\n - handling operations that arrive out of order on a replica and have a\r\n   version conflict with a previous operation were never marked as\r\n   processed\r\n\r\nRelates #10708 Tighten sequence numbers recovery >>> 1"
690,"This commit adds the parsing fromXContent() methods to the `IndexResponse` class. The method is based on a `ObjectParser` because it is easier to reuse parsing method from parent abstract classes like `DocWriteResponse`.\r\n    \r\nIt also changes a bit the toXContent() method of `ReplicationResponse.ShardInfo` so that it now starts its own object. This way, the `ShardInfo.fromXContent()` method can be directly referenced by the IndexResponse's ObjectParser.\r\n Add fromxcontent methods to index response >>> 1"
691,"With this commit, we introduce a cache to the geoip ingest processor.\r\nThe cache is enabled by default and caches the 1000 most recent items.\r\nThe cache size is controlled by the setting `ingest.geoip.cache_size`.\r\n\r\nCloses #22074 Cache results of geoip lookups >>> 1"
692,With this commit we enable the Jackson feature 'STRICT_DUPLICATE_DETECTION'\r\nby default for all XContent types (not only JSON).\r\n\r\nWe have also changed the name of the system property to disable this feature\r\nfrom `es.json.strict_duplicate_detection` to the now more appropriate name\r\n`es.xcontent.strict_duplicate_detection`.\r\n\r\nRelates elastic/elasticsearch#19614\r\nRelates elastic/elasticsearch#22073 Enable strict duplicate checks for all XContent types >>> 1
693,With this commit we change the data type of the 'TIMESTAMP'\r\nmeta-data field from a formatted date string to a plain\r\n`java.util.Date` instance. The main reason for this change is\r\nthat our benchmarks have indicated that this contributes\r\nsignificantly to the time spent in the ingest pipeline.\r\n\r\nThe overhead in terms of indexing throughput of the ingest\r\npipeline is about 15% and breaks down roughly as follows:\r\n\r\n* 5% overhead caused by the conversion from `XContent` -> `Map`\r\n* 5% overhead caused by the timestamp formatting (eliminated by this PR)\r\n* 5% overhead caused by the conversion `Map` -> `XContent`\r\n\r\nRelates #22074 Change type of ingest doc meta-data field 'TIMESTAMP' to `Date` >>> 1
694,It returns whether the last merged mapping has `_all` enabled rather than\r\nwhether any of the types has `_all` enabled. Fix MapperService.allEnabled(). >>> 1
695,This commit adds the parsing methods `fromXContent()` to the `ReplicationResponse.ShardInfo` and `ReplicationResponse.ShardInfo.Failure` classes.\r\n\r\nNote that it requires some utility methods from #22082 Add fromXContent() methods for ReplicationResponse >>> 1
696,"In some cases, it might happen that the `_all` field gets a field type that is\r\nnot totally configured, and in particular lacks analyzers. This is due to the\r\nfact that `AllFieldMapper.TypeParser.getDefault` uses `Defaults.FIELD_TYPE` as\r\na default field type, which does not have any analyzers configured since it\r\ndoes not know about the default analyzers. The `_all` default mapper is not completely configured. >>> 1"
697,"When using a bulk processor in test, you might write something like:\r\n\r\n```java\r\nBulkProcessor bulkProcessor = BulkProcessor.builder(client, new BulkProcessor.Listener() {\r\n    @Override public void beforeBulk(long executionId, BulkRequest request) {}\r\n    @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {}\r\n    @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) {}\r\n})\r\n        .setBulkActions(10000)\r\n        .setFlushInterval(TimeValue.timeValueSeconds(10))\r\n        .build();\r\n\r\nfor (int i = 0; i < 10000; i++) {\r\n    bulkProcessor.add(new IndexRequest(""foo"", ""bar"", ""doc_"" + i)\r\n            .source(jsonBuilder().startObject().field(""foo"", ""bar"").endObject()\r\n    ));\r\n}\r\n\r\nbulkProcessor.flush();\r\nclient.admin().indices().prepareRefresh(""foo"").get();\r\nSearchResponse response = client.prepareSearch(""foo"").get();\r\n// response does not contain any hit\r\n```\r\n\r\nThe problem is that by default bulkProcessor defines the number of concurrent requests to 1 which is using behind the scene an Async BulkRequestHandler.\r\nWhen you call `flush()` in a test, you expect it to flush all the content of the bulk so you can search for your docs.\r\nBut because of the async handling, there is a great chance that none of the documents has been indexed yet when you call the `refresh` method.\r\n\r\nWe should advice in our Java guide to explicitly set concurrent requests to `0` so users will use behind the scene the Sync BulkRequestHandler.\r\n\r\n```java\r\nBulkProcessor bulkProcessor = BulkProcessor.builder(client, new BulkProcessor.Listener() {\r\n    @Override public void beforeBulk(long executionId, BulkRequest request) {}\r\n    @Override public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {}\r\n    @Override public void afterBulk(long executionId, BulkRequest request, Throwable failure) {}\r\n})\r\n        .setBulkActions(5000)\r\n        .setFlushInterval(TimeValue.timeValueSeconds(10))\r\n        .setConcurrentRequests(0)\r\n        .build();\r\n```\r\n\r\nCloses #22158. Explain how to use bulk processor in a test context >>> 1"
698,"Subclasses of TransportReplicationAction can currently chose to implement block levels for which the request will be blocked.\r\n\r\n- Refresh/Flush was using the block level METADATA_WRITE although they don't operate at the cluster meta data level (but more like shard level meta data which is not represented in the block levels). Their level has been changed to `null` so that they can operate freely in the presence of blocks.\r\n- GlobChkptSync was using WRITE although it does not make any changes to the actual documents of a shard. The level has been changed to `null` so that it can operate freely in the presence of blocks.\r\n\r\nThe PR also adds a check for closed indices in TRA so that the right exception is thrown if refresh/flush/checkpoint syncing is attempted on a closed index (before it was throwing an IndexNotFoundException, now it's throwing IndexClosedException).\r\n\r\nAn alternative to the approach taken here would be to introduce a new block level that represents writing shard ""metadata"" that does not change the actual documents. Use correct block levels for TRA subclasses >>> 1"
699,"LatLonPoint polygon query in Lucene 6 supports multipolygons and polygons with holes. This PR exposes this capability through the ES query DSL by adding `multipolygon` support to the `GeoPolygonQueryBuilder` parser such that the following can now be parsed:\r\n\r\n```javascript\r\n""query"" : {\r\n  ""geo_polygon"" : {\r\n    ""track.location"" :{\r\n      ""multipolygon"": [ [ [\r\n        [-70, 40],\r\n        [-80, 30],\r\n        [-90, 20]\r\n      ] ] ]\r\n    }\r\n  }\r\n}\r\n```\r\nTo maintain backcompat the following is still supported:\r\n\r\n```javascript\r\n""query"" : {\r\n  ""geo_polygon"" : {\r\n    ""track.location"" :{\r\n      ""points"": [ \r\n        [-70, 40],\r\n        [-80, 30],\r\n        [-90, 20]\r\n      ] \r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe `GeoPolygonQueryBuilder` now creates a `Polygon` object and passes it to `LatLonPoint.newPolygonQuery`. \r\n\r\nNOTE:\r\n\r\n* Performs minimal polygon validation (e.g., enforcing minimum number of points, validating coordinates).\r\n* Dateline crossing is not supported. \r\n\r\nTODO:\r\n\r\n* update docs with multipolygon support\r\n\r\ncloses #20789 Add multipolygon support to GeoPolygonQueryBuilder >>> 0"
700,"Today in the codebase we refer to seccomp everywhere instead of system\r\ncall filter even if we are not specifically referring to Linux. This\r\ncommit is a purely mechanical change to refer to system call filter\r\nwhere appropriate instead of the general seccomp, and only leaves\r\nseccomp in place when actually referring to the Linux implementation.\r\n\r\nRelates #22226\r\n Refer to system call filter instead of seccomp >>> 1"
701,In #22094 we introduce a test-only setting to simulate transport\r\nimpls that don't support handshakes. This commit implements the same logic\r\nwithout a setting. Remove `doHandshake` test-only settings from TcpTransport >>> 1
702,This commit exposes public getters for the aggregations in\r\nAggregatorFactories.Builder. The reason is that it allows to\r\nparse the aggregation object from elsewhere (e.g. a plugin) and then\r\nbe able to get the aggregation builders in order to set them in\r\na SearchSourceBuilder.\r\n\r\nThe alternative would have been to expose a setter for the\r\nAggregatorFactories.Builder object. But that would be making\r\nthe API a bit trappy. Allow setting aggs after parsing them elsewhere >>> 1
703,"Today if a settings object has many keys ie. if somebody specifies\r\na gazillion synonyms in-line (arrays are keys ending with ordinals) operations like\r\n`Settings#getByPrefix` have a linear runtime. This can cause index creations to be\r\nvery slow producing lots of garbage at the same time. Yet, `Settings#getByPrefix` is called\r\nquite frequently by group settings etc. which can cause heavy load on the system.\r\n\r\nWhile it's not recommended to have synonym lists with 25k entries in-line these use-cases should\r\nnot have such a large impact on the cluster / node. This change introduces a view-like map\r\nthat filters based on the prefixes referencing the actual source map instead of copying all values\r\nover and over again. A benchmark that adds a single key with 25k random synonyms between 2 and 5 chars\r\ntakes 16 seconds to get the synonym prefix 200 times while the filtered view takes 4 ms for the 200 iterations.\r\n\r\nThis relates to https://discuss.elastic.co/t/200-cpu-elasticsearch-5-index-creation-very-slow-with-a-huge-synonyms-list/69052\r\n Speed up filter and prefix settings operations >>> 1"
704,"This PR completes the refactoring of the cluster allocation explain API and improves it in the following two high-level ways:\r\n\r\n1. The explain API now uses the same allocators that the `AllocationService` uses to make shard allocation decisions.  Prior to this PR, the explain API would run the deciders against each node for the shard in question, but this was not executed on the same code path as the allocators, and many of the scenarios in shard allocation were not captured due to not executing through the same code paths as the allocators.\r\n\r\n2. The APIs have changed, both on the Java and JSON level, to accurately capture the decisions made by the system.  The APIs also now report on shard moving and rebalancing decisions, whereas the previous API did not report decisions for moving shards which cannot remain on their current node or rebalancing shards to form a more balanced cluster.\r\n\r\nNote: this change affects plugin developers who may have a custom implementation of the `ShardsAllocator` interface.  The method `weighShards` has been removed and no longer has any utility.  In order to support the new explain API, however, a custom implementation of `ShardsAllocator` must now implement `ShardAllocationDecision decideShardAllocation(ShardRouting shard, RoutingAllocation allocation)` which provides a decision and explanation for allocating a single shard.   For implementations that do not support explaining a single shard allocation via the cluster allocation explain API, this method can simply return an `UnsupportedOperationException`.\r\n\r\nRelates #21691 #21662 #21103 #20634 #20347 Cluster Explain API uses the allocation process to explain shard allocation decisions >>> 1"
705,"Problem: So far all rank eval requests are being executed in parallel. If there\r\nare more than the search thread pool can handle, or if there are other search\r\nrequests executed in parallel rank eval can fail.\r\n\r\nSolution: Make number of max_concurrent_searches configurable.\r\n\r\nName of configuration parameter is analogous to msearch. Default\r\nmax_concurrent_searches set to 10: Rank_eval isn't particularly time critical so\r\ntrying to avoid being more clever than probably needed here. Can set this value\r\nthrough the API to a higher value anytime.\r\n\r\nFixes #21403 Make maximum number of parallel search requests configurable. >>> 1"
706,Problem: We introduced the ability to shorten the rank eval request by using a\r\ntemplate in #20231. When playing with the API it turned out that there might be\r\nuse cases where - e.g. due to various heuristics - folks might want to translate\r\nthe original user query into more than just one type of Elasticsearch query.\r\n\r\nProposed solution: Give each template an id that can later be referenced in the\r\nactual requests.\r\n\r\nCloses #21257 Support specifying multiple templates >>> 1
707,As per https://github.com/elastic/elasticsearch/pull/22192#discussion_r92999532 this adds a mutation based test to RankEvalSpec RankEvaluation: Add mutation based testing to RankEvalSpec >>> 1
708,"Closes #22149\r\n\r\nThis commit is adding an ability to remove pipelines with wildcards. I used a similar approach as in templates. When deleting using single asterisk it won't throw the exception. If the user specifies a name of the pipeline which cannot be found, the exception is thrown (included unit test). Added ability to remove pipelines via wildcards (#22149) >>> 1"
709,"Our `float`/`double` fields generally assume that `-0` compares less than `+0`,\r\nexcept when bounds are exclusive: an exclusive lower bound on `-0` excludes\r\n`+0` and an exclusive upper bound on `+0` excludes `-0`.\r\n\r\nCloses #22167 Make `-0` compare less than `+0` consistently. >>> 1"
710,"The way aggregations on scripts work is by hiding scripts behind the same API\r\nthat we use for regular fields. However, there is no native support for boolean\r\nfields, those need to be exposed as integers, with `0` standing for `false` and\r\n`1` for true.\r\n\r\nRelates #20941 Allow terms aggregations on pure boolean scripts. >>> 1"
711,"In pre 2.x versions, if the repository was set to compress snapshots,\r\nthen snapshots would be compressed with the LZF algorithm.  In 5.x,\r\nElasticsearch no longer supports the LZF compression algorithm.  This\r\npresents an issue when retrieving snapshots in a repository or upgrading\r\nrepository data to the 5.x version, because Elasticsearch throws an\r\nexception when it tries to read the snapshot metadata because it was\r\ncompressed using LZF.\r\n\r\nThis commit gracefully handles the situation by introducing a new\r\nincompatible-snapshots blob to the repository.  For any pre-2.x snapshot\r\nthat cannot be read, that snapshot is removed from the list of active\r\nsnapshots, because the snapshot could not be restored anyway.  Instead,\r\nthe snapshot is recorded in the incompatible-snapshots blob.  When\r\nlisting snapshots, both active snapshots and incompatible snapshots will\r\nbe listed, with incompatible snapshots showing a `INCOMPATIBLE` state.\r\nAny attempt to restore an incompatible snapshot will result in an\r\nexception. Gracefully handles pre 2.x compressed snapshots >>> 1"
712,Also adds some initial tests for some aggregations.\r\n\r\nFirst PR for #22278. Add base class for writing unit test for aggregations >>> 1
713,"This changes SuggestionBuilder from extending ToXContentToBytes to implementing the ToXContent interface only. The former could lead to unexpected behaviour when\r\ntrying to display instances of the object, since the inherited `toString()` method would create an error message because the SuggestionBuilders toXContent() methods only render json fragments (see #22264 as an example).\r\n SuggestionBuilder doesn't need to extend ToXContentToBytes >>> 1"
714,"#22191 added the ability for wildcarding but is missing docs, this PR aims to reflect the new API. [docs] update ingest-node delete docs to mention wildcarding >>> 1"
715,This adds test classes that can be used to test the wire serialisation and (optionally) the XContent serialisation of objects that implement Streamable/Writeable and ToXContent.\r\n\r\nThese test classes will enable classes such as InternalAggregation (or at least its implementations) to be tested in a consistent way when is comes to testing serialisation. Adds abstract test classes for serialisation >>> 1
716,"The `UnicastZenPing` shows it's age and is the result of many small changes. The current state of affairs is confusing and is hard to reason about. This PR cleans it up (while following the same original intentions). Highlights of the changes are:\r\n\r\n1) Clear 3 round flow - no interleaving of scheduling.\r\n2) The previous implementation did a best effort attempt to wait for ongoing pings to be sent and completed. The pings were guaranteed to complete because each used the total ping duration as a timeout. This did make it hard to reason about the total ping duration and the flow of the code. All of this is removed now and ping should just complete within the given duration or not be counted (note that it was very handy for testing, but I move the needed sync logic to the test).\r\n3) Because of (2) the pinging scheduling changed a bit, to give a chance for the last round to complete. We now ping at the beginning, 1/3 and 2/3 of the duration.\r\n4) To offset for (3) a bit, incoming ping requests are now added to on going ping collections.\r\n5) UnicastZenPing never establishes full blown connections (but does reuse them if there). Relates to #22120\r\n6) Discovery host providers are only used once per pinging round. Closes #21739\r\n7) Usage of the ability to open a connection without connecting to a node ( #22194 ) and shorter connection timeouts helps with connections piling up. Closes #19370\r\n8) Beefed up testing and sped them up.\r\n9) removed light profile from production code  Simplify Unicast Zen Ping >>> 1"
717,"added `ignore_missing` flag to:\r\n\r\n- Attachment Processor\r\n- GeoIP Processor\r\n- User-Agent Processor\r\n\r\n\r\nThis is an oversight, and should have been done alongside the other ingest-common processors a while back. add `ignore_missing` flag to ingest plugins >>> 1"
718,Now you can parse field values of the `key=value` variety and have\r\n`key` be inserted as a field name in an ingest document.\r\n\r\nCloses #22222. introduce KV Processor in Ingest Node >>> 1
719,The allocation decider explanation messages where improved in #21771 to\r\ninclude the specific Elasticsearch setting that contributed to the\r\ndecision taken by the decider.  This commit improves upon the\r\nexplanation message output by including whether the setting was an index\r\nlevel setting or a cluster level setting.  This will further help the\r\nuser understand and locate the setting that is the cause of shards\r\nremaining unassigned or remaining on their current node. Adds setting level to allocation decider explanations >>> 1
720,"If we conditionally do random things, e.g. initialize a node only after the first test, we have to make sure that we unconditionally create a new seed calling `random.nextLong()`, then initialize the node under a private randomness context. This makes sure that any random usage through `Randomness.get()` will retrieve the proper random instance through `RandomizedContext.current().getRandom()`. When running under private randomness, the context  will return the `Random` instance that was created with the provided seed (forked from the main random instance) rather than the main `Random` that's exposed to tests as well. Otherwise tests become non repeatable because that initialization part happens only before the first executed test. [TEST] Improve ESSingleNodeTestCase repeatability >>> 1"
721,"This change makes it possible for custom routing values to go to a subset of shards rather than\r\njust a single shard. This enables the ability to utilize the spatial locality that custom routing can\r\nprovide while mitigating the likelihood of ending up with an imbalanced cluster or suffering\r\nfrom a hot shard.\r\n\r\nThis is ideal for large multi-tenant indices with custom routing that suffer from one or both of\r\nthe following:\r\n- The big tenants cannot fit into a single shard or there is so many of them that they will likely\r\nend up on the same shard\r\n- Tenants often have a surge in write traffic and a single shard cannot process it fast enough\r\n\r\nBeyond that, this should also be useful for use cases where most queries are done under the context\r\nof a specific field (e.g. a category) since it gives a hint at how the data can be stored to minimize\r\nthe number of shards to check per query. While a similar solution can be achieved with multiple\r\nconcrete indices or aliases per value today, those approaches breakdown for high cardinally fields.\r\n\r\nA partitioned index enforces that mappings have routing required, that the partition size does not\r\nchange when shrinking an index (the partitions will shrink proportionally), and rejects mappings\r\nthat have parent/child relationships.\r\n\r\nCloses #21585 Allow an index to be partitioned with custom routing >>> 1"
722,"This is related to #22116. In order to remove SocketPermissions from core we need to isolate some code that can be given SocketPermissions. The mocksocket jar includes wrapped versions of Socket and SocketServer. This is one step towards migrating permissions out of core. Replace Socket, ServerSocket, and HttpServer usages in tests with mocksocket versions >>> 1"
723,"Replaces `IndicesQueriesRegistry` with` XContentParser#namedObject`.\r\n\r\nThis is the first of the real payoff for https://github.com/elastic/elasticsearch/pull/22003, one less thing to pass around the entire application. Now the parsers it contained are wrapped in the `XContentParser`. Replace IndicesQueriesRegistry >>> 1"
724,- checks for index-out-of-bounds\r\n- added unit tests for failed `field_split` and `value_split` scenarios\r\n\r\nmissed this test in #22272. fix index out of bounds error in KV Processor >>> 1
725,"Currently, stored scripts use a namespace of (lang, id) to be put, get, deleted, and executed.  This is not necessary since the lang is stored with the stored script.  A user should only have to specify an id to use a stored script.  This change makes that possible while keeping backwards compatibility with the previous namespace of (lang, id).  Anywhere the previous namespace is used will log deprecation warnings.  The goal is to backport this change to 5.2 and then remove the previous namespace altogether in 6.0 giving users the opportunity to change their scripts to the new namespace.\r\n\r\nWhy is this change so large?\r\n\r\nThis change seems huge, but it's not really.  The main files changed are ScriptMetaData and ScriptService with StoredScriptSource being added.  Secondarily, most of the Request files have changed a bit to support the new/old namespaces.  Many lines that have changed are added JavaDocs and some new tests.  Also maintaining back compat eats up some lines for XContent and the wire format.\r\n\r\nWhat is the new behavior?\r\n\r\nWhen a user specifies a stored script, that script will be stored under both the new namespace and old namespace.\r\n\r\nTake for example script 'A' with lang 'L0' and data 'D0'.  If we add script 'A' to the empty set, the scripts map will be [""A"" -- D0, ""A#L0"" -- D0].  If a script 'A' with lang 'L1' and data 'D1' is then added, the scripts map will be [""A"" -- D1, ""A#L1"" -- D1, ""A#L0"" -- D0].\r\n\r\nWhen a user deletes a stored script, that script will be deleted from both the new namespace (if it exists) and the old namespace.\r\n\r\nTake for example a scripts map with {""A"" -- D1, ""A#L1"" -- D1, ""A#L0"" -- D0}. If a script is removed specified by an id 'A' and lang null then the scripts map will be {""A#L0"" -- D0}.  To remove the final script, the deprecated namespace must be used, so an id 'A' and lang 'L0' would need to be specified.\r\n\r\nWhen a user gets/executes a stored script, if the new namespace is used then the script will be retrieved/executed using only 'id', and if the old namespace is used then the script will be retrieved/executed using 'id' and 'lang' Change Namespace for Stored Script to Only Use Id >>> 1"
726,"Relates to Issue #22294\r\n\r\nAdd a Note to the foreach processor for processing an array of attachments, as this is required to be able to process an array of attachments.\r\n Add link to foreach processor to ingest-attachment.asciidoc >>> 1"
727,Relates to Issue #22294\r\n\r\nAdded an example to the foreach processor when dealing with attachments noting the requirement to use the target_field option with the attachment processor to ensure the properties extracted from the attachment are added in the correct location.\r\n Add an attachment foreach example to ingest-node.asciidoc >>> 0
728,This is coming from this thread on discuss: https://discuss.elastic.co/t/ingest-attachment-plugin-exception/69167/10\r\n Adds more information about ingest attachment properties extraction >>> 1
729,Don't inline inner hits if the query the inner hits is inlined into can't resolve mappings and ignore_unmapped has been set to true.\r\n\r\nCloses #21620 Inner hits and ignore unmapped >>> 1
730,"As the translog evolves towards a full operations log as part of the\r\nsequence numbers push, there is a need for the translog to be able to\r\nrepresent operations for which a sequence number was assigned, but the\r\noperation did not mutate the index. Examples of how this can arise are\r\noperations that fail after the sequence number is assigned, and gaps in\r\nthis history that arise when an operation is assigned a sequence number\r\nbut the operation never completed (e.g., a node crash). It is important\r\nthat these operations appear in the history so that they can be\r\nreplicated and replayed during recovery as otherwise the history will be\r\nincomplete and local checkpoints will not be able to advance. This\r\ncommit introduces a no-op to the translog to set the stage for these\r\nefforts.\r\n\r\nRelates #10708\r\n Introduce translog no-op >>> 1"
731,"The deprecation warning gives now the same message as 5.x. The deprecation warning was previously removed, but given that we are still lenient with old indices we should still output the warning. Restore deprecation warning for invalid match_mapping_type values >>> 1"
732,"Today we ship with default jvm.options for server Elasticsearch that\r\nprevents Netty from using some unsafe optimizations. Yet, the settings\r\ndo nothing for the transport client since it is embedded in other\r\napplications that will not read and use those settings. This commit adds\r\nthese settings for the transport client, and is done so in a way that\r\nstill enables users to go unsafe if they want to go unsafe (they\r\nshouldn't, but the option is there). Tell Netty not to be unsafe in transport client >>> 1"
733,As per issue #22306 I am submitting PR for eu-west-london support for discovery-ec2 and repository-s3.\r\n\r\nThanks! \r\nNic\r\n Support for eu-west-2 (London) cloud-aws plugin >>> 1
734,"Today we only expose `value_type` in scriptable aggregations, however it is\r\nalso useful with unmapped fields. I suspect we never noticed because\r\n`value_type` was not documented (fixed) and most aggregations are scriptable.\r\n\r\nCloses #20163 `value_type` is useful regardless of scripting. >>> 1"
735,"Currently we only apply date detection on strings that contain either `:`, `-`\r\nor `/`. This commit inverses the heuristic in order to only apply date detection\r\non strings that are not parseable as a number, so that more date formats can be\r\nused as dynamic dates formats.\r\n\r\nCloses #1694 Date detection should not rely on a hardcoded set of characters. >>> 1"
736,Added a new section detailing how to use the attachment processor\r\nwithin an array.\r\n\r\nThis reverts commit #22296 and instead links to the foreach processor.\r\n\r\nNote this invalidates the previous pull request (#22295) which was made against the wrong branch.\r\n Add ingest-attachment-with-arrays section to ingest attachments doc >>> 1
737,"Today if an older version of a plugin exists, we fail to notify the user\r\nwith a helpful error message. This happens because during plugin\r\nverification, we attempt to read the plugin descriptors for all existing\r\nplugins. When an older version of a plugin is sitting on disk, we will\r\nattempt to read this old plugin descriptor and fail due to a version\r\nmismatch. This leads to an unhelpful error message. Instead, we should\r\ncheck for existence of the plugin as part of the verification phase, but\r\nbefore attempting to read plugin descriptors for existing plugins. This\r\nenables us to provide a helpful error message to the user.\r\n\r\nRelates #22084\r\n Provide helpful error message if a plugin exists >>> 1"
738,"* Remove a checked exception, replacing it with `ParsingException`.\r\n* Remove all Parser classes for the yaml sections, replacing them with static methods.\r\n* Remove `ClientYamlTestFragmentParser`. Isn't used any more.\r\n* Remove `ClientYamlTestSuiteParseContext`, replacing it with some static utility methods.\r\n\r\nI did not rewrite the parsers using `ObjectParser` because I don't think it is worth it right now. Remove much ceremony from parsing client yaml test suites >>> 1"
739,"We don't *want* to use negative numbers with `writeVLong`\r\nso throw an exception when we try. On the other\r\nhand unforeseen bugs might cause us to write negative numbers (some versions of Elasticsearch don't have the exception, only an assertion)\r\nso this fixes `readVLong` so that instead of reading a wrong\r\nvalue and corrupting the stream it reads the negative value.\r\n Support negative numbers in readVLong >>> 1"
740,"Today we try to pull stats from index writer but we do not get a\r\nconsistent view of stats. Under heavy indexing, this inconsistency can\r\nbe very skewed indeed. In particular, it can lead to the number of\r\ndeleted docs being reported as negative and this leads to serialization\r\nissues. Instead, we should provide a consistent view of the stats by\r\nusing an index reader.\r\n\r\nCloses #22285 Use reader for doc stats >>> 1"
741,"`ShardCoreKeyMap.add` is called on each segment for all search requests, which\r\nmeans it might become a bottleneck under a cocurrent load of cheap search\r\nrequests since this method acquires a mutex. This change proposes to use a\r\n`ConcurrentHashMap` which allows to only take the mutex in the case that the\r\n`LeafReader` has never been seen before. Improve concurrency of ShardCoreKeyMap. >>> 1"
742,"Adds an `intersection_buckets` property to `filters` agg so that keyed filters ""A"", ""B"" and ""C"" would also return buckets for the intersections of these sets i.e. ""A&B"", ""A&C"" and ""B&C"".\r\n\r\nSome areas that need work are:\r\n* Efficiency tweaks:\r\n   * Currently lacks an option to trim empty intersections (of which there may be many)\r\n   * No safety-limit on combinatorial explosions (maybe we say no more than 100 filter keys?)\r\n   * Option to filter single-value results e.g. `key1` if you only want intersections (key1&key2)\r\n* Key-naming strategy is ampersand-ed pairs e.g. `key1&key2`. Is this OK?\r\n* Docs\r\n* More tests\r\n\r\nBut it would be good to review this approach before I take it further @colings86 @jpountz \r\n\r\nCloses #22169 Initial version of an adjacency matrix using the Filters aggregation >>> 0"
743,"This PR factors out the cluster state update tasks that are published to the nodes from those that are not, serving as a basis for future refactorings to separate the publishing mechanism out of cluster service. Separate cluster update tasks that are published from those that are not >>> 1"
744,Does pretty much what the title says.\r\n\r\nOptimistically check for `tag` of an unknown processor for better tracking of which \r\nprocessor declaration is to blame in an invalid configuration.\r\n\r\nCloses #21429. Adds ingest processor headers to exception for unknown processor. >>> 1
745,"Recoveries are tracked on the target node using RecoveryTarget objects that are kept in a RecoveriesCollection. Each recovery has a unique id that is communicated from the recovery target to the source so that it can call back to the target and execute actions using the right recovery context.  In case of a network disconnect, recoveries are retried. At the moment, the same recovery id is reused for the restarted recovery. This can lead to confusion though if the disconnect is unilateral and the recovery source continues with the recovery process. If the target reuses the same recovery id while doing a second attempt, there might be two concurrent recoveries running on the source for the same target.\r\n\r\nThis PR changes the recovery retry process to use a fresh recovery id. It also waits for the first recovery attempt to be fully finished (all resources locally freed) to further prevent concurrent access to the shard. Finally, in case of primary relocation, it also fails a second recovery attempt if the first attempt moved past the finalization step, as the relocation source can then be moved to RELOCATED state and start indexing as primary into the target shard (see TransportReplicationAction). Resetting the target shard in this state could mean that indexing is halted until the recovery retry attempt is completed and could also destroy existing documents indexed and acknowledged before the reset.\r\n\r\nRelates to #22043 Use a fresh recovery id when retrying recoveries >>> 1"
746,"This commit adds a document describing our data replication model in high level terms. The goal is give people basic insight into how things work in order to better understand how read and writes interact, both during normal operations and under failures.\r\n Add a high level description of ES's data replication model >>> 1"
747,nan Remove deprecated percolate and mpercolate apis >>> 1
748,It looks to me like there is no gradle task that cleans the idea build directory. There are times when wiping out the build directory is very useful. I added a task and made cleanIdea depend on it. Add cleanIdeaBuildDir gradle task that cleans the idea build directory >>> 1
749,"When starting a standalone cluster, we do not able assertions. This is\r\nproblematic because it means that we miss opportunities to catch\r\nbugs. This commit enables assertions for standalone integration tests,\r\nand fixes a couple bugs that were uncovered by enabling these.\r\n\r\n Enable assertions in integration tests >>> 1"
750,"The backwards compatibility tests rely on gradle's built-in mechanisms for resolving dependencies to get the zip of the older version we test against. By default, this will cache snapshots for 24 hours, which can lead to unexpected failures in CI. This change makes the special configurations for backwards compatibility always update their snapshots by setting the amount of time to cache to 0 seconds. build: do not use cached snapshots for backwards compatibility tests >>> 1"
751,Closes #22114 Add documentation for Delete By Query Java API >>> 1
752,Switches custom cluster state components from PROTO-based de-serialization to named objects based de-serialization\r\n\r\nCloses #21868 \r\n\r\nThis PR removes PROTO-based serialization from all custom objects except custom IndexMetaData. Custom IndexMetaData is not currently used in any know code and will be removed in v7.0. I will open a separate PR deprecating it and disabling creation of Custom IndexMetaData for new indices in v6.0. Remove PROTO-based custom cluster state components >>> 1
753,"Before, snapshot/restore would synchronize all operations on the cluster\r\nstate except for deleting snapshots.  This meant that only one\r\nsnapshot/restore operation would be allowed in the cluster at any given\r\ntime, except for deletions - there could be two or more snapshot\r\ndeletions running at the same time, or a deletion could be running,\r\nunbeknowest to the rest of the cluster, and thus a snapshot or restore\r\nwould be allowed at the same time as the snapshot deletion was still in\r\nprogress.  This could cause any number of synchronization issues,\r\nincluding the situation where a snapshot that was deleted could reappear\r\nin the index-N file, even though its data was no longer present in the\r\nrepository.\r\n\r\nThis commit introduces a new custom type to the cluster state to\r\nrepresent deletions in progress.  Now, another deletion cannot start if\r\na deletion is currently in progress.  Similarily, a snapshot or restore\r\ncannot be started if a deletion is currently in progress.  In each case,\r\nif attempting to run another snapshot/restore operation while a deletion\r\nis in progress, a ConcurrentSnapshotExecutionException will be thrown.\r\nThis is the same exception thrown if trying to snapshot while another\r\nsnapshot is in progress, or restore while a snapshot is in progress.\r\n\r\nCloses #19957 Synchronize snapshot deletions on the cluster state >>> 1"
754,"As we have here an ingest processor, we can offer extracting all possible metadata instead of only a small subset.\r\nThat makes even more interesting the ingest processor as it can receive for example a picture and it will be possible to extract much more information than before.\r\n\r\nThis PR adds a new property `raw_metadata` which is not set by default. That means that nothing change for users unless they explicitly ask for `""properties"": [ ""raw_metadata"" ]`.\r\n\r\nFor example:\r\n\r\n```\r\nPUT _ingest/pipeline/attachment\r\n{\r\n  ""description"" : ""Extract all metadata"",\r\n  ""processors"" : [\r\n    {\r\n      ""attachment"" : {\r\n        ""field"" : ""data"",\r\n        ""properties"": [ ""raw_metadata"" ]\r\n      }\r\n    }\r\n  ]\r\n}\r\nPUT my_index/my_type/my_id?pipeline=attachment\r\n{\r\n  ""data"": ""e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=""\r\n}\r\nGET my_index/my_type/my_id\r\n```\r\n\r\ngives back:\r\n\r\n```json\r\n{\r\n  ""found"": true,\r\n  ""_index"": ""my_index"",\r\n  ""_type"": ""my_type"",\r\n  ""_id"": ""my_id"",\r\n  ""_version"": 1,\r\n  ""_source"": {\r\n    ""data"": ""e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0="",\r\n    ""attachment"": {\r\n      ""raw_metadata"": {\r\n        ""X-Parsed-By"": ""org.apache.tika.parser.rtf.RTFParser"",\r\n        ""Content-Type"": ""application/rtf""\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nOf course, much more metadata can be extracted. For example, this is what a `docx` Word document can generate:\r\n\r\n```json\r\n""attachment"": {\r\n  ""raw_metadata"": {\r\n    ""date"": ""2015-02-20T11:36:00Z"",\r\n    ""cp:revision"": ""22"",\r\n    ""Total-Time"": ""6"",\r\n    ""extended-properties:AppVersion"": ""15.0000"",\r\n    ""meta:paragraph-count"": ""1"",\r\n    ""meta:word-count"": ""15"",\r\n    ""dc:creator"": ""Windows User"",\r\n    ""extended-properties:Company"": ""JDI"",\r\n    ""Word-Count"": ""15"",\r\n    ""dcterms:created"": ""2012-10-12T11:17:00Z"",\r\n    ""meta:line-count"": ""1"",\r\n    ""Last-Modified"": ""2015-02-20T11:36:00Z"",\r\n    ""dcterms:modified"": ""2015-02-20T11:36:00Z"",\r\n    ""Last-Save-Date"": ""2015-02-20T11:36:00Z"",\r\n    ""meta:character-count"": ""92"",\r\n    ""Template"": ""Normal.dotm"",\r\n    ""Line-Count"": ""1"",\r\n    ""Paragraph-Count"": ""1"",\r\n    ""meta:save-date"": ""2015-02-20T11:36:00Z"",\r\n    ""meta:character-count-with-spaces"": ""106"",\r\n    ""Application-Name"": ""Microsoft Office Word"",\r\n    ""extended-properties:TotalTime"": ""6"",\r\n    ""modified"": ""2015-02-20T11:36:00Z"",\r\n    ""Content-Type"": ""application/vnd.openxmlformats-officedocument.wordprocessingml.document"",\r\n    ""X-Parsed-By"": ""org.apache.tika.parser.microsoft.ooxml.OOXMLParser"",\r\n    ""creator"": ""Windows User"",\r\n    ""meta:author"": ""Windows User"",\r\n    ""meta:creation-date"": ""2012-10-12T11:17:00Z"",\r\n    ""extended-properties:Application"": ""Microsoft Office Word"",\r\n    ""meta:last-author"": ""Luka Lampret"",\r\n    ""Creation-Date"": ""2012-10-12T11:17:00Z"",\r\n    ""xmpTPg:NPages"": ""1"",\r\n    ""Character-Count-With-Spaces"": ""106"",\r\n    ""Last-Author"": ""Luka Lampret"",\r\n    ""Character Count"": ""92"",\r\n    ""Page-Count"": ""1"",\r\n    ""Revision-Number"": ""22"",\r\n    ""Application-Version"": ""15.0000"",\r\n    ""extended-properties:Template"": ""Normal.dotm"",\r\n    ""Author"": ""Windows User"",\r\n    ""publisher"": ""JDI"",\r\n    ""meta:page-count"": ""1"",\r\n    ""dc:publisher"": ""JDI""\r\n  }\r\n}\r\n```\r\n Support extraction of all metadata >>> 0"
755,`scaled_float` should be used as FLOAT in aggregations but currently they are used as LONG.\r\nThis change fixes this issue and adds a simple it test for it.\r\n\r\nFixes #22350 Fix scaled_float numeric type in aggregations >>> 1
756,throw execption when custom analyzers have the same names as built-in analyzers according to issue:\r\nhttps://github.com/elastic/elasticsearch/issues/22263\r\n Do not allow custom analyzers to have the same names as built-in analyzers >>> 0
757,Fixes #22355 Fix NPE in percolator's 'now' range check for percolator queries with range queries >>> 1
758,Unless the dynamic templates define an explicit format in the mapping\r\ndefinition: in that case the explicit mapping should have precedence.\r\n\r\nCloses #9410 Dynamic `date` fields should use the `format` that was used to detect it is a date. >>> 1
759,nan Docs fix native script usage >>> 1
760,Backport of #22146\r\n\r\nNote to reviewers: the diff between this change and #22146 is contained in the 2nd commit. Format doc_value fields >>> 0
761,"This commit checks for a null BytesReference being passed to CompressorFactory#uncompressIfNeeded and simply returns null. Previously this would have resulted in a NPE. While this does seem internal at first glance, it can affect user code as a GetResponse could trigger this when the document is missing. prevent NPE when trying to uncompress a null BytesReference >>> 1"
762,`ParseFieldMatcher` is no longer needed as of #22130.  We can now remove all of its usages step by step. This is the first step which removes some usages of `ParseFieldMatcher` from request parsing code.\r\n\r\nRelates to #19552 Remove some ParseFieldMatcher usages >>> 1
763,"In #22313 we added a check that prevents the SnapshotDeletionsInProgress custom cluster state objects from being sent to older elasticsearch nodes. This commits make this check generic and available to other cluster state custom objects if needed.\r\n\r\n@abeyad, could you take a look when you are back? That's the change that we discussed in #22313.\r\n Add a generic way of checking version before serializing custom cluster object >>> 1"
764,"1. Escape sequences we're working. For example `\` is now correctly\r\ninterpreted as `\` instead of `\`. Same with `\'` being `'` and\r\n`\""` being `""`.\r\n2. `'` delimited strings weren't allowed to contain `""`s but it looked\r\nlike they were intended to support it. Now they do.\r\n3. Improves the error message when the script contains an invalid\r\nescape sequence inside a string to include a list of the valid\r\nescape sequences.\r\n\r\nCloses #22372 Fix some issues with painless's strings >>> 1"
765,These are the last usages of `ParseFieldMatcher#match` hence the method can now be removed.\r\n\r\nRelates to #19552\r\nRelates to #22130 Remove some more usages of ParseFieldMatcher >>> 1
766,"The `RestHighLevelClient` class takes as as an argument a low level client instance (`RestClient`). The first method added is `ping`, which returns true if the call to HEAD / went ok and false if an IOException was thrown. Any other exception gets bubbled up.\r\n\r\nThere are two kinds of tests, a unit test (`RestHighLevelClientTests`) that verifies the interaction between high level and low level client, and an integration test (`MainActionIT`) which relies on an externally started es cluster to send requests to.\r\n\r\nThis PR essentially holds the current content of the feature/high-level-rest-client public branch. We decided to add parsing code to java api responses directly upstream. Also we can add new methods to the high level rest client as we go without the need to maintain a separate branch. The important part is that we don't release/publish any artifact until we are ready to do so. Add REST high level client gradle submodule and first simple method >>> 1"
767,"Makes `Version` implement `Comparable`.\r\n\r\nThis allows use streams to calculate min/max of a collection of Versions, etc.\r\n\r\n Implement Comparable in Version >>> 1"
768,This PR is yet another step on the way of removing ParseFieldMatcher. It contains mostly mechanical removals of ParseFieldMatcher arguments from parse methods. \r\n\r\nRelates to #19552\r\nRelates to #22130 Remove some more usages of ParseFieldMatcher >>> 1
769,"`ToXContentObject` extends `ToXContent` without adding new methods to it, while allowing to mark classes that output complete xcontent objects to distinguish them from classes that require starting and ending an anonymous object externally.\r\n\r\nIdeally `ToXContent` would be renamed to `ToXContentFragment` or something along those lines, but that would be a huge change in our codebase, hence we can simply document the fact that `ToXContent` outputs fragments with no guarantees that the output is valid per se without an external ancestor.\r\n\r\nAs part of this PR also some of our response classes are migrated to the new interface. In general all of our response should implement `ToXContentObject`, but some don't even implement yet `ToXContent`, while some that do will be migrated step by step as part of the effort made for REST high level client (essentially while writing parsing code, we can also adjust the `toXContent` methods.\r\n\r\n`RestToXContentListener`, `String#toString` and `XContentHelper#toXContent` used to take a `boolean` argument to indicate whether the output needed to be wrapped into a new object. This decision doesn't have to be made case by case now. We can rely on whether a class implements `ToXContent` (fragment) or `ToXContentObject` (complete valid object) instead.\r\n\r\nThere are many more things that can be done to improve how deal with `ToXContent` classes, like unifying their toStrings output etc. those improvements will come later, one at a time.\r\n\r\nRelates to # 3889\r\nRelates to #16347\r\n Introduce ToXContentObject interface >>> 1"
770,"Currently we have getters an setters for both ""minimumNumberShouldMatch"" and ""minimumShouldMatch"", which both access the same internal value (minimumShouldMatch). Since we only document the `minimum_should_match` parameter for the query DSL, I think we can deprecate the other getters and setters for 5.x and remove with 6.0, also deprecating the  `minimum_number_should_match` query DSL parameter. Deprecate and remove ""minimumNumberShouldMatch"" in BoolQueryBuilder >>> 1"
771,"This is a POC for implementing field collapsing for search requests.\r\nThe field collapsing is implemented for `query_then_fetch` and  `dfs_query_then_fetch` using the `query` and `fetch` phase. No additional phase is required even when `inner_hits` for each collapsed result are computed. For instance the following query:\r\n\r\n````\r\nGET _search\r\n{\r\n   ""collapse"": {\r\n      ""field"": ""category"",\r\n      ""inner_hits"": {\r\n         ""name"": ""collapsed_hits"",\r\n         ""size"": 3\r\n      }\r\n   }\r\n}\r\n````\r\n... would return the top hits ""collapsed"" on the category field and for each collapsed result the inner_hits named `collapsed_hits` contains the 3 best hits within this category.\r\nThe `inner_hits` retrieval is done in the `fetch` phase and does not require any index routing to be accurate.\r\n\r\nThe main advantages of using this versus a combo terms + top_hits aggregation (as described here https://www.elastic.co/guide/en/elasticsearch/guide/current/top-hits.html) are:\r\n* The collapse + inner_hits is done in two phases (instead of 1 in the combo agg) so the top hits of each collapsed key are always accurate.\r\n* The field collapsing is done at the top hits level only. We don't need to compute the values for each collapsed key in the result set, just the top hits. This saves a lot of memory compared to the `terms` aggregation behavior.\r\n* Since it's applied on the top hits only the collapsing can be much faster than using the combo agg.\r\n* The collapsing top docs collector does not use global ordinals for strings which saves extra memory compared to the aggs solution.\r\n* Paging is possible, though with the same limitation than for regular search.\r\n* `search_after` and `scroll` are not implemented yet but could be achieved as well (though without the optimization when sorting on `_doc` only).\r\n\r\nI've opened this PR to show approximatively the amount of changes required. The implementation should be cleaned and at the moment there is no test or docs so the only way to test this is to launch a node with this branch. It's implemented directly in the existing search actions since I did not wanted to create a new request/response/action for this. \r\nI though it would be less intrusive which is why I need feedback at this point to decide if this POC should be continued.\r\n\r\nFixes #21833 Add field collapsing for search request >>> 1"
772,"After deprecating getters and setters and the query DSL parameter in 5.x,\r\nsupport for `minimum_number_should_match` can be removed entirely. Also\r\nconsolidated comments with the ones on 5.x branch and added an entry to the\r\nmigration docs. Remove deprecated `minimum_number_should_match` in BoolQueryBuilder >>> 1"
773,This new snapshot has the synonym graph filters and new helpers for\r\nexclusive bounds on ranges. @mattweber @mikemccand Could you\r\nhave a look at the synonym part? Upgrade to lucene-6.4.0-snapshot-084f7a0. >>> 1
774,Closes #19040\n Make path syntax for lower and upper standard deviation bounds in the extended_stats aggregation more obvious >>> 0
775,"This change is the first towards providing the ability to store\r\nsensitive settings in elasticsearch. It adds the\r\n`elasticsearch-keystore` tool, which allows managing a java keystore.\r\nThe keystore is loaded upon node startup in Elasticsearch, and used by\r\nthe Setting infrastructure when a setting is configured as secure.\r\n\r\nThere are a lot of caveats to this PR. The most important is it only\r\nprovides the tool and setting infrastructure for secure strings. It does\r\nnot yet provide for keystore passwords, keypairs, certificates, or even\r\nconvert any existing string settings to secure string settings. Those\r\nwill all come in follow up PRs. But this PR was already too big, so this\r\nat least gets a basic version of the infrastructure in.\r\n\r\nThe two main things to look at.  The first is the `SecureSetting` class,\r\nwhich extends `Setting`, but removes the assumption for the raw value of the\r\nsetting to be a string. SecureSetting provides, for now, a single\r\nhelper, `stringSetting()` to create a SecureSetting which will return a\r\nSecureString (which is like String, but is closeable, so that the\r\nunderlying character array can be cleared). The second is the\r\n`KeyStoreWrapper` class, which wraps the java `KeyStore` to provide a\r\nsimpler api (we do not need the entire keystore api) and also extend\r\nthe serialized format to add metadata needed for loading the keystore\r\nwith no assumptions about keystore type (so that we can change this in\r\nthe future) as well as whether the keystore has a password (so that we\r\ncan know whether prompting is necessary when we add support for keystore\r\npasswords). Add infrastructure for elasticsearch keystore >>> 1"
776,Provides an example of using is and an example return description\r\nand explains that we've added descriptions for some tasks but not\r\neven close to all of them. And that we expect to change the\r\ndescriptions as we learn more.\r\n\r\nCloses #22407\r\n Document the `detailed` parameter of tasks API >>> 1
777,Currently `geo_point` and `geo_shape` field are treated as `text` field by the field stats API and we\r\ntry to extract the min/max values with MultiFields.getTerms.\r\nThis is ok in master because a `geo_point` field is always a Point field but it can cause problem in 5.x (and 2.x) because the legacy\r\n `geo_point` are indexed as terms.\r\n As a result the min and max are extracted and then printed in the FieldStats output using BytesRef.utf8ToString\r\n which can throw an IndexOutOfBoundException since it's not valid UTF8 strings.\r\n This change ensure that we never try to extract min/max information from a `geo_point` field.\r\n It does not add a new type for geo points in the fieldstats API so we'll continue to use `text` for this kind of field.\r\n This PR is targeted to master even though we could only commit this change to 5.x. I think it's cleaner to have it in master too before we make any decision on\r\n  https://github.com/elastic/elasticsearch/pull/21947.\r\n\r\nFixes #22384 Implement stats for geo_point and geo_shape field >>> 1
778,"Fixes #16838\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Allows to use nested aggregation and terms aggregation in sort by deep metric >>> 0"
779,Previous to his change when the range query was rewritten to an unbounded range (`[* TO *]`) it maintained the timezone and format for the query. This means that queries with different timezones and format which are rewritten to unbounded range queries actually end up as different entries in the search request cache.\r\n\r\nThis is inefficient and unnecessary so this change nulls the timezone and format in the rewritten query so that regardless of the timezone or format the rewritten query will be the same.\r\n\r\nAlthough this does not fix #22412 (since it deals with the WITHIN case rather than the INTERSECTS case) it is born from the same arguments RangeQuery WITHIN case now normalises query >>> 1
780,"Previously, we could run into a situation where attempting to delete an\r\nindex due to a cluster state update would cause an unhandled exception\r\nto bubble up to the ClusterService and cause the cluster state applier\r\nto fail.  The result of this situation is that the cluster state never\r\ngets updated on the ClusterService because the exception happens before\r\nall cluster state appliers have completed and the ClusterService only\r\nupdates the cluster state once all cluster state appliers have\r\nsuccessfully completed.\r\n\r\nAll other methods on IndicesService properly handle all exceptions and\r\nnot just IOExceptions, but there were two instances with respect to\r\nindex deletion where only IOExceptions where handled by the\r\nIndicesService.  If any other exception occurred during these delete\r\noperations, the exception would be bubbled up to the ClusterService,\r\ncausing the aforementioned issues.\r\n\r\nThis commit ensures all methods in IndicesService properly capture all\r\ntypes of Exceptions, so that the ClusterService manages to update the\r\ncluster state, even in the presence of shard creation/deletion failures.\r\n\r\nNote that the lack of updating the cluster state in the presence of such\r\nexceptions can have many unintended consequences, one of them being\r\nthe tripping of the assertion in IndicesClusterStateService#removeUnallocatedIndices\r\nwhere the assumption is that if there is an `IndexService` to remove with\r\nan unassigned shard, then the index must exist in the cluster state, but if \r\nthe cluster state was never updated due to the aforementioned exceptions,\r\nthen the cluster state will not have the index in question. IndicesService handles all exceptions during index deletion >>> 1"
781,"#22325 changed the recovery retry logic to use unique recovery ids. The change also introduced an issue, however, which made it possible for the shard store to be closed under CancellableThreads, triggering assertions in the node locking logic. This PR limits the use of CancellableThreads only to the part where we wait on the old recovery target to be closed.\r\n\r\nTest failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.x+multijob-darwin-compatibility/106/consoleFull\r\n\r\nLog output:\r\n\r\n```\r\n1> [2017-01-04T14:29:36,961][WARN ][o.e.e.NodeEnvironment    ] [node_sd2] lock assertion failed\r\n  1> java.nio.channels.ClosedByInterruptException: null\r\n  1> \tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[?:1.8.0_60]\r\n  1> \tat sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:315) ~[?:?]\r\n  1> \tat org.apache.lucene.mockfile.FilterFileChannel.size(FilterFileChannel.java:85) ~[lucene-test-framework-6.4.0-snapshot-ec38570.jar:6.4.0-snapshot-ec38570 ec385708c6e0c47440127410c1223f14703c24e1 - jim - 2016-11-29 01:11:32]\r\n  1> \tat org.apache.lucene.mockfile.FilterFileChannel.size(FilterFileChannel.java:85) ~[lucene-test-framework-6.4.0-snapshot-ec38570.jar:6.4.0-snapshot-ec38570 ec385708c6e0c47440127410c1223f14703c24e1 - jim - 2016-11-29 01:11:32]\r\n  1> \tat org.apache.lucene.mockfile.FilterFileChannel.size(FilterFileChannel.java:85) ~[lucene-test-framework-6.4.0-snapshot-ec38570.jar:6.4.0-snapshot-ec38570 ec385708c6e0c47440127410c1223f14703c24e1 - jim - 2016-11-29 01:11:32]\r\n  1> \tat org.apache.lucene.store.NativeFSLockFactory$NativeFSLock.ensureValid(NativeFSLockFactory.java:170) ~[lucene-core-6.4.0-snapshot-ec38570.jar:6.4.0-snapshot-ec38570 ec385708c6e0c47440127410c1223f14703c24e1 - jim - 2016-11-29 01:11:32]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:902) ~[main/:?]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment.availableShardPaths(NodeEnvironment.java:782) ~[main/:?]\r\n  1> \tat org.elasticsearch.env.NodeEnvironment.deleteShardDirectoryUnderLock(NodeEnvironment.java:492) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.IndicesService.deleteShardStore(IndicesService.java:657) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.IndexService.onShardClose(IndexService.java:442) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.IndexService.access$100(IndexService.java:93) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:524) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.IndexService$StoreCloseListener.handle(IndexService.java:509) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.store.Store.closeInternal(Store.java:366) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.store.Store.access$000(Store.java:126) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.store.Store$1.closeInternal(Store.java:147) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64) ~[main/:?]\r\n  1> \tat org.elasticsearch.index.store.Store.decRef(Store.java:348) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.RecoveryTarget.closeInternal(RecoveryTarget.java:330) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.RecoveryTarget.resetRecovery(RecoveryTarget.java:193) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.RecoveriesCollection.lambda$resetRecovery$306(RecoveriesCollection.java:113) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.CancellableThreads.executeIO(CancellableThreads.java:105) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.RecoveriesCollection.resetRecovery(RecoveriesCollection.java:113) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.retryRecovery(PeerRecoveryTargetService.java:156) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.retryRecovery(PeerRecoveryTargetService.java:152) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.doRecovery(PeerRecoveryTargetService.java:289) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService.access$900(PeerRecoveryTargetService.java:73) ~[main/:?]\r\n  1> \tat org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRunner.doRun(PeerRecoveryTargetService.java:555) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) ~[main/:?]\r\n  1> \tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[main/:?]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_60]\r\n  1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_60]\r\n  1> \tat java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_60]\r\n``` Don't close store under CancellableThreads >>> 1"
782,This change adds 5.3 and updates 5.x branch to have 5.3 as latest. Add 5.3 version constant >>> 1
783,"Randomized runner uses a flag, tests.asserts, which we have previously\r\nnot used, but is used in lucene for disabling assertions. This change\r\nmodifies the gradle configuration to look for this flag and pass through\r\nto the test runner to determine whether -ea and -esa are added to the\r\njava commandline for tests. Use tests.asserts flag to allow disabling assertions >>> 1"
784,"Today we execute the low level handshake on the TCP layer in #connectToNode.\r\nIf #openConnection is used directly, which is truly expert, no handshake is executed\r\nwhich allows connecting to nodes that are not necessarily compatible. This change\r\nmoves the handshake to #openConnection to prevent bypassing this logic. Execute low level handshake in #openConnection >>> 1"
785,"In preparation to be able to parse search hits from rest responses for the java rest client, \r\nthis adds methods to parse InternalSearchHit and InternalSearchHits from their\r\nxContent representation. Most of the information in the original object is\r\npreserved when rendering the object to xContent and then parsing it back.\r\nHowever, some pieces of information are lost which we currently cannot parse\r\nback from the rest response, most notably:\r\n\r\n* the ""match"" property of the lucene explanation is not rendered in the\r\n  ""_explain"" section and cannot be reconstructed on the client side\r\n* the original ""shard"" information (SearchShardTarget) is only rendered if the\r\n  ""explanation"" is also set. However, we always render the ""_index"" name that \r\n  is contained in SearchShardTarget. In order to be able to parse back only \r\n  the index name, this information is now stored separately in InternalSearchHit.\r\n  Also we loose the indexUUID of the contained ShardId because we don't write \r\n  it out. As a replacement we can use ClusterState.UNKNOWN_UUID on the receiving side.\r\n Add parsing from xContent to InternalSearchHit and InternalSearchHits >>> 1"
786,"This is the backport of the gradle changes made as part of #22371.\r\n\r\nBackporting these gradle changes will make backporting the high level rest client submodule easier in the future when ready (it is currently under development in master), by simply copying its directory from master to 5.x ideally. Backport gradle changes made in #22371 >>> 1"
787,"This PR removes some more usages of ParseFieldMatcher. ObjectParser and friends no longer require a `ParseFieldMatcherSupplier`, which allows to start moving away from carrying `ParseFieldMatcher` around in parsers.\r\n\r\nRelates to #19552\r\nRelates to #22130 Remove some more usages of ParseFieldMatcher >>> 1"
788,"Netty plays a lot of games with recycling byte buffers in thread local\r\ncaches.\r\n\r\nThe recycler in particular appears to be fraught with peril. It appears\r\nthat there are circumstances where the recycler does not recycle quickly\r\nenough and can exceed its capacity leading to heap exhaustion and out of\r\nmemory errors. If you spend a few minutes reading the history of the\r\nrecycler on the Netty GitHub issues, it appears it has been nothing but\r\na source of trouble, and the project itself has an open issue that\r\nproposes disabling by default and possibly even removing the recycler.\r\n\r\nWe are seeing users struggle with issues in 5.x that I think are largely\r\ndriven by some of the problems here with Netty.\r\n\r\nThis change proposes to disable the recycler. I think that disabling\r\nthis feature will return some of the stablity that this feature appears\r\nto be losing us.\r\n\r\nI have done performance testing on my workstation with disabling this\r\nand I do not see a difference in performance. I propose that we make\r\nthis change in master and let some nightly benchmarks run to confirm\r\nthat there is not a difference in performance. If we are comfortable\r\nwith the performance changes, I propose backporting this to all active\r\nbranches.\r\n\r\nRelates netty/netty#5904, #22406, #22360, #22189 Disable the Netty recycler >>> 1"
789,This change makes the terms aggregation work when the buckets coming from different indices are a mix of decimal numbers and non-decimal numbers. In this case non-decimal number (longs) are promoted to decimal (double) which can result in a loss of precision for big numbers.\r\n\r\nFixes #22232 Promote longs to doubles when a terms agg mixes decimal and non-decimal numbers >>> 1
790,This is related to #22116. A logIfNecessary() call makes a call to\r\nNetworkInterface.getInterfaceAddresses() which requires SocketPermission connect\r\nprivileges. By moving this to bootstrap the logging call can be made before\r\ninstalling the SecurityManager.\r\n\r\n\r\nAs a note:\r\n\r\nIFConfig and its method need to be made public in order for this to work. Another option would be to move the class into a different package - but that would cause issues as it depends on package private methods in NetworkUtils. Move IfConfig.logIfNecessary call into bootstrap >>> 1
791,"Someone else also submitted a merge/pull request (https://github.com/elastic/elasticsearch/pull/22457), please consider this one since I filed the request. Thank you.\r\n\r\nCloses #22454. Add support for ca-central-1 region to EC2 and S3 plugins >>> 1"
792,"Fixes #22454 \r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Add support for ca-central1 region #22454 >>> 0"
793,"When I migrated from ES 1.7 to 5.1, I have found that there is no alternative to `inQuery` function described in docs. After some investigation i found #13145, where someone wrote that `termsQuery` should  be used instead. Adds removed inQuery method alternative >>> 0"
794,"When I migrated from ES 1.7 to 5.1, I have found that there is no described alternative to inQuery function in the docs. After some investigation i found #13145, where someone wrote that termsQuery should be used instead. Adds removed inQuery method alternative >>> 1"
795,"This  PR contains some minor improvements made to `SearchShardTarget`:\r\n\r\n- removed duplicated getters with and without get prefix, kept the latter only\r\n- removed redundant indexText member field\r\n- make its instance members final, possible as it already implements `Writeable`\r\n\r\nAlso removed a couple of unused methods from `InternalSearchHit`. Clean up SearchShardTarget >>> 1"
796,Today when an index is shrunk the version information is not carried over\r\nfrom the source to the target index. This can cause major issues like mapping\r\nincompatibilities for instance if an index from a previous major version is shrunk.\r\n\r\nThis commit ensures that all version information from the soruce index is preserved\r\nwhen a shrunk index is created.\r\n\r\nCloses #22373 Ensure shrunk indices carry over version information from its source >>> 1
797,"Right now closing a shard looks like it strands refresh listeners,\r\ncausing tests like\r\n`delete/50_refresh/refresh=wait_for waits until changes are visible in search`\r\nto fail. Here is a build that fails:\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+multi_cluster_search+multijob-darwin-compatibility/4/console\r\n\r\nThis attempts to fix the problem by implements `Closeable` on\r\n`RefreshListeners` and rejecting listeners when closed. More importantly\r\nthe act of closing the instance flushes all pending listeners\r\nso we shouldn't have any stranded listeners on close.\r\n\r\nBecause it was needed for testing, this also adds the number of\r\npending listeners to the `CommonStats` object and all API to which\r\nthat flows: `_cat/nodes`, `_cat/indices`, `_cat/shards`, and\r\n`_nodes/stats`.\r\n Close and flush refresh listeners on shard close >>> 1"
798,"This commit adds the Gradle logic so that a marker file is created in 'elasticsearch/build/vagrant/' for every Vagrant box. This marker file is deleted when the project is cleaned up (`gradle clean`) or when the Vagrantfile is changed. In both of these cases, it will force the destruction of the VM (`vagrant destroy --force`) before the next VM start. This should help the CI infrastructure to run packaging tests for external projects. [TESTS] Use a marker file to check if a Vagrant box must be destroyed >>> 0"
799,"Primary relocation for shadow replicas is broken as the recovery process accesses the engine on the source shard after it's been closed. This results in the source shard failing itself. This has not been detected by tests as, after failing source and target shard of the relocation, the master simply reassigns the primary to the node where it wanted to relocate to as every node has access to the shard files.\r\n\r\nTest failure exhibiting behavior (might be an un-related issue though):\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1282/consoleFull\r\n\r\nRelates to #20300 Fix primary relocation for shadow replicas >>> 1"
800,"We have a custom logger implementation known as a prefix logger that is used to write every message by the logger with a given prefix. This is useful for node-level, index-level, and shard-level messages where we want to log the node name, index name, and shard ID, respectively, if possible. The mechanism that we employ is that of a marker. Log4j has a built-in facility for managing these markers, but its effectively a memory leak because these markers are held in a map and can never be released. This is problematic for us since indices and shards do not necessarily have infinite life spans and so on a node where there are many indices being creted and destroyed, this infinite lifespan can be a problem indeed. To solve this, we use our own cache of markers. This is necessary to prevent too many instances of the marker for the same prefix from being created (just think of all the shard-level components that exist in the system), and to workaround the effective leak in Log4j. These markers are stored as weak references in a weak hash map. It is these weak references that are unneeded. When a key is removed from a weak hash map, the corresponding entry is placed on a reference queue that is eventually cleared. This commit simplifies prefix logger by removing this unnecessary weak reference wrapper.\r\n\r\nRelates #20429 Remove unneeded weak reference from prefix logger >>> 1"
801,"This commit introduces sequence-number-based recovery. When a replica has fallen out of sync, rather than performing a file-based recovery we first attempt to replay operations since the last local checkpoint on the replica. To do this, at the start of recovery the replica tells the primary what its local checkpoint is. The primary will then wait for all operations between that local checkpoint and the current maximum sequence number to complete; this is to ensure that there are no gaps in the operations that will be replayed from the primary to the replica. This is a best-effort attempt as we currently have no guarantees on the primary that these operations will be available; if we are not able to replay all operations in the desired range, we just fallback to file-based recovery. Later work will strengthen the guarantees.\r\n\r\nRelates #10708 Introduce sequence-number-based recovery >>> 1"
802,"This patch fixes the incorrect indentation in the REST tests, which makes tests in language runners (eg. Ruby, Python) to fail, since the `skip` clause is parsed as an empty value. Tha Java YAML parser is smarter/lenient about whitespace, so it doesn't catch this. Fixed the incorrect indentation for the `skip` clauses in the REST tests >>> 1"
803,"This patch fixes the incorrect indentation in the REST tests, which makes tests in language runners (eg. Ruby, Python) to fail, since the skip clause is parsed as an empty value. Tha Java YAML parser is smarter/lenient about whitespace, so it doesn't catch this.\r\n [TEST] Fixed the incorrect indentation for the `skip` clauses in the REST tests >>> 1"
804,Changes the error message when `action.auto_create_index` or\r\n`index.mapper.dynamic` forbids automatic creation of an index\r\nfrom `no such index` to one of:\r\n* `no such index and [action.auto_create_index] is [false]`\r\n* `no such index and [index.mapper.dynamic] is [false]`\r\n* `no such index and [action.auto_create_index] contains [-<pattern>] which forbids automatic creation of the index`\r\n* `no such index and [action.auto_create_index] ([all patterns]) doesn't match`\r\n\r\nThis should make it more clear *why* there is `no such index`.\r\n\r\nCloses #22435\r\nCloses #21448 Better error when can't auto create index  >>> 1
805,"This removes `AllocationCommandRegistry` entirely and replaces\r\nit with `XContentParser#namedObject`, removing another class from\r\nguice.\r\n\r\n Use namedObject to parse AllocationCommands >>> 1"
806,This is the last thing in `SearchRequestParsers`. After this\r\nis merged we should be able to drop it.\r\n\r\nBreaks java plugins registering `SearchExtParser`s by making\r\nthe registration more standard. Replace SearchExtRegistry with namedObject >>> 1
807,"Currently, such tasks are only created for default boxes (`centos-7`, `ubuntu-1404`) and not for all boxes. This can be misleading for developers who want to debug testing scripts on non-default boxes like Fedora 24. [TESTS] Create vagrant up task for all boxes >>> 1"
808,ES v5.0.0+ uses the `O_EXCL | O_CREAT` file creation flags when writing snapshot blobs to ensure that existing files in the repository are not mistakenly overridden. Some NFS implementations have issues with the `O_EXCL` file creation flag though. This commit adds a FS-repository setting `enforce_write_once` (default `true`) that provides the possibility to revert to the pre-v5.0.0 file creation flags. Add setting to FsRepository to disable O_EXCL file creation flag >>> 0
809,"The Java APIs for unmapping are changing, and Lucene 6.4.0 will support it correctly, so until then we need an assume in this test case.\r\n\r\nSee https://issues.apache.org/jira/browse/LUCENE-6989 for details.\r\n\r\nCloses #22495\r\n Lucene with Java 1.9 doesn't support unmap in MMapDirectory until Lucene 6.4.0 >>> 1"
810,"Removes `AggregatorParsers`, replacing all of its functionality with\r\n`XContentParser#namedObject`.\r\n\r\nThis is the third bit of payoff from #22003, one less thing to pass\r\naround the entire application.\r\n Replace AggregatorParsers with namedObject >>> 1"
811,If the remote doesn't return a content type then reindex\r\ntried to guess the content-type. This didn't work most\r\nof the time and produced a rather useless error message.\r\nGiven that Elasticsearch always returns the content-type\r\nwe are dropping content-type detection in favor of just\r\nfailing the request if the remote didn't return a content-type.\r\n\r\nCloses #22329\r\n\r\n Remove content type detection from reindex-from-remote >>> 1
812,"Today Elasticsearch can act as a tribe node that fully joins another cluster to support operations across all different clusters. While tribe is a popular feature it also has it's downsides like:\r\n * non-trivial testing\r\n * special configuration\r\n * receives a non trivial amount of clusterstate updates\r\n * maintains connections to all nodes in all clusters and vice versa\r\n * has to be restarted to join another cluster\r\n * needs special strategies to disambiguate indices with identical names\r\n\r\nThat said, from a functionality standpoint the only feature in elasticsearch that needs cross cluster communication is in-fact the search layer. Everything else can be done on the client side by communicating with each cluster individually. For `_search` the merge of aggregations etc. is non trivial and can't be done on the client side. \r\n\r\nThe feature added in this PR is called `cross cluster search` and allows any node to act as a federated search node without joining any of the other clusters. There are 2 basic modes of operations: \r\n\r\n * globally configured remote clusters via cluster state settings\r\n * locally configured remote clusters via elasticsearch.yaml\r\n\r\nA node with such a configuration will `connect` to the remote cluster and discover a set of nodes that it can communicate with for federated search (Default # of nodes is 3). It won't connect to all nodes but only eligible nodes in the remote cluster (depending on their version and optionally on a node attribute (`node.addr`).\r\n\r\nRemote clusters can be configured and updated at any time via cluster settings without the need of a node-restart.  Each remote cluster specified via a `name` -> `seed node IP list` ie:\r\n\r\n * `search.remote.my_cluster.seeds: 127.0.0.1:9300, 127.0.0.1:9301`\r\n\r\nindices of this cluster can then be addressed via the clusters alias: `GET index_1,my_cluster:index_1, my_cluster:other*/_search` \r\n\r\n\r\nCloses #21473\r\n  Add federated cross cluster search capabilities >>> 1"
813,Make match queries that use phrase prefix or cutoff frequency options\r\ngraph aware.\r\n\r\nCloses #22490 Additional Graph Support in Match Query >>> 1
814,This can be confusing when unexpected.\r\n\r\nResolves #4707 Document simple_query_string negation with default_operator of OR >>> 1
815,Removes another parser registery type thing in favor of\r\n`XContentParser#namedObject`. Yet another parser registry\r\nconsolidated!\r\n\r\nBreaks that plugin API for suggesters by making defining an\r\nextra suggester more like anything else. Replace Suggesters with namedObject >>> 1
816,"This change converts repository-s3 to use the new secure settings. In\r\norder to support the multiple ways we allow aws creds to be configured,\r\nit also moves the main methods for the keystore wrapper into a\r\nSecureSettings interface, in order to allow settings prefixing to work. Make s3 repository sensitive settings use secure settings >>> 1"
817,"-e is for environment variable which do not seems to be be used by es to override settings. -E is passed to the command, which is what we want.\r\n\r\nThe original command line does not change the elastiscsearch instance configuration.\r\n\r\nMoreover, with this configuration http.publish_host expose the public ip of the container, thus preventing the use of a SniffingConnectionPool on the host. While not really usefull on a development machine with a single node, it avoids using different connection pool based on the environment. Fixed command line for development environment >>> 0"
818,Reindex-from-remote was accepting source filtering in the request\r\nbut ignoring it and setting `_source=true` on the search URI. This\r\nfixes the filtering so it is piped through to the remote node and\r\nadds tests for that.\r\n\r\nCloses #22507\r\n Fix source filtering in reindex-from-remote >>> 1
819,"This commit updates the cluster allocation explain API documentation to\r\nexplain the new request parameters and response formats, and gives\r\nexamples of the explain API responses under various scenarios. Cluster allocation explain API documentation >>> 1"
820,"The `NodeConnectionsService` currently determines which nodes to connect to / disconnect from by inspecting cluster state changes and connecting to added nodes / disconnecting from removed nodes. When a master steps down (for example due to another master-eligible node shutting down which brings the number of master-eligible nodes below minimum_master_master), and the connection to other existing nodes was dropped while pinging, however, the connection to these nodes is not re-established while publishing the first cluster state that establishes the node as master.\r\nThis PR changes the `NodeConnectionsService` connect / disconnect logic to always rely on the state that is to be / was published, looking not only at the added / removed nodes, but validating that exactly all nodes that are currently registered in `NodeConnectionsService` are connected (corresponds to a NOOP if the node is already connected). Keep NodeConnectionsService in sync with current nodes in the cluster state >>> 1"
821,"The document in the randomized `GetResult` can exist with no source (like if the `_source` was disabled in mappings) but the test should not always expect a non null source when the doc exists.\r\n\r\nRelated to #22386.\r\n\r\nIt can be reproduced with: \r\n<details>\r\n\r\n>  gradle :core:test -Dtests.seed=9F97D124A3385FBB -Dtests.class=org.elasticsearch.index.get.GetResultTests -Dtests.method=""testGetSourceAsBytes"" -Dtests.security.manager=true -Dtests.locale=fr-BE -Dtests.timezone=Africa/Porto-Novo\r\n</details>\r\n\r\n [TESTS] Fix GetResultTests.testGetSourceAsBytes() >>> 1"
822,"Affix settings are useful to namespace a certain setting. Yet, affix settings\r\nmust be specialized for their concrete type which causes lot of code duplication.\r\nThis commit allows to reuse an existing setting with and affix setting as soon as\r\na concrete key is available. Allow affix settings to delegate to actual settings >>> 1"
823,"Reindex-from-remote had a race when it tried to clear the scroll. It\r\nfirst starts the request to clear the scroll and then submits a task\r\nto the generic threadpool to shutdown the client. These two things\r\nrace and, in my experience, closing the scroll generally loses. That\r\nmeans that most of the time reindex-from-remote isn't clearing the\r\nscrolls that it uses. This isn't the end of the world because we\r\nflush old scroll contexts after a while but this isn't great.\r\n\r\nNoticed while experimenting with #22514. Fix reindex from remote clearing scroll >>> 1"
824,"Cluster state publishing has special handling to apply the cluster state on the active master: The master publishes the new cluster state to all nodes except itself, waits for up to 30 seconds for the nodes to apply the cluster state, and then applies the cluster state to itself.\r\nThis PR changes ClusterService so that the master publishes cluster state changes to itself in the same way as it publishes changes to other nodes. This is achieved by adding a second thread pool executor to ClusterService so that there is a dedicated thread for publishing and one for applying cluster states. A future step will be to separate the publishing aspects out of the ClusterService class into a dedicated component. Apply cluster state on active master using standard publishing mechanism >>> 0"
825,Relates to #14899 Use general cluster state batching mechanism for snapshot state updates >>> 1
826,"As the title says, we can reuse code rather than duplicate it in BytesRestResponse. use ElasticsearchException#renderException in BytesRestResponse#convert >>> 1"
827,Today affix settings are not dynamic since it's required to know\r\nit's namespace in order to pull a concrete setting from it. This is not possible\r\nin practice since the namespaces are dynamic by design. This change allows to register\r\na specialized settings consumer that consumes the namespace and the actual value if\r\na setting gets updated.\r\n Allow affix settings to be dynamic / updatable >>> 1
828,"Previously, we removed all unneeded backward compatibility logic\r\nfrom the BlobStoreRepository because 6.0 does not need to support\r\n2.x snapshot formats.  During the process of removing this backward\r\ncompatibility logic, some code was leftover that is no longer\r\nnecessary.  This commit removes all the remaining unnecessary\r\nbackwards compatibility code in BlobStoreRepository. Removes remaining snapshot backwards compatibility logic >>> 1"
829,Moves fetching the local node id into `NodeClient` which is a\r\nfairly useful place to put it so you can generate task ids from\r\n`NodeClient#executeLocally`.\r\n Remove ClusterService from ctors in reindex >>> 1
830,Adds a message about how the remote is unlikely to be Elasticsearch.\r\nThis isn't as good as including the whole message from the remote but\r\nwe can't do that because we are stream parsing it and we don't want\r\nto mark the whole request.\r\n\r\nCloses #22330\r\n Improve error message when reindex-from-remote gets bad json >>> 1
831,"Translog.Delete used `.writeVLong` instead of `.writeLong` for the sequence\r\nnumber and primary term (and their respective ""read"" variants). This could lead\r\nto issues where a 5.x node sent a translog operation with a negative sequence\r\nnumber (-2 for unassigned seq no) that tripped an assertion serializing a\r\nnegative number and causing ES to exit.\r\n\r\nAdds a unit test for serialization and a mixed-cluster REST test, since that was\r\nhow this was originally caught. Fix Translog.Delete serialization for sequence numbers >>> 1"
832,It is empty now that we've moved all the parsing into `namedObject`.\r\n\r\n Remove SearchRequestParsers >>> 1
833,\r\n Make it clear that bulk API actions are processed sequentially on each shard rather than sequentially overall >>> 1
834,"This is related to #22116. A number of plugins (discovery-azure-classic, discovery-ec2, discovery-gce, repository-azure, repository-gcs, and repository-s3) open socket connections. As `SocketPermissions` are transitioned out of core, these plugins will require `connect` permission. This pull request wraps operations that require these permissions in `doPrivileged` blocks. Add doPrivilege blocks for socket connect operations in plugins >>> 1"
835,The low level TCP handshake can cause channel / connection leaks if it's interrupted\r\nsince the caller doesn't close the channel / connection if the handshake was not successful.\r\nThis commit fixes the channel leak and adds general test infrastructure to detect channel leaks\r\nin the future. Prevent open channel leaks if handshake times out or is interrupted >>> 1
836,The config template that ships with Elasticsearch distributions contains\r\nlinks to various pieces of documentation. Links go out of date and get\r\nbroken. This commit removes such links from the config template.\r\n\r\nCloses #22545\r\n Remove doc links from config template >>> 1
837,"Add support for graph token streams to ""query_string"" and\r\n""simple_query_string"" queries. QueryString and SimpleQueryString Graph Support >>> 1"
838,This function introduced in #20838 wasn't documented at all.\r\n\r\nRelated to #22459 [DOC] Document {{url}} mustache function >>> 1
839,"Considering this a bug as it probably should have been higher to support updates in the first place, but still provides some infinite loop protection.  Updated to 1000000 from 10000.\r\n\r\nThis counter applies on a loose basis to statements (not operations) within a loop in Painless.  It is cumulative on a per function basis including the main function.  Calls external to Painless functions count as one statement.  This will hopefully become smarter in the future.\r\n\r\nRelates: (#22508) Update Painless Loop Counter to be Higher >>> 1"
840,"The constructors were left behind when filter_path was introduced, now we always provide includes and excludes hence we can remove the constructors that don't take those two arguments. Remove unused *XContentGenerator constructors >>> 1"
841,"There are some parameters that are accepted by each and every api we expose. Those (pretty, source, error_trace and filter_path)  are not explicitly listed in the spec of every api, rather whitelisted in clients test runners so that they are always accepted. The `human` flag has been treated up until now as a parameter that's accepted by only some stats and info api, but that doesn't reflect reality as es core treats it exactly like `pretty` (relevant especially now that we validate params and throw exception when we find one that is not supported). Furthermore, the human flag has effect on every api that outputs a date, time, percentage or byte size field. For instance the tasks api outputs a date field although they don't have the human flag explicitly listed in their spec. There are other similar cases. This commit removes the human flag from the rest spec and makes it an always accepted query_string param. Human flag to be moved under always accepted query_string params >>> 1"
842,"When trying to write parsing code for the SearchResponse `profile` section I ran into problems getting back the original time information, because both `ProfileResult` and `CollectorResult` currently print their time parameter in a human readable string format (e.g. `""time"": ""55.20315000ms""`). When trying to parse this back to a long value, for example to use in the planned high level java rest client, we can lose precision because of conversion and rounding issues. I was wondering if we should introduce an additional field (e.g. `time_in_nanos`) to be able to get the raw value back and only print the `time` field for human consumption if requested with the `human=true` flag.\r\nAnother question would be if we can backport this in some way to 5.x, I'm not sure if the fact that the profile API is experimental means we can introduce additional fields in the response in 5.x. This PR is only a basis for discussion, maybe @polyfractal or @clintongormley have an opinion on this. ProfileResult and CollectorResult should print machine readable timing information >>> 1"
843,"Today when you do not specify a port for an entry in\r\ndiscovery.zen.ping.unicast.hosts, the default port is the value of the\r\nsetting transport.profiles.default.port and falls back to the value of\r\ntransport.tcp.port if this is not set. For a node that is explicitly\r\nbound to a different port than the default port, this means that the\r\ndefault port will be equal to this explicitly bound port. Yet, the docs\r\nsay that we fall back to 9300 here. This commit corrects the docs.\r\n\r\n Fix default port for unicast zen ping hosts >>> 1"
844,Closes #11638\r\n Include global query string parameters in the REST spec >>> 1
845,There are presently 7 ctor args used in any rest handlers:\r\n* `Settings`: Every handler uses it to initialize a logger and\r\n  some other strange things.\r\n* `RestController`: Every handler registers itself with it.\r\n* `ClusterSettings`: Used by `RestClusterGetSettingsAction` to\r\n  render the default values for cluster settings.\r\n* `IndexScopedSettings`: Used by `RestGetSettingsAction` to get\r\n  the default values for index settings.\r\n* `SettingsFilter`: Used by a few handlers to filter returned\r\n  settings so we don't expose stuff like passwords.\r\n* `IndexNameExpressionResolver`: Used by `_cat/indices` to\r\n  filter the list of indices.\r\n* `Supplier<DiscoveryNodes>`: Used to fill enrich the response\r\n  by handlers that list tasks.\r\n\r\nWe probably want to reduce these arguments over time but\r\nswitching construction away from guice gives us tighter\r\ncontrol over the list of available arguments.\r\n\r\nThese parameters are passed to plugins using\r\n`ActionPlugin#initRestHandlers` which is expected to build and\r\nreturn that handlers immediately. This felt simpler than\r\nreturning an reference to the ctors given all the different\r\npossible args.\r\n\r\nBreaks java plugins by moving rest handlers off of guice. Deguice rest handlers >>> 1
846,Remove some more ParseFieldMatcher usages\r\n\r\nRelates to #19552\r\nRelates to #22130 \r\n Remove some more ParseFieldMatcher usages >>> 1
847,"With the recent addition of `synonym_graph` token filter, [LUCENE-7619](https://issues.apache.org/jira/browse/LUCENE-7619) almost ready, and all the related query support for graph token streams I think it is important that the analyze api show when a token has a non-default position length (anything > 1).  \r\n\r\nThis PR adds support to the analyze api for showing the position length only when a token has a non-default value without requiring to go into the `explain` mode.  When in explain mode, the position length attribute will still be output even when it has a default value. Analyze API Position Length Support >>> 1"
848,"Before, the default chunk size for Azure repositories was\r\n-1 bytes, which meant that if the chunk_size was not set on\r\nthe Azure repository, nor as a node setting, then no data\r\nfiles would get written as part of the snapshot (because\r\nthe BlobStoreRepository's PartSliceStream does not know\r\nhow to process negative chunk sizes).\r\n\r\nThis commit fixes the default chunk size for Azure repositories\r\nto be the same as the maximum chunk size.  This commit also\r\nadds tests for both the Azure and Google Cloud repositories to\r\nensure only valid chunk sizes can be set.\r\n\r\nCloses #22513 Fixes default chunk size for Azure repositories >>> 1"
849,"In preparation to be able to parse SearchResponse from its rest representation\r\nfor the java rest client, this adds fromXContent to SearchResponse. Most of the\r\ninformation in the original object is preserved when parsing it back. However,\r\nthe exceptions in the ""failure"" section won't be identical to the original ones\r\non the server side since they are parsed back to a generic\r\nElasticsearchException on the receiving side. Also the ""aggregations"", ""suggest""\r\nand ""profile"" section parsing is currently skipped and will be added by\r\nsubsequent PRs. Add parsing from xContent to SearchResponse >>> 0"
850,"This commit adds the `fromXContent()` method to the `UpdateResponse` class, so that it can be used with the high level rest client. Add parsing methods for UpdateResponse >>> 1"
851,This PR removes all of the remaining usages of `ParseFieldMatcher` so that we can remove the class and `ParseFieldMatcherSupplier`.\r\n\r\nCloses #19552 Remove ParseFieldMatcher and ParseFieldMatcherSupplier >>> 1
852,"This commit upgrades the Netty dependency to version 4.1.7.Final,\r\npicking up some important bug fixes.\r\n\r\n Upgrade to Netty 4.1.7 >>> 1"
853,"Instead of forcing each task to register all nodes where its children are running, this commit runs cancellation on all nodes. The task cancellation operation doesn't run too frequently, so this optimization doesn't seem to be worth additional complexity of the interface.\r\n Remove taskManager.registerChildTask >>> 1"
854,"Previously, certain settings that could take multiple comma delimited\r\nvalues would pick up incorrect values for all entries but the first if\r\neach comma separated value was followed by a whitespace character.  For\r\nexample, the multi-value ""A,B,C"" would be correctly parsed as\r\n[""A"", ""B"", ""C""] but the multi-value ""A, B, C"" would be incorrectly parsed\r\nas [""A"", "" B"", "" C""].\r\n\r\nThis commit allows a comma separated list to have whitespace characters\r\nafter each entry.  The specific settings that were affected by this are:\r\n\r\n  cluster.routing.allocation.awareness.attributes\r\n  index.routing.allocation.require.*\r\n  index.routing.allocation.include.*\r\n  index.routing.allocation.exclude.*\r\n  cluster.routing.allocation.require.*\r\n  cluster.routing.allocation.include.*\r\n  cluster.routing.allocation.exclude.*\r\n  http.cors.allow-methods\r\n  http.cors.allow-headers\r\n\r\nFor the allocation filtering related settings, this commit also provides\r\nvalidation of each specified entry if the filtering is done by _ip,\r\n_host_ip, or _publish_ip, to ensure that each entry is a valid IP\r\naddress.\r\n\r\nCloses #22297 Allow comma delimited array settings to have a space after each entry >>> 1"
855,Today there are several races / holes in TcpTransport and MockTcpTransport\r\nthat can allow connections to be opened and remain unclosed while the actual\r\ntransport implementation is closed. A recently added assertions in #22554 exposes\r\nthese problems. This commit fixes several issues related to missed locks or channel\r\ncreations outside of a lock not checking if the resource is still open. Ensure new connections won't be opened if transport is closed or closing >>> 1
856,Source filtering was incorrectly always accepting array items even if the include pattern did not match.\r\n\r\nCloses #22557\r\n Source filtering: only accept array items if the previous include pattern matches >>> 1
857,closes #22266 Update Jackson to 2.8.6 >>> 1
858,Bumps version and adds the BWC indices Bump version to 5.1.3 >>> 1
859,Without this whitelist painless can't use ip or binary doc values.\r\n\r\nCloses #22584\r\n Whitelist some ScriptDocValues in painless >>> 1
860,"We already have `CheckedConsumer`, `CheckedFunction` is useful too. I exposed it and used it in a few places. It could be used in more places and replace some more custom functional interfaces, but it would also make things slightly less readable I'm afraid (e.g. ScoreFunctionParser, QueryParser where the method would be called apply instead of parse etc.). Expose CheckedFunction and use it in a few places >>> 1"
861,"Beforehand, the DateProcessor constructs its joda pattern formatter during processor\r\nconstruction. This led to newly ingested documents being defaulted to\r\nthe year that the pipeline was constructed, not that of processing.\r\n\r\nFixes #22547. fix date-processor to a new default year for every new pipeline execution >>> 1"
862,"throw exception when user set negative size or negative from paramter in search request, which is describe in : https://github.com/elastic/elasticsearch/issues/22530 handler negative size/from paramter in search request >>> 0"
863,The IndexingOperationListener interface did not provide any\r\ninformation about the shard id when a document was indexed.\r\n\r\nThis commit adds the shard id as the first parameter to all methods\r\nin the IndexingOperationListener. Indexing: Add shard id to indexing operation listener >>> 1
864,"ClusterService and TransportService expect the local discovery node to be set\r\nbefore they are started but this requires manual interaction and is error prone since\r\nto work absolutely correct they should share the same instance (same ephemeral ID).\r\n\r\nTransportService also has 2 modes of operation, mainly realted to transport client vs. internal\r\nto a node. This change removes the mode where we don't maintain a local node and uses a dummy local\r\nnode in the transport client since we don't bind to any port in such a case.\r\n\r\nLocal discovery node instances are now managed by the node itself and only suppliers and factories that allow\r\ncreation only once are passed to TransportService and ClusterService. Remove setLocalNode from ClusterService and TransportService >>> 1"
865,"```sh\r\n$ bin/elasticsearch-keystore create\r\nCreated elasticsearch keystore in /Users/dpilato/Documents/Elasticsearch/apps/elasticsearch/elasticsearch-6.0.0-alpha1/config\r\n$ bin/elasticsearch-keystore add\r\nEnter value for null: xyz\r\nException in thread ""main"" java.lang.NullPointerException: invalid null input\r\n\tat java.security.KeyStore.setEntry(KeyStore.java:1552)\r\n\tat org.elasticsearch.common.settings.KeyStoreWrapper.setString(KeyStoreWrapper.java:264)\r\n\tat org.elasticsearch.common.settings.AddStringKeyStoreCommand.execute(AddStringKeyStoreCommand.java:83)\r\n\tat org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:58)\r\n\tat org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122)\r\n\tat org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)\r\n\tat org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122)\r\n\tat org.elasticsearch.cli.Command.main(Command.java:88)\r\n\tat org.elasticsearch.common.settings.KeyStoreCli.main(KeyStoreCli.java:39)\r\n```\r\n NPE when no setting name passed to elasticsearch-keystore >>> 1"
866,"This pull request tries to simplify the way `ElasticsearchException` are rendered to xcontent. It adds some documentation and renames and merges some methods. Current behavior is preserved, the goal is to be more readable and centralize everything in the `ElasticsearchException` class. Simplify ElasticsearchException rendering as a XContent >>> 1"
867,This is related to #22116. netty channels require socket `connect` and \r\n`accept` privileges. Netty does not currently wrap these operations \r\nwith `doPrivileged` blocks. These changes extend the netty channels \r\nand wrap calls to the relevant super methods in doPrivileged blocks. Wrap netty accept/connect ops with doPrivileged >>> 1
868,"This is a follow up to #22479, where storing credentials secure way was\r\nadded.\r\n\r\ncloses #21041 Deprecate specifying credentials through env vars, sys props, and remove profile files >>> 1"
869,"Minor tweak to raise the visibility of `doExecute()`, so that other modules/plugins/libraries depending on Reindex can use it.  The alternative is to make `AsyncDeleteBySearchAction` and `ParentBulkByScrollTask` public so they can be used directly, but it seemed cleaner to just elevate `doExecute()` instead so the user doesn't need to fiddle with the internal bits.\r\n\r\n/cc @nik9000  Increase visibility of doExecute so it can be used directly >>> 1"
870,"There was still small race in MockTcpTransport where channesl that are concurrently\r\nclosing are not yet removed from the reference tracking causing tests to fail. Compared to\r\nthe other races before this is a rather small windown and requires very very short test durations.\r\n\r\nan example failure is here: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=ubuntu/510/console\r\n\r\n```\r\n  2> java.lang.AssertionError: there are still open channels: [MockChannel{profile='none', isOpen=false, localAddress=/127.0.0.1:9301, isServerSocket=false}]\r\n```\r\n\r\nwhere the channel is already marked as closed but not yet removed from the open channels.\r\n Harden close and remove reference concurrency in MockTcpTransport >>> 1"
871,TransportInterceptors are commonly used to enrich requests with headers etc.\r\nwhich requires access the the thread context. This is not always easily possible\r\nsince threadpools are hard to access for instance if the interceptor is used on a transport client.\r\n\r\nThis commit passes on the thread context to all the interceptors for further consumption.\r\n\r\nCloses #22585 Pass ThreadContext to transport interceptors to allow header modification >>> 1
872,We are already explaining how to debug remotely in `Debugging from an IDE` section.\r\nWe can remove one.\r\n gradle run --debug-jvm is explained twice >>> 1
873,"This is related to #22116. A number of modules (reindex, etc) use the \r\nrest client. The rest client opens connections using the apache http \r\nclient. To avoid throwing `SecurityException` when using the \r\n`SecurityManager` these operations must be privileged. This is tricky \r\nbecause connections are opened within the httpclient code on its \r\nreactor thread. The way I confronted this was to wrap the creation \r\nof the client (and creation of reactor thread) in a `doPrivileged` \r\nblock. The new thread inherits the existing security context. Wrap rest httpclient with doPrivileged blocks >>> 1"
874,"For certain situations, end-users need the base path for Elasticsearch logs. Exposing this as a property is better than hard-coding the path into the logging configuration file as otherwise the logging configuration file could easily diverge from the Elasticsearch configuration file. Additionally, Elasticsearch will only have permissions to write to the log directory configured in the Elasticsearch configuration file. This commit adds a property that exposes this base path.\r\n\r\nOne use-case for this is configuring a rollover strategy to retain logs for a certain period of time. As such, we add an example of this to the documentation.\r\n\r\nAdditionally, we expose the property es.logs.cluster_name as this is used as the name of the log files in the default configuration.\r\n\r\nFinally, we expose es.logs.node_name in cases where node.name is explicitly set in case users want to include the node name as part of the name of the log files.   Expose logs base path >>> 1"
875,Deleting indices is an important event in a cluster and as such should be logged at the info level. This commit changes the logging level on index deletion to the info level.\r\n\r\nCloses #22605\r\n Log deleting indices at info level >>> 1
876,"When logger.level is set, we end up configuring a logger named ""level"" because we look for all settings of the form ""logger\..+"" as configuring a logger. Yet, logger.level is special and is meant to only configure the default logging level. This commit causes is to avoid not configuring a logger named level.  Do not configure a logger named level >>> 1"
877,"`EngineClosedException` is a ES level exception that is used to indicate that the engine is closed when operation starts. It doesn't really add much value and we can use `AlreadyClosedException` from Lucene (which may already bubble if things go wrong during operations). Having two exception can just add confusion and lead to bugs, like wrong handling of `EngineClosedException` when dealing with document level failures. The latter was exposed by `IndexWithShadowReplicasIT`.\r\n\r\nThis PR also removes the AwaitFix from the `IndexWithShadowReplicasIT` tests (which was what cause this to be discovered). While debugging the source of the issue I found some mismatches in document uid management in the tests. The term that was passed to the engine didn't correspond to the uid in the parsed doc - those are fixed as well.\r\n\r\n`EngineClosedException` is kept around for BWC. Once this goes into 5.3, I will remove it from master. Replace EngineClosedException with AlreadyClosedExcpetion >>> 1"
878,"An operation that completed successfully on a primary can result in a version conflict on a replica due to the asynchronous nature of operations. When a replica operation results in a version conflict, the operation is not added to the translog. This leads to gaps in the translog which is problematic as it can lead to situations where a replica shard can never advance its local checkpoint. As such operations are just normal course of business for a replica shard, these operations should be treated as if they completed successfully. This commit adds these operations to the translog.  Add replica ops with version conflict to translog >>> 1"
879,"The class is incorrectly sharing a `StempelStemmer` instance, which is not thread safe, across multiple threads.  Instead it should create a new instance for each call to `create`.\r\n\r\nCloses #21911 \r\n Fix thread safety of Stempel's token filter factory >>> 1"
880,Adding some missing version constants to keep them complete Version: Add missing releases from 2.x in Version.java >>> 1
881,"All the language clients support a special `ignore` parameter that doesn't get passed to elasticsearch with the request, but is used to indicate which error code(s) should not lead to an exception if returned for a specific request.\r\n\r\nMoving this to the low level REST client will allow the high level REST client to make use of it too, for instance so that it doesn't have to intercept ResponseExceptions when the get api returns a 404, which is in that specific case a perfectly valid response code (e.g. the body returned is not an exception in that case). move ignore parameter support from yaml test client to low level rest client >>> 1"
882,"This commit removes the option to use the blocking variants of the TCP\r\ntransport server, TCP transport client, or http server. Remove blocking TCP clients and servers >>> 1"
883,Today we have quite some abstractions that are essentially providing a simple\r\ndispatch method to the plugins defining a `HttpServerTransport`. This commit\r\nremoves `HttpServer` and `HttpServerAdaptor` and introduces a simple `Dispatcher` functional\r\ninterface that delegates to `RestController` by default.\r\n\r\nRelates to #18482 Remove HttpServer and HttpServerAdapter in favor of a simple dispatch method >>> 1
884,"Currently both ProfileResult and CollectorResult print the timing field in a\r\nhuman readable string format (e.g. ""time"": ""55.20315000ms""). When trying to\r\nparse this back to a long value, for example to use in the planned high level\r\njava rest client, we can lose precision because of conversion and rounding\r\nissues. This change introduces the additional field `time_in_nanos` that prints\r\nthe raw timing value in nanoseconds. ProfileResult and CollectorResult should print machine readable timing information >>> 1"
885,"This token filter is vital if you use a graph token stream at index time ... it'll be first available in Lucene 6.3.0, so I think we should include it in ES 5.2.0 Expose FlattenGraphTokenFilter >>> 1"
886,"Relates to #22024\r\n\r\nOn top of documentation, the PR adds deprecation loggers and deals with the resulting warning headers.\r\n\r\nThe yaml test is set exclude versions up to 6.0. This is need to make sure bwc tests pass until this is backported to 5.2.0 . Once that's done, I will change the yaml test version limits Add a deprecation notice to shadow replicas  >>> 1"
887,"In preparation for being able to parse SearchResponse from its rest representation\r\nfor the java rest client, this adds fromXContent to SearchProfileShardResults and its\r\nnested classes.  Add parsing from xContent to SearchProfileShardResults and nested classes >>> 1"
888,Replace the hardcoded global parameters in the yaml test suite with parameters parsed from the newly added _common.json file.\r\n\r\nRelates to #22569 [TEST] parse global parameters from _common.json >>> 1
889,"The extra plugins that may be attached to the elasticsearch build\r\ncontain their own license. In the past, the ASL license elasticsearch\r\nuses was avoided by specially checking for the gradle project prefix of\r\n`:x-plugins`. However, since refactoring to the elasticsearch-extra dir\r\nstructure, this mechanism was broken. This change fixes the pom license\r\nadding to only be applied to projects that fall under the root project\r\n(ie elasticsearch).\r\n\r\nCloses #22651 Only add ASL license to pom for elasticsearch project >>> 1"
890,"Use consistend formatting for field names in the profile documentation, replaces occurences quotes and backticks with just backticks ( `""someField""` with `someField` ).\r\n [DOCS] Fix inconsistent formatting for fieldnames in profile.asciidoc >>> 1"
891,"This exposes the least and most used disk usage estimates within the ""fs"" nodes\r\nstats output:\r\n\r\n```json\r\nGET /_nodes/stats/fs?pretty&human\r\n{\r\n  ""nodes"" : {\r\n    ""34fPVU0uQ_-wWitDzDXX_g"" : {\r\n      ""fs"" : {\r\n        ""timestamp"" : 1481238723550,\r\n        ""total"" : {\r\n          ""total"" : ""396.1gb"",\r\n          ""total_in_bytes"" : 425343254528,\r\n          ""free"" : ""140.6gb"",\r\n          ""free_in_bytes"" : 151068725248,\r\n          ""available"" : ""120.5gb"",\r\n          ""available_in_bytes"" : 129438912512\r\n        },\r\n        ""least_usage_estimate"" : {\r\n          ""path"" : ""/home/hinmanm/es/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.0.0-alpha1-SNAPSHOT/data/nodes/0"",\r\n          ""total"" : ""396.1gb"",\r\n          ""total_in_bytes"" : 425343254528,\r\n          ""available"" : ""120.5gb"",\r\n          ""available_in_bytes"" : 129438633984,\r\n          ""used_disk_percent"" : 69.56842912023208\r\n        },\r\n        ""most_usage_estimate"" : {\r\n          ""path"" : ""/home/hinmanm/es/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.0.0-alpha1-SNAPSHOT/data/nodes/0"",\r\n          ""total"" : ""396.1gb"",\r\n          ""total_in_bytes"" : 425343254528,\r\n          ""available"" : ""120.5gb"",\r\n          ""available_in_bytes"" : 129438633984,\r\n          ""used_disk_percent"" : 69.56842912023208\r\n        },\r\n        ""data"" : [{...}],\r\n        ""io_stats"" : {...}\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nResolves #8686 Expose disk usage estimates in nodes stats >>> 0"
892,"Add tests for `GlobalAggregator`, `MaxAggregator`, and `InternalMax`.\r\n\r\nRelates to #22278\r\n Improve unit test coverage of aggs >>> 1"
893,"Today we do not preserve response headers if they are present on a transport protocol\r\nresponse. While preserving these headers is not always desired, in the most cases we\r\nshould pass on these headers to have consistent results for depreciation headers etc.\r\nyet, this hasn't been much of a problem since most of the deprecations are detected early\r\nie. on the coordinating node such that this bug wasn't uncovered until #22647\r\n\r\nThis commit allow to optionally preserve headers when a context is restored and also streamlines\r\nthe context restore since it leaked frequently into the callers thread context when the callers\r\ncontext wasn't restored again. Streamline foreign stored context restore and allow to perserve response headers >>> 1"
894,"I missed the docs in the my first try, and also failed to actually map `flatten_graph` to the new factory.\r\n\r\nI fixed an unrelated typo too. Finish exposing FlattenGraphTokenFilter >>> 1"
895,"Everything that extended `AbstractAsyncBulkByScrollAction` also\r\nextended `AbstractAsyncBulkIndexByScrollAction` so this removes\r\n`AbstractAsyncBulkIndexByScrollAction`, merging it into\r\n`AbstractAsyncBulkByScrollAction`.\r\n\r\nI'd like to get this before doing #22616. Consolidate some reindex utility classes >>> 1"
896,"This changes build files so that building Elasticsearch works with both Gradle 2.13 as well as higher versions of Gradle (tested 2.14 and 3.3), enabling a smooth transition from Gradle 2.13 to 3.x. Make build Gradle 2.14 / 3.x compatible >>> 1"
897,We don't want to expose `String#getBytes` which is required for\r\n`Base64.getEncoder.encode` to work because we're worried about\r\ncharacter sets. This adds `encodeBase64` and `decodeBase64`\r\nmethods to `String` in Painless that are duals of one another\r\nsuch that:\r\n`someString == someString.encodeBase64().decodeBase64()`.\r\n\r\nBoth methods work with the UTF-8 encoding of the string.\r\n\r\nCloses #22648\r\n Painless: Add augmentation to String for base 64 >>> 1
898,This commit adds the parsing fromXContent() methods to the IndexResponse class.\r\n\r\nIt's a pale copy of what has been done in #22229.\r\n\r\nThere are a lot of code in common between IndexResponse and DeleteResponse including in tests.\r\nDo we want to keep it as it is or avoid code duplication? Add fromxcontent methods to delete response >>> 1
899,Supported settings so far:\r\n\r\n```yml\r\ncloud:\r\n    azure-arm:\r\n        client_id: FILL_WITH_YOUR_CLIENT_ID\r\n        secret: FILL_WITH_YOUR_SECRET\r\n        tenant_id: FILL_WITH_YOUR_TENANT\r\n        subscription_id: FILL_WITH_YOUR_SUBSCRIPTION_ID\r\n\r\ndiscovery:\r\n    zen.hosts_provider: azure-arm\r\n    azure-arm:\r\n        host:\r\n            type: private_ip\r\n            name: azure-esnode-master-*\r\n            group_name: azure-preprod\r\n            region: westeurope\r\n        refresh_interval: 10s\r\n```\r\n\r\nCloses #19146\r\n Add Azure ARM discovery plugin >>> 0
900,"This commit adds a TestWithDependenciesPlugin to the gradle build. \r\nThe main piece of functionality that it adds is to copy plugin-metadata \r\nfrom dependencies into the generated-resources for the current test source. \r\nThis is necessary to ensure that permissions for dependencies are applied \r\nwhen running the tests.\r\n\r\nA current limitation is that the permissions are applied differently \r\nthan in the distribution sources. When permissions are granted to all \r\ndepedencies for a module or plugin, the permissions are granted to all \r\ndependencies on the classpath for tests besides a few hardcoded \r\nexclusions:\r\n- es core\r\n- es test framework\r\n- lucene test framework\r\n- randomized runner\r\n- junit library Add TestWithDependenciesPlugin to build >>> 1"
901,"This change is a simple adaptation of https://github.com/elastic/elasticsearch/pull/19587 for the current state of master.\r\nIt allows to define search response listener in the form of `BiConsumer<SearchRequest, SearchResponse>`s in a search plugin.\r\nI plan to use this feature to cleanup the current PR for field collapsing:\r\nhttps://github.com/elastic/elasticsearch/pull/22337 Add the ability to define search response listeners in search plugin >>> 1"
902,"**This is work in progress**\r\n\r\nWhen rendered to XContent, ElasticsearchException's headers and other metadata are added to the XContent as new fields/objects.\r\n    \r\nFor example, a `new CircuitBreakingException(""Excessive stuff"", 30L, 20L)` is rendered as the following JSON:\r\n```\r\n    {\r\n        ""type"": ""circuit_breaking_exception"",\r\n        ""reason"": ""Excessive stuff"",\r\n        ""bytes_wanted"": 30,\r\n        ""bytes_limit"": 20\r\n    }\r\n```\r\n    \r\nThe same exception with an internal header ""es.uuid""=""foo"" and a custom header ""bar""=""baz"" would be rendered as:\r\n```\r\n    {\r\n        ""type"": ""circuit_breaking_exception"",\r\n        ""reason"": ""Excessive stuff"",\r\n        ""uuid"": ""foo""\r\n        ""bytes_wanted"": 30,\r\n        ""bytes_limit"": 20,\r\n        ""header"": {\r\n            ""bar"": ""baz""\r\n        }\r\n    }\r\n```\r\n    \r\nWhen this JSON is parsed back, there is no way to know if `bytes_wanted` is an internal header or a metadata because only custom headers (ie, which does not start with ""es."") are added as sub fields of a parent `header` field. The internal headers are just added in the stream. In the best world, headers and metadata would have their own namespaces to avoid name conflicts but for now we can just assume that everything is a header.\r\n    \r\nThis commit changes the ElasticsearchException so that:\r\n- innerFromXContent() method is added and is in charge of parsing what is produced by innerToXContent()\r\n- every field/object is considered as a header\r\n- header objects/arrays  names are flattened\r\n- header values are converted to String (because headers only accept strings)\r\n [WIP] Add headers parsing to ElasticsearchException >>> 0"
903,In 5.2 the FieldStats API can return null min/max values.\r\nThese values cannot be deserialized by a node with version pre 5.2 so if this node\r\nis pick to coordinate a FieldStats request in a mixed cluster an NPE can be thrown.\r\nThis change prevents the NPE by removing the non serializable FieldStats object directly in the field stats shard request.\r\nThe filtered fields will not be present in the response when a node pre 5.2 acts as a coordinating node. Fix NPE on FieldStats with mixed cluster on version pre/post 5.2 >>> 1
904,"Since #22200 the `Settings` class now uses a static `DeprecationLogger`. This makes Log4j2 status logger yells when Elasticsearch starts with the message `ERROR StatusLogger No log4j2 configuration file found. Using default configuration` because this deprecation logger is instantiated before the status logger has been configured.\r\n\r\nThis commit changes the `LogConfigurator.configureWithoutConfig()` method so that the status logger is initialized without using any `Settings`, making the message disappear. Fix status logger message at startup >>> 0"
905,nan [Docs] Remove outdated info about enabling/disabling doc_values >>> 1
906,"This change adds a strict mode for xcontent parsing on the rest layer. The strict mode will be off by default for 5.x and in a separate commit will be enabled by default for 6.0. The strict mode, which can be enabled by setting `http.content_type.required: true` in 5.x, will require that all incoming rest requests have a valid and supported content type header before the request is dispatched. In the non-strict mode, the Content-Type header will be inspected and if it is not present or not valid, we will continue with auto detection of content like we have done previously.\r\n\r\nThe content type header is parsed to the matching XContentType value with the only exception being for plain text requests. This value is then passed on with the content bytes so that we can reduce the number of places where we need to auto-detect the content type.\r\n\r\nAs part of this, many transport requests and builders were updated to provide methods that\r\naccepted the XContentType along with the bytes and the methods that would rely on auto-detection have been deprecated.\r\n\r\nIn non strict mode, deprecation warnings are issued whenever a request with body doesn't provide the Content-Type header.\r\n\r\nSee #19388  Optionally require a valid content type for all rest requests with content >>> 1"
907,"This commit fixes an issue with deprecation logging for lenient booleans. The underlying issue is that adding deprecation logging for lenient booleans added a static deprecation logger to the Settings class. However, the Settings class is initialized very early and in CLI tools can be initialized before logging is initialized. This leads to status logger error messages. Additionally, the deprecation logging for a lot of the settings does not provide useful context (for example, in the token filter factories, the deprecation logging only produces the name of the setting, but gives no context which token filter factory it comes from). This commit addresses both of these issues by changing the call sites to push a deprecation logger through to the lenient boolean parsing.\r\n\r\nRelates #22200, supersedes #22687 Fix deprecation logging for lenient booleans >>> 1"
908,Makes `PainlessLexer` abstract and adds the hacks that it needs as abstract methods implemented in `EnhancedPainlessLexer`. This *feels* a little cleaner than referencing the hacks statically. Mechanical changes to make PainlessLexer a bit more obvious >>> 1
909,Related to #22387\r\n Fix broken TaskInfo.toString() >>> 1
910,"In preparation for being able to parse SearchResponse from its rest\r\nrepresentation, this adds fromXContent to ShardSearchFailure. Add parsing from xContent to ShardSearchFailure >>> 1"
911,Those services validate their setting before submitting an AckedClusterStateUpdateTask to the cluster state service. An acked cluster state may be completed by a networking thread when the last acks as received. As such it needs special care to make sure that thread context headers are handled correctly. Index creation and setting update may not return deprecation logging >>> 1
912,The simplest way to do that is to move the public API into a\r\nnew package and generate javadoc for that package. Generate javadoc jar for painless's public API >>> 1
913,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Add option to disable source logging for search slow log closes #22683 >>> 0"
914,"This commit adds the parsing fromXContent() methods to the DeleteResponse class. The method is based on a ObjectParser because it is easier to use when parsing parent abstract classes like DocWriteResponse.\r\n\r\nIt also changes the ReplicationResponse.ShardInfo so that it now implements ToXContentObject. This way, the ShardInfo.fromXContent() method can be used by the DeleteResponse's ObjectParser.\r\n\r\nBackport of #22680 in 5.x branch\r\n Add fromxcontent methods to delete response >>> 1"
915,Not closing the InputStream will leak native memory as the DeflateCompressor/Inflater won't be closed. Close InputStream when receiving cluster state in PublishClusterStateAction >>> 1
916,"Move ""es."" internal headers to separate metadata set in `ElasticsearchException` and stop returning them as response headers. These headers are printed out as part of the response body already, they weren't meant to be sent back as response headers too. They were introduced to prevent having to add custom exceptions to our codebase whenever we need to throw an exception that has to hold some additional metadata that `ElasticsearchException` doesn't support out of the box. The header notion stays in `ElasticsearchException` but only for what actually needs to be returned as response header (no ""es."" prefix).\r\n\r\nAlso removed `ESExceptionTests` and moved its tests under `ElasticsearchExceptionTests` or `ExceptionSerializationTests`\r\n\r\nCloses #17593\r\n Stop returning ""es."" internal exception headers as http response headers >>> 1"
917,"By default, the JVM GC log file grows without\r\nlimitation. This is inconvenient for a long running\r\nprocess like Elasticsearch.\r\n\r\nWith this commit we add an example configuration\r\nfor a rotating GC log in `conig/jvm.options`. Add example configuration for rotating GC log >>> 1"
918,Adds unit tests for the date histogram aggregator.\r\n\r\nRelates #22278 Add unit tests for DateHistogramAggregator >>> 1
919,"As a follow up to #22649, this changes the resent tests for parsing parts of search responses to randomly set the humanReadable() flag of\r\nthe XContentBuilder that is used to render the responses. This should help to test that we can parse back thoses classes if the user specifies `?human=true` in the request url. Include human readable responses in response parsing tests >>> 1"
920,This is related to #22116. Core no longer needs `SocketPermission` \r\n`accept`. This permission is relegated to the transport-netty4 module \r\nand (for tests) to the mocksocket jar. Remove accept SocketPermissions from core >>> 1
921,"This also adds the infrastructure for adding new methods in the future.\r\n\r\nThere are a few TODOs left in the code that should be resolved before this PR gets merged, in particular around exception parsing which will be improved in other parallel PRs. Add get/exists method to RestHighLevelClient >>> 1"
922,Today we try to be smart and make a generic decision if an exception should\r\nbe treated as a document failure but in some cases concurrency in the index writer\r\nmake this decision very difficult since we don't have a consistent state in the case\r\nanother thread is currently failing the IndexWriter/InternalEngine due to a tragic event.\r\n\r\nThis change simplifies the exception handling and makes specific decisions about document failures\r\nrather than using a generic heuristic. This prevent exceptions to be treated as document failures\r\nthat should have failed the engine but backed out of failing since since some other thread has\r\nalready taken over the failure procedure but didn't finish yet.\r\n Fix handling of document failure exception in InternalEngine >>> 1
923,Elasticsearch 6.0 removes support for lenient\r\nbooleans (see #22200). With this commit we\r\ndeprecate all usages of non-strict booleans in\r\nElasticsearch 5.x so users can already spot\r\nimproper usages.\r\n\r\nRelates #22200\r\nRelates #22696 Deprecate lenient booleans >>> 1
924,The weight factor function does not check if the delegate score function needs to access the score of the query.\r\nThis results in a _score equals to 0 for all score function that set a weight.\r\nThis change modifies the WeightFactorFunction#needsScore to delegate the call to its underlying score function.\r\n\r\nFix #21483 Fix script score function that combines _score and weight >>> 1
925,"This commit ensures that the index.latest blob is first examined to\r\ndetermine the latest index-N blob id, before attempting to list all\r\nindex-N blobs and picking the blob with the highest N.\r\n\r\nIt also fixes the MockRepository#move so that tests are able to handle\r\nnon-atomic moves.  This is done by adding a special setting to the\r\nMockRepository that requires the test to specify if it can handle\r\nnon-atomic moves.  If so, then the MockRepository#move operation will be\r\nnon-atomic to allow testing for against such repositories. Fixes retrieval of the latest snapshot index blob >>> 1"
926,Introduce CheckedSupplier and CheckedRunnable functional interfaces \r\ninto core. These offer a checked version of the Supplier and Runnable\r\ninterfaces for use with lambda apis. Add CheckedSupplier and CheckedRunnable to core >>> 1
927,"This commit adds a SpecialPermission constant and uses that constant\r\nopposed to introducing new instances everywhere.\r\n\r\nAdditionally, this commit introduces a single static method to check that\r\nthe current code has permission. This avoids all the duplicated access\r\nblocks that exist currently. Add single static instance of SpecialPermission >>> 1"
928,This change updates the dependencies to Lucene 6.4.0 and fixes the compilation errors due to some changes in Lucene API. Upgrade to Lucene 6.4.0 >>> 1
929,To effectively allow a plugin to intercept a transport handler it needs\r\nto know if the handler must be executed even if there is a rejection on the\r\nthread pool in the case the wrapper forks a thread to execute the actual handler. Pass `forceExecution` flag to transport interceptor >>> 1
930,Adds unit tests for the value count aggregator.\r\n\r\nRelates #22278 [TEST] Add unit tests for ValueCountAggregator and InternalValueCount >>> 1
931,"When users need to specify a custom location for configuration files, they also need to specify a custom location for the jvm.options file yet our docs are absent in this regard. This commit adds a note to the rolling upgrade docs explaining this situation.\r\n\r\nCloses #22744\r\n Add note regarding custom jvm.options >>> 1"
932,"Closes #22521 \r\n\r\nTests don't check for the amount of used memory because that will tie-up the tests to the implementation. \r\nFrom what I saw there are no tests to check that the used memory is calculated correctly, but it may be just my lack of knowledge of the codebase. Also it is out of the scope of original ticket.  Add used memory amount to CircuitBreakingException message (#22521) >>> 1"
933,There was a typo in the `ParseField` declaration. I know\r\nwe want to port these parsers to `ObjectParser` eventually\r\nbut I don't have the energy for that today and want to get\r\nthis fixed.\r\n\r\nCloses #22722\r\n Fix parsing for `max_determinized_states` >>> 1
934,"This is a bespoke backport of #20109 for 5.x:\r\n\r\nCurrently, bulk item requests can be any ActionRequest, this PR restricts bulk\r\nitem requests to DocumentRequest. This simplifies handling failures during bulk\r\nrequests. Additionally, a new enum is added to DocumentRequest to represent the\r\nintended operation to be performed by a document request (create, index, update\r\nand delete), which was previously represented with a mix of strings and index\r\nrequest operation type.\r\n\r\nNow, index request operation type reuses the new enum to specify whether the\r\nrequest should create or index a document. Restricting bulk requests to\r\nDocumentRequest further simplifies execution of shard-level bulk operations to\r\nuse the same failure handling for index, delete and update operations. This PR\r\nalso fixes a bug which executed delete operations twice for replica copies while\r\nexecuting bulk requests.\r\n\r\nRelates to #19105 and #20109 Simplify bulk request execution >>> 1"
935,"This is related to #22116. `URLRepository` requires `SocketPermission` \r\n`connect`. This commit introduces a new module called ""repository-url"" \r\nwhere `URLRepository` will reside. With the new module, permissions can \r\nbe removed from core. Add repository-url module and move URLRepository >>> 1"
936,This commit replaces specialized functional interfaces in various\r\nplugins with generic options. Instead of creating `StorageRunnable`\r\ninterfaces in every plugin we can just use `Runnable` or `CheckedRunnable`.\r\n Use generic interfaces for checking socket access >>> 1
937,Add unit tests for `TopHitsAggregator` and convert some snippets in\r\ndocs for `top_hits` aggregation to `// CONSOLE`.\r\n\r\nRelates to #22278\r\nRelates to #18160 Add more tests for top_hits aggregation >>> 1
938,"Docker cgroups are mounted in the wrong place (i.e., inconsistently with /proc/self/cgroup). This commit adds an undocumented hack for working around, for now.\r\n\r\nRelates #21029 Add hack for Docker cgroups >>> 1"
939,"This change implements named configurations for s3 repository as\r\nproposed in #22520. The access/secret key secure settings which were\r\nadded in #22479 are reverted, and the only secure settings are those\r\nwith the new named configs. All other previously used settings for the\r\nconnection are deprecated.\r\n\r\ncloses #22520\r\n S3 repository: Add named configurations >>> 1"
940,"This PR adds a new option for `host_type`: `tag:TAGNAME` where `TAGNAME` is the tag field you defined for your ec2 instance.\r\n\r\nFor example if you defined a tag `my-elasticsearch-host` in ec2 and set it to `myhostname1.mydomain.com`, then\r\nsetting `host_type: tag:my-elasticsearch-host` will tell Discovery Ec2 plugin to read the host name from the\r\n`my-elasticsearch-host` tag. In this case, it will be resolved to `myhostname1.mydomain.com`.\r\n\r\nCloses #22566.\r\n Read ec2 discovery address from aws instance tags >>> 1"
941,"In some cases (apparently with outlook files), mime4j library is needed.\r\nWe removed it in the past which can cause elasticsearch to crash when you are using ingest-attachment (and probably mapper-attachments as well in 2.x series) with a file which requires this library.\r\n\r\nSimilar problem as the one reported at #22077. Add missing mime4j library >>> 1"
942,"Adds ""Appendix B. Painless API Reference"", a reference of all classes\r\nand methods available from Painless. Removes links to java packages\r\nbecause they contain methods that we don't expose and don't contain\r\nmethods that we do expose (the ones in Augmentation). Instead this\r\ngenerates a list of every class and every exposed method using the same\r\ntype information available to the\r\ninterpreter/compiler/whatever-we-call-it. From there you can jump to\r\nthe relevant docs.\r\n\r\nRight now you build all the asciidoc files by running\r\n```\r\ngradle generatePainlessApi\r\n```\r\n\r\nThese files are expected to be committed because we build the docs\r\nwithout running `gradle`.\r\n\r\nYou can also run it in an IDE safely if you pass the path to the\r\ndirectory in which to generate the docs as the first parameter. It'll\r\nblow away the entire directory an recreate it from scratch so be careful.\r\n\r\nAnd then you can build the docs by running something like:\r\n```\r\n../docs/build_docs.pl --out ../built_docs/ --doc docs/reference/index.asciidoc --open\r\n```\r\n\r\nThat is, if you have checked out https://github.com/elastic/docs in\r\n`../docs`. Wait a minute or two and your browser will pop open in with\r\nall of Elasticsearch's reference documentation. If you go to\r\n`http://localhost:8000/painless-api-reference.html` you can see this\r\nlist. Or you can get there by following the links to `Modules` and\r\n`Scripting` and `Painless` and then clicking the link in the paragraphs\r\nbelow titled `Appendix B. Painless API Reference`.\r\n\r\nI like having these in asciidoc because we can deep link to them from the\r\nrest of the guide with constructs like\r\n`<<painless-api-reference-Object-hashCode-0>>` and\r\n`<<painless-api-reference->>` and we get link checking. Then the only\r\nbrittle link maintenance bit is the link generation for javadoc. Which\r\nsucks. But I think it is important that we link to the methods directly\r\nso they are easy to find.\r\n Generate reference links for painless API >>> 1"
943,Quick fix to `scripted_metric` aggregation to provide a more intuitive behavior when `params` without `_agg` are being specified.\n\nCloses #19768\n Scripted_metric _agg parameter disappears if params are provided >>> 0
944,"This PR adds support for `histogram` and `date_histogram` agg compound `order` by refactoring and reusing `terms` agg `order` code. The major change is that the `Terms.Order` and `Histogram.Order` classes have been replaced/refactored into a new class `BucketOrder`. This is a **breaking change** for the Java Transport API. For backward compatibility with previous ES versions the `(date)histogram` compound `order` will use the first order. Also the `_term` and `_time` aggregation order keys have been **deprecated**; replaced by `_key`.\r\n\r\nRelates to #20003: now that all these aggregations use the same `order` code, it should be easier to move validation to parse time (as a follow up PR).\r\n\r\nRelates to #14771: `histogram` and `date_histogram` aggregation `order` will now be validated at reduce time.\r\n\r\nCloses #23613: if a single `BucketOrder` that is not a tie-breaker is added with the Java Transport API, it will be converted into a `CompoundOrder` with a tie-breaker. Compound order for histogram aggregations  >>> 1"
945,The output of the `ElasticsearchException.generateThrowableXContent()` method can be parsed back by the `ElasticsearchException.fromXContent()` method.\r\n\r\nThis commit adds unit tests in the style of the `to-and-from-xcontent` tests we already have for other parsing methods. It also relax the strict parsing of the `ElasticsearchException.fromXContent()` so that it does not throw an exception when custom metadata of type array & objects are parsed. Add parsing method for ElasticsearchException.generateThrowableXContent() >>> 1
946,Fixes https://github.com/elastic/elasticsearch/issues/22770 Add null check in case of orphan child document >>> 1
947,This commit removes the search type `dfs_query_and_fetch` without a\r\nreplacement. We don't allow to use this type via REST since 2.x\r\nbut still keep it around for no particular reason. There we no users\r\ncomplaining about the availability. This should now be removed from the\r\ncodebase. `query_and_fetch` is still used internally to safe a roundtrip\r\nif there is only one shard but it can't be used via the rest interface.\r\n Remove DFS_QUERY_AND_FETCH as a search type >>> 1
948,"Currently, any write (e.g. `index`, `delete`) operation failure can be categorized as:\r\n- request failure (e.g. analysis, parsing error, version conflict)\r\n- transient operation failure (e.g. due to shard initializing, relocation)\r\n- environment failure (e.g. out of disk, corruption, lucene tragic event)\r\n\r\nThe main motivation of the PR is to handle these failure types appropriately for a\r\nwrite request. Each failure type needs to be handled differently:\r\n- request failure (being request specific) should be replicated and then failed\r\n- transient failure should be retried (eventually succeeding)\r\n- environment failure (persistent primary shard failure) should fail the request\r\n  immediately.\r\n\r\nCurrently, transient operation failures are retried in replication action but no distinction\r\nis made between request and environment failures, both fails write request immediately.\r\n\r\nIn this PR, we distinguish between request and environment failures for a write operation.\r\nIn case of environment failures, the exception is bubbled up failing the request and in case\r\nof request failures, the exception is captured and replication continues (we ignore performing\r\non replicas when such failures occur in primary). Transient operation failures are bubbled up\r\nto be retried by the replication operation, as before.\r\n\r\nrelates to #19105\r\n\r\nNOTE: https://github.com/elastic/elasticsearch/pull/22718 has to be backported as well to fix document failure handling in InternalEngine, \r\nI intend on backporting it after this is merged.\r\n Simplify write failure handling (backport of #19105) >>> 1"
949,"This is related to #22116. The repository-hdfs plugin opens socket\r\nconnections. As `SocketPermission` is transitioned out of core, hdfs\r\nwill require `connect` permission. This pull request wraps operations\r\nthat require this permission in `doPrivileged` blocks. Add doPrivilege blocks for socket connect ops in repository-hdfs >>> 1"
950,Some tests verify that all connection have been closed but due to the\r\nasync / concurrent nature of `RemoteClusterConnection` there are situations\r\nwhere we notify listeners that trigger tests to finish before we actually\r\nclosed all connections. The race is very very small and has no impact on the\r\ncode correctness. This commit documents and improves the way we close\r\nconnections to ensure test won't fail with false positives.\r\n\r\nCloses #22803 Improve connection closing in `RemoteClusterConnection` >>> 1
951,These changes will be effective when we upgrade to Lucene 7. But I'm afraid that I forget to document them when that happens so I think it's fine to add them now. Document upcoming scoring changes. >>> 1
952,"Today we cache query results even if the query timed out. This is obviously\r\nproblematic since results are not complete. Yet, the decision if a query timed\r\nout or not happens too late to simply not cache the result since if we'd just throw\r\nan exception all currently waiting requests with the same request / cache key would\r\nfail with the same exception without the option to access the result or to re-execute.\r\nInstead, this change will allow the request to enter the cache but invalidates it immediately.\r\nConcurrent request might not get executed and return the timed out result which is not absolutely\r\ncorrect but very likely since identical requests will likely timeout as well. As a side-effect\r\nwe won't hammer the node with concurrent slow searches but rather only execute one of them\r\nand return shortly cached result.\r\n\r\nCloses #22789 Invalidate cached query results if query timed out >>> 1"
953,"Cancelling tasks with no cancellable children can cause the cancellation operation to hang. This commit fixes this issue.\r\n\r\n@nik9000, I found a bug in my previous PR, could you take a look? Fix hanging cancelling task with no children >>> 1"
954,"At this point AbstractSearchAsyncAction is just a base-class for the first phase of a search where we have multiple replicas\r\nfor each shardID. If one of them is not available we move to the next one. Yet, once we passed that first stage we have to work with\r\nthe shards we succeeded on the initial phase.\r\nUnfortunately, subsequent phases are not fully detached from the initial phase since they are all non-static inner classes.\r\nIn future changes this will be changed to detach the inner classes to test them in isolation and to simplify their creation.\r\nThe AbstractSearchAsyncAction should be final and it should just get a factory for the next phase instead of requiring subclasses\r\netc. First step towards separating individual search phases >>> 1"
955,This commit adds a `ElasticsearchException.failureFromXContent()` that can be used to parse the result of `ElasticsearchException.generateFailureXContent()`. Add parsing method for ElasticsearchException.generateFailureXContent() >>> 1
956,Remove `FromXContent` and use `CheckedFunction` instead.\r\nRemove `FromXContentWithContext` and use `ContentParser` instead. Merge some equivalent interfaces >>> 1
957,"Today, the relationship between Lucene and the translog is rather simple: every document not in Lucene is guaranteed to be in the translog. We need a stronger guarantee from the translog though, namely that it can replay all operations after a certain sequence number. For this to be possible, the translog has to made sequence-number aware. As a first step, we introduce the min and max sequence numbers into the translog so that each generation knows the possible range of operations contained in the generation. This will enable future work to keep around all generations containing operations after a certain sequence number (e.g., the global checkpoint).\r\n\r\nRelates #10708 Introduce sequence-number-aware translog >>> 1"
958,"Performance testing by @danielmitterdorfer revealed single\r\nindex/delete operations have similar performance (indexing\r\nthroughput) to equivalent single item bulk request.\r\nThis PR reduces the code paths to executing single write\r\noperations, by reusing the logic in (shard) bulk action for\r\nexecuting single operation as a single-item bulk request.\r\n\r\nrelates to #21964\r\n Make index and delete operation execute as single bulk item (backport of #21964) >>> 1"
959,"Currently, if a previously allocated shard has no in-sync copy in the\r\ncluster, but there is a stale replica copy, the explain API does not\r\ninclude information about the stale replica copies in its output.  This\r\ncommit includes shard copy information available (even for stale\r\ncopies or corrupt copies) for all nodes in the cluster, when explaining \r\nan unassigned primary shard that was previously allocated in the cluster.\r\n\r\nThis situation can arise as follows: imagine an index with 1 primary and\r\n1 replica and a cluster with 2 nodes.  If the node holding the replica\r\nis shut down, and data continues to be indexed, only the primary will\r\nhave the latest data and the replica that has gone offline will be\r\nmarked as stale.  Now, suppose the node holding the primary is shut\r\ndown.  There are no copies of the shard data in the cluster.  Now, start\r\nthe first stopped node (holding the stale replica) back up.  The cluster\r\nis red because there is no in-sync copy available.  Running the explain\r\nAPI before would inform the user that there is no valid shard copy in\r\nthe cluster for that shard, but it would not provide any information\r\nabout the existence of the stale replica that exists on the restarted\r\nnode.  With this commit, the explain API provides information about all\r\nthe shard copy information when explaining the unassigned primary. Include stale replica shard info when explaining an unassigned primary >>> 1"
960,This permission is relegated to these modules/plugins:\r\n- transport-netty4 module\r\n- reindex module\r\n- repository-url module\r\n- discovery-azure-classic plugin\r\n- discovery-ec2 plugin\r\n- discovery-gce plugin\r\n- repository-azure plugin\r\n- repository-gcs plugin\r\n- repository-hdfs plugin\r\n- repository-s3 plugin\r\n\r\nAnd for tests:\r\n- mocksocket jar\r\n- rest client\r\n- httpcore-nio jar\r\n- httpasyncclient jar Remove connect SocketPermissions from core >>> 1
961," #22194 gave us the ability to open low level temporary connections to remote node based on their address. With this use case out of the way, actual full blown connections should validate the node on the other side, making sure we speak to who we think we speak to. This helps in case where multiple nodes are started on the same host and a quick node restart causes them to swap addresses, which in turn can cause confusion down the road. TransportService.connectToNode should validate remote node ID >>> 1"
962,"This moves the building blocks for delete by query into core. This\r\nshould enabled two things:\r\n1. Plugins other than reindex to implement ""bulk by scroll"" style\r\noperations.\r\n2. Plugins to directly call delete by query. Those plugins should\r\nbe careful to make sure that task cancellation still works, but\r\nthis should be possible.\r\n\r\nNotes:\r\n1. I've mostly just moved classes and moved around tests methods.\r\n2. I haven't been super careful about cohesion between these core\r\nclasses and reindex. They are quite interconnected because I wanted\r\nto make the change as mechanical as possible.\r\n\r\nCloses #22616 Move delete by query helpers into core >>> 1"
963,"This PR adds a new option for `host_type`: `tag:TAGNAME` where `TAGNAME` is the tag field you defined for your ec2 instance.\r\n\r\nFor example if you defined a tag `my-elasticsearch-host` in ec2 and set it to `myhostname1.mydomain.com`, then\r\nsetting `host_type: tag:my-elasticsearch-host` will tell Discovery Ec2 plugin to read the host name from the\r\n`my-elasticsearch-host` tag. In this case, it will be resolved to `myhostname1.mydomain.com`.\r\n\r\nRelates to #22566.\r\nBackport of #22743 for 5.x branch (5.3)\r\n Read ec2 discovery address from aws instance tags >>> 1"
964,"This migration script existed from moving plugins into the ES repo in\r\n2.0. It is no longer needed (and contains eg maven commands), and if we\r\ndid need any part of it, it will still exist in git history. Build: Remove legacy migration script >>> 1"
965,nan Docs: Add setup section for the keystore tool and secure settings >>> 1
966,These are deprecated in 5.x. This commit removes support for them in 6.0. S3 Repository: Remove env var and sysprop credentials support >>> 1
967,relates #22761 S3 Repository: Deprecate auto creation of s3 bucket for repository >>> 1
968,closes #22761 S3 Repository: Remove bucket auto create >>> 1
969,"The region and endpoint settings overlap, and have complicated logic for\r\nwhich to use. Futhermore, region is not necessary, as for most cases it\r\nis automatically figured out by the s3 client, given the bucket name,\r\nand for custom cases like a new region, the endpoint is necessary\r\nanyways. This commit adds a deprecation warning when specifying region.\r\n\r\nrelates #22758  S3 Repository: Deprecate specifying region >>> 1"
970,This change removes the ability to set region for s3 repositories.\r\nEndpoint should be used instead if a custom s3 location needs to be\r\nused.\r\n\r\ncloses #22758 S3 Repository: Remove region setting >>> 1
971,This PR adds deprecation logging to `GeoDistanceRangeQueryBuilder` for 5.x and a deprecation note in the reference documentation.\r\n Geo distance range deprecation >>> 1
972,"Reported at: https://discuss.elastic.co/t/combine-elasticsearch-5-1-1-and-repository-hdfs/69659\r\n\r\nIf you define as described [in our docs](https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-hdfs-config.html) the following `elasticsearch.yml` settings:\r\n\r\n```yml\r\nrepositories:\r\n  hdfs:\r\n    uri: ""hdfs://es-master:9000/"" # optional - Hadoop file-system URI\r\n    path: ""some/path"" # required - path with the file-system where data is stored/loaded\r\n```\r\n\r\nIt fails at startup because we don't register the global setting `repositories.hdfs.path` in `HdfsPlugin`.\r\n\r\nThis PR removes that from our docs so people must provide those settings only when registering the repository with:\r\n\r\n```\r\nPUT _snapshot/my_hdfs_repository\r\n{\r\n  ""type"": ""hdfs"",\r\n  ""settings"": {\r\n    ""uri"": ""hdfs://namenode:8020/"",\r\n    ""path"": ""elasticsearch/respositories/my_hdfs_repository"",\r\n    ""conf.dfs.client.read.shortcircuit"": ""true""\r\n  }\r\n}\r\n```\r\n\r\nBased on issue #22800.\r\n\r\nCloses #22301 repositories.hdfs.path can not be set >>> 1"
973,"This PR prepares us for changes coming in a new build of `elastic/sles-12-x86_64` Vagrant box, mostly related to upgrading to SLES-12-SP2.\r\n\r\nWith SLES-12 SP2, there is a system package available for `java-1_8_0-openjdk` and thus we can deprecate the OpenSUSE external repo.\r\n\r\nWe also remove the `puppetlabs-pc1` repo from Puppetlabs. With the existing SLES-12 vagrant boxes Puppet+facter get installed from source but going forward the official Puppetlabs repo is pre-installed.\r\n\r\nAlso we remove the `src rpm` repo which is not needed as we don't build packages from source and leads to errors about missing `media.2` directory with `zypper addrepo`. Prepare Vagrantfile tests for SLES12-SP2 >>> 1"
974,"This patch significantly reduces memory usage when indexing large documents.\r\n\r\nApparently the whole input document (`source`) is rendered into `IndexRequest` `toString()` result.\r\nAnd two distinct copies of such `String`s are then used as `ReplicationTask` descriptions.\r\n(in `TransportReplicationAction.PrimaryPhase` and `TransportReplicationAction.ReroutePhase`)\r\n\r\nThus, for 10 MB input in UTF-8 there would be about 40 MB of 'excessively allocated' memory.\r\n\r\nThe patch changes `IndexRequest.toString()` implementation so that only piece of large `source` is used.\r\n\r\n Limit IndexRequest toString() length >>> 1"
975,"The sampler and diversified_sampler aggs were missing executable CONSOLE tests.\r\n\r\nThe scores produced by the example queries are sensitive to changes in the index (e.g. numbers of deleted docs) so I have deliberately added logic to copy actual result values into the expected results we document in the example. I also wanted to provide examples to users that provide doc counts from multiple-shards but actually for test purposes wanted to use a single shard to produce deterministic results. These actual-vs-expected differences were, again, patched by test-response manipulations.\r\n\r\nCloses #22746\r\n docs / test enhancement - added CONSOLE scripts for sampler aggs >>> 1"
976,"The region setting was added for named clients before the decision was\r\nmade to remove region altogether. This change removes the setting,\r\nbefore it has been released, so as not to introduce an already\r\ndeprecated setting.\r\n S3 Repository: Remove client region >>> 1"
977,"Follow up of #22857 where we deprecate automatic creation of azure containers.\r\n\r\nBTW I found that the `AzureSnapshotRestoreServiceIntegTests` does not bring any value because it runs basically a Snapshot/Restore operation on local files which we already test in core.\r\n\r\nSo instead of trying to fix it to make it pass with this PR, I simply removed it.\r\n Remove auto creation of container for azure repository >>> 1"
978,Similar to #22843 but for azure\r\n\r\nNote that we need to add also this information to the deprecation documentation once it will be available (see #22856).\r\n Deprecate auto creation of container for azure repository >>> 1
979,This commit adds a BytesRestResponse.errorFromXContent() method to parse the error returned by BytesRestResponse. Add parsing method to BytesRestResponse's error >>> 1
980,"The seq# base recovery logic relies on rolling back lucene to remove any operations above the global checkpoint. This part of the plan is not implemented yet but have to have these guarantees. Instead we should make the seq# logic validate that the last commit point (and the only one we have) maintains the invariant and if not, fall back to file based recovery.\r\n\r\n This commit adds a test that creates situation where rollback is needed (primary failover with ops in flight) and fixes another issue that was surfaced by it - if a primary can't serve a seq# based recovery request and does a file copy, it still used the incoming `startSeqNo` as a filter.\r\n\r\n Relates to #22484 & #10708 Seq Number based recovery should validate last lucene commit max seq# >>> 1"
981,"Removed by https://github.com/elastic/elasticsearch/pull/19649 [Doc]Not support ""M"" time unit in offset param >>> 1"
982,"With the new secure settings, methods like getAsMap() no longer work\r\ncorrectly as a means of checking for empty settings, or the total size.\r\nThis change converts the existing uses of that method to use methods\r\ndirectly on Settings. Note this does not update the implementations to\r\naccount for SecureSettings, as that will require a followup which\r\nchanges how secure settings work.\r\n Internal: Convert empty and size checks of settings to not use getAsMap() >>> 1"
983,"This commit completes the bump of the Elasticsearch version to 5.3.0.  The version number was bumped to 5.3.0 in the places required, there were only a couple things left over to generate for backward compatibility tests.\r\n Bumps the Elasticsearch version number to 5.3.0 >>> 1"
984,"* Adds `ScriptDocValues.Longs#getDates` which can be used to iterate over all date doc values. Attempts to minimize the number of allocations involved without adding a lot of management burden.\r\n* Documents how date functions are exposed to painless. Other than `getDates` above, Painless had fairly decent, undocumented date support.\r\n* Adds some tests for painless and dates.\r\n* Improves the error message when a script attempts to write to a `ScriptDocValues`.\r\n* Removes some unneeded wrapping on `getValues`. Now that we have the nicer error messages it is more obvious *why* it is unneeded.\r\n\r\nCloses #22162 Expose multi-valued dates to scripts and document painless's date functions >>> 1"
985,"This disallows object mappings that would accidentally create something like\r\n`foo..bar`, which is then unparsable for the `bar` field as it does not know\r\nwhat its parent is.\r\n\r\nResolves #22794 Disallow introducing illegal object mappings (double '..') >>> 1"
986,"In preparation for being able to parse SearchResponse from its rest\r\nrepresentation, this adds fromXContent to Suggest and nested Suggestion\r\nclasses and its child objects.\r\n\r\nThis is still WIP since we might get a better way to determine the parsed suggestion type with another PR.  Add parsing from xContent to Suggest >>> 0"
987,"In some cases (apparently with outlook files), mime4j library is needed.\r\nWe removed it in the past which can cause elasticsearch to crash when you are using ingest-attachment (and probably mapper-attachments as well in 2.x series) with a file which requires this library.\r\n\r\nBackport of #22764 in 5.x branch\r\nApplying same changes for mapper-attachments plugin (5.x branch) Add missing mime4j library >>> 1"
988,"This changes the way that replica failures are handled such that not all\r\nfailures will cause the replica shard to be failed or marked as stale.\r\n\r\nIn some cases such as refresh operations, or global checkpoint syncs, it is\r\n""okay"" for the operation to fail without the shard being failed (because no data\r\nis out of sync). In these cases, instead of failing the shard we should simply\r\nfail the operation, and, in the event it is a user-facing operation, return a\r\n5xx response code including the shard-specific failures.\r\n\r\nThis was accomplished by having two forms of the `Replicas` proxy, one that is\r\nfor non-write operations that does not fail the shard, and one that is for write\r\noperations that will fail the shard when an operation fails.\r\n\r\nRelates to #10708 Change certain replica failures not to fail the replica shard >>> 1"
989,"This commit adds clearer error message when trying to find the version of java, but\r\nthe jjs binary is not found.\r\n\r\ncloses #22898\r\n Build: Add nicer error message for pre java 8 >>> 1"
990,We were incorrectly resolving qualified method references at run\r\ntime when invoked on `def`. This lead to errors like\r\n`The struct with name [org] has not been defined.` when attempting\r\n\r\n```\r\ndoc.date.dates.stream().map(\r\n  org.joda.time.ReadableDateTime::centuryOfEra\r\n).collect(Collectors.toList())\r\n```\r\n Fix def invoked qualified method refs >>> 1
991,"This commit change `ElasticsearchException.failureFromXContent()` method so that it now parses root causes which were ignored before, and adds them as suppressed exceptions of the returned exception.\r\n\r\nSadly, it is very difficult to parse the `root_causes` array in the `failureFromXContent()` method while delegating the parsing of other fields to `innerFromXContent()`. So the parsing of root causes has been directly implemented in `innerFromXContent()` but only activated when parsing failures. This is a tradeoff to keep parsing methods simple. Parse elasticsearch exception's root causes >>> 1"
992,"Currently, update action internally uses deprecated index and delete\r\ntransport actions. As of #21964, these transport actions were deprecated\r\nin favour of using single item bulk request. In this commit, update action\r\nuses single item bulk action.\r\n Use bulk action internally for update >>> 1"
993,Backport of https://github.com/elastic/elasticsearch/pull/22718\r\n\r\nToday we try to be smart and make a generic decision if an exception should\r\nbe treated as a document failure but in some cases concurrency in the index writer\r\nmake this decision very difficult since we don't have a consistent state in the case\r\nanother thread is currently failing the IndexWriter/InternalEngine due to a tragic event.\r\n\r\nThis change simplifies the exception handling and makes specific decisions about document failures\r\nrather than using a generic heuristic. This prevent exceptions to be treated as document failures\r\nthat should have failed the engine but backed out of failing since since some other thread has\r\nalready taken over the failure procedure but didn't finish yet.\r\n Fix handling of document failure exception in InternalEngine (backport #22718) >>> 1
994,This commit adds two additional test cases that can be used to verify correct diff serialization in additional to binary and xcontent serialization.\r\n\r\nExtends #22281 to also include tests for diff-based serialization Expand AbstractSerializingTestCase and AbstractWireSerializingTestCase to test diff serialization >>> 1
995,"Hi,\r\n\r\nAdded check for null plugin name passed to removePlugin.\r\n\r\nAlso renamed pluginId to pluginName in install for consistency with docs.\r\nhttps://www.elastic.co/guide/en/elasticsearch/plugins/current/listing-removing.html\r\n\r\nAppears to fix issue when I recreated the distribution. Let me know your thoughts.\r\n\r\nCloses #22922  Add check for null pluginName in remove command >>> 1"
996,"In 5.2 we stopped sending the source parameter if the user didn't\r\nspecify it. This was a mistake as versions before 2.0 look like\r\nthey don't always include the `_source`. This is because reindex\r\nrequests some metadata fields. Anyway, now we say `""_source"": true`\r\nif there isn't a `_source` configured in the reindex request.\r\n\r\nCloses #22893\r\n Fix reindex-from-remote from <2.0 >>> 1"
997,This adds fromXContent to MainResponse so we can use it in the java rest client.\r\n Add parsing from xContent to MainResponse >>> 1
998,"GeoDistance query, sort, and scripts make use of a crazy GeoDistance enum for handling 4 different ways of computing geo distance: `SLOPPY_ARC`, `ARC`, `FACTOR`, and `PLANE`. Only two of these are useful: `ARC`, `PLANE`. This PR acomplishes the following:\n- remove `SLOPPY_ARC`, and `FACTOR`. \n- remove overkill `FixedSourceDistance` helper classes for comuputing Geo distance\n- Refactor and deprecate legacy GeoDistance query classes and helpers\n Reduce GeoDistance Insanity >>> 1"
999,"This commit adds a ""how to"" guide on how to bump the version number in Elasticsearch.  Its a very error prone process at the moment, which presents the opportunity to miss things easily, so this document's goal is to capture all of the steps involved in bumping the version number. A ""how to"" guide on bumping the version number in Elasticsearch >>> 0"
1000,This change also removes the reference to the difference between full name and index name.\r\nThey are always the same since 2.x and `name` does not refer anymore to `author.name` automatically.\r\nA simple pattern must be used instead.\r\nRemove redundant code that checks the field name twice. Consilify get-field-mapping docs >>> 1
1001,"Imagine you are doing something like this:\r\n```java\r\nStringBuilder sb = new StringBuilder();\r\n// ... adding some heavy data to 'sb'\r\nsb.append(""indices["");\r\nStrings.arrayToDelimitedString(indices, "","", sb);\r\nsb.append(""], "");\r\nsb.append(""types["");\r\nStrings.arrayToDelimitedString(types, "","", sb);\r\nsb.append(""]"");\r\n```\r\n\r\nAnd before you know it, you've just allocated two extra 5 MB `String` instances on the heap. Do not create String instances in 'Strings' methods accepting StringBuilder >>> 1"
1002,"When a primary is relocated from an old node to a new node, it can have ops in its translog that do not have a sequence number assigned. When a file-based recovery is started, this can lead to skipping these ops when replaying the translog due to a bug in the recovery logic. This commit addresses this bug and adds a test in the BWC tests.\r\n\r\nRelates #22484\r\n Avoid losing ops in file-based recovery >>> 1"
1003,"The current rest backcompat tests, which run against a mixed cluster of\r\n5.x and 6.0 nodes, depend on snapshot builds of 5.x. However, this has\r\nthe potential for inconsistency that results in CI failures, and happens\r\nquite often, whenever some backcompat logic is added to 5.x, but the bwc\r\ntest on master fails because the 5.x code has not yet been published as\r\na snapshot.\r\n\r\nThis change creates a git worktree of the 5.x branch (detached),\r\nbuilds the zip distribution, and ties that into gradle substitutions for\r\nthe 5.x version.\r\n Tests: Use local clone build of 5.x with bwc tests >>> 1"
1004,"This adds a callout to note that the Low Level REST Client example sets the `ContentType` explicitly and that users should do the same (`ContentType` is required by all of the `HttpEntity`'s that I am aware of, but it's not required to be set properly).\r\n\r\nI will drop the `Future releases of Elasticsearch will require this to be set properly.` sentence from the master branch in a follow-up commit.\r\n\r\nRelates to #19388 [DOCS] Low Level REST Client should emphasize ContentType >>> 1"
1005,Instead of longs. If you want millis since epoch you can call `doc.date_field.value.millis`.\r\n\r\nRelates to #22875 Make dates be ReadableDateTimes in scripts >>> 1
1006,"This change switches to using jrunscript, instead of jjs, for detecting\r\nversion properties of java, which is available on java versions prior to 8.\r\n\r\ncloses #22898 Build: Improve detection of older java versions >>> 1"
1007,"Versions of Elasticsearch prior to 2.0 would return a scroll id\r\neven with the last scroll response. They'd then automatically\r\nclear the scroll because it is empty. When terminating reindex\r\nwill attempt to clear the last scroll it received, regardless of\r\nthe remote version. This quiets the warning when the scroll cannot\r\nbe cleared for versions before 2.0.\r\n\r\nCloses #22937\r\n Reindex: do not log when can't clear old scroll >>> 1"
1008,Relates to #22278\r\n [TEST] Added unit tests for sum aggs. >>> 0
1009,"This commit upgrades the checkstyle configuration from version 5.9 to\r\nversion 7.5, the latest version as of today. The main enhancement\r\nobtained via this upgrade is better detection of redundant modifiers.\r\n Upgrade checkstyle to version 7.5 >>> 1"
1010,"Let's make our life easier when debugging/testing.\r\nAlso having a flat dir helps us to compare or ""synchronize"" more easily with Tika project files.\r\n\r\nCloses #22958. Replace tika-files.zip by a tika-files dir >>> 1"
1011,It was accidentally renamed `enable_position_increment` in the cleanups\r\nfor 5.0. This adds `enable_position_increment` as a deprecated alias\r\nso it will continue to work.\r\n Fix name of `enabled_position_increments` >>> 1
1012,Debian 8 has been having issues with the openjdk package dependencies\r\nbeing broken. This comment comments out debian-8 from the boxes which\r\npackaging tests will run on CI. Build: Temporarily remove debian 8 for packaging tests >>> 1
1013,Painless can cast anything into the magic type `def` but it\r\nreally shouldn't try to cast **nothing** into `def`. That causes\r\nthe byte code generation library to freak out a little.\r\n\r\nCloses #22908\r\n Don't allow casting from void to def in painless >>> 1
1014,"This commit adds parsing support for the newline delimited JSON Content-Type, which is how\r\nthe bulk, multi-search, and multi-search template APIs expect data to be formatted. The\r\n`elasticsearch-js` client has also been using this content type for these types of requests.\r\n\r\nCloses #22943\r\n Add support for newline delimited JSON Content-Type >>> 1"
1015,"Now that debian is disabled, we are seeing similar failures with fedora\r\nnot able to install java. This commit temporarily disables fedora until\r\nit is once again stable. Build: Disable fedora for packaging tests >>> 1"
1016,"This pull request adds a new parameter to the REST Search API named `prefixed_name`. When set to true, the aggregation names in the search response are prefixed with a prefix that reflects the internal type of the aggregation.\r\n\r\nHere is a simple example with a `terms` aggregation called ""tweets_per_user"":\r\n```\r\nGET /_search?prefixed_name\r\n{\r\n    ""aggs"": {\r\n        ""tweets_per_user"": {\r\n            ""terms"": {\r\n                ""field"": ""user""\r\n            }\r\n        }\r\n    },\r\n    ""size"": 0\r\n}\r\n```\r\n\r\nIn, the response, the aggregation appears with the name ""sterms:tweets_per_user"":\r\n\r\n```\r\n{\r\n    ""aggs"": {\r\n        ""sterms:tweets_per_user"": {\r\n            ...\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis parameter is intended to make life easier for REST clients that could parse back the prefix and detect the type of the aggregation to parse. It could also be implemented for suggesters.\r\n\r\nNote that the name is changed when the XContent is printed out, it should be an issue for derivative aggregations that use bucket_paths. Also, name suggestion for `prefixed_name` as well as feedback on test coverage are warmly welcome. Add parameter to prefix aggs name with type in search responses >>> 1"
1017,"This is related to #22116. This commit adds calls that require\r\n`SocketPermission` `connect` to forbidden APIs.\r\n\r\nThe following calls are now forbidden:\r\n- java.net.URL#openStream()\r\n- java.net.Socket#connect(java.net.SocketAddress)\r\n- java.net.Socket#connect(java.net.SocketAddress, int)\r\n- java.nio.channels.SocketChannel#open(java.net.SocketAddress)\r\n- java.nio.channels.SocketChannel#connect(java.net.SocketAddress) Add methods requiring connect to forbidden apis >>> 1"
1018,Relates to #22970 Deprecate the include_in_all mapping parameter >>> 1
1019,"In order to support the evolving GeoPoint encodings in Lucene 5 and 6, ES 2.x and 5.x implements an abstraction layer to the `GeoPointFieldMapper` classes. As of 5.0 the `geo_point` field mapper settled on using Lucene's more performant `LatLonPoint` field type and deprecated all other encodings but used the abstraction layer to support backward compatibility. In ES 6.0 all deprecated encodings have been removed rendering this abstraction layer useless. This PR removes the abstraction layer and renames `LatLonPointFieldMapper` back to `GeoPointFieldMapper` to mantain consistency with ES field naming.\r\n Remove GeoPointFieldMapper abstractions >>> 1"
1020,You can read changes at https://github.com/apache/lucene-solr/blob/branch_6_4/lucene/CHANGES.txt.\r\n Upgrade to Lucene 6.4.1. >>> 1
1021,"Generalizes two previously hard coded things in painless into\r\ngeneric concepts:\r\n\r\n1. The ""main method"" is no longer hardcoded to:\r\n```\r\npublic abstract Object execute(Map<String, Object> params,\r\n        Scorer scorer, LeafDocLookup doc, Object value);\r\n```\r\nInstead Painless's compiler takes an and implements that. It looks like:\r\n```\r\npublic interface SomeScript {\r\n    // Argument names we expose to Painless scripts\r\n    String[] ARGUMENTS = new String[] {""a"", ""b""};\r\n    // Method implemented by Painless script. Must be named execute but can have any parameters or return any value.\r\n    Object execute(String a, int b);\r\n    // Is the ""a"" argument used by the script?\r\n    boolean uses$a();\r\n}\r\nSomeScript script = scriptEngine.compile(SomeScript.class, null, ""the_script_here"", emptyMap());\r\nObject result = script.execute(""a"", 1);\r\n```\r\n\r\n`PainlessScriptEngine` now compiles all scripts to the new\r\n`GenericElasticsearchScript` interface by default for compatibility\r\nwith the rest of Elasticsearch until it is able to use this new\r\nability.\r\n\r\n2. `_score` and `ctx` are no longer hardcoded to be extracted from\r\n`#score` and `params` respectively. Instead Painless's default\r\nimplementation of Elasticsearch scripts uses the `uses$_score` and\r\n`uses$ctx` methods to determine if it is used and gives them\r\ndummy values if they are not used.\r\n\r\n3. Throwing the `ScriptException` is now handled by the Painless\r\nscript itself. That way Painless doesn't have to leak the metadata\r\nthat is required to build the fancy stack trace. And all painless scripts\r\nget the fancy stack trace. Allow painless to implement more interfaces >>> 1"
1022,to numeric datatypes\r\n Add half_float and scaled_float >>> 1
1023,Heading should refer to custom normalizers\r\n Update normalizers.asciidoc >>> 1
1024,Relates to #22278 Add unit tests to histogram aggregations. >>> 1
1025,"When a node receives a new cluster state from the master, it opens up connections to any new node in the cluster state. That has always been done serially on the cluster state thread but it has been a long standing TODO to do this concurrently, which is done by this PR.\r\n\r\nThis is spin off of #22828, where an extra handshake is done whenever connecting to a node, which may slow down connecting. Also, the handshake is done in a blocking fashion which triggers assertions w.r.t blocking requests on the cluster state thread. Instead of adding an exception, I opted to implement concurrent connections which both side steps the assertion and compensates for the extra handshake. \r\n\r\nThe change is well tested under current tests. Sadly, I could find an easy way to simulate rejections as we have locked down the thread pool settings (good!) and the management still has an unbound queue. Connect to new nodes concurrently >>> 1"
1026,"This is in order to trigger listeners for disconnect events, most importantly the NodeFaultDetection\r\n\r\nThis is the cause of failure in https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.2+multijob-unix-compatibility/os=debian/72 MockTransportService should physically disconnect when simulating it >>> 1"
1027,"Change `org.elasticsearch.bootstrap.JNAKernel32Library$SizeT` constructor's modifier to public.\r\nOtherwise `NativeMappedConverter` can't construct this class. @peteharverson Has verified that this fix works.\r\n\r\nThis bug never got released in any version, so marking this pr as non-issue.\r\n\r\nCloses #22991 Change `JNAKernel32Library$SizeT` constructor's modifier to public >>> 1"
1028,This setting was missed in the great settings refactoring and should be exposed\r\nvia node level settings. Expose `search.highlight.term_vector_multi_value` as a node level setting >>> 1
1029,`QUERY_AND_FETCH` has been treated as an internal optimization for 2 major\r\nversions. This commit removes the search type and it's implementation details and\r\nfolds the optimization in the case of a single shard into the search controller such\r\nthat every search with a single shard (non DFS) will receive this optimization.\r\n Remove QUERY_AND_FETCH search type >>> 1
1030,Adding tests for AvgAggregator and InternalAvg.\r\n\r\nRelates to #22278  [Tests] Adding tests for AvgAggregator and InternalAvg >>> 1
1031,"This commit changes the exception type thrown when trying to\r\ncreate a snapshot with a name that already exists in the repository.\r\nInstead of throwing a SnapshotCreateException, which results in a\r\ngeneric 500 status code, a duplicate snapshot name will throw a\r\nInvalidSnapshotNameException, which will result in a 400 status code\r\n(bad request).\r\n\r\nNote that before 5.0, a duplicate snapshot name would throw an \r\nInvalidSnapshotNameException.  After #18228, the exception type was\r\ninadvertently changed. Duplicate snapshot name throws InvalidSnapshotNameException >>> 1"
1032,"Secure settings from the elasticsearch keystore were not yet validated.\r\nThis changed improves support in Settings so that secure settings more\r\nseamlessly blend in with normal settings, allowing the existing settings\r\nvalidation to work. Note that the setting names are still not validated\r\n(yet) when using the elasticsearch-keystore tool. Add secure settings validation on startup >>> 1"
1033,the client header is not set properly because of missing the i++.\r\n fix the index of header when build rest client >>> 0
1034,Today either all nodes in the cluster connect to remote clusters of only nodes\r\nthat have remote clusters configured in their node config. To allow global remote\r\ncluster configuration but restrict connections to a set of nodes in the cluster\r\nthis change adds a new setting `search.remote.connect` (defaults to `true`) to allow\r\nto disable remote cluster connections on a per node basis.\r\n Add a setting to disable remote cluster connections on a node >>> 1
1035,Elasticsearch v5.0.0 uses allocation IDs to safely allocate primary shards whereas prior versions of ES used a version-based mode instead. Elasticsearch v5 still has support for version-based primary shard allocation as it needs to be able to load 2.x shards. ES v6 can drop the legacy support. Remove legacy primary shard allocation mode based on versions >>> 1
1036,"As part of #22116 we are going to forbid usage of api\r\n`java.net.URL#openStream()`. However in a number of places across the\r\nwe use this method to read files from the local filesystem. This commit\r\nintroduces a helper method `openFileURLStream(URL url)` to read files\r\nfrom URLs. It does specific validation to only ensure that `file:/`\r\nurls are read.\r\n\r\nAdditionlly, this commit removes unneeded method\r\n`FileSystemUtil.newBufferedReader(URL, Charset)`. This method used the\r\n`openStream ()` method which will soon be forbidden. Instead we use the\r\n`Files.newBufferedReader(Path, Charset)`. Add FileSystemUtil method to read 'file:/' URLs >>> 1"
1037,"Painless uses Ruby-like method dispatch (reciever type, method name,\r\nand arity) rather than Java-like (reciever type, method name, and\r\nargument compile time types) or Groovy-like method dispatch (receiver\r\ntype, method name, and argument run time types). We do this for\r\nmostly good reasons but we never documented it.\r\n\r\nRelates to #22720\r\n\r\n Explain painless's method dispatch >>> 1"
1038,nan Bump version to 5.4 >>> 1
1039,"Today we carry on all search results including aggs, suggest and profile results\r\nuntil we have successfully fetched all hits for the search request. This can potentially\r\nhold on to a large amount of memory if there are heavy aggregations involved. With\r\nthis change aggs and profiles are entirely consumed an released for GC before the fetch\r\nphase is executing. This is a first step towards reducing results on-the-fly if the number\r\nof non-empty response are large.\r\n Separate reduce (aggs, suggest and profile) from merging fetched hits >>> 1"
1040,This is related to #23001.\r\n Docs: CONSOLEify transport docs >>> 1
1041,relates #23001\r\n\r\n Docs: Consoleify cluster and indices settings docs >>> 1
1042,"Since `_all` is now deprecated and cannot be set for new indices, we should also\r\ndisallow any field that has the `include_in_all` parameter set.\r\n\r\nResolves #22923 Disallow `include_in_all` for 6.0+ indices >>> 1"
1043,This adds parsing from xContent to Suggestion.Entry.Option and\r\nTermsuggestion.Entry.Option. Add xcontent parsing to suggestion options >>> 1
1044,"After the first cluster state from a new master is processed, NodeConnectionService guarantees we connect to the new master. This removes the need to explicitly connect to the master in the MasterFaultDetection code making it simpler and bypasses the assertion triggered due to the blocking operation on the cluster state thread.\r\n\r\nRelates to #22828\r\n MasterFaultDetection can start after the initial cluster state has been processed >>> 1"
1045,"We have a bunch of interfaces that have only a single implementation\r\nfor 6 years now. These interfaces are pretty useless from a SW development\r\nperspective and only add unnecessary abstractions. They also require\r\nlots of casting in many places where we expect that there is only one\r\nconcrete implementation. This change removes the interfaces, makes\r\nall of the classes final and removes the duplicate `foo` `getFoo` accessors\r\nin favor of `getFoo` from these classes. Fold InternalSearchHits and friends into their interfaces >>> 1"
1046,Today we account for too many response with an `IllegalStateException` in\r\n`AbstractSearchAsyncAction` while this is something that should never happen\r\nwe should rather assert that we are always have less or equal the number of\r\nexpected ops when waiting for responses.\r\n Harden ops counting in AbstractSearchAsyncAction >>> 1
1047,"The method is not needed anymore, was needed only when we supported setting a legacy default lang, which was removed with #21607\r\n\r\nRelates to #21607 Remove getDefaultScriptingLanguage from QueryParseContext >>> 1"
1048,Relates to #23001 \r\n Docs: CONSOLEify termvectors.asciidoc >>> 1
1049,Relates #23001 [Docs] Consoleify multi-search and search-template docs >>> 1
1050,"This commit deprecates support for the `application/x-ldjson` Content-Type header as this was only used in the first draft of the spec and had very little uptake. Additionally, the docs for bulk and msearch have been updated to specifically call out ndjson and mention that the newline character may be preceded by a carriage return.\r\n\r\nFinally, the bulk and multi percolate request handling of the carriage return has been improved to remove this character from the source.\r\n\r\nCloses #23025 \r\n\r\nNote: this is the backport of #23049 that deprecates instead of removes support Deprecate ldjson support and document ndjson for bulk/msearch >>> 1"
1051,Relates to #23001 Consolify docs/reference/analysis/tokenfilters/pattern-capture-tokenfilter.asciidoc. >>> 1
1052,"This commit removes support for the `application/x-ldjson` Content-Type header as this was only used in the first draft of the spec and had very little uptake. Additionally, the docs for bulk and msearch have been updated to specifically call out ndjson and mention that the newline character may be preceded by a carriage return.\r\n\r\nFinally, the bulk request handling of the carriage return has been improved to remove this character from the source.\r\n\r\nCloses #23025 Remove ldjson support and document ndjson for bulk/msearch >>> 1"
1053,This changes removes the SearchResponseListener that was used by the ExpandCollapseSearchResponseListener to expand collapsed hits.\r\nInstead the expand calls are done directly in the SearchPhaseController in a asynchronous fashion.\r\nThe removal of SearchResponseListener is not a breaking change because it was never release.\r\nThis functionnality is not very useful as is and should be rethink during the great search refactoring that Simon is doing.\r\n\r\nCloses #23048 Replace blocking calls in ExpandCollapseSearchResponseListener by asynchronous requests >>> 1
1054,"Previously, write requests were mutated at the\r\ntransport level to update request version, version type\r\nand sequence no before replication.\r\nNow that all write requests go through the shard bulk\r\ntransport action, we can use the primary response stored\r\nin item level bulk requests to pass the updated version,\r\nsequence no. to replicas.\r\n Make document write requests immutable >>> 1"
1055,This commit adds support for the Index API in the High Level Rest Client.\r\n\r\nIt changes few things in the way request's parameters map is created in the high level client and changea bit  RefreshPolicy in core. Add Index API to High Level Rest Client >>> 1
1056,"Empty response bodies should only be sent back for HEAD requests, otherwise we should always send back info about the exception that was thrown. Removed some manual exception handling in the REST action that should be rather bubbled up and handled by our rest action infra like every other rest action does.\r\n Cluster allocation explain to never return empty response body >>> 1"
1057,This commit upgrades the Netty dependency to version 4.1.8.Final. Upgrade to Netty 4.1.8 >>> 1
1058,All the warnings were upsetting me. This doesn't change behavior.\r\n Fix generics on LeadDocLookup >>> 1
1059,We only ever tested it with `Strings`.\r\n\r\nRelates to #23060\r\n Drop toString from LeafDocLookup >>> 0
1060,Changed CreateIndexResponse.java and TransportCreateIndexAction.java to include index name.\r\nTests still need changing to pass with new response info.\r\n Create index request should return the index name #23044 >>> 0
1061,"The human flag is centrally handled in RestChannel, no need to have Rest actions manually read it and set it to the builder. Remove redundant reads of human flag >>> 1"
1062,"When nested objects are present in the mappings, many queries get deoptimized\r\ndue to the need to exclude documents that are not in the right space. For\r\ninstance, a filter is applied to all queries that prevents them from matching\r\nnon-root documents (`+*:* -_type:__*`). Moreover, a filter is applied to all\r\nchild queries of `nested` queries in order to make sure that the child query\r\nonly matches child documents (`_type:__nested_path`), which is required by\r\n`ToParentBlockJoinQuery` (the Lucene query behing Elasticsearch's `nested`\r\nqueries).\r\n\r\nThese additional filters slow down `nested` queries. In 1.7-, the cost was\r\nsomehow amortized by the fact that we cached filters very aggressively. However,\r\nthis has proven to be a significant source of slow downs since 2.0 for users\r\nof `nested` mappings and queries, see #20797.\r\n\r\nThis change makes the filtering a bit smarter. For instance if the query is a\r\n`match_all` query, then we need to exclude nested docs. However, if the query\r\nis `foo: bar` then it may only match root documents since `foo` is a top-level\r\nfield, so no additional filtering is required.\r\n\r\nAnother improvement is to use a `FILTER` clause on all types rather than a\r\n`MUST_NOT` clause on all nested paths when possible since `FILTER` clauses\r\nare more efficient.\r\n\r\nHere are some examples of queries and how they get rewritten:\r\n\r\n```\r\n""match_all"": {}\r\n```\r\n\r\nThis query gets rewritten to `ConstantScore(+*:* -_type:__*)` on master and\r\n`ConstantScore(_type:AutomatonQuery {\norg.apache.lucene.util.automaton.Automaton@4371da44})`\r\nwith this change. The automaton is the complement of `_type:__*` so it matches\r\nthe same documents, but is faster since it is now a positive clause. Simplistic\r\nperformance testing on a 10M index where each root document has 5 nested\r\ndocuments on average gave a latency of 420ms on master and 90ms with this change\r\napplied.\r\n\r\n```\r\n""term"": {\r\n  ""foo"": {\r\n    ""value"": ""0""\r\n  }\r\n}\r\n```\r\n\r\nThis query is rewritten to `+foo:0 #(ConstantScore(+*:* -_type:__*))^0.0` on\r\nmaster and `foo:0` with this change: we do not need to filter nested docs out\r\nsince the query cannot match nested docs. While doing performance testing in\r\nthe same conditions as above, response times went from 250ms to 50ms.\r\n\r\n```\r\n""nested"": {\r\n  ""path"": ""nested"",\r\n  ""query"": {\r\n    ""term"": {\r\n      ""nested.foo"": {\r\n        ""value"": ""0""\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis query is rewritten to\r\n`+ToParentBlockJoinQuery (+nested.foo:0 #_type:__nested) #(ConstantScore(+*:* -_type:__*))^0.0`\r\non master and `ToParentBlockJoinQuery (nested.foo:0)` with this change. The\r\ntop-level filter (`-_type:__*`) could be removed since `nested` queries only\r\nmatch documents of the parent space, as well as the child filter\r\n(`#_type:__nested`) since the child query may only match nested docs since the\r\n`nested` object has both `include_in_parent` and `include_in_root` set to\r\n`false`. While doing performance testing in the same conditions as above,\r\nresponse times went from 850ms to 270ms. Nested queries should avoid adding unnecessary filters when possible. >>> 1"
1063,Netty 4.1.8 wraps connect and accept operations in doPrivileged blocks.\r\nThis means that we not need to give permissions to the entire transport\r\nmodule. Additionally this commit deletes `PrivilegedNioSocketChannel`\r\nand `PrivilegedNioServerSocketChannel`. Isolate SocketPermissions to Netty >>> 1
1064,EvillPeerRecoveryIT checks scenario where recovery is happening while there are on going indexing operation that *already* have been assigned a seq# . This is fairly hard to achieve and the test goes through a couple of hoops via the plugin infra to achieve that. This PR extends the unit tests infra to allow for those hoops to happen in unit tests. This allows the test to be moved to `RecoveryDuringReplicationTests`\r\n\r\nRelates to #22484 Move EvilPeerRecoveryIT to a unit test in RecoveryDuringReplicationTests >>> 1
1065,"This pull request reuses the `typed_keys` parameter added in #22965, but this time it applies it to suggesters. When set to true, the suggester names in the search response will be prefixed with a prefix that reflects their type. Use `typed_keys` parameter to prefix suggester names by type in search responses >>> 1"
1066,As @bleskes pointed out in #23069 there were inconsistencies\r\nin version handling on 5.x and 5.3 from master due to backport\r\nof #21964. This change ensures versions are handled uniformly\r\nand fixes minor issues in shard bulk action to be similar to master\r\n\r\nfixes https://github.com/elastic/elasticsearch/issues/23069\r\n Fix backport executing ops as single item bulk >>> 1
1067,This commit adds methods to the BulkProcessor that accept bytes and a XContentType to avoid content type detection. The\r\nmethods that do not accept XContentType with bytes have been deprecated by this commit.\r\n\r\nRelates #22691 Add BulkProcessor methods with XContentType parameter >>> 1
1068,https://bugs.openjdk.java.net/browse/JDK-8162520\r\nSome filesystems can be so large that they return a negative value for their\r\nfree/used/available disk bytes due to being larger than `Long.MAX_VALUE`.\r\n\r\nThis adds protection for our `FsProbe` implementation and adds a test that it\r\ndoes the right thing.\r\n\r\nYou can see a failure from this here: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+nfs/4/console Fix total disk bytes returning negative value >>> 1
1069,This refactors the `TransportShardBulkAction#executeBulkItemRequest` method into\r\nmultiple methods so that it can be more easily read. It does not change the\r\nbehavior in any way.\r\n\r\n(This is a precursor to unit testing `TransportShardBulkAction`) Refactor TransportShardBulkAction#executeBulkItemRequest >>> 0
1070,"This commit is just a code cleanup of RestGetAliasesAction.java. For example, we remove an unnecessary class, simplify a convenience method, and simplify some code flow.\r\n Cleanup RestGetAliasesAction.java >>> 1"
1071,"Alias HEAD requests incorrectly return a content-length header of 0. This commit addresses this by removing the special handling for alias HEAD requests, and just relying on the general mechanism that exists for handling HEAD requests in the REST layer.\r\n\r\nRelates #21125 Fix alias HEAD requests >>> 1"
1072,"This is an in-progress PR as I have a failure in MatchQueryIT that looks related to recent changes around graph token streams. If someone could help me dig it, I would appreciate. Upgrade to lucene-6.5.0-snapshot-f919485. >>> 1"
1073,This commit changes the `RefreshPolicy` enum so that string representation are exposed. This will help the high level rest client to simply use `refreshPolicy.getValue()` to get the corresponding parameter value of a given refresh policy. Expose WriteRequest.RefreshPolicy string representation >>> 1
1074,"Index HEAD requests incorrectly return a content-length header of\r\n0. This commit addresses this by removing the special handling for index\r\nHEAD requests, and just relying on the general mechanism that exists for\r\nhandling HEAD requests in the REST layer.\r\n\r\nRelates #21125\r\n\r\n Fix index HEAD requests >>> 1"
1075,"In #23013 I explain how I expected the `min_score` filter to behave as the query filter; I find it would be useful if it were explicitly stated that the two filters - albeit having same syntax - will behave in a completely different way.\r\n\r\nPerhaps I should add a link to the query filter page on the words ""query filter""? Add note about min_score filtering efficiency >>> 1"
1076,Relates #23001 Documentation: Consoleify cat shards/recovery API docs >>> 1
1077,"GraphQueries are now generated as simple clauses in BooleanQuery. So for instance a multi terms synonym will generate\r\n a GraphQuery but only for the side paths, the other part of the query will not be impacted. This means that we cannot apply\r\n `minimum_should_match` or `cutoff_frequency` on GraphQuery anymore (only ES 5.3 does that because we generate all possible paths if a query has at least one multi terms synonym).\r\nStarting in 5.4 multi terms synonym will now be treated as a single term when `minimum_should_match` is computed and will be ignored when `cutoff_frequency` is set.\r\nFixes #23102 Fix GraphQuery expectation after Lucene upgrade to 6.5 >>> 1"
1078,"This commit adds a parsing method to the `BulkItemResponse` class. In order to do that, the way `DocWriteResponse`s are parsed has to be changed: `ConstructingObjectParser`/`ObjectParser` is removed in favor of a simpler (and more readable, I think) way to parse these objects.\r\n\r\n`DocWriteResponse` now provides the `parseInnerToXContent(`) method that can be used by subclasses (`IndexResponse`, `UpdateReponse` and `DeleteResponse`) to parse the current token/field and potentially update a `ParsingContext`. \r\n\r\nThe `ParsingContext` is a simple POJO used to contain parsed values. It can be passed around from one parsing method to another parsing method. This is what is done in IndexResponse: a `ParsingContext` is created in `IndexResponse.fromXContent()`, and the parsing context is passed to `IndexResponse.parseXContentFields()` that parses fields specific to `IndexResponse` (like ""created"") and updates the context, delegating to `DocWriteResponse.parseInnerToXContent()`\r\nthe parsing of any other field. Once all XContent is parsed, `IndexResponse.fromXContent(`) uses the method `IndexResponse.fromParsingContext()` to create the new instance of `IndexResponse`.\r\n\r\nThis behavior allow to reuse parsing code among the class hierarchy while keeping the current behavior. It also allows other objects like `BulkItemResponse` to reuse the same parsing code to parse `DocWriteResponse`s.\r\n\r\nFinally, IndexResponseTests, UpdateResponseTests and DeleteResponseTests have been updated to introduce some random shuffling of fields before the XContent is parsed in order to ensure that the parsing code does not rely on field order. Add parsing methods to BulkItemResponse >>> 1"
1079,Today all search phases are inner classes of AbstractSearchAsyncAction or one of it's\r\nsubclasses. This makes unittesting of these classes practically impossible. This commit\r\nExtracts `DfsQueryPhase` and `FetchSearchPhase` or of the code that composes the actual\r\nquery execution types and moves most of the fan-out and collect code into an `InitialSearchPhase`\r\nclass that can be used to build initial search phases (phases that retry on shards). This will\r\nmake modification to these classes simpler and allows to easily compose or add new search phases\r\ndown the road if additional roundtrips are required. Detach SearchPhases from AbstractSearchAsyncAction >>> 1
1080,nan remove old 2.x percolator bwc logic. >>> 1
1081,This was an undocumented and unsettable setting that allowed id generation.\r\n\r\nResolves #23088 Remove action.allow_id_generation setting >>> 1
1082,"This gives Lucene the choice to use index/point-based queries or\r\ndoc-values-based queries depending on which one is more efficient. This commit\r\nintegrates this feature for:\r\n - long/integer/short/byte/double/float/half_float/scaled_float ranges,\r\n - date ranges,\r\n - geo bounding box queries,\r\n - geo distance queries.\r\n\r\nSee https://issues.apache.org/jira/browse/LUCENE-7643. Integrate IndexOrDocValuesQuery. >>> 1"
1083,"Template HEAD requests incorrectly return a content-length header of 0. This commit addresses this by removing the special handling for template HEAD requests, and just relying on the general mechanism that exists for handling HEAD requests in the REST layer.\r\n\r\nRelates #21125\r\n\r\n Fix template HEAD requests >>> 1"
1084,"#21817 introduced the notion of a cluster state applier and banned those for sampling the cluster state directly (as it is not applied yet). Testing has exposed one exceptional use case - if the appliers want to spawn off a follow up it may require waiting for specific new cluster state (for example, the shard started action, called by the IndicesClusterStateService, may run into trouble connecting to the master and wait for a new master to be elected). This requires creating an observer which, in turn, samples the cluster state. \r\n\r\nAn example failure can be seen at https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/1701/console\r\n\r\nThis commit allows creating an observer from a cluster state applier. The observer is adapted to exclude any potential old cluster state in its logic. Allow a cluster state applier to create an observer and wait for a better state >>> 1"
1085,"Today when trying to encode the location header to ASCII, we rely on the Java URI API. This API requires a proper URI which blows up whenever the URI contains, for example, a space (which can happen if the type, ID, or routing contain a space). This commit addresses this issue by properly encoding the URI. Additionally, we remove the need to create a URI simplifying the code flow.\r\n\r\nCloses #23115, relates #21057 Properly encode location header >>> 1"
1086,"The legacy ip fields, which still use the term dictionary, support only\r\nipv4 addresses. This change improves the error message when an ipv6\r\naddress is parsed for a legacy field, to indicate reindexing is\r\nnecessary to use ipv6 addresses.\r\n\r\nrelates #23126 Improve error message for ipv6 on legacy ip fields >>> 1"
1087,This commit adds a new method to the TransportChannel that provides access to the version of the\r\nremote node that the response is being sent on and that the request came from. This is helpful\r\nfor serialization of data attached as headers. Make the version of the remote node accessible on a transport channel >>> 1
1088,This commit creates a keystore and adds settings to it during the\r\ncluster formation for integration tests. Users can define a\r\n`keyStoreSetting` in build files for settings that need to be placed in\r\nthe keystore. Setup keystore during integration tests >>> 1
1089,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Append details about the default `script.max_compilations_per_minute` >>> 1"
1090,"When Netty decodes a bad HTTP request, it marks the decoder result on the HTTP request as a failure, and reroutes the request to GET /bad-request. This either leads to puzzling responses when a bad request is sent to Elasticsearch (if an index named ""bad-request"" does not exist then it produces an index not found exception and otherwise responds with the index settings for the index named ""bad-request""). This commit addresses this by inspecting the decoder result on the HTTP request and dispatching the request to a bad request handler preserving the initial cause of the bad request and providing an error message to the client.\r\n\r\nCloses #23034 Handle bad HTTP requests >>> 1"
1091,relates #23001\r\n Docs: CONSOLEify multi-get.asciidoc >>> 1
1092,"With #22977, network disruption also disconnects nodes from the transport service. That has the side effect that when the disruption is healed, the disconnected node stay disconnected until the `NodeConnectionsService` restores the connection. This can take too long for the tests. This PR adds logic to the cluster healing to restore connections immediately. \r\n\r\nSee https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=debian/611/console for an example failure. Ensure network connections are restored after disruptions >>> 1"
1093,"When configuring which repositories to pull from, we currently add\r\nmavenLocal() when the `repos.mavenLocal` flag is set. However, this is\r\nonly done in normal projects, but not the special buildSrc project. This\r\nchange adds that support. Note that this was not possible before gradle\r\n2.13, as there was a bug which prevented sys props from reaching the\r\nbuildSrc project (https://issues.gradle.org/browse/GRADLE-2475).\r\nHowever, we already require 2.13+.\r\n Build: Support mavenLocal flag in buildSrc >>> 1"
1094,This change modifies the deprecation log message emitted when a setting\r\nis found which is deprecated. The new message indicates docs for the\r\ndeprecated settings can be found in the breaking changes docs for the\r\nnext major version.\r\n\r\ncloses #22849 Improve setting deprecation message >>> 1
1095,"Now, analyze API doesn't support keyword type field.\r\nThis PR allow you to analyze keyword type field with/without normalizer.\r\n\r\nIf user request keyword field name, analyze API uses KeywordAnalyzer if the field doesn't have  normalizer prop or Normalizer if the field have normalizer prop.\r\nAnd this PR does not support to use normalizer request param yet. Support Keyword type in Analyze API >>> 1"
1096,Closes #23152\r\n Add a note about `cluster.routing.allocation.node_concurrent_recoveries` >>> 1
1097,"This adds parsing from xContent to the CompletionSuggestion.Entry.Option.\r\nThe completion suggestion option also inlines the xContent rendering of the\r\ncontaines SearchHit, so in order to reuse the SearchHit parser this also changes\r\nthe way SearchHit is parsed from using a loop-based parser to using a\r\nConstructingObjectParser that creates an intermediate map representation and\r\nthen later uses this output to create either a single SearchHit or use it with\r\nadditional fields defined in the parser for the completion suggestion option. Add xcontent parsing to completion suggestion option >>> 1"
1098,Now that we have more flexible search phases we should move the rather\r\nhacky integration of the collapse feature as a real search phase that can\r\nbe tested and used by itself. This commit adds a new CollapseSearchPhase\r\nincluding a unittest for the phase. It's integrated into the fetch phase\r\nas an optional successor. Add CollapseSearchPhase as a successor for the FetchSearchPhase >>> 1
1099,"Today, we don't check if the replica request has the correct version during replica write operation. Add request version asserting during replica operation >>> 1"
1100,"Discovered in https://github.com/elastic/elasticsearch-migration/issues/103\r\n\r\nIn 5.x we renamed the ""default"" similarity to ""classic"". Though in 2.x it is possible to override\r\n the ""default"" similarity and to define a new type for ""default"":\r\n ````\r\n PUT t\r\n{\r\n  ""settings"": {\r\n    ""index"": {\r\n      ""similarity"": {\r\n        ""default"": {\r\n          ""type"": ""BM25""\r\n        }\r\n      }\r\n    }\r\n  },\r\n  ""mappings"": {\r\n    ""t"": {\r\n      ""properties"": {\r\n        ""title"": {\r\n          ""type"": ""text"",\r\n          ""similarity"": ""default""\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n````\r\n\r\nWhen we upgrade this index to 5.x the similarity name for ""text"" is renamed to ""classic"" even though the ""default"" similarity type has been overriden.\r\nThis change fixes this upgrade bug and adds a test for it. Fix similarity upgrade when ""default"" similarity is overridden >>> 1"
1101,"This commit is just a code cleanup of RestGetIndicesAction.java. For example, we remove an unnecessary class, remove some unnecessary local variables, and simplify some code flow.\r\n Cleanup RestGetIndicesAction.java >>> 1"
1102,"Get source HEAD requests incorrectly return a content-length header of\r\n0. This commit addresses this by removing the special handling for get\r\nsource HEAD requests, and just relying on the general mechanism that\r\nexists for handling HEAD requests in the REST layer.\r\n\r\nRelates #21125\r\n\r\n Fix get source HEAD requests >>> 1"
1103,This commit adds unit tests for two cases where Elasticsearch violates expect header handling. These tests are marked as awaits fix.\r\n\r\nRelates #23172 Add failing tests for expect header violations >>> 1
1104,Closes #22933 Restore support for the `include/pattern` syntax. >>> 1
1105,The backport of #22691 caused plain text bodies with a scroll id to fail with an IllegalStateException as the wrong method was being called. This commit adds tests to ensure plain text bodies work and fixes the search scroll action so that it properly handles a request with a plain text body. Fix search scroll request with a plain text body >>> 1
1106,nan Add delete API to the High Level Rest Client >>> 1
1107,This change improves the notice file present in our distributions to\r\ninclude notice and license files from each included dependency.\r\n\r\ncloses #22546\r\n Build: Add notice file generation >>> 1
1108,"Get HEAD requests incorrectly return a content-length header of 0. This\r\ncommit addresses this by removing the special handling for get HEAD\r\nrequests, and just relying on the general mechanism that exists for\r\nhandling HEAD requests in the REST layer.\r\n\r\nRelates #21125\r\n Fix get HEAD requests >>> 1"
1109,Relates #22278 Add unit tests for terms aggregation objects. >>> 1
1110,"In update scripts, `ctx._now` uses the same milliseconds value used by the\r\nrest of the system to caluclate deltas. However, that time is not\r\nactually epoch milliseconds, as it is derived from `System.nanoTime()`.\r\nThis change reworks the estimated time thread in ThreadPool which this\r\ntime is based on to make available both the relative time, as well as\r\nabsolute milliseconds (epoch) which may be used with calendar system. It\r\nalso renames the EstimatedTimeThread to a more apt CachedTimeThread.\r\n\r\ncloses #23169 Script: Fix value of `ctx._now` to be current epoch time in milliseconds >>> 1"
1111,This commit fixes the snapshot documentation to be in conformity\r\nwith the CONSOLE format. Consolify snapshot documentation >>> 1
1112,"This commit adds the elasticsearch LICENSE.txt to all plugins that\r\nreleased with elasticsearch, as well as a generated NOTICE.txt specific\r\nto the dependencies of each plugin. Plugins: Include license and notice files in zip >>> 1"
1113,Relates to #23001 Console-ify curl statements for allocation explain API docs >>> 1
1114,"Get mappings HEAD requests incorrectly return a content-length header of 0. This commit addresses this by removing the special handling for get mappings HEAD requests, and just relying on the general mechanism that exists for handling HEAD requests in the REST layer.\r\n\r\nRelates #21125\r\n Fix get mappings HEAD requests >>> 1"
1115,Clarified where the stopwords file needs to live Update stop-analyzer.asciidoc >>> 1
1116,"A previous change aligned the handling of the GET document and HEAD document APIs. This commit aligns the specification for these two APIs as well, and fixes a failing test.\r\n\r\nRelates #23186\r\n\r\n Fix REST spec for exists >>> 1"
1117,"This adds parsing from xContent to Suggestion.Entry and its subclasses for Terms-, Phrase- \r\nand CompletionSuggestion.Entry.  Adding fromXContent to Suggestion.Entry and subclasses >>> 1"
1118,"Today when users start Elasticsearch with their Java configuration pointing to a pre-Java 8 install, they encounter a cryptic message:\r\n    \r\n> `Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 52.0`\r\n\r\nThey often think that they have Java 8 installed but if their JAVA_HOME or other configuration is causing them to start with a pre-Java 8 install, this error message does not help them.\r\n\r\nWe introduce a Java version checker that runs on Java 6 as part of the startup scripts. If the Java version is pre-Java 8, we can display a helpful error message to the user informing them of the Java version that the runtime was started with. Otherwise, Elasticsearch starts as it does today.\r\n\r\nRelates #21102 Introduce Java version check >>> 1"
1119,"\r\nThis change adds a new parameter called `auto_generate_synonyms_phrase_query`.\r\nThis param can be used in conjunction with `synonym_graph` token filter to generate phrase queries\r\nwhen multi terms synonyms are encountered.\r\nFor example, a synonym like ""ny, new york"" would produce the following boolean query when ""ny city"" is parsed:\r\n((ny OR ""new york"") AND city)\r\n\r\nNote how the multi terms synonym ""new york"" produces a phrase query. By default this parameter is set to false. Add support for auto_generate_synonyms_phrase_query in match_query, multi_match_query, query_string and simple_query_string >>> 0"
1120,"Let's make our life easier when debugging/testing.\r\nAlso having a flat dir helps us to compare or ""synchronize"" more easily with Tika project files.\r\n\r\nRelated to #22958.\r\nSame as of #22959 but for mapper-attachments plugin\r\n Replace tika-files.zip by a tika-files dir >>> 1"
1121,"These images have been rebuilt to be preloaded with java 8 installed.\r\nThis change re-enables the systems. It also removes some redundancy in\r\nthe rpm checks I found while testing the new images, and fixes a\r\npotential issue with generated resources in plugins where a stale dir\r\ncan cause junk to get into the distribution.\r\n Tests: Re-enable debian-8 and fedora-24 packaging tests >>> 1"
1122,"Related to #22077\r\n\r\nThis PR comes with 2 changes, one for `ingest-attachment` and the other for `mapper-attachments`.\r\nIt's essentially a backport of #22079 for 5.x series.\r\n\r\n## Ingest Attachment Plugin\r\n\r\n* Send a non supported document to an ingest pipeline using `ingest-attachment`\r\n* If Tika is not able to parse the document because of a missing class (we are not importing all jars needed by Tika), Tika throws a Throwable which is not catch.\r\n\r\nThis commit removes support for Visio and POTM office files.\r\n\r\nSo elasticsearch is not killed anymore when you run a command like:\r\n\r\n```\r\nGET _ingest/pipeline/_simulate\r\n{\r\n  ""pipeline"" : {\r\n    ""processors"" : [\r\n      {\r\n        ""attachment"" : {\r\n          ""field"" : ""file""\r\n        }\r\n      }\r\n    ]\r\n  },\r\n  ""docs"" : [\r\n    {\r\n      ""_source"" : {\r\n        ""file"" : ""BASE64CONTENT""\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe good news is that it does not kill the node anymore and allows to extract the text which is in the Office document even if we have a Visio content (which is not extracted anymore).\r\n\r\n\r\n## Mapper Attachments Plugin\r\n\r\n* Parse a non supported document using `mapper-attachments`\r\n* If Tika is not able to parse the document because of a missing class (we are not importing all jars needed by Tika), Tika throws a Throwable which is not catch.\r\n\r\nThis commit removes support for Visio and POTM office files.\r\n\r\nThe good news is that it does not kill the node anymore and allows to extract the text which is in the Office document even if we have a Visio content (which is not extracted anymore).\r\n\r\nNote that for this one as we did not apply yet #22963 it hides the fact that we removed the potm sample file from the tika big ZIP file.\r\n Remove support for Visio and potm files >>> 1"
1123,"The file /proc/self/cgroup lists the control groups to which the process\r\nbelongs. This file is a colon separated list of three fields:\r\n 1. a hierarchy ID number\r\n 2. a comma-separated list of hierarchies\r\n 3. the pathname of the control group in the hierarchy\r\n\r\nThe regex pattern for this contains a bug for the second field. It\r\nallows one or two entries in the comma-separated list, but not\r\nmore. This commit fixes the pattern to allow one or more entires in the\r\ncomma-separated list.\r\n\r\nCloses #23218 Fix control group pattern >>> 1"
1124,"The elastic images used for testing different systems now have java\r\ninstalled in the base image. This commit removes the installation of\r\njava, which should make the packagingTest runs more stable, as they will\r\nnot depend on flaky system repository mirrors. Tests: Remove java installation from vagrant provisioning >>> 1"
1125,"This pull requests cleans up some parsing tests added from the High Level Rest Client: IndexResponseTests, DeleteResponseTests, UpdateResponseTests, BulkItemResponseTests.\r\n\r\nThese tests are now more uniform with the others _test-from-to-XContent_ tests we have, they now shuffle the XContent fields before parsing, the asserting method for parsed objects does not used a Map<String, Object> anymore, and buggy equals/hasCode methods in ShardInfo and ShardInfo.Failure have been removed. [Tests] Cleans up DocWriteResponse parsing tests >>> 1"
1126,"This commit enforces the requirement of Content-Type for the REST layer and removes the deprecated methods in transport requests and their usages.\r\n\r\nWhile doing this, it turns out that there are many places where *Entity classes are used from the apache http client libraries and many of these usages did not specify the content type. The methods that do not specify a content type explicitly have been added to forbidden apis to prevent more of these from entering our code base.\r\n\r\nRelates #19388 Enforce Content-Type requirement on the rest layer and remove deprecated methods >>> 1"
1127,Relates to #22278\r\n\r\n Adds unit test for sampler aggregation >>> 1
1128,"Today all query results are buffered up until we received responses of\r\nall shards. This can hold on to a significant amount of memory if the number of\r\nshards is large. This commit adds a first step towards incrementally reducing\r\naggregations results if a, per search request, configurable amount of responses\r\nare received. If enough query results have been received and buffered all so-far\r\nreceived aggregation responses will be reduced and released to be GCed.\r\n\r\n\r\nthis PR really needs some reviews and potential discussions but it's a start and outlines what it takes to make this feature work  First step towards incremental reduction of query responses >>> 1"
1129,"Change behavior synonym filter factory.\nNow, synonyms are tokenized with the whitespace tokenizer or a tokenizer specified ""tokenizer"" parameter.\nThis PR is to tokenize synonyms with whatever tokenizer and token filters appear before it in the chain.\n- change parsing synonyms from constructor to building custom analyzer\n- clone SynonymTokenFilterFactory with the analysis chain in CustomAnalyzerProvider\n\nClose #7199\n Parse synonyms with the same analysis chain >>> 1"
1130,"A follow up to #23202, this adds parsing from xContent and tests to the four Suggestion implementations\r\nand the top level `suggest` element to be used later when parsing the entire SearchResponse. Adding fromXContent to Suggest and Suggestion class >>> 1"
1131,This pull request adds the `fromXContent()` parsing method to BulkResponse. Add parsing method to bulk response >>> 1
1132,Relates #22278 Add unit tests for BinaryRangeAggregator/InternalBinaryRange >>> 1
1133,Relates #22278 Add unit tests for GeoBoundsAggregator/InternalGeoBounds >>> 1
1134,Relates to #22278\r\n\r\n Tests: Add unit test for InternalChildren >>> 1
1135,"The `MultiPhraseQuery` isn't exposed directly in the query dsl, but can endup being used if simple query parser is used or third party plugin uses this query. Add term extraction support for MultiPhraseQuery >>> 1"
1136,Fix #23201\r\nThis PR replace the usage of `TermsQuery` with `TermInSetQuery`. There are two tests i have commented out because i think they will no longer be needed. Migrate TermsQuery to TermInSetQuery >>> 1
1137,I doubt I've formatted the intra-doc link properly. If I could get a hand with that I'd appreciate it. Add note and link to 'tune for disk usage' >>> 1
1138,This pull request adds support for UpdateRequest to the High Level Rest client. Add UpdateRequest support to High Level Rest client >>> 1
1139,Add some comments for DocWriteResponse.Builders and rename the inner classes too. Add javadoc for DocWriteResponse.Builders >>> 1
1140,"The warning header used by Elasticsearch for delivering deprecation warnings has a specific format (RFC 7234, section 5.5). The format specifies that the warning header should be of the form\r\n\r\n    warn-code warn-agent warn-text [warn-date]\r\n\r\nHere, the warn-code is a three-digit code which communicates various meanings. The warn-agent is a string used to identify the source of the warning (either a host:port combination, or some other identifier). The warn-text is quoted string which conveys the semantic meaning of the warning. The warn-date is an optional quoted date that can be in a few different formats.\r\n\r\nThis commit corrects the warning header within Elasticsearch to follow this specification. We use the warn-code 299 which means a ""miscellaneous persistent warning."" For the warn-agent, we use the version of Elasticsearch that produced the warning. The warn-text is unchanged from what we deliver today, but is wrapped in quotes as specified (this is important as a problem that exists today is that multiple warnings can not be split by comma to obtain the individual warnings as the warnings might themselves contain commas). For the warn-date, we use the RFC 1123 format.\r\n\r\nCloses #22986\r\n Correct warning header to be compliant >>> 1"
1141,"This commit adds a boundary_scanner property to the search highlight\r\nrequest (defaults to ""simple"", per current behavior) so the user can\r\nspecify ""break_iterator"" and its type (""sentence"", ""word"", ""line"" and\r\n""character""). Also adds ""boundary_scanner_locale"" to define which one\r\nshould be used when highlighting the text. Add BreakIteratorBoundaryScanner support >>> 1"
1142,"- Added sections for memlock, nproc, and as\r\n- changed links in bootstrap configs to point to limits.conf\r\n  section\r\n Added examples of /etc/security/limits.conf changes >>> 0"
1143,In #23253 we added the ability to incrementally reduce search results.\r\nThis change exposes the parameter to control the batch size and therefore\r\nthe memory consumption of a large search request.\r\n Expose `batched_reduce_size` via `_search` >>> 1
1144,Previously we calculated Netty's receive predictor size for HTTP and transport\r\ntraffic based on available memory and worker nodes. This resulted in a receive\r\npredictor size between 64kb and 512kb. In our benchmarks this leads to increased\r\nGC pressure.\r\n\r\nWith this commit we set Netty's receive predictor size to 32kb. This value is in\r\na sweet spot between heap memory waste (-> GC pressure) and effect on request\r\nmetrics (achieved throughput and latency numbers).\r\n\r\nCloses #23185 Set network receive predictor size to 32kb >>> 1
1145,"Add tests for InternalStats, InternalExtendedStats and StatsAggregator/ExtendedStatsAggregator\r\n\r\nRelates #22278  Add unit tests for stats and extended stats aggregations >>> 1"
1146,This commit changes UpdateRequest so that it implements the ToXContentObject interface. This will be useful for the High Level Rest client when executing UpdateRequests. UpdateRequest implements ToXContent >>> 1
1147,Previously when multiple reduces occured for the terms aggregation we would add up the errors for the aggregations but would not take into account the errors that had already been calculated for the previous reduce phases.\r\n\r\nThis change corrects that by adding the previously created errors to the new error value.\r\n\r\nCloses #23286 Fixes terms error count for multiple reduce phases >>> 1
1148,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Misprint parenthesis in repositories.azure example >>> 1"
1149,This fixes several bugs related to boxing/unboxing in casting.\r\n\r\nAn original PR with examples of the failures was posted by @nik9000 at #23238.  This PR goes further to correct some issues that have been around for a long time.  In ASM both boxing and unboxing expect an ASM unboxed type to be passed in rather than having symmetry where you can pass in a boxed type to unbox and an unboxed type to box.  This PR will correct that problem along with the casting issues mentioned in the other issue by passing in a separate parameter for the box/unbox type.\r\n\r\nAlso applied the tests @nik9000 wrote in the other PR. Fix Bad Casts In Painless >>> 1
1150,"From #23093, we fixed the issue where a filesystem can be so large that it\r\noverflows and returns a negative number. However, there is another issue when\r\nadding a path as a sub-path to another `FsInfo.Path` object, when adding the\r\ntotals the values can still overflow.\r\n\r\nThis adds the same safety to return `Long.MAX_VALUE` instead of the negative\r\nnumber, as well as a test exercising the logic. Handle long overflow when adding paths' totals >>> 1"
1151,"Fixes Painless to properly implement scripts that return primitives and void. Adds some simple tests that we emit sane opcodes and some other tests that we implement primitives as expected.\r\n\r\nMostly this is just a fix following up from #22983 but there is one thing I did really worth talking about, I think. So, before this script Painless scripts could only ever return `Object` and they did would always return `null` for paths that didn't return any values. Now that they can return primitives the question is ""what should Painless return from paths that don't return any values?"" In those cases Painless will now return `0` or `false`.\r\n\r\nEdit: The last sentence in the last paragraph used to be: And I answered that with ""Painless should refuse to compile scripts like this"". Fix Painless's implementation of interfaces returning primitives >>> 1"
1152,This commit fixes an issue that was missed in #22534.\r\n`AWSCredentialsProvider.getCredentials()` appears to potentially open a\r\nsocket connect. This operation needed to be wrapped in `doPrivileged()`.\r\n\r\nThis should fix issue #23271. Wrap getCredentials() in a doPrivileged() block >>> 1
1153,"When a trailing / is present at the end of base_path in s3 repository creation, an extra slash still gets prepended and this causes S3 to create an empty folder. This was not a problem in our ES 2.4 cluster as we always included the / at the end and was dealt with here:\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/v2.4.4/plugins/cloud-aws/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java#L140-L146. \r\n\r\nSpecifically, the splitStringToArray on L142 got rid of the trailing slashes. \r\n\r\nThis pull request fixes the trailing / issue. It also adds extra test case to BlobPathTests.java. Handle BlobPath's trailing separator case. Add test cases to BlobPathTests.java >>> 1"
1154,related to issue #21984 Added types options to DeleteByQueryRequest >>> 1
1155,"The current implementation of `RestClient.performAsync()` methods can throw exceptions before the request is asynchronously executed. Since it only throws unchecked exceptions, it's easy for the user/dev to forget to catch them, leaving the listener in the wild... Instead I think async methods should never throw exceptions and should always call the listener onFailure() method. RestClient asynchronous execution should not throw exceptions >>> 1"
1156,"This PR introduces randomization of the content type for requests sent as part of our REST tests. Requests can be sent in any supported format, and responses can be read from any of the supported formats too, according to the returned Content-Type header.\r\n\r\nThis new randomization revealed a couple of problems:\r\n- update indices settings only supports yaml or json (#23309)\r\n- a yaml tests for filters aggregations using wrapper query builder would only work with json or yaml content types, not with binary formats (which don't write binary inner objects as base64)\r\n- Scripts and templates should be converted to json while parsing them (#23308) Randomize Content-Type in REST tests >>> 1"
1157,"Elasticsearch accepts multiple content-type formats, hence scripts can be stored/provided in json, yaml, cbor or smile. Yet the format that should be used internally is json. This is a problem mainly around search templates, as they only support json out of the four content-types, so instead of maintaining the content-type of the request we should rather convert the scripts/templates to json.\r\n\r\nBinary formats were not previously supported. If you stored a template in yaml format, you'd get back an error ""No encoder found for MIME type [application/yaml]"" when trying to execute it. With this commit the request content-type is independent from the template, which always gets converted to json internally. That is transparent to users and doesn't affect the content type of the response obtained when executing the template.\r\n\r\nAlso given that search templates always get converted to json, we don't need to try and auto-detect their content-type, which anyways didn't work as expected before given that only json was really working.\r\n\r\nRelates to #23245 Convert script/template objects to json format internally >>> 1"
1158,"Gradle's finalizedBy on tasks only ensures one task runs after another,\r\nbut not immediately after. This is problematic for our integration tests\r\nsince it allows multiple project's integ test clusters to be\r\nsimultaneously. While this has not been a problem thus far (gradle 2.13\r\nhappened to keep the finalizedBy tasks close enough that no clusters\r\nwere running in parallel), with gradle 3.3 the task graph generation has\r\nchanged, and numerous clusters may be running simultaneously, causing\r\nmemory pressure, and thus generally slower tests, or even failure if the\r\nsystem has a limited amount of memory (eg in a vagrant host).\r\n\r\nThis commit reworks how integ tests are configured. It adds an\r\n`integTestCluster` extension to gradle which is equivalent to the current\r\n`integTest.cluster` and moves the rest test runner task to\r\n`integTestRunner`.  The `integTest` task is then just a dummy task,\r\nwhich depends on the cluster runner task, as well as the cluster stop\r\ntask. This means running `integTest` in one project will both run the\r\nrest tests, and shut down the cluster, before running `integTest` in\r\nanother project. Build: Rework integ test setup and shutdown to ensure stop runs when desired >>> 1"
1159,"Indices update settings api should support requests in binary formats like cbor or smile, not just json or yaml. Also expanded testing on the different ways to provide index settings and remove dead code around ability to provide settings as query string parameters. The latter didn't work since 5.0 , where we introduced validation of query_string parameters. Given that we received no complaint about that, rather than restoring that ability, I will add a note to the 5.0 migration guide.\r\n\r\nRelates to #23245\r\n\r\nCloses #23242  Update indices settings api to support CBOR and SMILE format >>> 1"
1160,"When sending a response to a client, we attach a releasing listener to the channel promise. If the client disappears before the response is sent, the releasing listener was never notified. The reason the listeners were never notified was due to a mistaken invocation of write and flush on the channel which has two overrides: one that takes an existing promise, and one that does not and instead creates a new promise. When the client disappears, it is this latter promise that is notified, which does not contain the releasing listener. This commit addreses this issue by invoking the override that passes our channel promise through.\r\n Ensure that releasing listener is called >>> 1"
1161,"When a node wants to join a cluster, it sends a join request to the master. The master then sends a join validation request to the node. This checks that the node can deserialize the current cluster state that exists on the master and that it can thus handle all the indices that are currently in the cluster (see #21830).\r\n\r\nThe current code can trip an assertion as it does not take the cluster state as is but sets itself as the local node on the cluster state. This can result in an inconsistent DiscoveryNodes object as the local node is not yet part of the cluster state and a node with same id but different address can still exist in the cluster state. Also another node with the same address but different id can exist in the cluster state if multiple nodes are run on the same machine and ports have been swapped after node crashes/restarts.\r\n\r\nBuild failure:\r\n\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=ubuntu/643/consoleFull\r\n\r\nFrom the logs:\r\n\r\n```\r\nat java.lang.Thread.run(Thread.java:745)Throwable #3: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=7061, name=elasticsearch[node_t4][__mock_network_thread][T#6], state=RUNNABLE, group=TGRP-MinimumMasterNodesIT]\r\n   > Caused by: java.lang.AssertionError: building disco nodes from network doesn't pass preflight: can't add node {node_t4}{YTZdF3xkQQmT-vN8PRNOvA}{K8X3Bf9fRb26y3FQWWmgkg}{127.0.0.1}{127.0.0.1:9783}, found existing node {node_t3}{cP9AQejSTBWvJmopV5y4Mw}{pT5FBfktS5-3OX1PiNEVvg}{127.0.0.1}{127.0.0.1:9783} with same address\r\n   > \tat __randomizedtesting.SeedInfo.seed([840C6FA44DFAF8D3]:0)\r\n   > \tat org.elasticsearch.cluster.node.DiscoveryNodes.readFrom(DiscoveryNodes.java:543)\r\n   > \tat org.elasticsearch.cluster.ClusterState.readFrom(ClusterState.java:662)\r\n   > \tat org.elasticsearch.discovery.zen.MembershipAction$ValidateJoinRequest.readFrom(MembershipAction.java:173)\r\n   > \tat org.elasticsearch.transport.TcpTransport.handleRequest(TcpTransport.java:1441)\r\n   > \tat org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1328)\r\n   > \tat org.elasticsearch.transport.MockTcpTransport.readMessage(MockTcpTransport.java:175)\r\n   > \tat org.elasticsearch.transport.MockTcpTransport.access$1000(MockTcpTransport.java:72)\r\n   > \tat org.elasticsearch.transport.MockTcpTransport$MockChannel$1.lambda$doRun$0(MockTcpTransport.java:359)\r\n   > \tat org.elasticsearch.common.util.CancellableThreads.executeIO(CancellableThreads.java:105)\r\n   > \tat org.elasticsearch.transport.MockTcpTransport$MockChannel$1.doRun(MockTcpTransport.java:359)\r\n   > \tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n   > \tat java.lang.Thread.run(Thread.java:745)Throwable #4: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=7057, name=elasticsearch[node_t5][__mock_network_thread][T#9], state=RUNNABLE, group=TGRP-MinimumMasterNodesIT]\r\n``` Don't set local node on cluster state used for node join validation >>> 1"
1162,This commit adds support for BulkRequest execution in the High Level Rest client. Add BulkRequest support to High Level Rest client >>> 1
1163,Adds unit tests for the `children` aggregation.\r\nThis change also add the ability to mock Mapperservice in subtests of\r\nAggregatorTestCase.\r\n\r\nRelates to #22278\r\n Add unit tests for ParentToChildAggregator >>> 1
1164,"This commit moves the LICENSE.txt and NOTICE.txt files for each plugin\r\nto be alongside the other plugin files, inside the elasticsearch subdir.\r\nThis ensures those files are installed alongside the plugin.\r\n Build: Change location in zip of license and notice inclusion for plugins >>> 1"
1165,"In oder to use lucene's utilities to merge top docs the results\r\nneed to be passed in a dense array where the index corresponds to the shard index in\r\nthe result list. Yet, we were sorting results before merging them just to order them\r\nin the incoming order again for the above mentioned reason. This change removes the\r\nobsolet sort and prevents unnecessary materializing of results.\r\n Remove unnecessary result sorting in SearchPhaseController >>> 1"
1166,"When pipelined responses are sent to the pipeline handler for writing, they are not necessarily written immediately. They must be held in a priority queue until all responses preceding the given response are written. This means that when write is invoked on the handler, the promise that is attached to the write invocation will not necessarily be the promise associated with the responses that are written while the queue is drained. To address this, the promise associated with a pipelined response must be held with the response and then used when the channel context is actually written to. This was introduced when ensuring that the releasing promise is always chained through on write calls lest the releasing promise never be invoked. This leads to many failing test cases, so no new test cases are needed here.\r\n\r\nRelates #23310, closes #23322\r\n Respect promises on pipelined responses >>> 1"
1167,Previous changes aligned HEAD requests to be consistent with GET\r\nrequests to the same endpoint. This commit aligns the REST spec for the\r\nimpacted endpoints.\r\n\r\nRelates #21125\r\n Align REST specs for HEAD requests >>> 1
1168,"This class doesn't seem to do much other than to group together\r\ncertain types of aggregations. If that is the reason for its existence, we can\r\nprobably use a marker interface for that.\r\n Can we remove abstract InternalMetricsAggregation? >>> 1"
1169,"NamedXContentRegistry will be used by the high level REST client to parse aggregation responses, and any section whose class type depends on a json key. There are currently on entries in the standard registry, but soon aggs and most likely suggesters will be added. Also it is possible for subclasses to provide additional named xcontent parsers. Add support for named xcontent parsers to high level REST client >>> 1"
1170,This change moves codes that validate the search request in the SearchRequest#validate method.\r\nIt also adds the following validation for the search request:\r\n* scroll cannot be used if size equals 0\r\n* scroll cannot be used if from is greater than 0 (from was ignored in the scroll context before this change so it's not a breaking change)\r\n\r\nFixes #22552 and #9373 Move search request validations to SearchRequest#validate >>> 0
1171,"This is fallout from #23297. That commit wrapped\r\n`InstanceProfileCredentialsProvider` to ensure that the `getCredentials`\r\nand `refresh` methods had privileged access. However, it looks like\r\nthere was a test ensuring that `buildCredentials` returned the correct\r\nclazz type. This commit adjusts that test to check that the correct\r\nwrapper is returned. Test that buildCredentials returns correct clazz >>> 1"
1172,"This commit sets the intial size of the pipeline handler queue small to prevent waste if pipelined requests are never sent. Since the queue will grow quickly if pipeline requests are indeed set, this should not be problematic.\r\n Keep the pipeline handler queue small initially >>> 1"
1173,"There are two ways to determine the latest index-N blob that contains\r\nthe truth of the contents of the repository: (1) list all index-N blobs\r\nand figure out what the latest value of N is, and (2) read the\r\nindex.latest blob, which contains the latest value of N explicitely.\r\nNote that the index.latest blob is not written atomically and can be\r\nre-written, as opposed to the index-N blobs which are never re-written\r\n(to create an updated index blob, index-{N+1} is written).\r\n\r\nPreviously, the latest index-N was determined by first trying to read\r\nthe index.latest blob and if that blob was missing (it was deleted\r\nbefore being re-written and in between deleting it and re-writing it,\r\nthe system crashed), then all index-N blobs were listed to pick the\r\nhighest N value.\r\n\r\nFor non-read-only repositories, this could produce race conditions with\r\nthe file system.  In particular, it is possible that the index.latest\r\nblob is being read in order to serve a read request (e.g. get snapshots)\r\nand while doing so, an attempt is made to delete the index.latest blob\r\nand re-write it in order to finalize a snapshot operation.  On some file\r\nsystems (e.g. Windows), it is forbidden to delete a file while it is\r\nopen for reading by another process/thread.\r\n\r\nThis commit changes the priority so that figuring out the latest index-N\r\nblob is first done by listing all index-N blobs and determining the\r\nlatest N value.  If that values because the repository does not\r\nsupport listing blobs (e.g. the URL repository), then the index.latest\r\nblob is read.  This is safe because in read-only repositories that do\r\nnot support listing blobs, the index.latest blob is never deleted and\r\nthen re-written, so the aforementioned issue does not arise. Prioritize listing index-N blobs over index.latest in reading snapshots >>> 1"
1174,"Load the geoip database the first time a pipeline gets created that has a geoip processor.\r\nThis saves memory (measured ~150MB for the city db) in cases when the plugin is installed, but not used. Lazy load the geoip databases >>> 1"
1175,This change exposes the new Lucene graph based word delimiter token filter in the analysis filters.\r\nUnlike the `word_delimiter` this token filter named `word_delimiter_graph` correctly handles multi terms expansion at query time.\r\n\r\nCloses #23104 Expose WordDelimiterGraphTokenFilter >>> 1
1176,"Console.readText may return null in certain cases. This commit fixes a\r\nbug in Terminal.promptYesNo which assumed a non-null return value.  It\r\nalso adds a test for this, and modifies mock terminal to be able to\r\nhandle null input values. Settings: Fix keystore cli prompting for yes/no to handle console returning null >>> 1"
1177,"This change adds a new test task, platformTest, which runs `gradle test\r\nintegTest` within a vagrant host. This will allow us to still test on\r\nall the supported platforms, but be able to standardize on the tools\r\nprovided in the host system, for example, with a modern version of git\r\nthat can allow #22946.\r\n\r\nIn order to have sufficient memory and cpu to run the tests, the\r\nvagrantfile has been updated to use 8GB and 4 cpus by default. This can\r\nbe customized with the `VAGRANT_MEMORY` and `VAGRANT_CPUS` environment\r\nvariables.  Also, to save time to show this can work, it currently uses\r\nthe same Vagrantfile the packaging tests do. There are a lot of cleanups\r\nwe can do to how the gradle-vagrant tasks work, including generating\r\nVagrantfile altogether, but I think this is fine for now as the same\r\nmachines that would run platformTest run packagingTest, and they are\r\nrelatively beefy machines, so the higher memory and cpu for them, with\r\neither task, should not be an issue. Tests: Add platformTest to vagrant >>> 1"
1178,"The IndexShardOperationsLock has a mechanism to delay operations if there is currently a block on the lock. These delayed operations are executed when the block is released and are executed by a different thread. When the different thread executes the operations, the ThreadContext is that of the thread that was blocking operations. In order to preserve the ThreadContext, we need to store it and wrap the listener when the operation is delayed. Always restore the ThreadContext for operations delayed due to a block >>> 1"
1179,This commit adds support for an info() method to the High Level Rest\r\nclient that returns the cluster information usually obtained by performing a\r\n`GET hostname:9200` request.\r\n Add info method to High Level Rest client >>> 1
1180,The REST Client is split into 2 parts:\r\n\r\n* Low level\r\n* High level\r\n\r\nThe High level client has a main common section and the document delete API documentation as a start.\r\n\r\nI'd really like that we have a QA module where we test all the methods we are documenting and even better only include code snippets into the documentation so we know it's always up to date.\r\n\r\nI'd like to start with a QA project if no one objects.\r\n Add first High level client documentation >>> 1
1181,"This change puts guava and jimfs into versioning to fix a guava version conflict leading to jar-hell in a dependent project. For ease of use I also sorted the version file (keeping the original sections, just order by alphabet inside a section) as it gets harder to read now with more and more entries. Build: Versioning for guava and jimfs >>> 0"
1182,Make it easier to check two MainResponse instances for equality. \r\n\r\nRelates to #23350\r\n Adding equals/hashCode to MainResponse >>> 1
1183,"The dependencyLicenses check has the ability to map multiple jar files\r\nto the same license file. However, netty was not taking advantage of\r\nthis, and had duplicate copies of its license/notice files for each jar.\r\nThis commit reduces the copies to one and uses the mapping feature. Build: Remove extra copies of netty license >>> 1"
1184,"This commit fixes the reproduce line output when the vagrant packagingTest\r\nfails. Before only the `gradle packagingTest` would be output, but the\r\nseed and list of versions was swallowed by groovy with an ancillary\r\nfailure (due to the `+` being on the wrong line for a string\r\ncontinuation).  With the new reproduce line, it is now output next to\r\nthe task right after failure, contains the actual task (specific to the\r\nbox that fails), and contains the seed. It also no longer contains the\r\nupgrade versions list, as the seed is used to determine which of those\r\nto use, and the same file would be read when testing a failure on a\r\nparticular git commit. Finally, this also ties bats test setup directly\r\nto packagingTest, instead of to the vagrant up command. Tests: Fix reproduce line for packagingTest >>> 1"
1185,"This commit modifies the `BulkProcessor` to be decoupled from the\r\nclient implementation. Instead it just takes a\r\n`BiConsumer<BulkRequest, ActionListener<BulkResponse>>` that executes\r\nthe `BulkRequest`. \r\n Decouple BulkProcessor from client implementation >>> 1"
1186,Fix #23269 \r\n\r\nThis is a first approach to automatically release a search Context. To-do is to access all unique contexts from the same scroll request after that last fetch and calculate total elements in an index. Automatically release searchContext >>> 0
1187,"Currently, when a primary write operation fails after generating\r\na sequence number, the failure is not communicated to the replicas.\r\nIdeally, every operation which generates a sequence number on primary\r\nshould be recorded in all replicas.\r\n\r\nIn this change, a sequence number is associated with write operation\r\nfailure. When a failure with an assinged seqence number arrives at a\r\nreplica, the failure cause and sequence number is recorded in the translog\r\nand the sequence number is marked as completed via executing `Engine.noOp`\r\non the replica engine.\r\n Replicate write failures >>> 1"
1188,Previously this code was duplicated across the 3 different topdocs variants\r\nwe have. It also had no real unittest (where we tested with holes in the results)\r\nwhich caused a sneaky bug where the comparison used `result.size()` vs `results.size()`\r\ncausing several NPEs downstream. This change adds a static method to fill the top docs\r\nthat is shared across all variants and adds a unittest that would have caught the issue\r\nvery quickly.\r\n\r\nCloses #19356\r\nCloses #23357 Factor out filling of TopDocs in SearchPhaseController >>> 1
1189,"We recently added parsing code to parse suggesters responses into java api objects. This was done using a switch based on the type of the returned suggestion. We can now replace the switch with using NamedXContentRegistry, which will also be used for aggs parsing. Convert suggestion response parsing to use NamedXContentRegistry >>> 1"
1190,Supersedes https://github.com/elastic/elasticsearch/pull/23362 that also fixes the removal of the GraphQuery Upgrade to lucene-6.5.0-snapshot-d00c5ca >>> 1
1191,"This change adds a new class, CloseableChars, that can be retrieved from an instance of a SecureString. This wrapper class contains a copy of the char[] that backs the SecureString and can also be closed. Closing the wrapper will wipe the char[] that backs the wrapper but the SecureString itself remains unaffected.\r\n\r\nAdditionally, while making a separate change I found that SecureSettings will fail when diff is called on them and there is no fallback setting. Given the idea behind SecureSetting, I think that diff should just be a no-op and I have implemented this here as well. Provide a method to retrieve a closeable char[] from a SecureString >>> 1"
1192,"The ThreadedActionListener may not be invoked by the same thread that creates it. When this happens, the ThreadContext from the thread that created the ThreadedActionListener should be used. This change makes the ThreadedActionListener always wrap the ActionListener in a ContextPreservingActionListener to prevent accidental loss of the ThreadContext.\r\n\r\nRelates #23349 ThreadedActionListener always wraps listener to preserve context >>> 0"
1193,Change TaskOperationFailure#getCause() return type from Throwable to Exception.  Change TaskOperationFailure#getCause() return type >>> 1
1194,Relates to #22278 Tests: Add unit test for InternalScriptedMetric >>> 1
1195,This change fixes the retrieval of the merge scheduler config that previously used IndexSettings#getValue and failed to\r\nretrieve the current value of the max_thread_count when max_merge_count was set.\r\nInstead we now use Setting#get(Settings) which correctly retrieves the value (and default value) for all settings.\r\nThis fixes a bug that appears when only max_thread_count was updated. Any value for the setting was accepted leading to a state where the\r\n merge scheduler throws exceptions (and fails any merge) and the index cannot be opened anymore (if it was closed before). Fix merge scheduler config settings >>> 1
1196,"Replaces the `Cast` object which had 7 parameters with casting\r\nstrategy objects that ought to be much easier to read. The trouble\r\nwith the `Cast` object is that it is fairly difficult to figure\r\nout from reading the constructor invocation what the instructions\r\nare emitted for the cast. The casting strategies are much easier\r\nto read. Compare:\r\n```\r\n- return new Cast(LONG_TYPE, expected, true, LONG_TYPE, null, null, null);\r\n+ return new Cast.Unbox(LONG_TYPE, new Cast.Numeric(LONG_TYPE, expected));\r\n```\r\n\r\nThey both do the same thing but the bottom one can be read\r\n""unbox into a `long` and then demote to the expected type"".\r\n\r\nOr compare:\r\n```\r\n- return new Cast(BOOLEAN_OBJ_TYPE, DEF_TYPE, explicit, null, null, BOOLEAN_TYPE, null);\r\n+ return new Cast.Box(BOOLEAN_TYPE);\r\n```\r\n\r\nBoth of them do the same thing but the bottom one reads\r\n""box a boolean"" while the top one is kind of an eye chart.\r\n\r\n\r\nI reordered many of the `case` clauses so that they were\r\nin a consistent order: primitives (smallest to largest),\r\n`def`, `Object`, `Number`, boxed numbers (smallest to\r\nlargest). I tried to be consistent though I imagine I wasn't\r\nalways successful.\r\n\r\nI also added a ton of test cases for casting. There are four\r\nsorts of ""regular"" casts in painless, and I made sure to cover\r\nthem fairly extensively.\r\n\r\n* ""implicit non-internal"": Generally only works when promoting\r\nnumbers or widening types. For example, assigning the\r\n`int` `1` to `long`:\r\n```\r\nlong n = 1;\r\n```\r\n\r\n* ""implicit internal"": Like ""implicit, non-internal"" but also\r\nsupports boxing. Generally used for return values. For\r\nexample, a function returning an `Integer` can accept\r\nthe `int` `1`:\r\n```\r\nInteger foo() {return 1}\r\n```\r\n\r\n* ""explicit non-internal"": Fine for promoting or demoting\r\nnumbers and widening or narrowing types. For example,\r\nassigning the `long` `1` to an `int`:\r\n```\r\nlong l = 1L;\r\nint n = (int) l;\r\n```\r\n\r\n* ""explicit internal"": Fine for promoting or demoting\r\nnumbers, widening or narrowing types, and boxing.\r\nInterestingly, you can only trigger these in a foreach\r\nloop:\r\n```\r\nint[] array = new int[1]\r\narray[0] = 1\r\nfor (Short s : array) {   // see the demotion and boxing?!\r\n```\r\n\r\nI labeled this a `bug` because it fixes a few issues that I found while testing the ""explicit, internal"" casts. Replace Painless's Cast with casting strategies >>> 0"
1197,When multiple reduce phases were needed the per term error got lost in subsequent reduces in some situations:\r\n\r\nWhen a previous reduce phase had calculated a non-zero error for a particular bucket we were not accounting for this error in subsequent reduce phases and instead were relying on the overall error for the agg which meant we were implicitly assuming that all shards that made up that aggregation had returned the term. This is plainly not true so we need to make sure the per term error for the aggregation is used when calcualting the error for that term in the new reduced aggregation. Fixes the per term error in the terms aggregation >>> 1
1198,"We should have the same behavior for Azure repositories as we have for S3 (see #22762).\r\n\r\nSo we want to use a similar name for keys and define explicitly the configuration name to use with `client` of instead `account`.\r\nAlso we secure our key/secret combination.\r\n\r\nInstead of:\r\n\r\n```yml\r\ncloud:\r\n    azure:\r\n        storage:\r\n            my_account1:\r\n                account: your_azure_storage_account1\r\n                key: your_azure_storage_key1\r\n                default: true\r\n            my_account2:\r\n                account: your_azure_storage_account2\r\n                key: your_azure_storage_key2\r\n```\r\n\r\nSupport something like:\r\n\r\n```sh\r\nbin/elasticsearch-keystore add azure.client.default.account\r\nbin/elasticsearch-keystore add azure.client.default.key\r\nbin/elasticsearch-keystore add azure.client.my_account2.account\r\nbin/elasticsearch-keystore add azure.client.my_account2.key\r\n```\r\n\r\nThen instead of:\r\n\r\n```\r\nPUT _snapshot/my_backup3\r\n{\r\n    ""type"": ""azure"",\r\n    ""settings"": {\r\n        ""account"": ""my_account2""\r\n    }\r\n}\r\n```\r\n\r\nUse:\r\n\r\n```\r\nPUT _snapshot/my_backup3\r\n{\r\n    ""type"": ""azure"",\r\n    ""settings"": {\r\n        ""client"": ""my_account2""\r\n    }\r\n}\r\n```\r\n\r\nIf someone uses:\r\n\r\n```\r\nPUT _snapshot/my_backup3\r\n{\r\n    ""type"": ""azure""\r\n}\r\n```\r\n\r\nIt will use the `default` azure repository settings.\r\n\r\nAnd mark as deprecated old settings.\r\nThe plan is to then remove deprecated settings as a follow up in master branch only.\r\n\r\nCloses #22763.\r\n Azure repository: Move to named configurations as we do for S3 repository and secure settings >>> 1"
1199,"Global repositories settings we were able to set in elasticsearch config file under `repositories.s3`\r\nname space are now deprecated and will be removed in master (6.x) - see #23276.\r\n\r\nThis includes `repositories.s3.bucket`, `repositories.s3.server_side_encryption`,\r\n`repositories.s3.buffer_size`, `repositories.s3.max_retries`, `repositories.s3.use_throttle_retries`,\r\n`repositories.s3.chunk_size`, `repositories.s3.compress`, `repositories.s3.storage_class`, `repositories.s3.canned_acl`,\r\n`repositories.s3.base_path` and `repositories.s3.path_style_access`.\r\n\r\nWe must set those settings per repository instead. Respectively `bucket`, `server_side_encryption`, `buffer_size`,\r\n`max_retries`, `use_throttle_retries`, `chunk_size`, `compress`, `storage_class`, `canned_acl`, `base_path` and\r\n`path_style_access`.\r\n\r\nRelated to #22800\r\n Deprecate repositories.s3 settings >>> 1"
1200,"This commit removes a deprecation log message that is printed any time\r\nthe Groovy script engine is loaded. This is unnecessary because we print\r\ndeprecation messages any time a Groovy script is used, so this message\r\nappears independently of whether or not a user is actually using Groovy\r\nscripts.\r\n\r\nCloses #23401\r\n Remove unnecessary Groovy deprecation logging >>> 1"
1201,This commit adds deprecation warnings for the use of Netty 3 for transport or HTTP requests.\r\n\r\n Deprecate Netty 3 >>> 1
1202,"Adds a common base class for testing subclasses of\r\n`InternalSingleBucketAggregation`. They are so similar they\r\ncall into question the utility of having all of these classes.\r\nWe maybe could just use `InternalSingleBucketAggregation` in\r\nall those cases.... But for now, let's test the classes!\r\n\r\nRelates to #22278\r\n Tests InternalSingleBucketAggregation subclasses >>> 1"
1203,Relates to #22278\r\n Tests: Add unit test for InternalScriptedMetricAggregator >>> 1
1204,"Throw error when skip or do sections are malformed, such as they don't start with the proper token (START_OBJECT). That signals bad indentation, which would be ignored otherwise. Thanks (or due to) our pull parsing code, we were still able to properly parse the sections, yet other runners weren't able to.\r\n\r\nCloses #21980 [TEST] improve yaml test sections parsing >>> 1"
1205,"Previously, cluster.routing.allocation.same_shard.host was not a dynamic\r\nsetting and could not be updated after startup.  This commit changes the\r\nbehavior to allow the setting to be dynamically updatable.  The\r\ndocumentation already states that the setting is dynamic so no\r\ndocumentation changes are required.\r\n\r\nCloses #22992 Makes the same_shard host dynamically updatable >>> 1"
1206,Relates to #22278  Tests: Add unit test for InternalDateHistogram >>> 1
1207,"We have many version constants in master that have already been\r\nreleased, but are still marked (by naming convention) as unreleased.\r\nThis commit renames those version constants.\r\n Internal: Change version constant names for already released versions >>> 1"
1208,"This refactors the `TransportShardBulkAction` to split it appart and make it\r\nunit-testable, and then it also adds unit tests that use these methods.\r\n\r\nIn particular, this makes `executeBulkItemRequest` shorter and more readable\r\n\r\nI'd like to add more tests for the update execution, however, I think a follow-up\r\n after the refactoring would be better than having this PR get too large. Refactor TransportShardBulkAction and add unit tests >>> 1"
1209,"This commit fixes the date format in warning headers. There is some confusion around whether or not RFC 1123 requires two-digit days. However, the warning header specification very clearly relies on a format that requires two-digit days. This commit removes the usage of RFC 1123 date/time format from Java 8, which allows for one-digit days, in favor of a format that forces two-digit days (it's otherwise identical to RFC 1123 format, it is just fixed width).\r\n\r\nRelates #23275\r\n Fix date format in warning headers >>> 1"
1210,"This commit sets the version on the repository-hdfs Guava dependency to version 11.0.2. This change is made to align the version here with the version that is [defined in the POM for Hadoop 2.7.1](https://github.com/apache/hadoop/blob/release-2.7.1/hadoop-project/pom.xml#L396-L400), the version of Hadoop that the repository-hdfs plugin is based on. See [HADOOP-10101](https://issues.apache.org/jira/browse/HADOOP-10101) and [HADOOP-11319](https://issues.apache.org/jira/browse/HADOOP-11319) for the ridiculous history of trying to upgrade Guava past this version in the Hadoop project.\r\n Correct version on repository-hdfs Guava dependency >>> 1"
1211,Don't extend `AbstractComponent` in `MustacheScriptEngine` because\r\nit doesn't buy anything.\r\n Don't extend AbstractComponent in MustacheScriptEngine >>> 1
1212,Part of #22278 [TEST] Add unit tests for GeoHashGridAggregator and InternalGeoHashGrid >>> 1
1213,Relates #22278 Add unit tests to percentile ranks aggregations. >>> 1
1214,The code was testing `PointRangeQuery` however we now use the\r\n`IndexOrDocValuesQuery` in field mappers. This makes the test generate queries\r\nthrough mappers so that we test the actual queries that would be generated. Avoid adding unnecessary nested filters when ranges are used. >>> 1
1215,Relates to #22278  Tests: Add unit test for SignificantLongTerms and SignificantStringTerms >>> 1
1216,Today the status is lost when parsing back a` BulkItemResponse.Failure`. This commit changes the `BulkItemResponse.Failure` parsing method so that it correctly instantiates a failure with the parsed status instead of realying on the parsed ElasticsearchException (that always return an internal server error status). Correctly parse BulkItemResponse.Failure's status >>> 1
1217,"The skip will come into play when we run backwards tests. We changed the deprecation headers format with 5.3.0 and we can't properly test that pre 5.3.0 nodes return the previous format warnings, so we just skip these tests whenever a pre 5.3.0 node is in the cluster. [TEST] skip yaml tests that expect warnings with nodes pre 5.3.0 >>> 1"
1218,"Currently ""foo:\*"" is parsed as prefix query on the field `foo` unless the field is defined in `default_field` or `fields`.\r\nThis commit fixes this behavior, ""foo:\*"" is now rewritten to an exists query on the field name.\r\nThis change also removes the assumption that ""_all:\*"" should return all docs.\r\n\r\nrelates #23356 Fix query_string_query to transform ""foo:*"" in an exists query on the field name >>> 1"
1219,"For the response parsing we want to be lenient when it comes to parsing new\r\nxContent fields. In order to ensure this in our testing, this change adds a\r\nutility method to ESTestCase that takes xContent bytes representation as input\r\nand recursively adds random fields on each object level.\r\n\r\nSome exceptions to consider: sometimes we want to exclude a whole subtree from\r\nthis treatment (e.g. skipping ""_source""), other times an element (e.g. ""fields"",\r\n""highlight"" in SearchHit) can have arbitraryly named objects. Those cases can be\r\nspecified as exceptions.\r\n\r\nThree use cases (SearchHitsTests, SearchHitTest, MainResponseTests) are also\r\nincluded.\r\n Tests: Add ability to generate random new fields for xContent parsing test >>> 1"
1220,"This pr fixes two issues:\r\n* Accumulate any potential other processor parse errors before failing instead of failing upon first processor parsing error. This is useful if more than one processor type is unknown, so that the client creating the pipeline knows about all missing processors. Note that suppressed exceptions are not rendered yet on the [rest layer](https://github.com/elastic/elasticsearch/issues/23392). However suppressed exceptions are being serialized in the internal node-to-node layer.\r\n* Attach type and tag header to the error that is thrown when ingest processor isn't available on other ingest nodes. Improve missing ingest processor error >>> 1"
1221,This should make it simpler to include example snippets from\r\ntests.\r\n\r\nRelates to #23351 Use include-tagged macro for high level client docs >>> 1
1222,"Today when resetting the deprecation logger after a test is torn down, we attach a new thread context to the deprecation logger. This thread context is never cleared and we are left with a thread context attached to the deprecation logger for every test method that ran in the same JVM. This commit adds a flag when resetting the deprecation logger to not attach a new thread context when the test is being torn down.\r\n Properly clean up thread context after tests >>> 1"
1223,"This commit adds the size of the cluster state to the response for the\r\nget cluster state API call (GET /_cluster/state).  The size that is\r\nreturned is the size of the full cluster state in bytes when compressed.\r\nThis is the same size of the full cluster state when serialized to\r\ntransmit over the network.  Specifying the ?human flag displays the\r\ncompressed size in a more human friendly manner.  Note that even if the\r\ncluster state request filters items from the cluster state (so a subset\r\nof the cluster state is returned), the size that is returned is the\r\ncompressed size of the entire cluster state.\r\n\r\nCloses #3415 Adds cluster state size to /_cluster/state response >>> 1"
1224,"Previously, the RestController would stash the context prior to copying headers. However, there could be deprecation log messages logged and in turn warning headers being added to the context prior to the stashing of the context. These headers in the context would then be removed from the request and also leaked back into the calling thread's context.\r\n\r\nThis change moves the stashing of the context to the HttpTransport so that the network threads' context isn't accidentally populated with warning headers and to ensure the headers added early on in the RestController are not excluded from the response. HTTP transport stashes the ThreadContext instead of the RestController >>> 1"
1225,This commit adds deprecation warnings for the use of Netty 3 for transport or HTTP requests.\r\n\r\nSupersedes #23411 Deprecate Netty 3 >>> 1
1226,Recent changes in the Lucene query that the ExistsQueryBuilder creates broke\r\nthis test. Tests: Adapt ExistsQueryBuilderTests to changes in ExistQueryBuilder#toQuery() >>> 1
1227,"When downloading Gradle to install inside the VMs used for testing, the\r\ndownload progress logs do not play well with the Gradle progress logger\r\nso we see garbage like:\r\n\r\n```\r\n==> centos-6: ==> Installing gradle\r\n==> centos-6:\r\n==> centos-6:\r\n==> centos-6: %\r\n==> centos-6:\r\n==> centos-6: T\r\n==> centos-6: o\r\n==> centos-6: t\r\n==> centos-6: a\r\n==> centos-6: l\r\n==> centos-6:\r\n==> centos-6:\r\n==> centos-6:\r\n==> centos-6:\r\n==> centos-6: %\r\n==> centos-6:\r\n==> centos-6: R\r\n==> centos-6: e\r\n==> centos-6: c\r\n==> centos-6: e\r\n==> centos-6: i\r\n==> centos-6: v\r\n==> centos-6: e\r\n==> centos-6: d\r\n==> centos-6:\r\n==> centos-6: %\r\n==> centos-6:\r\n==> centos-6: X\r\n==> centos-6: f\r\n==> centos-6: e\r\n==> centos-6: r\r\n==> centos-6: d\r\n==> centos-6:\r\n==> centos-6:\r\n==> centos-6: A\r\n==> centos-6: v\r\n==> centos-6: e\r\n==> centos-6: r\r\n==> centos-6: a\r\n==> centos-6: g\r\n==> centos-6: e\r\n==> centos-6:\r\n==> centos-6: S\r\n==> centos-6: p\r\n==> centos-6: e\r\n==> centos-6: e\r\n==> centos-6: d\r\n==> centos-6:\r\n==> centos-6:\r\n```\r\n\r\nThis commit addresses this by setting curl to be silent and only show\r\nerrors. This instead gives:\r\n\r\n```\r\n==> centos-6: ==> Installing gradle\r\n==> centos-6: Archive:  /tmp/gradle.zip\r\n```\r\n\r\n Install Gradle quietly >>> 1"
1228,This commit adds a note to the docs regarding the requirements for Bash.\r\n\r\nRelates #23443\r\n Add documentation for Bash requirement >>> 1
1229,"For rest handlers that support plain text, we did not always try to auto detect the content type and instead just sent the request along. This breaks sending of JSON or other XContent requests without a content type header as the body will be parsed as plain text. This commit ensures we auto detect for these requests if auto detection is enabled.\r\n Ensure we try to autodetect content type for handlers that support plain text >>> 1"
1230,"This commit adds an `ignoreSha` configuration to the `dependencyLicense`\r\ntask, which allows to not check for a sha for a given dependency jar.\r\nThis is useful for locally built jars, which will constantly change. Build: Allow ignoring sha checks for dependencies >>> 1"
1231,"While the esplugin extension already had an input for the base notice\r\nfile of the plugin, the NoticeTask did not actually know how to use\r\nthat, and always used the base notice file from Elasticsearch. Build: Fix notice generation to use configured notice file >>> 1"
1232,"Previously, the Azure blob store would depend on a 404 StorageException\r\ncoming back from Azure if trying to open an input stream to a\r\nnon-existent blob.  This works for Azure repositories which access a\r\nprimary location path.  For those configured to access a secondary\r\nlocation path, the Azure SDK keeps trying for a long while before\r\nreturning a 404 StorageException, causing potential delays in the\r\nsnapshot APIs.  This commit makes an initial check if the blob exists in\r\nAzure and returns immediately with a NoSuchFileException, instead of\r\ntrying to open the input stream to the blob.\r\n\r\nCloses #23480  Azure blob store's readBlob() method first checks if the blob exists >>> 1"
1233,"When parsing the control groups to which the Elasticsearch process belongs, we extract a map from subsystems to paths by parsing /proc/self/cgroup. This file contains colon-delimited entries of the\r\nform hierarchy-ID:subsystem-list:cgroup-path. For control group version 1 hierarchies, the subsystem-list is a comma-delimited list of the subsystems for that hierarchy. For control group version 2 hierarchies (which can only exist on Linux kernels since version 4.5), the subsystem-list is an empty string. The previous parsing of /proc/self/cgroup incorrectly accounted for this possibility (a + instead of a * in a regular expression). This commit addresses this issue, adds a test case that covers this possibility, and simplifies the code that parses /proc/self/cgroup.\r\n\r\nCloses #23486 Handle existence of cgroup version 2 hierarchy >>> 1"
1234,"The hyper-link 'downloaded from our website' for the debian section was pointing to rpm. Updating the href.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Small update in deb.asciidoc to fix href mismatch (deb -> rpm) >>> 1"
1235,"Fixed NPEs caused by requests without content.\r\nREST handlers that require a body will throw an an `ElasticsearchParseException` ""request body required"".\r\nREST handlers that require a body OR source param will throw an `ElasticsearchParseException` ""request body or source parameter is required"".\r\nReplaced asserts in `BulkRequest` parsing code with a more descriptive `IllegalArgumentException` if the line contains an empty object.\r\n\r\nBefore this fix all the following requests threw confusing exceptions:\r\n```\r\nPOST _bulk\r\nPOST _search/template/test\r\nPOST _scripts/test\r\nPOST _mapping/test\r\nPOST test-index/test-type\r\nresponse:\r\n{\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""null_pointer_exception"",\r\n        ""reason"": null\r\n      }\r\n    ],\r\n    ""type"": ""null_pointer_exception"",\r\n    ""reason"": null\r\n  },\r\n  ""status"": 500\r\n}\r\n\r\nPOST _template/test\r\nresponse:\r\n{\r\n\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""not_x_content_exception"",\r\n        ""reason"": ""Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes""\r\n      }\r\n    ],\r\n    ""type"": ""not_x_content_exception"",\r\n    ""reason"": ""Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes""\r\n  },\r\n  ""status"": 500\r\n}\r\n\r\nPOST _bulk\r\n{}\r\nresponse (note strange null in reason):\r\n{\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""illegal_argument_exception"",\r\n        ""reason"": ""Malformed action/metadata line [1], expected START_OBJECT or END_OBJECT but found [null]""\r\n      }\r\n    ],\r\n    ""type"": ""illegal_argument_exception"",\r\n    ""reason"": ""Malformed action/metadata line [1], expected START_OBJECT or END_OBJECT but found [null]""\r\n  },\r\n  ""status"": 400\r\n}\r\n```\r\n\r\nAfter this fix, requests that require a body will throw this exception:\r\n```\r\n{\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""parse_exception"",\r\n        ""reason"": ""request body required""\r\n      }\r\n    ],\r\n    ""type"": ""parse_exception"",\r\n    ""reason"": ""request body required""\r\n  },\r\n  ""status"": 400\r\n}\r\n```\r\n\r\nAfter this fix, requests require a body or `source` param:\r\n```\r\n{\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""parse_exception"",\r\n        ""reason"": ""request body or source parameter is required""\r\n      }\r\n    ],\r\n    ""type"": ""parse_exception"",\r\n    ""reason"": ""request body or source parameter is required""\r\n  },\r\n  ""status"": 400\r\n}\r\n```\r\nFixed confusing `_bulk` response:\r\n```\r\nPOST _bulk\r\n{}\r\nresponse:\r\n{\r\n  ""error"": {\r\n    ""root_cause"": [\r\n      {\r\n        ""type"": ""illegal_argument_exception"",\r\n        ""reason"": ""Malformed action/metadata line [1], expected FIELD_NAME but found [END_OBJECT]""\r\n      }\r\n    ],\r\n    ""type"": ""illegal_argument_exception"",\r\n    ""reason"": ""Malformed action/metadata line [1], expected FIELD_NAME but found [END_OBJECT]""\r\n  },\r\n  ""status"": 400\r\n}\r\n```\r\n\r\nCloses #24701 Fixed NPEs caused by requests without content. >>> 1"
1236,This commit adds a note to the resiliency status page regarding the fact that replicas can fall out of sync with the primary shard after primary promotion occurs due to a failing primary shard.\r\n\r\nRelates #10708\r\n Add note regarding out-of-sync replicas >>> 1
1237,"The YAML test has been failing for the Ruby runner, because of the incorrect syntax in the query definition and the missing document separator.\r\n\r\n* Added the YAML document separator to the beginning of the file\r\n* Fixed the incorrect JSON syntax in the query\r\n [TEST] Fixed the ""Msearch"" typed keys YAML test >>> 1"
1238,"We previously removed setting the vagrant group because sles-12 and\r\nopensuse-13 did not have this group. Now that those images have the\r\ngroup, we can go back to setting both user and group to vagrant.\r\n Build: Set vagrant group for gradle install in vagrant >>> 1"
1239,"This commit upgrades to the newest version of randomized runner. There\r\nis a new additional check that allows ensuring the working directory\r\nfor each child jvm is empty. By default, this check will fail the test\r\nrun. However, for elasticsearch, we default to wipe the directory. For\r\nexample, if you previously told the runner to not wipe the directory, in\r\norder to investigate a failure, the wipe option will delete this data\r\nupon re-running the test. Test: Upgrade randomized runner to 2.5.0 >>> 1"
1240,PR for #23482 Changed DisMaxQueryBuilder to extract inner hits from leaf queries >>> 1
1241,"Today when handling a multi-search request, we asynchornously execute as many search requests as the minimum of the number of search requests in the multi-search request and the maximum number of concurrent requests. When these search requests return, we poll more search requests from a queue of search requests from the original multi-search request. The implementation of this was recursive, and if the number of requests in the multi-search request was large, a stack overflow could arise due to the recursive invocation. This commit replaces this recursive implementation with a simple iterative implementation.\r\n\r\nCloses #23523\r\n\r\n Avoid stack overflow in multi-search >>> 1"
1242,Also added unit tests for `BestBucketsDeferringCollector` and `BestDocsDeferringCollector`.\r\nMoved and renamed the diversified sampler builder unit test and made some public constructors package protected. Added unit tests for diversified sampler aggregator >>> 1
1243,"This fixes an NPE in finding scaled float stats. The type of min/max\r\nmethods on the wrapped long stats returns a boxed type, but in the case\r\nthis is null, the unbox done for the FieldStats.Double ctor primitive\r\ntypes will cause the NPE. These methods would have null for min/max when\r\nthe field exists, but does not actually have points values.\r\n\r\nfixes #23487\r\n\r\n Fix NPE with scaled floats stats when field is not indexed >>> 1"
1244,"A previous change to the multi-search request execution to avoid stack overflows regressed on limiting the number of concurrent search requests from a batched multi-search request. In particular, the replacement of the tail-recursive call with a loop could asynchronously fire off all of the remaining search requests in the batch while max concurrent search requests are already executing. This commit attempts to address this issue by taking a more careful approach to the initial problem of recurisve calls. The cause of the initial problem was due to possibility of individual requests completing on the same thread as invoked the search action execution. This can happen, for example, in cases when an individual request does not resolve to any shards. To address this problem, when an individual request completes we check if it completed on the same thread as fired off the request. In this case, we loop and otherwise safely recurse. Sadly, there was a unit test to check that the maximum number of concurrent search requests was not exceeded, but that test was broken while modifying the test to reproduce a case that led to the possibility of stack overflow. As such, we randomize whether or not search actions execute on the same thread as the thread that invoked the action.\r\n\r\nRelates #23527 Honor max concurrent searches in multi-search >>> 1"
1245,With this commit we change the default receive predictor size for Netty\r\nfrom 32kB to 64kB as our testing has shown that this leads to less\r\nallocations on smaller heaps like the default out of the box\r\nconfiguration and this value also works reasonably well for larger\r\nheaps.\r\n\r\nCloses #23185 Adjust default Netty receive predictor size to 64k >>> 1
1246,"This commit upgrades the Netty dependencies from version 4.1.8 to version 4.1.9. This commit picks up a few bug fixes that impacted us:\r\n - Netty was incorrectly ignoring interfaces with self-assigned MAC addresses (e.g., instances running in Docker containers or on EC2)\r\n - incorrect handling of the Expect: 100-continue header\r\n\r\nCloses #23172, relates netty/netty#6308, relates netty/netty#6374\r\n Upgrade to Netty 4.1.9 >>> 1"
1247,"When plugins are installed on a union filesystem (for example, inside a Docker container), removing them can fail because we attempt an atomic move which will not work if the plugin is not installed in the top layer. This commit modifies removing a plugin to fall back to a non-atomic move in cases when the underlying filesystem does not support atomic moves.\r\n\r\nCloses elastic/elasticsearch-docker#35 Fall back to non-atomic move when removing plugins >>> 1"
1248,"This commit improves the output when jrunscript fails to include the\r\nfull output of the command. It also makes the quoting that is needed for\r\nwindows only happen on windows (which worked on java 8, but for some\r\nreason does not work with java 9) Build: Give better output for java version introspection >>> 1"
1249,"While trying to improve the failure output in #23547, the stderr was\r\nalso captured from jrunscript. This was under the assumption that stderr\r\nis only written to in case of an error. However, with java 9, when\r\nJAVA_TOOL_OPTIONS are set, they are output to stderr. And our CI sets\r\nJAVA_TOOL_OPTIONS for some reason. This commit fixes the jrunscript call\r\nto use a separate buffer for stderr. Build: Split output for jrunscript to stdout and stderr >>> 1"
1250,removing retries from s3 blob store. Close #22845.\r\n removing retries from s3 blob store. Close #22845 >>> 0
1251,Currently GeoHashGridAggregatorTests#testWithSeveralDocs increases the expected\r\ndocument count per hash for each geo point added to a document. When points\r\nadded to the same doc fall into one bucket (one hash cell) the document should\r\nonly be counted once.\r\n\r\nCloses #23555 Tests: fix GeoHashGridAggregatorTests expectations >>> 1
1252,This commit changes the listener passed to sendMessage from a Runnable\r\nto a Consumer<Exception>. An exception will be passed to the listener \r\nin the case of a send failure. This allows the callback to know the\r\ndifference between a success and failure. \r\n\r\nThis change also removes IOException from the sendMessage signature.\r\nThat signature is misleading as it allows implementers to assume an\r\nexception will be thrown in case of failure. That does not happen due\r\nto Netty's async nature.  Pass exception from sendMessage to listener >>> 1
1253,"This will allow us to get rid of deprecation warnings that appear when\r\nusing 3.3, and also get rid of extra logic for 2.13 required because of\r\nthe progress logger. This commit simply upgrades the minimum version to 3.3, and those other cleanups will be done as followups. Build: Upgrade min gradle to 3.3 >>> 1"
1254,"We'd like to be able to support context-sensitive whitelists in\r\nPainless but we can't now because the whitelist is a static thing.\r\nThis begins to de-static the whitelist, in particular removing\r\nthe static keyword from `Definition#getRuntimeClass` and plumbing\r\nthe static instance into the appropriate spots as though it weren't\r\nstatic. Once we de-static all the methods we should be able to\r\nfairly simply build context-sensitive whitelists.\r\n\r\nThe only ""fun"" bit of this is that I added another layer in the\r\nchain of methods that bootstraps `def` calls. Instead of running\r\n`invokedynamic` directly on `DefBootstrap` we now `invokedynamic`\r\n`$bootstrapDef` on the script itself loads the `Definition` that\r\nthe script was compiled against and then calls `DefBootstrap`.\r\n Start on custom whitelists for Painless >>> 1"
1255,Pinging the local node address doesn't really add to discovering other nodes. It just pollutes the logs with unneeded information. UnicastZenPing shouldn't ping the address of the local node >>> 1
1256,"The issue is reproduced in 5.1, 5.2, 5.3, 5.x and master branch. My fix is on master.\r\n\r\nThis commit just checks the ranges array size at the spot where the exception is raised. I am not sure if this should be fixed at a higher level in the code.\r\n\r\nCloses #22881 \r\n Fix ArrayIndexOutOfBoundsException when no ranges are specified in the query >>> 1"
1257,"This commit adds a system property that enables end-users to explicitly enforce the bootstrap checks, indepdently of the binding of the transport protocol. This can be useful for single-node production systems that do not bind the transport protocol (and thus the bootstrap checks would not be enforced).\r\n\r\nCloses #21864\r\n Enable explicitly enforcing bootstrap checks >>> 1"
1258,I discovered this obsolete setting with the following:\r\nCreate a data dir with an index created with ES 2.4.2.\r\nStart ES 5.x with this data dir and it upgrades the index.\r\nUse the upgrade segments functionality to upgrade the lucene segments of the index to the current lucene version.\r\nRestart the ES 5.x node -> then the following assertion is failing:\r\nhttps://github.com/crate/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java#L77\r\n\r\nbecause the setting is not added here: https://github.com/crate/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java#L186\r\n\r\nAfter searching a bit I saw that the setting is actually not really used... Remove obsolete index setting `index.version.minimum_compatible`. >>> 1
1259,"In this repository, `Settings.builder` is used everywhere although it does exactly same as `Settings.settingsBuilder`. With the reference of the commit [Remove Settings.settingsBuilder.](https://github.com/elastic/elasticsearch/commit/42526ac28e07da0055faafca1de6f8c5ec96cd85) , I think mistakenly this `Settings.settingsBuilder` remains in.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n Remove Settings.settingsBuilder. >>> 1"
1260,"Fix section ""Document API changes"" to contain the correct Delete API response. 5.x branches only. 5.x breaking changes doc corrections >>> 1"
1261,closes #20321 Docs: Add note about updating plugins requiring removal and reinstallation >>> 1
1262,"We currently use POSIX exit codes in all of our CLIs. However, posix\r\nonly suggests these exit codes are standard across tools. It does not\r\nprescribe particular uses for codes outside of that range. This commit\r\nadds 2 exit codes specific to plugin installation to make distinguishing\r\nan incorrectly built plugin and a plugin already existing easier.\r\n\r\ncloses #15295 Plugins: Add plugin cli specific exit codes >>> 1"
1263,This commit removes the hardcoded list of plugins used by the smoke\r\ntester and instead loads this list the same way the build does: by\r\nlooking for directories under the `plugins` dir.\r\n\r\ncloses #13096 Build: Make plugin list for smoke tester dynamic >>> 1
1264,"This commit adds a single node discovery type. With this discovery type, a node will elect itself as master and never form a cluster with another node. Introduce single-node discovery >>> 1"
1265,"In Gradle 3.4, the buildSrc plugin seems to be packaged into a jar before it is accessed by the rest of the build and the signatures file for the third-party audit task cannot be accessed as \r\n `getClass().getResource('/forbidden/third-party-audit.txt')` then points to a file entry in a JAR, which cannot be loaded directly as a `File` object. This commit changes the third-party audit task to pass the content of the signatures file as a `String` instead. Fix third-party audit task for Gradle 3.4 >>> 1"
1266,"This commit adds support for the pattern keyword marker filter in\r\nLucene.  Previously, the keyword marker filter in Elasticsearch\r\nsupported specifying a keywords set or a path to a set of keywords.\r\nThis commit exposes the regular expression pattern based keyword marker\r\nfilter also available in Lucene, so that any token matching the pattern\r\nspecified by the `keywords_pattern` setting is excluded from being\r\nstemmed by any stemming filters.\r\n\r\nCloses #4877 Adds pattern keyword marker filter support >>> 1"
1267,"This commit introduce a new break iterator (a BoundedBreakIterator) designed for the `unified` highlighter that is able to limit the size of fragments produced by generic break iterator like `sentence`.\r\nThe `unified` highlighter now supports `boundary_scanner` with the following mode: `words` and `sentence`. The `sentence` mode uses the new bounded break iterator in order to limit the size of the sentence to `fragment_length`.\r\nWhen sentences bigger than `fragment_length` are produced, this mode will break the sentence at the next word boundary **after** `fragment_length` is reached. Add support for fragment_length in the unified highlighter >>> 1"
1268,MapperService#parentTypes is rewrapped in an UnmodifiableSet in MapperService#internalMerge every time the cluster state is updated. After thousands of updates the collection is wrapped so deeply that calling a method on it generates a StackOverflowError.\r\n\r\nI have been running this patch in my cluster to address issue #23604 Fix MapperService StackOverflowError >>> 1
1269,"When a thread blocking on an adapter action future is interrupted, we throw an illegal state exception. This is documented, but it is rude to not restore the interrupt flag. This commit restores the interrupt flag in this situation, and adds a test.\r\n\r\nCloses #23617 Adapter action future should restore interrupts >>> 1"
1270,"Currently the task manager is tied to the transport and can only create tasks based on TransportRequests. This commit enables task manager to support tasks created by non-transport services such as the persistent tasks service.\r\n\r\n@martijnvg, @bleskes this is the part of the effort to decouple persistent tasks from the transport service, that we talked about. Task Manager should be able to support non-transport tasks >>> 1"
1271,"In cases where the user specifies only the `text` option on the top level\r\nsuggest element (either via REST or the java api), this gets transferred to the\r\n`text` property in the SuggestionSearchContext. CompletionSuggestionContext\r\ncurrently requires prefix or regex to be specified, otherwise errors. We should\r\nuse the global `text` property as a fall back if provided in this case.\r\n\r\nNote that when `text` is set on the CompletionSuggestionBuilder directly, we already \r\noverwrite a missing `prefix` property in `SuggestionBuilder#populateCommonFields`. \r\nThis, however, currently doesn't work with the global text overwrite taking place in \r\n`SuggestBuilder#build`\r\n\r\nCloses to #23340 Completion suggestion should also consider text if prefix/regex is missing >>> 1"
1272,"This commit introduces a maximum size for a translog generation and automatically rolls the translog when a generation exceeds the threshold into a new generation. This threshold is configurable per index and defaults to sixty-four megabytes. We introduce this constraint as sequence numbers will require keeping around more than the current generation (to ensure that we can rollback to the global checkpoint). Without keeping the size of generations under control, having to keep old generations around could consume excessive disk space. A follow-up will enable commits to trim previous generations based on the global checkpoint.\r\n\r\nRelates #10708\r\n Introduce translog generation rolling >>> 1"
1273,"The reindex API is mature now, and we will work to maintain backwards compatibility in accordance with our backwards compatibility policy. This commit unmarks the reindex API as experimental.\r\n Unmark reindex as experimental >>> 1"
1274,"As the query of a search request defaults to match_all,\r\ncalling _delete_by_query without an explicit query may\r\nresult in deleting all data.\r\n\r\nIn order to protect users against falling into that\r\npitfall, this commit adds a check to require the explicit\r\nsetting of a query.\r\n\r\nCloses #23629\r\n Require explicit query in _delete_by_query API >>> 1"
1275,"Without this change, if write a script with multiple regexes\r\n*sometimes* the lexer will decide to look at them like one\r\nbig regex and then some trailing garbage. Like this discuss post:\r\nhttps://discuss.elastic.co/t/error-with-the-split-function-in-painless-script/79021\r\n\r\n```\r\ndef val = /\\/.split(ctx._source.event_data.param17);\r\nif (val[2] =~ /\./) {\r\n  def val2 = /\./.split(val[2]);\r\n  ctx._source['user_crash'] = val2[0]\r\n} else {\r\n  ctx._source['user_crash'] = val[2]\r\n}\r\n```\r\n\r\nThe error message you get from the lexer is `lexer_no_viable_alt_exception`\r\nright after the *second* regex.\r\n\r\nWith this change each regex is just a single regex like it ought to be.\r\n\r\nAs a bonus, while looking into this issue I found that the error\r\nreporting for regexes wasn't very nice. If you specify an invalid\r\npattern then you get an error marker on the start of the pattern\r\nwith the JVM's regex error message which attempts to point you to the\r\nlocation in the regex but is totally unreadable in the JSON response.\r\n\r\nThis change fixes the location to point to the appropriate spot\r\ninside the pattern and removes the portion of the JVM's error message\r\nthat doesn't render well. It is no longer needed now that we point\r\nusers to the appropriate spot in the pattern.\r\n Fix painless's regex lexer and error messages >>> 1"
1276,nan Deprecate delete_by_query requests without an explicit query >>> 1
1277,"When adding filesystem stats from individual filesystems, free and available can overflow. This commit guards against this by adjusting these situations to Long.MAX_VALUE.\r\n\r\n Avoid overflow when computing total FS stats >>> 1"
1278,"Broken long lines of code for improved readability\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Visual improvements >>> 0"
1279,"with detailed description, parameter and throws listing. Add Javadoc Comment to execute method >>> 1"
1280,Minor improvements and additions in the indentation (1 line instead of 2) and in the syntax of some comments. Small improvements: indentation and syntax >>> 1
1281,"Short summary: contributing code you wrote for a class is fine, just\r\ndon't make the contribution process part of a class.\r\n Document some guidelines for students and teachers >>> 1"
1282,"Fixes issue #22748.\r\nThis commit deprecates the use of 'request_cache' to clear the cache. Instead, 'request' should be used.\r\nThis is my first contribution, so any feedback is appreciated, particularly around code conventions and the versioning I specified in the test case.\r\n Deprecate request_cache for clear-cache >>> 1"
1283,"Internal indexing requests in Elasticsearch may be processed out of order and repeatedly. This is important during recovery and due to concurrency in replicating requests between primary and replicas. As such, a replica/recovering shard needs to be able to identify that an incoming request contains information that is old and thus need not be processed. The current logic is based on external version. This is sadly not sufficient. This PR moves the logic to rely on sequences numbers and primary terms which give the semantics we need.\r\n\r\nThe change also refactors `InternalEngine.index` and `InternalEngine.delete`. The current implementations tries to share as much code as possible between the different execution paths (primary vs replica, versioned vs unversioned etc.) but the end result is hard to read and is complex to reason about. The PR proposes a slightly more verbose version but where the code flows are clearer (IMO) and rely on immutable variables which makes easier to reason about guarantees.\r\n\r\nThe PR also beefed up all the versioning tests in `InternalEngineTests`  as the current tests are not sufficient (and didn't expose some subtle but minor existing bugs).\r\n\r\nRelates to #10708  Use sequence numbers to identify out of order delivery in replicas & recovery >>> 0"
1284,This reverts #21971 which should only have been applied in master\r\nto preserve backwards compatibility. Instead of throwing an error\r\nwhen you specify `include_in_all` inside a multifield we instead\r\nreturn a deprecation warning. `include_in_all` in a multifield\r\nstill doesn't do anything. But at least people who use it erroneously\r\nwon't break.\r\n\r\nCloses #23654\r\n\r\n Switch include_in_all in multifield to warning >>> 1
1285,Some uppercase cleanup. Update aliases.asciidoc >>> 1
1286,The validation of mappings in a template should take the settings for the template into account\r\nas settings may be provided to increase soft limits such as the number of fields or nested\r\nobjects. This commit changes the dummy index service to use the settings provided in the template\r\nand overwrite those with specific values as necessary. Validation of mappings in templates needs to take settings into account >>> 0
1287,Relates to #22278 Add unit tests for ReverseNestedAggregator >>> 1
1288,"Search took time uses an absolute clock to measure elapsed time, and then tries to deal with the complexities of using an absolute clock for this purpose. Instead, we should use a high-precision monotonic relative clock that is designed exactly for measuring elapsed time. This commit modifies the search infrastructure to use a relative clock for measuring took time, but still provides an absolute clock for the components of search that require a real clock (e.g., index name expression resolution, etc.).\r\n\r\n Search took time should use a relative clock >>> 1"
1289,"After the removal of the joda time hack we used to have, we can cleanup\r\nthe codebase handling in security, jarhell and plugins to be more picky\r\nabout uniqueness. This was originally in #18959 which was never merged.\r\n\r\ncloses #18959\r\n Packaging: Remove classpath ordering hack >>> 1"
1290,"This commit makes closing a ReleasableBytesStreamOutput release the underlying BigArray so\r\nthat we can use try-with-resources with these streams and avoid leaking memory by not returning\r\nthe BigArray. As part of this change, the ReleasableBytesStreamOutput adds protection to only release the BigArray once.\r\n\r\nIn order to make some of the changes cleaner, the ReleasableBytesStream interface has been\r\nremoved. The BytesStream interface is changed to a abstract class so that we can use it as a\r\nuseable return type for a new method, Streams#flushOnCloseStream. This new method wraps a\r\ngiven stream and overrides the close method so that the stream is simply flushed and not closed.\r\nThis behavior is used in the TcpTransport when compression is used with a\r\nReleasableBytesStreamOutput as we need to close the compressed stream to ensure all of the data\r\nis written from this stream. Closing the compressed stream will try to close the underlying stream\r\nbut we only want to flush so that all of the written bytes are available.\r\n\r\nAdditionally, an error message method added in the BytesRestResponse did not use a builder\r\nprovided by the channel and instead created its own JSON builder. This changes that method to use the channel builder and in turn the bytes stream output that is managed by the channel.\r\n Closing a ReleasableBytesStreamOutput closes the underlying BigArray >>> 1"
1291,nan Confusing sentence in allocation-explain doc >>> 1
1292,Makes it explicit that the `node_id` has to be included when canceling a task. I had to look into the code to figure this out.\r\n\r\nThere might be a different and maybe better way of making this clear - though this change seems intuitive to me. Clarify task cancellation command >>> 1
1293,According to https://www.elastic.co/guide/en/elasticsearch/reference/5.2/docs-reindex.html#_url_parameters_3 the default is indeed `true` [API] change wait_for_completion default according to docs >>> 1
1294,"Changes reindex and friends to wait until the entire request has\r\nbeen ""cleaned up"" before responding. ""Clean up"" in this context\r\nis clearing the scroll and (for reindex-from-remote) shutting\r\ndown the client. Failures to clean up are still only logged, not\r\nreturned to the user.\r\n\r\nCloses #23653\r\n Make reindex wait for cleanup before responding >>> 1"
1295,"This commit catches the underlying failure when trying to list plugin\r\ninformation when a plugin is incompatible with the current version of\r\nelasticsearch. This could happen when elasticsearch is upgraded but old\r\nplugins still exist. With this change, all plugins will be output,\r\ninstead of failing at the first out of date plugin.\r\n\r\ncloses #20691\r\n Plugins: Output better error message when existing plugin is incompatible >>> 1"
1296,"Currently the user can set an analyzer on e.g. a numeric query, but explicitely\r\nsetting an analyzer only make sense for input text. Otherwise we might\r\nanalyze e.g. floats in scientific notation like `1.34e-6` to just an `e` which\r\nleads to NumberFormatExceptions on each shard later (if targeting a numeric\r\nfield).\r\n\r\nCloses #21665\r\n Match- and MultiMatchQueryBuilder should only allow setting analyzer on string values >>> 0"
1297,"This commit adds a build listener to the integ test runner which will\r\nprint out an excerpt of the logs from the integ test cluster if the test\r\nrun fails.  There are future improvements that can be made (for example,\r\nto dump excerpts from dependent clusters like in tribe node tests), but\r\nthis is a start, and would help with simple rest tests failures that we\r\ncurrently don't know what actually happened on the node. Test: Add dump of integ test cluster logs on failure >>> 1"
1298,"We are now on minimum gradle 3.3, so we no longer need the groovy/gradle\r\nhacks used to support both 2.13 and 2.14+. Build: remove progress logger hack for gradle 2.13 >>> 1"
1299,"The output of the different implementations of terms aggs is always very similar. The `toXContent` methods for each of those classes though was duplicating almost the same code multiple times. This commit centralizes the code for rendering `XContent` to a single place, which can be reused from the different terms aggs implementations. Share XContent rendering code in terms aggs >>> 1"
1300,This is especially useful when we rewrite the query because the result of the rewrite can be very different on different shards. See #18254 for example.\r\n Make it possible to validate a query on all shards instead of a single random shard >>> 1
1301,"This PR is meant to address the permission errors that are encountered in the HDFS Repository Plugin as described in https://github.com/elastic/elasticsearch/issues/22156.\r\n\r\nWhen Hadoop security is enabled, the HDFS client requests the current logged in Subject for a hadoop based Credentials object, which trips a missing permission in the plugin's policy file. This is not caught during testing since we neither use the actual HDFS client code nor do we execute with Kerberos security enabled.\r\n\r\nI'm working on testing this on a local environment at the moment since it requires a secured HDFS service to activate the code path. My main concern is that there may be other permissions that have not yet had the chance to trip up the plugin because they have not yet been reached in the code.\r\n\r\nCloses #22156 Fixing permission errors for `KERBEROS` security mode for HDFS Repository >>> 1"
1302,This PR completely removes all trace of the `distance_type` parameter and supporting `GeoDistance` class(es) that was deprecated in 5.3.\r\n\r\ncloses #22914 \r\ncloses #15616 Remove distance_type support from GeoDistance >>> 0
1303,"This commit fixes an issue manifested in the\r\nSharedClusterSnapshotRestoreIT#testGetSnapshotsRequest where a delete\r\nrequest on a snapshot encounters an in-progress snapshot, so it first\r\ntries to abort the snapshot.  During the aborting process, an exception\r\nis thrown which is handled by the snapshot listener's onSnapshotFailure\r\nmethod.  This method retries the delete snapshot request, only to\r\nencounter that the snapshot is missing, throwing an exception.  It is\r\npossible that the snapshot failure resulted in the snapshot never having\r\nbeen written to the repository, and hence, there is nothing to delete.\r\nThis commit handles the SnapshotMissingException by logging it and\r\nnotifying the listener of the missing snapshot.\r\n\r\nCloses #23663 Fixes snapshot deletion handling on in-progress snapshot failure >>> 1"
1304,"Each search hit object can be very small e.g. when using ""stored_fields"": [""_none_""]\r\nwe only get back ""_index"" and ""_score"" fields. This adds a test that checks that we \r\ncan still parse back the object.\r\n\r\n Test: Check that parsing SearchHit without _type/_id works >>> 1"
1305,With this change we remove a TODO from `CommonFields`. Also this will be useful when parsing aggs response for the high level REST client. Use ParseField for aggs CommonFields rather than String >>> 1
1306,In gradle 3.3 use of getConfiguration on a ModuleDependency was\r\ndeprecated. This commit changes it to use getTargetConfiguration. Build: Use targetConfiguration to remove gradle deprecation warning >>> 1
1307,This change ports the regeneration of antlr parser/lexer into gradle\r\n(but does still take advantage of ant calls where appropriate). Build: Rewrite antlr regeneration in gradle >>> 1
1308,"The OpenJDK project provides early-access builds of upcoming releases. These early-access builds are not suitable for production. These builds sometimes end up on systems due to aggressive packaging (e.g., Ubuntu). This commit adds a bootstrap check to ensure these early-access builds are not being used in production.\r\n\r\nRelates #23668\r\n Add early-access check >>> 1"
1309,First: I'm genuinely sorry to whoever reviews this. It is big and will need to be looked at carefully.\r\n\r\nThis starts the process of breaking scripts and templates. I've intentionally preserved abstractions that will be lost eventually in an attempt to keep this change as small as I figured I could get away with while still enabling us to change things later. I believe I've preserved 95% of backwards compatibility. This is what I know I've broken:\r\n\r\n* The plugin interface\r\n* The transport client with regard to templates\r\n* The stored template APIs during the 5.x to 6.0 upgrade. They should work just fine outside of the upgrade.\r\n\r\nHere are some things I intentionally didn't change that we probably will want to change later:\r\n\r\n* Internally we use the `Script` object to refer to templates. This is pretty silly and the new `TemplateService` doesn't require it. I didn't change this to keep the PR small. It should be a followup.\r\n* I believe as a followup to *that* we should be able to remove some things from `Script`.\r\n* `TemplateService.Backend` is the same as `ScriptEngineService`. I didn't change this to keep the PR small. It should be a followup.\r\n\r\n\r\nThings I didn't change that we might want to change at some point but we'll need to be super careful about so we don't break backwards compatibility:\r\n\r\n* File templates are still loaded from the `config/scripts` directory every few seconds.\r\n* Templates stored in the cluster state are still stored in the `ScriptMetaData`.\r\n* Statistics related to templates and scripts are still collected together.\r\n\r\nOriginally I'd planned to backport this to 5.x as well as commit to master but right now I don't have any appetite for it. Begin to break templates away from scripts >>> 0
1310,"Today, when parsing mget requests, we silently ignore keys in the top level that do not match ""docs"" or ""ids"". This commit addresses this situation by throwing an exception if any other key occurs here, and providing the names of valid keys.\r\n\r\nCloses #23720 Validate top-level keys when parsing mget requests >>> 1"
1311,"The InternalEngine Index/Delete methods (plus satellites like version loading from Lucene) have accumulated some cruft over the years making it hard to clearly the code flows for various use cases (primary indexing/recovery/replicas etc). This PR refactors those methods for better readability. As a follow up we intend to take certain parts of these method and extract them to another help methods to improve things even more. This will be done as a follow up.\r\n\r\nTo support the refactoring I have considerably beefed up  the versioning tests.\r\n\r\nThis PR is a spin-off from #23543 , which made it clear this is needed. Refactor InternalEngine's index/delete flow for better clarity >>> 1"
1312,Change the error response when using a non UTC timezone for range queries with epoch_millis or epoch_second formats to an illegal argument exception. The goal is to provide a better explanation of why the query has failed. The current behavior is to respond with a parse exception. Improve error handling for epoch format parser with time zone (#22621) >>> 1
1313,nan corrected windows options for plugin installation via proxy >>> 1
1314,The last commit is worth reviewing. It contains a small hack to retrieve the merge rate limiter hidden by https://issues.apache.org/jira/browse/LUCENE-7700. Upgrade to Lucene 6.5.0 >>> 1
1315,An array of values is illegal in the `fuzzy` query and should result in a parsing error.\r\n\r\nCloses #23759\r\n\r\n FuzzyQueryBuilder should error when parsing array of values >>> 1
1316,"Relates to https://github.com/elastic/elasticsearch/issues/23331#issuecomment-286830030.\r\n\r\nThe goal is to (eventually) move all the analyzers in `lucene-analzyers-common.jar` into a module so the core ES jar doesn't have to depend on `lucene-analyzer-common.jar`. This would only affect the high level rest client and the transport client. The download would still include the the jar, just in a different spot.\r\n\r\nSo the real question here is, ""is this worth our time?"" This would shave maybe 2mb off of the high level rest client and transport client. `lucene-analzyers-common.jar` is 1.4mb. I'm generously assuming that the code I extract from core will be 600kb.\r\n\r\nSome of the extraction is fairly easy - move files around, rig them up like plugins, etc. The effect on tests isn't super difficult, but means that the process takes time.\r\n\r\nIn the end the only analyzer available to tests in core would be `standard` because that is all that is in `lucene-core`. The `standard` tokenizer would be available in core as well. No token would be available though. Only the `lowercase` and `mock` tokenfilters are available in `lucene-core` and we can't expose them in core because `lowercase` in Elasticsearch is linked with `GreekLowerCaseFilter`, `IrishLowerCaseFilter`, and `TurkishLowerCaseFilter` and `mock` is just for testing. Useful for writing tests, mind, but not a thing we can expose.\r\n\r\nSo I'm opening this up for discussion: should we do it?\r\n\r\nMy thoughts:\r\n1. It'd make the core theoretically cleaner. Lucene has had this separation for a long time.\r\n2. It'd be a pretty big change. Not as long hanging fruit as we thought. This PR isn't small and it only does three token filters.\r\n3. It wouldn't really save that many bits unless I'm reading it wrong.\r\n\r\nI'm quite happy to kill this if we decide it isn't a useful savings. Start building analysis-common module >>> 1"
1317,This commit modifies the handling of plugins that require special permissions to cover a case that was not previously covered.\r\n\r\n Modify permissions dialog for plugins >>> 1
1318,"This moves `updateReplicaRequest` to `createPrimaryResponse` and separates the\r\ntranslog updating to be a separate function so that the function purpose is more\r\neasily understood (and testable).\r\n\r\nIt also separates the logic for `MappingUpdatePerformer` into two functions,\r\n`updateMappingsIfNeeded` and `verifyMappings` so they don't do too much in a\r\nsingle function. This allows finer-grained error testing for when a mapping\r\nfails to parse or be applied.\r\n\r\nFinally, it separates parsing and version validation for\r\n`executeIndexRequestOnReplica` into a separate\r\nmethod (`prepareIndexOperationOnReplica`) and adds a test for it.\r\n\r\nRelates to #23359 Further refactor and extend testing for `TransportShardBulkAction` >>> 1"
1319,"This commit switches from executing gradle when building the bwc testing\r\nzip through Exec, to using GradleBuild. In addition to not depending on\r\ngradle being in the PATH, it also has the added benefit of much better\r\nlogging while the bwc build is going on (the actual tasks show up as\r\ntasks of a subproject within the current build).\r\n Build: Use GradleBuild task for invoking 5.x checkout build >>> 1"
1320,"As per https://github.com/elastic/elasticsearch/issues/23131#issuecomment-280307124 it is not clear if  introducing took time for multi search is a desired behavior or if a further discussion is needed.\r\n\r\nBut as there are no comments against it so far I am submitting this PR.   Introducing ""took"" time (in ms) for `_msearch` >>> 1"
1321,"We default to allowing remote connections from a node, which makes the getting started experience ideal.  However, with respect to documentation, we should scope ""dedicated"" nodes to within a single cluster.  This PR reflects that. Reflect cross-cluster search in ""dedicated"" terminology >>> 1"
1322,"This commit adds the boolean similarity scoring from Lucene to\r\nElasticsearch.  The boolean similarity provides a means to specify that\r\na field should not be scored with typical full-text ranking algorithms,\r\nbut rather just whether the query terms match the document or not.\r\nBoolean similarity scores a query term equal to its query boost only.\r\nBoolean similarity is available as a default similarity option and thus\r\na field can be specified to have boolean similarity by declaring in its\r\nmapping:\r\n```\r\n""similarity"": ""boolean""\r\n```\r\n\r\nCloses #6731 Adds boolean similarity to Elasticsearch >>> 1"
1323,"The translog already occupies 43 bytes on disk when empty. If the translog generation threshold is below this, the flush thread can get stuck in an infinite loop repeatedly rolling the generation. This commit adds a lower bound on the translog generation to avoid this problem, however we keep the lower bound small for convenience in testing.\r\n\r\nRelates #23606\r\n\r\n Add lower bound for translog generation threshold >>> 1"
1324,Resolves https://github.com/elastic/elasticsearch/issues/23793 Clarify summary description >>> 1
1325,We currently have the last minor version of the previous major hardcoded\r\nin tests like rolling upgrade. This change programmatically finds this\r\nduring gradle initialization by parsing versions from Version.java. Build: Find bwc version during build >>> 1
1326,"This will use File.toString() for the `git clone` command, which will\r\nautomatically be correct for whatever system the build is running on.\r\n\r\ncloses #23784 Build: Use filesystem agnostic printing of bwc dir path >>> 1"
1327,issue around this enhancement here #23085 ingest convert processor add long and double type. >>> 0
1328,"The LoggedExec task does not capture output when info logging is\r\nenabled. This commit changes the upstream check to use Exec directly,\r\nso as not to break when info logging is enabled. Build: Ensure upstream check works even when using info logging >>> 1"
1329,Discussed in https://discuss.elastic.co/t/5-3-0-breaks--reindex/80388 and https://github.com/elastic/elasticsearch/pull/22931#issuecomment-290093349\r\n\r\nThis commit https://github.com/elastic/elasticsearch/commit/7520a107bee67099338813728147d2aee25ed240#diff-0c925d1f41390dd5d7dd09b6ff5d51d1 in master and v5.3 breaks `reindex from remote` when the source is on a version before 2.0.0. The `scroll_id` is always extracted from a plain text body in ES 1.7 (and before) and does not accept the JSON version that appears in 2.0.0.\r\nThis change checks the version of the remote cluster and builds the scroll_id in the body depending on the remote version. Fix reindex with a remote source on a version before 2.0.0 >>> 1
1330,"This commit adds information about connecting the same repository to clusters of different versions, namely 2.x and 5.x clusters, which is only supported if the 2.x clusters create the repository in `readonly` mode. [DOCS] Multi-cluster snapshots connected to the same repository >>> 1"
1331,"Now that we are on gradle 3.3, we can take advantage of a fix that was\r\nmade in 2.14 which properly handles disabling transitive dependencies in\r\npom generation.  As it was currently, we actually ended up generated two\r\nexclusions sections in the generated pom. This is yet another example of\r\nwhy we need validation on the pom files with our generation here, but I\r\nleave that for another day because I still don't know a good way to do\r\nit. Build: Remove pom exclusions hack used for gradle 2.13 >>> 1"
1332,"There is a missing backtick `""false"" \r\nCorrected it\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n Missing backticks >>> 1"
1333,"As an addition to https://github.com/elastic/elasticsearch/issues/22278 we should probably also have base tests for InternalSimpleValue and InternalDerivative, although it seems to me they don't have a corresponding Aggregator implementation. We will need to extend this test when writing the aggregation parsing as well. Tests: Add base tests for InternalSimpleValue and InternalDerivative >>> 1"
1334,"Today we have the shard target and the target request ID available in SearchPhaseResults.\r\nYet, the coordinating node maintains a shard index to reference the request, response tuples\r\ninternally which is also used in many other classes to reference back from fetch results to\r\nquery results etc. Today this shard index is implicitly passed via the index in AtomicArray\r\nwhich causes an undesirable dependency on this interface.\r\nThis commit moves the shard index into the SearchPhaseResult and removes some dependencies\r\non AtomicArray. Further removals will follow in the future. The most important refactoring here\r\nis the removal of AtomicArray.Entry which used to be created for every element in the atomic array\r\nto maintain the shard index during result processing. This caused an unnecessary indirection, dependency\r\nand potentially thousands of unnecessary objects in every search phase.\r\n Streamline shard index availability in all SearchPhaseResults >>> 1"
1335,This fixes a bug in 'date_histogram' when using 'extended_bounds'\r\ntogether with some 'offset'. Offsets should be applied after rounding the\r\nextended bounds and also be applied when adding empty buckets during the reduce\r\nphase in InternalDateHistogram.\r\n\r\nCloses #23776\r\n DateHistogram: Fix `extended_bounds` with `offset` >>> 1
1336,"This change introduces a new API called `_field_caps` that allows to retrieve the capabilities of specific fields.\r\nThis field centric API relies solely on the mapping of the requested indices to extract the following infos:\r\n* `types`: one or many field types if the type is not the same across the requested indices\r\n* `searchable`: Whether this field is indexed for search on at least one requested indices.\r\n* `aggregatable`:  Whether this field can be aggregated on at least one requested indices.\r\n\r\nExample:\r\n\r\n````\r\nGET t,v/_field_caps?fields=field1,field2,field3\r\n````\r\n\r\nreturns:\r\n\r\n````\r\n{\r\n   ""fields"": {\r\n      ""field1"": {\r\n         ""_all"": {\r\n            ""types"": ""text"",\r\n            ""searchable"": true,\r\n            ""aggregatable"": false\r\n         }\r\n      },\r\n      ""field3"": {\r\n         ""_all"": {\r\n            ""types"": ""text"",\r\n            ""searchable"": true,\r\n            ""aggregatable"": false\r\n         }\r\n      },\r\n      ""field2"": {\r\n         ""_all"": {\r\n            ""types"": [\r\n               ""keyword"",\r\n               ""long""\r\n            ],\r\n            ""searchable"": true,\r\n            ""aggregatable"": true\r\n         }\r\n      }\r\n   }\r\n}\r\n````\r\n\r\nIn this example `field1` and `field3` have the same type `text` across the requested indices `t` and `v`.\r\nConversely `field2` is defined with two conflicting types `keyword` and `long`.\r\nNote that `_field_caps` does not treat this case as an error but rather return the list of unique types seen for this field.\r\n\r\nIt is also possible to get a view of each index field capabilities with the `level` parameter:\r\n\r\n````\r\nGET t,v/_field_caps?fields=field2&level=indices\r\n````\r\n\r\n````\r\n{\r\n   ""fields"": {\r\n      ""field2"": {\r\n         ""t"": {\r\n            ""types"": ""keyword"",\r\n            ""searchable"": true,\r\n            ""aggregatable"": true\r\n         },\r\n         ""v"": {\r\n            ""types"": ""long"",\r\n            ""searchable"": true,\r\n            ""aggregatable"": true\r\n         }\r\n      }\r\n   }\r\n}\r\n````\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/22438#issuecomment-276459451 Add FieldCapabilities (_field_caps) API >>> 1"
1337,Closes #23448. Update to Azure Storage 5.0.0 >>> 1
1338,"With this commit, Azure repositories are now using an Exponential Backoff policy before failing the backup.\r\nIt uses Azure SDK default values for this policy:\r\n\r\n* `30s` delta backoff base with\r\n   * `3s` min\r\n   * `90s` max\r\n* `3` retries max\r\n\r\nUsers can define the number of retries they wish by setting `cloud.azure.storage.xxx.max_retries` where `xxx` is the azure named account.\r\n\r\nCloses #22728.\r\n Add Backoff policy to azure repository >>> 1"
1339,You can define a proxy using the following settings:\r\n\r\n```yml\r\nazure.client.default.proxy.host: proxy.host\r\nazure.client.default.proxy.port: 8888\r\nazure.client.default.proxy.type: http\r\n```\r\n\r\nSupported values for `proxy.type` are `http` or `socks`. Defaults to no proxy.\r\n\r\nNote that this commit depends on #23517 which needs to be merged first (upgrade azure client to 5.0.0).\r\n\r\nCloses #23506\r\n Support for accessing Azure repositories through a proxy >>> 1
1340,"When executing an update request, the request timeout is not transferred to the index/delete request executed on behalf of the update request. This leads to update requests not timing out when they should (e.g., if not all shards are available when the request specifies wait_for_shards=all with a small timeout). This commit causes the index/delete requests to honor the update request timeout.\r\n Honor update request timeout >>> 1"
1341,Today we have no way to mark an execution as internal. This commit adds\r\na simple thread context header that allows executing code in a system context.\r\nThis allows intercepting code can make better decisions down the road when\r\nit gets to authentication.\r\n Add infrastructure to mark contexts as system contexts >>> 1
1342,"Currently for field sorting we always use a custom sort field and a custom comparator source.\r\nThough for numeric fields this custom sort field could be replaced with a standard SortedNumericSortField (unless the field is nested) especially since we removed the FieldData for numerics.\r\nWe can also use a SortedSetSortField for string sort based on doc_values when the field is not nested.\r\n\r\nThis change replaces IndexFieldData#comparatorSource with IndexFieldData#sortField that returns a Sorted{Set,Numeric}SortField when possible or a custom sort field when the field sort spec is not handled by the SortedSortFields. Replace custom sort field with SortedSetSortField and SortedNumericSortField when possible >>> 1"
1343,"When executing an index operation on the primary shard,\r\n`TransportShardBulkAction` first parses the document, sees if there are any\r\nmapping updates that needs to be applied, and then updates the mapping on the\r\nmaster node. It then re-parses the document to make sure that the mappings have\r\nbeen applied and propagated.\r\n\r\nThis adds a check that skips the second parsing of the document in the event\r\nthere was not a mapping update applied in the first case.\r\n\r\nFixes a performance regression introduced in #23665 Only re-parse operation if a mapping update was needed >>> 1"
1344,"If a snapshot is taken on multiple indices, and some of them are ""good""\r\nindices that don't contain any corruption or failures, and some of them\r\nare ""bad"" indices that contain missing shards or corrupted shards, and\r\nif the snapshot request is set to partial=false (meaning don't take a\r\nsnapshot if there are any failures), then the good indices will not be\r\nsnapshotted either.  Previously, when getting the status of such a\r\nsnapshot, a 500 error would be thrown, because the snap-*.dat blob for\r\nthe shards in the good index could not be found.\r\n\r\nThis commit fixes the problem by reporting shards of good indices as\r\nfailed due to a failed snapshot, instead of throwing the\r\nNoSuchFileException.\r\n\r\nCloses #23716 Fixes snapshot status on failed snapshots >>> 1"
1345,Relates to #22278 Adds tests for cardinality and filter aggregations >>> 1
1346,Closes #22768\r\n Improves disabled fielddata error message >>> 1
1347,SearchPhaseController is tightly coupled to AtomicArray which makes\r\nnon-dense representations of results very difficult. This commit removes\r\nthe coupling and cuts over to Collection rather than List to ensure no\r\norder or random access lookup is implied.\r\n Cleanup SearchPhaseController interface >>> 1
1348,Today we have multiple ways to define settings when a user needs to create a repository:\r\n\r\n* in `elasticsearch.yml` file using `repositories.azure` prefix\r\n* when creating the repository itself with `PUT _snaphot/repo`\r\n\r\nThe plan is to:\r\n\r\n* Deprecate `repositories.azure` settings in 5.x (done with #22856)\r\n* Remove in 6.x (this PR)\r\n\r\nRelated to #22800\r\n Remove global `repositories.azure` settings >>> 1
1349,"Today we prevent nodes from joining when indices exists that are too old.\r\nYet, the opposite can happen too since lucene / elasticsearch is not forward\r\ncompatible when it gets to indices we won't let nodes join the cluster once\r\nthere are indices in the clusterstate that are newer than the nodes version.\r\nThis prevents forward compatibility issues which we never test against. Yet,\r\nthis will not prevent rolling restarts or anything like this since indices\r\nare always created with the minimum node version in the cluster such that an index\r\ncan only get the version of the higher nodes once all nodes are upgraded to this version.\r\n Prevent nodes from joining if newer indices exist in the cluster >>> 1"
1350,"Create index accepts only `settings`, `mappings` or `aliases` as top-level keys.\r\n\r\nCloses #23755 Validate top-level keys for create index request  >>> 0"
1351,"This commit changes `ClusterStatsNodes.NetworkTypes` so that is does\r\nnot print out empty field names when no Transport or HTTP type is defined:\r\n\r\n```\r\n{\r\n      ...\r\n      ""network_types"": {\r\n        ...\r\n        ""http_types"": {\r\n          """": 2\r\n        }\r\n      }\r\n}\r\n```\r\n\r\nis now rendered as:\r\n\r\n```\r\n{\r\n      ...\r\n      ""network_types"": {\r\n        ...\r\n        ""http_types"": {\r\n        }\r\n      }\r\n}\r\n```\r\n\r\nThis should almost never happen because `transport-netty4` is shipped by default, but we can have integration tests that use mocked transport plugins without registering HTTP transport types. Cluster stats should not render empty http/transport types >>> 1"
1352,"This class is unused, so it can be removed.\r\n Cleanup: Remove unused FieldMappers class >>> 1"
1353,"Fielddata can no longer be configured to be loaded eagerly (it only accepts\r\n`true` and `false`), so this line is a little misleading because it talks about\r\na procedure we can no longer do. [DOCS] Remove line about eager loading global ordinals >>> 1"
1354,TopDocs et.al. got additional parameters to incrementally reduce\r\ntop docs. In order to add incremental reduction `CollapseTopFieldDocs`\r\nneeds to have the same properties.\r\n Synchronized CollapseTopFieldDocs with lucenes relatives >>> 1
1355,"Eclipse can't deal with the generics, maybe the fixed but\r\nunreleased https://bugs.eclipse.org/bugs/show_bug.cgi?id=511750\r\n Fix FieldCapabilities compilation in Eclipse >>> 1"
1356,"The method Boolean#getBoolean is dangerous. It is too easy to mistakenly invoke this method thinking that it is parsing a string as a boolean. However, what it actually does is get a system property with the specified string, and then attempts to use usual crappy boolean parsing in the JDK to parse that system property as boolean with complete leniency (it parses every input value into either true or false); that is, this method amounts to invoking Boolean#parseBoolean on the result of System#getProperty(String). Boo. This commit bans usage of this method.\r\n\r\nRelates #23863\r\n Ban Boolean#getBoolean >>> 1"
1357,"Remote nodes in cross-cluster search can be marked as eligible for acting a gateway node via a remote node attribute setting. For example, if search.remote.node.attr is set to ""gateway"", only nodes that have node.attr.gateway set to ""true"" can be connected to for cross-cluster search. Unfortunately, there is a bug in the handling of these attributes due to the use of a dangerous method Boolean#getBoolean(String) which obtains the system property with specified name as a boolean. We are not looking at system properties here, but node settings. This commit fixes this situation, and adds a test. A follow-up will ban the use of Boolean#getBoolean. Fix cross-cluster remote node gateway attributes >>> 1"
1358,This adds parsing from xContent to one of the simple single metric aggregations. I open this as a WIP for discussion since I expect some overlap with work currently going on with adding parsing to other aggregations.\r\n\r\n Add parsing to InternalMax aggregation >>> 0
1359,"Related to #23847, this is a slightly more complicated single value aggregation to parse, since it can \r\nrender two distinct formatted ""value_as_string"" fields which we need to provide back. Also, the ""normalized_value""\r\nthat we can parse from the REST response needs to be converted back to the original normalization factor used in\r\nthe InternalDerivative object for the calculation.\r\n Add parsing to InternalDerivative aggregation >>> 0"
1360,"This change adds a setting property `Final` that sets the value of a setting as final.\r\nUpdating a `Final` setting is prohibited in any context, for instance an index setting\r\nmarked as final must be set at index creation and will refuse any update even if the index is closed. Add a property to mark setting as final >>> 1"
1361,Release notes: https://github.com/junit-team/junit4/blob/master/doc/ReleaseNotes4.12.md\r\n Upgrade to JUnit 4.12 >>> 1
1362,Today we have several code paths to merge top docs based on the number of\r\nsearch results returned from the shards. If there is a only a single shard\r\nholding any hits we go a different code path with quite some complexity while\r\nif there are more than one the code is basically duplicated to safe the\r\ncreation of a dense array of top docs which can be large if there are many results.\r\nThis commit removes the need of the dense array and in-turn the justification for\r\nthe optimization. This commit introduces a single code path to merge top docs. Simplify sorted top docs merging in SearchPhaseController >>> 1
1363,"I check source code https://github.com/elastic/elasticsearch/blob/v5.3.0/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L172\r\n\r\nI conclude current document contains bugs.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n Fix bulk queue_size and size in thread pool document >>> 1"
1364,Batch size example had a trailing comma\r\n\r\n Remove trailing comma from reindex example >>> 1
1365,This pull request aims to handle a list of `alias` in `GetAliasRequest` instead of a single one.\r\n\r\nCloses #23661  Handle multiple aliases in _cat/aliases api >>> 1
1366,"When terminating an executor service or a thread pool, we first shutdown. Then, we do a timed await termination. If the await termination fails because there are still tasks running, we then shutdown now. However, this method does not wait for actively executing tasks to terminate, so we should again wait for termination of these tasks before returning. This commit does that.\r\n Await termination after shutting down executors >>> 1"
1367,Relates #22278 Add unit tests for the missing aggregator >>> 1
1368,"While there are use-cases where a single-node is in production, there are also use-cases for starting a single-node that binds transport to an external interface where the node is not in production (for example, for testing the transport client against a node started in a Docker container). It's tricky to balance the desire to always enforce the bootstrap checks when a node might be in production with the need for the community to perform testing in situations that would trip the bootstrap checks. This commit enables some flexibility for these users. By setting the discovery type to ""single-node"", we disable the bootstrap checks independently of how transport is bound. While this sounds like a hole in the bootstrap checks, the bootstrap checks can already be avoided in the single-node use-case by binding only HTTP but not transport. For users that are genuinely in production on a single-node use-case with transport bound to an external use-case, they can set the system property ""es.enable.bootstrap.checks"" to force running the bootstrap checks. It would be a mistake for them not to do this.\r\n\r\nRelates #23585, relates #23595\r\n Disable bootstrap checks for single-node discovery >>> 1"
1369,"This commit renames the random ASCII helper methods in ESTestCase. This is because this method ultimately uses the random ASCII methods from randomized runner, but these methods actually only produce random strings generated from [a-zA-Z].\r\n Rename random ASCII helper methods >>> 1"
1370,We shuffle the keys before we parse our responses for the high level client so that we make sure we never rely on keys ordering. [TEST] make sure that fromXContent doesn't rely on keys ordering >>> 1
1371,"This commit puts all the classes in the repository-s3 plugin into a\r\nsingle package.  In addition to simplifying the plugin, it will make it\r\neasier to test as things that should be package private will not be\r\ndifficult to use inside tests alone. Collapse packages in repository-s3 >>> 1"
1372,"Windoes rest tests consistenly fail because the filesystem appears to be\r\nan order of magnitude slower than that of *nix, at least in the context\r\nof our rest tests. This commit overrides the suite timeout to 30 mins\r\nfor windows. From past failures, it appears this should be enough, as\r\nthe tests seem to fail when they are almost complete. The default suite\r\ntimeout for ESTestCase is 20 mins, so this leaves ample buffer for\r\nwindows shenanigans. Tests: Extend rest test timeout to 30 minutes for windows >>> 1"
1373,"This is important for some queries like `terms`, which are parsed differently\r\ndepending on whether we want to get a query or a filter. The `filter` and `significant_terms` aggregations should parse the `filter` as a filter, not a query. >>> 1"
1374,This pull request prevent from creating cluster name and index name containing `:` character\r\n\r\nClose #23892\r\n Bans ':' in cluster and index name >>> 0
1375,"The purpose of this validation is to make sure that the master doesn't step down\r\ndue to a change in master nodes, which also means that there is no way to revert\r\nan accidental change. Since we validate using the current cluster state (and\r\nnot the one from which the settings come from) we have to be careful and only\r\nvalidate if the local node is already a master. Doing so all the time causes\r\nsubtle issues. For example, a node that joins a cluster has no nodes in its\r\ncurrent cluster state. When it receives a cluster state from the master with\r\na dynamic minimum master nodes setting int it, we must make sure we don't reject it.\r\n\r\nCloses #23695\r\n ZenDiscovery - only validate min_master_nodes values if local node is master >>> 1"
1376,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Update client.asciidoc >>> 1"
1377,Shingle filters that produce shingles of different size and CJK filters that produce bigram AND unigram are problematic when\r\n we analyze the graph they produce. The position for each shingle size are not aligned so each position has at least two side paths.\r\n So in order to avoid paths explosion this change disables the graph analysis at query time for field analyzers that contain these filters\r\n with a problematic configuration.\r\n\r\nCloses #23918\r\n Disable graph analysis at query time for shingle and cjk filters producing tokens of different size >>> 1
1378,"`_field_stats` has evolved quite a lot to become a multi purpose API capable of retrieving the field capabilities and the min/max value for a field.\r\nIn the mean time a more focused API called `_field_caps` has been added, this enpoint is a good replacement for `_field_stats` since he can retrieve the field capabilities by just looking at the field mapping (no lookup in the index structures).\r\nAlso the recent improvement made to range queries makes the _field_stats API obsolete since this queries are now rewritten per shard based on the min/max found for the field.\r\nThis means that a range query that does not match any document in a shard can return quickly and can be cached efficiently.\r\nFor these reasons this change deprecates `_field_stats` endpoint. The deprecation should happen in 5.4 but we won't remove this API in 6.x yet which is why this PR is made directly to 6.0.\r\nThe rest tests have also been adapted to not throw an error while this change is backported to 5.4. Deprecate _field_stats endpoint >>> 1"
1379,"This PR adds the option for a plugin to specify extra directories containing\r\nnotices and licenses files to be incorporated into the overall notices file that\r\nis generated for the plugin.\r\n\r\nThis can be useful, for example, where the plugin has a non-Java dependency\r\nthat itself incorporates many 3rd party components.\r\n\r\nTo avoid changing many build files, licenses/notices are still picked up from\r\nthe licenses sub-directory of the plugin project if such a directory exists.  If\r\nextra directories are specified, however, they _must_ exist or the build will be\r\nfailed. Add the ability to include extra notices in a plugin's NOTICES file >>> 1"
1380,This commit collapses all the classes inside ec2 discovery to a single\r\npackage name. Collapse packages in ec2 discovery plugin >>> 1
1381,"`ESTestCase` has methods to shuffle xContent keys given a builder or a parser. Shuffling wasn't actually doing what was expected but rather reordering the keys in their natural ordering, hence the output was always the same at every run. Corrected that and added tests, also fixed a couple of tests that were affected by this fix. [TEST] fix shuffling of xContent keys >>> 1"
1382,They needed to be updated now that Painless is the default and\r\nthe non-sandboxed scripting languages are going away or gone.\r\n\r\nI dropped the entire section about customizing the classloader\r\nwhitelists. In master this barely does anything (exposes more\r\nthings to expressions) and in 5.x it isn't advisable anyway.\r\n Rewrite the scripting security docs >>> 1
1383,Upgrade 5.3 to the latest minor of Lucene 6.4: `6.4.2` upgrade to Lucene 6.4.2 >>> 1
1384,ClusterStateApplier cannot access cluster state because it's not yet applied. This commit will allow ClusterStateAppliers to unregister tasks if needed.\r\n Allow task to be unregistered by ClusterStateApplier >>> 0
1385,"Before now ranges where forbidden, because the percolator query itself could get cached and then the percolator queries with now ranges that should no longer match, incorrectly will continue to match.\r\nBy disabling caching when the `percolator` is being used, the percolator can now correctly support range queries with now based ranges.\r\n\r\nI think this is the right tradeoff. The percolator query is likely to not be the same between search requests and disabling range queries with now ranges really disabled people using the percolator for some of their use cases.\r\n\r\n Also fixed an issue that existed in the percolator fieldmapper, it was unable to find forbidden queries inside `dismax` queries.\r\n\r\n PR for #23859 Allowing range queries with now ranges inside percolator queries >>> 1"
1386,"This commit adds support for incremental top N reduction if the number of\r\nexpected shards in the search request is high enough. The changes here\r\nalso clean up more code in SearchPhaseController to make the separation\r\nbetween values that are the same on each search result and values that\r\nare per response. The reduced search phase result doesn't hold an arbitrary\r\nresult to obtain values like `from`, `size` or sort values which is now\r\ncleanly encapsulated. Introduce incremental reduction of TopDocs >>> 1"
1387,This commit preserves the response headers when creating an index and updating settings for an\r\nindex.\r\n\r\nCloses #23947 Preserve response headers when creating an index >>> 1
1388,"Currently, both the Amazon S3 client provides a retry mechanism, and the\r\nS3 blob store also attempts retries for failed read/write requests.\r\nBoth retry mechanisms are controlled by the\r\n`repositories.s3.max_retries` setting.  However, the S3 blob store retry\r\nmechanism is unnecessary because the Amazon S3 client provided by the\r\nAmazon SDK already handles retries (with exponential backoff) based on\r\nthe provided max retry configuration setting (defaults to 3) as long as\r\nthe request is retryable.  Hence, this commit removes the unneeded retry\r\nlogic in the S3 blob store and the S3OutputStream.\r\n\r\nCloses #22845 Removes the retry mechanism from the S3 blob store >>> 1"
1389,Eclipse seems to have trouble with CrudIT. It throws an NPE back\r\nto the user. I've filed a bug:\r\nhttps://bugs.eclipse.org/bugs/show_bug.cgi?id=514884\r\nbut for now we should work around it.\r\n Fix compilation in Eclipse again >>> 1
1390,"This change adds secure settings for access/secret keys and proxy\r\nusername/password to ec2 discovery.  It adds the new settings with the\r\nprefix `discovery.ec2`, copies other relevant ec2 client settings to the\r\nsame prefix, and deprecates all other settings (`cloud.aws.*` and\r\n`cloud.aws.ec2.*`).  Note that this is simpler than the client configs\r\nin repository-s3 because discovery is only initialized once for the\r\nentire node, so there is no reason to complicate the configuration with\r\nthe ability to have multiple sets of client settings.\r\n\r\nrelates #22475 Settings: Migrate ec2 discovery sensitive settings to elasticsearch keystore >>> 1"
1391,"The `getProperty` method is an internal method needed to run pipeline aggregations and retrieve info by path from the aggs tree. It is not needed in the `Aggregation` interface, which is  returned to users running aggregations from the transport client. The method is moved to the `InternalAggregation` class as that's where it belongs. Move getProperty method out of Aggregation interface >>> 1"
1392,"ParsedAggregation is the base Aggregation implementation for the high level client, which parses aggs responses into java objects. Add ParsedAggregation as base Aggregation impl for high level client >>> 0"
1393,There is a very small race that can cause requests to hang if nodes are concurrently\r\ndisconnected while a connection is fetched form the transport and the client handler\r\nis installed in the TransportService. The code relies on the fact that closed connections\r\neither fully work or fail and cause the requests to be ended / finished with an exception.\r\nThis is not necessarily true in local transport since it doesn't really maintain connections\r\nin the classical sense.\r\nThis change adds additional checking for disconnected nodes when the connection is used to ensure\r\nthe connection is never used with a disconnected transport.\r\n\r\nCloses #23942\r\n Fix possible hang in local transport when nodes get concurrently disconnected >>> 1
1394,"Shuffling xContent breaks the order of the highlighter fields in the\r\ninternal list if the highlighter doesn't use the array syntax. In other tests we\r\navoid shuffling this json level, but since this is done in the base test for\r\naggregations we should ensure the highlight builder uses the array syntax here.\r\n Tests: Ensure highlighter fields order in TopHitsTests >>> 1"
1395,This change restores the `rewrite to a match all query` that we used to apply on wildcard query `*` on  the `query_string` parser before https://github.com/elastic/elasticsearch/pull/23433. Restore special case for wilcard on _all query to rewrite to a match all query >>> 1
1396,"This commit adds an API to discover information like seed nodes,\r\nhttp addresses and connection status of a configured remote cluster.\r\n\r\nCloses #23925\r\n Add cross-cluster search remote cluster info API >>> 1"
1397,"The buffer limit should have been configurable already, but the factory constructor is package private so it is truly configurable only from the org.elasticsearch.client package. Also the HttpAsyncResponseConsumerFactory interface was package private, so it could only be implemented from the org.elasticsearch.client package.\r\n\r\nCloses #23958 Make buffer limit configurable in HeapBufferedConsumerFactory >>> 1"
1398,reindex_from_remote was using `TimeValue#toString` to generate the\r\nscroll timeout which is bad because that generates fractional\r\ntime values that are useful for people but bad for Elasticsearch\r\nwhich doesn't like to parse them. This switches it to using\r\n`TimeValue#getStringRep` which spits out whole time values.\r\n\r\nCloses to #23945\r\n\r\nMakes #23828 even more desirable.\r\n Fix throttled reindex_from_remote >>> 1
1399,First stab at adding parsing one of the single value aggregations from xContent.\r\nThis is WIP against our current feature branch.\r\n Adding ParsedCardinality >>> 1
1400,This is a reorg of the classes to simplify making them mostly\r\npackage protected. Collapse repository gcs classes into a single java package >>> 1
1401,"When spawning a native controller, for now we should skip hidden directories in the plugin folder. Future versions of Elasticsearch will not be lenient here.\r\n\r\n Skip hidden files when spawning >>> 1"
1402,"This commit removes the ""legacy"" feature of secure settings, which setup\r\na parallel setting that was a fallback in the insecure\r\nelasticsearch.yml. This was previously used to allow the new secure\r\nsetting name to be that of the old setting name, but is now not in use\r\ndue to other refactorings. It is much cleaner to just have all secure\r\nsettings use new setting names. If in the future we want to reuse the\r\nprevious setting name, once support for the insecure settings have been\r\nremoved, we can then rename the secure setting.  This also adds a test\r\nfor the behavior.\r\n Settings: Disallow secure setting to exist in normal settings >>> 1"
1403,"This commit removes support for s3 signer type in 6.0, and adds a note\r\nto the migration guide.\r\n\r\ncloses #22599 AWS Plugins: Remove signer type setting >>> 1"
1404,This is an attempt to parse InternalHDRPercentilesRanks and InternalTDigestPercentilesRanks aggregation.\r\n Add parsing for percentiles ranks >>> 1
1405,We have both endpoint and region settings. Region was removed from s3 to\r\nsimplify configuration. This is the ec2 equivalent.\r\n\r\ncloses #22758 Discovery EC2: Remove region setting >>> 1
1406,This commit upgrades the Log4j dependencies from version 2.7 to version 2.8.2. This release includes a fix for a case where Log4j could lose exceptions in the presence of a security manager. Upgrade to Log4j 2.8.2 >>> 1
1407,"The refactoring in #23711 hardcoded version logic for replica to assume monotonic versions. Sadly that's wrong for `FORCE` and `VERSION_GTE`. Instead we should use the methods in VersionType to detect conflicts.\r\n\r\nNote - once replicas use sequence numbers for out of order delivery, this logic goes away.\r\n\r\n Engine: version logic on replicas should not be hard coded >>> 1"
1408,"The `getProperty` method is an internal method needed to run pipeline aggregations and retrieve info by path from the aggs tree. It is not needed in the `Aggregations` interface, which is returned to users running aggregations from the transport client. Furthermore, the method is currenty unused by pipeline aggs too, as only InternalAggregation#getProperty is used. It can then be removed Remove getProperty method from Aggregations interface and impl >>> 1"
1409,"These will be shared between internal objects and objects exposed through high level REST client, so they should be moved from internal classes. Move aggs CommonFields and TYPED_KEYS_DELIMITER from InternalAggregation to Aggregation >>> 1"
1410,"The `getProperty` method is an internal method needed to run pipeline aggregations and retrieve info by path from the aggs tree. It is not needed in the `MultiBucketsAggregation.Bucket` interface, which is returned to users running aggregations from the transport client. The method is moved to the `InternalMultiBucketAggregation` class as that's where it belongs. Move getProperty method out of MultiBucketsAggregation.Bucket interface >>> 1"
1411,This reverts the line limit change in #23623 - this PR doesn't touch the suppression file since we are moving towards automatic code formatting which makes it mainly obsolete.  Move back to 140chars line length >>> 1
1412,Now that we have incremental reduce functions for topN and aggregations\r\nwe can set the default for `action.search.shard_count.limit` to unlimited.\r\nThis still allows users to restrict these settings while by default we executed\r\nacross all shards matching the search requests index pattern. Set shard count limit to unlimited >>> 1
1413,"When a primary relocation completes while there are ongoing replica recoveries, the recoveries for these replicas need to be restarted (as a new primary is in charge of replicating changes). Before this PR, the need for a recovery restart was detected by the data nodes that had the replicas, by checking on each cluster state update if the recovery process had completed before the recovery source changed. That code had a race, however, which could lead to a not-fully recovered shard exposing itself as started (see #23904).\r\n\r\nThis PR takes a different approach: When the primary relocation completes and the master updates the cluster state to move the primary shard from relocating to started, it will reinitialize all initializing replica shards, by giving them a fresh allocation id. Data nodes that have the replica shard will simply detect that the allocation id changed and restart the recovery process (instead of trying to determine the need to restart based on ongoing recoveries).\r\n\r\nNote: Removal of the code in `IndicesClusterStateService` that checks whether the recovery source has changed will not be backported to the 5.x branch. This ensures backward compatibility for the situation where the master node is older and does not have the code changes that have been introduced in this PR.\r\n\r\nCloses #23904 Trigger replica recovery restarts by master when primary relocation completes >>> 1"
1414,This commit adds support for replacing a stashed value within a header of a REST test. This is\r\nuseful for requests that may want to use a value previously obtained within a header.\r\n Test: add support for replacing stashed values within headers of REST tests >>> 1
1415,"PR for #24009 \r\n\r\nBefore this change if `NestedChildrenQuery` were to be cached it could lead to memory leak, because this query keeps a reference to the IndexReader. The chance that it would be cached is low, because this query is different for each search request and search hit it is trying to fetch inner hits for. Replace NestedChildrenQuery with ParentChildrenBlockJoinQuery >>> 1"
1416,Closes #21887 Removes version 2.x constants from Version >>> 1
1417,"Today when a flush is performed, the translog is committed and if there are no outstanding views, only the current translog generation is preserved. Yet for the purpose of sequence numbers, we need stronger guarantees than this. This commit migrates the preservation of translog generations to keep the minimum generation that would be needed to recover after the local checkpoint.\r\n\r\nRelates #10708\r\n\r\n Preserve multiple translog generations >>> 1"
1418,"We are still carrying some legacy code that deals with lucene indices\r\nthat don't have checksums. Yet, we do not support these indices\r\nfor a while now, in fact since version 5.0 such an index is not supported\r\nanymore. This commit removes all the special handling and leniency involved.\r\n Remote support for lucene versions without checksums >>> 1"
1419,When parsing aggregations and sub-aggregations we encounter cases where the\r\naggregation name is written to the response as key for a new nested aggregation\r\nobject on the same level that we want to parse other fields. Currently we cannot\r\nuse ObjectParser in this situations because it requires some ParseField for\r\nmatching the field name.\r\n\r\nThis adds a new `declareUnknownFieldParser()` method to ObjectParser that can be\r\nused to declare one specific ContextParser that gets called if no other\r\nFieldParsers that have been declared match. ObjectParser: add method to parse inner objects with names matching a given predicate >>> 0
1420,This commit adds a note to the migration docs that duplicate keys in the configuration file are no longer permitted.\r\n\r\nCloses #24006\r\n Add note to docs on duplicate keys in config >>> 1
1421,"Some systems like GCE rely on a plaintext file containing credentials.\r\nRather than extract the information out of that credentials file and\r\nstore each peace individually in the keystore, it is cleaner to just\r\nstore the entire file.\r\n\r\nThis commit adds support to the keystore wrapper for secure file\r\nsettings. These are settings that contain an entire file that would\r\nnormally be stored on the local filesystem. Retrieving the file returns\r\nan input stream to the file contents. This also adds a `add-file`\r\ncommand to the keystore cli. Add secure file setting to keystore >>> 1"
1422,"The `AsyncBulkByScrollActionTests` were brittle because they used the\r\ncurrent time. That was a mistake. This removes the current time from\r\nthe test, instead adding it to the parameters passed in to the\r\nappropriate methods. This means that we take the current time slightly\r\nearlier in all cases, but that shouldn't make a difference.\r\n\r\nCloses #24005\r\n\r\nExample failure:\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+nfs/161/consoleFull\r\n Remove real time from tests >>> 1"
1423,Adds a packaging test that would have detected #23981. Add package test for bad data.path setting from #23981 >>> 1
1424,"This commit removes passing the repository metadata object through to\r\ns3 client creation. It is not needed, and in fact in tests was confusing\r\nbecause you could create the metadata but have it contain different\r\nsettings than were passed in as repository settings. Repository S3: Simplify client method >>> 1"
1425,"Indices wildcards were resolved against all indices and aliases. And if an alias matched, then all indices with this alias were returned as matching.\r\n\r\nIn other words\r\n```\r\nPOST /_aliases\r\n{\r\n    ""actions"" : [\r\n        { ""add"" : { ""index"" : ""foo_foo"", ""alias"" : ""foo"" } },\r\n        { ""add"" : { ""index"" : ""bar_bar"", ""alias"" : ""foo"" } }\r\n    ]\r\n}\r\n```\r\n```\r\nPOST /_aliases \r\n{\r\n  ""actions"": [\r\n    {\r\n      ""remove"": {\r\n        ""index"": ""foo*"",\r\n        ""alias"": ""foo""\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\nwill actually return both indices `foo_foo` and `bar_bar` as the common alias `foo` matched the wildcard. Which led to deleting the alias for both indices.\r\n\r\nRelated to #23960\r\n\r\nRelates to #10106 \r\n Wrong behavior deleting alias >>> 1"
1426,The bwc checkout for backcompat tests currently always tries to fetch\r\nthe latest from the upstream remote. This change makes fetching from\r\nupstream conditional on not running an offline build. Build: Support offline build of bwc zip >>> 1
1427,"The ""category"" in context suggester could be String, Number or Boolean. However with the changes in version 5 this is failing and only accepting String. This will have problem for existing users of Elasticsearch if they choose to migrate to higher version; as their existing Mapping and query will fail as mentioned in a bug #22358\r\n\r\nThis PR fixes the above mentioned issue and allows user to migrate seamlessly.\r\n\r\nNote: Providing the NULL value for a category at index and query time will throw an exception.\r\n\r\nCloses #22358\r\n Allow different data types for category in Context suggester >>> 1"
1428,Backport of #24016\r\n Replace `NestedChildrenQuery` with `ParentChildrenBlockJoinQuery` >>> 1
1429,All our actions that are invoked from rest actions have corresponding\r\ntransport actions. This adds the transport action for RestRemoteClusterInfoAction\r\nfor consistency.\r\n\r\nRelates to #23969 Add a dedicated TransportRemoteInfoAction for consistency >>> 1
1430,"This is related to #23893. This commit allows users to use wilcards for\r\ncluster names when executing a cross cluster search.\r\n\r\nSo instead of defining every cluster such as:\r\n\r\n`GET one:*,two:*,three:*/_search`\r\n\r\nA user could just search:\r\n\r\n`GET *:*/_search`\r\n\r\nAs "":"" characters are currently allowed in index names, if the text\r\nup to the first "":"" does not match a defined cluster name, the entire \r\nstring is treated as an index name.\r\n\r\nCloses #23893 Wildcard cluster names for cross cluster search >>> 1"
1431,This removes the ability to configure Elasticsearch to use custom username\r\nand/or group when it is run.\r\n\r\nResolves #23848\r\n Remove customization of ES_USER and ES_GROUP >>> 1
1432,Resolves #22024 Remove shadow replicas >>> 1
1433,"The S3 repostiory has many levels of settings it looks at to create a\r\nrepository, and these settings were read at repository creation time.\r\nThis meant secure settings like access and secret keys had to be\r\navailable after node construction. This change makes setting loading for\r\nevery except repository level settings eager, so that secure settings\r\ncan be stashed, and the keystore can once again be closed after\r\nbootstrapping the node is complete.\r\n S3 Repository: Eagerly load static settings >>> 1"
1434,"Drops any mention of non-sandboxed scripting languages other than a\r\nbrief ""we don't support them and we shouldn't because A and B""\r\nstatement.\r\n\r\nRelates to #23930\r\n Update scripts/security docs for sandboxed world >>> 1"
1435,"In Elasticsearch 5.3.0 a bug was introduced in the merging of default settings when the target setting existed as an array. This arose due to the fact that when a target setting is an array, the setting key is broken into key.0, key.1, ..., key.n, one for each element of the array. When settings are replaced by default.key, we are looking for the target key but not the target key.0. This leads to key, and key.0, ..., key.n being present in the constructed settings object. When this concerns path.data, we end up in a situation where path.data.0, ..., path.data.n are configured and path.data is configured too. Since our packaging sets default.path.data, users that configure multiple data paths vian an array and use the packaging are subject to having shards land in default.path.data when that is very likely not what they intended.\r\n\r\nThis commit is an attempt to rectify this situation. First, we fix the merging of default settings when the target setting exists an array. We have to hold on to default.path.data though so that we can detect its presence. For this, we elevate default.path.data to an actual setting and give it special treatment when merging settings.\r\n\r\nAfter we have done this, we take a lock on all configured data directories in path.data, and default.path.data too. We look for the presence of indices in default.path.data and if there are any, we fail the node. \r\n\r\nCloses #23981 Correct handling of default settings and path.data >>> 0"
1436,"This change makes the build info initialization only try to load a jar\r\nmanifest if it is the elasticsearch jar. Anything else (eg a repackaged\r\nES for use of transport client in an uber jar) will contain ""Unknown""\r\nfor the build info as it does for tests currently.\r\n\r\nfixes #21955 Restrict build info loading to ES jar, not any jar >>> 1"
1437,Today Elasticsearch and other CLI tools that rely on environment aware command leniently accept duplicate settings with the last one winning. This commit removes this leniency.\r\n\r\n Reject duplicate settings on the command line >>> 1
1438,nan Cleanup outdated comments for fixing up pom dependencies >>> 1
1439,"This commit adds a deprecation warning when elasticsearch.yml or\r\nelasticsearch.json is read during startup. It also fixes all docs\r\nreferences, tests, and renames the default file that comes in our packages.\r\n\r\nrelates #19391 Deprecate settings in .yml and .json >>> 1"
1440,"Internal indexing requests in Elasticsearch may be processed out of order and repeatedly. This is important during recovery and due to concurrency in replicating requests between primary and replicas. As such, a replica/recovering shard needs to be able to identify that an incoming request contains information that is old and thus need not be processed. The current logic is based on external version. This is sadly not sufficient. This PR moves the logic to rely on sequences numbers and primary terms which give the semantics we need.\r\n\r\nRelates to #10708 Use sequence numbers to identify out of order delivery in replicas & recovery >>> 1"
1441,This commit collapses the `SyncBulkRequestHandler` and\r\n`AsyncBulkRequestHandler` into a single `BulkRequestHandler`. The new\r\nhandler executes a bulk request and awaits for the completion if the\r\n`BulkProcessor` was configured with a concurrentRequests setting of 0.\r\nOtherwise the execution happens asynchronously.\r\n\r\nAs part of this change the `Retry` class has been refactored.\r\n`withSyncBackoff` and `withAsyncBackoff` have been replaced with two\r\nversions of `withBackoff`. One method takes a listener that will be\r\ncalled on completion. The other method returns a future that will been\r\ncomplete on request completion. Simplify BulkProcessor handling and retry logic >>> 1
1442,"When a shard is marked as relocating in the cluster state but the node that is to be expected to have the shard does not have the corresponding in-memory index structures loaded, then the node does not notify the master of this inconsistency. The cluster can end up in this situation for example when using `discovery.zen.no_master_block: all`, which makes the data nodes unload in-memory index structures when no master is available. As a result, all indexing / search activity to the supposedly relocating shard will fail, requiring manual intervention from an operator to fix the issue by restarting the data node with the relocating shard.\r\n\r\nIn ES 5.0+, this issue is fixed by elastic/elasticsearch#17270 Fail shard marked as relocating but missing in-memory index structures >>> 1"
1443,"This change adds an index setting to define how the documents should be sorted inside each Segment.\r\nIt allows any numeric, date, boolean or keyword field inside a mapping to be used to sort the index on disk.\r\nIt is not allowed to use a `nested` fields inside an index that defines an index sorting since `nested` fields relies on the original sort of the index.\r\nThis change does not add early termination capabilities in the search layer. This will be added in a follow up.\r\n\r\nRelates #6720 Enable index-time sorting >>> 1"
1444,This is a bug fix upgrade and removes support for very old JDK versions (<1.2).\r\nMore information here: https://commons.apache.org/proper/commons-logging/changes-report.html\r\n\r\n Upgrade apache commons-logging to current version (1.1.3 -> 1.2) >>> 0
1445,Because 5.4 is feature-frozen. Bump version to 5.5. >>> 1
1446,"JDK9 has made changes to LambdaMetaFactory that prevent Painless lambdas from being able to do proper adaptation of types from the interface method to the delegate method even using a bridge.  In particular boxed types are no longer widened appropriately, and Object can no longer be adapted to primitive types.  Though, the former is likely a bug based on the spec, whether the latter is allowed according to spec is somewhat ambiguous.  \r\n\r\nThis fix adds a LambdaBootstrap class that replaces LambdaMetaFactory in Painless using the same methodology that LambdaMetaFactory was based on (http://cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html).  This allows us to do whatever adaptations we believe are necessary without having to worry about future changes/bugs in LambdaMetaFactory.\r\n\r\nThis is related to #23473. Fix Painless Lambdas for Java 9 >>> 1"
1447,"TaskInfo is stored as a part of TaskResult and therefore can be read by nodes with older versions during rolling upgrades. If we add any additional information to TaskInfo (for example, to implement #23250), nodes with an older version should be able to ignore new elements, otherwise they will not be able to read TaskResults stored by newer nodes.\r\n Task Management: Make TaskInfo parsing forwards compatible >>> 1"
1448,"I wasn't sure what to do with the existing `NestedAggregatorTests` test, which is based on `ESSingleNodeTestCase`. Partly because I wasn't really sure what it was testing...so I just left it alone :)\r\n\r\nSorry for the delay on getting this done!\r\n\r\nRelates to #22278 Add unit tests for NestedAggregator >>> 1"
1449,This is required in master now that #24071 is in or else we fail during BWC testing because the 5.x branch contains 5.5 but the build thinks it should contain 5.4. Add version constant for 5.5 >>> 1
1450,It can easily happen that we touch a logger before logging is configured due to chains of static intializers and other such scenarios. This commit adds detection for this mechanism that will fail startup if we touch a logger before logging is configured. This is a bug that will cause builds to fail.\r\n Detect using logging before configuration >>> 1
1451,"Added 'OBJECT_ARRAY_BOOLEAN_OR_STRING' to the ObjectParser.ValueType enum to be in line with the current configuration for source filtering in the request body. Updated InnerHitBuilder to use the new enum type. InnerHitBuilder now supports Object, Array, String or Boolean values for _source filtering\r\n\r\nCloses https://github.com/elastic/elasticsearch/issues/24063 Fix Source filtering in new field collapsing feature >>> 1"
1452,"Script below states painless as language not groovy\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n--> Fix scripting documentation (groovy -> painless) >>> 1"
1453,"The JVM caches `Integer` objects. This is known. A test in Painless\r\nwas relying on the JVM not caching the particular integer `1000`.\r\nIn older versions of java only -127, 128 were cached. That isn't\r\nalways true anymore. It turns out that when you provide\r\n`-XX:+AggressiveOpts` the JVM *does* cache `1000`, causing the\r\ntest to fail when that is specified.\r\n\r\nThis replaces `1000` with a randomly selected integer that we test\r\nto make sure *isn't* cached by the JVM. *Hopefully* this test is\r\ngood enough. It relies on the caching not changing in between when\r\nwe check that the value isn't cached and when we run the painless\r\ncode. The cache now is a simple array but there is nothing\r\npreventing it from changing. If it does change in a way that thwarts\r\nthis test then the test fail fail again. At least when that happens\r\nthe next person can see the comment about how it is important\r\nthat the integer isn't cached and can follow that line of inquiry.\r\n\r\nCloses #24041\r\n Harden painless test against ""fun"" caching >>> 1"
1454,We added some leniency to the spawner in 5.4.0 with the intention to immediately remove it in 5.5.0. This commit removes that leniency.\r\n\r\nRelates #23980\r\n Remove hidden file leniency from spawner >>> 1
1455,"This new version of jna is rebuilt from the official release of jna, but\r\nwith native libs linked against older glibc in order to support all\r\nplatforms elasticsearch supports.\r\n\r\ncloses #23640 Build: Switch jna dependency to an elastic version >>> 1"
1456,"In Elasticsearch 5.3.0 a bug was introduced in the merging of default settings when the target setting existed as an array. This arose due to the fact that when a target setting is an array, the setting key is broken into key.0, key.1, ..., key.n, one for each element of the array. When settings are replaced by default.key, we are looking for the target key but not the target key.0. This leads to key, and key.0, ..., key.n being present in the constructed settings object. This commit addresses two issues here. The first is that we fix the merging of the keys so that when we try to merge default.key, we also check for the presence of the flattened keys. The second is that when we try to get a setting value as an array from a settings object, we check whether or not the backing map contains the top-level key as well as the flattened keys. This latter check would have caught the first bug. For kicks, we add some tests.\r\n\r\nRelates #24052, relates #23981 Correct handling of default and array settings >>> 1"
1457,"This PR adds a new thread pool type: `fixed_auto_queue_size`. This thread pool\r\nbehaves like a regular `fixed` threadpool, except that every\r\n`auto_queue_frame_size` operations (default: 10,000) in the thread pool,\r\n[Little's Law](https://en.wikipedia.org/wiki/Little's_law) is calculated and\r\nused to adjust the pool's `queue_size` either up or down by 25. A minimum and\r\nmaximum is taken into account also.\r\n\r\nThe `SEARCH` threadpool is changed to use this new type of thread pool.\r\n\r\nRelates to #3890 Automatically adjust search threadpool queue_size >>> 1"
1458,Elasticsearch runs as user elasticsearch with uid:gid 1000:1000 inside\r\nthe Docker container. Clarify that bind mounted local directories or files \r\nneed to be accessible by this user.\r\n\r\nCloses #24058  Clarify elasticsearch user uid:gid mapping in Docker docs (#24058) >>> 1
1459,"Today Elasticsearch allows default settings to be used only if the actual setting is not set. These settings are trappy, and the complexity invites bugs. This commit removes support for default settings with the exception of default.path.data, default.path.conf, and default.path.logs which are maintainted to support packaging. A follow-up will remove support for these as well.\r\n\r\nRelates #23981, relates #24052, relates #24074\r\n\r\n Remove support for default settings >>> 1"
1460,"This was accidentally being added to packagingTest, which then had two\r\nrepro lines. Test: Fix repro line for platformTest >>> 1"
1461,"In Elasticsearch 5.3.0 a bug was introduced in the merging of default settings when the target setting existed as an array. When this bug concerns path.data and default.path.data, we ended up in a situation where the paths specified in both settings would be used to write index data. Since our packaging sets default.path.data, users that configure multiple data paths via an array and use the packaging are subject to having shards land in paths in default.path.data when that is very likely not what they intended.\r\n\r\nThis commit is an attempt to rectify this situation. If path.data and default.path.data are configured, we check for the presence of indices there. If we find any, we log messages explaining the situation and fail the node.\r\n\r\nCloses #23981, supersedes #24052, relates #24074, relates #24093 Detect remnants of path.data/default.path.data bug >>> 1"
1462,"Similar to #23973 this adds parsing from xContent for the Min, Max, Avg, Sum and ValueCount aggregation.\r\nI tried to pull out common functionality to ParsedNumericMetricsAggregation.SingleValue. Since InternalCardinality \r\nand InternalValueCount store a long value instead of a double, parsing and getters/setters for the value are slightly \r\ndifferent so I didn't include their parsed counterparts under ParsedNumericMetricsAggregation.SingleValue for now.\r\nAlso included one case where I tried pulling out common xContent rendering into the shared interface (in Sum) for discussion.\r\nI'm unsure if I like that one but it would help reduce some code duplication.\r\n\r\nThis PR is WIP against a feature branch. Add parsing to some single value aggregations >>> 1"
1463,"When building headers for a REST response, we de-duplicate the warning headers based on the actual warning value. The current implementation of this uses a capturing regular expression that is prone to excessive backtracking. In cases a request involves a large number of warnings, this extraction can be a severe performance penalty. An example where this can arise is a bulk indexing request that utilizes a deprecated feature (e.g., using deprecated forms of boolean values). This commit is an attempt to address this performance regression. We already know the format of the warning header, so we do not need to use a regular expression to parse it but rather can parse it by hand to extract the warning value. This gains back the vast majority of the performance lost due to the usage of a deprecated feature. There is still a performance loss due to logging the deprecation message but we do not address that concern in this commit.\r\n\r\nCloses #24018 Improve performance of extracting warning value >>> 1"
1464,"and made constructors of GeoCentroidAggregator, GeoCentroidAggregatorFactory and InternalGeoCentroid package protected. Added unit test for GeoCentroidAggregator >>> 1"
1465,Related to #22278 [Test] Add unit tests for InternalTDigestPercentilesTests >>> 1
1466,"The percolator doesn't close the IndexReader of the memory index any more.\r\nPrior to 2.x the percolator had its own SearchContext (PercolatorContext) that did this,\r\nbut that was removed when the percolator was refactored as part of the 5.0 release.\r\n\r\nI think an alternative way to fix this is to let percolator not use the bitset and fielddata caches,\r\nthat way we prevent the memory leak.\r\n\r\nAdding a WIP label to this as I'm not happy with the current test. It is not easy to test that we don't use the bitset or fielddata cache for the percolator, because non percolator operations may use these caches, which is valid.\r\n\r\nPR for #24108 Fix memory leak when percolator uses bitset or field data cache >>> 1"
1467,"When indexing a document via the bulk API where IDs can be explicitly specified, we currently accept an empty ID. This is problematic because such a document can not be obtained via the get API. Instead, we should rejected these requets as accepting them could be a dangerous form of leniency. Additionally, we already have a way of specifying auto-generated IDs and that is to not explicitly specify an ID so we do not need a second way. This commit the individual requests where ID is specified but empty.\r\n\r\nCloses #24116 Reject empty IDs >>> 1"
1468,"When preparing the final settings in the environment, we unconditionally set path.data even if path.data was not explicitly set. This confounds detection for whether or not path.data was explicitly set, and this is trappy. This commit adds logic to only set path.data in the final settings if path.data was explicitly set, and provides a test case that fails without this logic.\r\n\r\nRelates #24099\r\n Do not set path.data in environment if not set >>> 1"
1469,"Highlighting supports an option `fragmenter` which is supported by the Plain fragmenter but the reference documentation does not mention it.\r\n\r\nReference documentation for the same was added by collating information from the following sources:\r\n- Expose fragmenter option for plain highlighter #2465\r\n- Lucene docs for [Fragmenter](https://lucene.apache.org/core/3_0_3/api/contrib-highlighter/org/apache/lucene/search/highlight/Fragmenter.html), [NullFragmenter](https://lucene.apache.org/core/6_4_0/highlighter/org/apache/lucene/search/highlight/NullFragmenter.html), [SimpleFragmenter](https://lucene.apache.org/core/6_4_0/highlighter/org/apache/lucene/search/highlight/SimpleFragmenter.html) and [SpansHighlighter](https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/spans/Spans.html)\r\n- Elasticsearch source code for [PlainHighlighter](/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/fetch/subphase/highlight/PlainHighlighter.java#L77-L89)\r\n- Discussion in the issue #23736\r\n\r\nCloses #23736 Update reference docs for Highlighter fragmenter >>> 1"
1470,"This commit removes all deprecated settings which start with\r\n`cloud.aws`, `repositories.s3` and repository level client settings. S3 Repository: Cleanup deprecated settings >>> 1"
1471,"The current shard recovery logic consists of three main phases:\r\n\r\n1) File transfer (for non-sequence based recoveries)\r\n2) Operation replay \r\n3) If needed primary hand off\r\n\r\nThese phase are currently executed by a single recovery flow. The [Sequence Numbers work](https://github.com/elastic/elasticsearch/issues/10708) requires introducing a live primary-replica sync which is in essence the same operation replay that the current seq# based recovery uses. In order to be able to reuse the relevant recovery logic, this PR breaks the current monolithic approach into 3 different recovery types that can be chained together to achieve what's needed:\r\n\r\n1) File & Ops recovery - copies over files and replays ops from the translog (based on the pre-sequence number logic)\r\n2) Ops Recovery - recovers operations based on the requested sequence numbers range (this is the part that will be reused)\r\n3) Primary Hand Off - takes care of the intricate logic that is needed when relocating a primary\r\n\r\nAs you can see, first type is still a combination of the file and ops phases. Sadly this is needed to guarantee that all ops that are indexed during the file copying will be kept in the translog and can be replayed. In the future we may be able to solve this in a different way. Also, there is the future option of introducing a file only type which will be used first, followed by an extra (much smaller) file resync plus ops replay.\r\n\r\nThe code still some no commits and needs polishing. I'm also not yet happy with the state of unit tests and there are some issues in the BWC layer. However, I felt it is good to get some feedback on the approach. It's a monstrous PR so if anyone has an idea on how to split it up in smaller pieces please do share. [WIP] Recovery break into sub phases >>> 0"
1472,This includes a link to the Wikipedia page explaining what a centroid\r\nis.\r\n\r\nCloses #24140\r\n\r\n Update reference docs for geocentroid aggregation. >>> 1
1473,"After splitting integ tests into cluster configuration and the test\r\nrunner task, we still have dependencies of the test runner added as deps\r\nof the cluster. This commit adds dependencies directly to the cluster,\r\nso that the runner can have other dependencies independent of what is\r\nneeded for the cluster.\r\n Tests: Move cluster dependencies from runner to cluster >>> 1"
1474,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Misspelled letter >>> 0"
1475,"Minor modifications for fluid reading.\r\nNote:\r\n...then it might have index statistics that are sufficiently different from the replica (which still have plenty of deleted documents) so that scores are different too...\r\nI had to read over this several times to identify what was being compared.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n Update recipes.asciidoc - Trying to clarify >>> 0"
1476,These were leftover from the removal of the signer type setting in #23984 S3 Repository: Remove unused files >>> 1
1477,"Prerequisites: In our company we need specific scoring. We have a lot of suppliers with huge amount of goods. Some suppliers are ""better"" than others. But if supplier rank affects score, goods from top ranked supplier supersede goods from other suppliers. What we really want to get is:\r\n```\r\nproduct1 (supplier1)\r\nproduct2 (supplier2)\r\nproduct3 (supplier3)\r\nproduct4 (supplier1)\r\nproduct5 (supplier2)\r\n...\r\n```\r\n\r\nThe most suitable and simple way to do that is to write specific rescorer (the only requirement that all products from specific supplier should be located on the same shard). Thus we decided to make rescorers pluggable. There were an [issue](#17331) and a [discussion](https://discuss.elastic.co/t/how-to-plug-in-alternative-rescorer/11555) for this feature.\r\n Pluggable rescorers >>> 0"
1478,"This change simplifies how the rest test runner finds test files and\r\nremoves all leniency.  Previously multiple prefixes and suffixes would\r\nbe tried, and tests could exist inside or outside of the classpath,\r\nalthough outside of the classpath never quite worked. Now only classpath\r\ntests are supported, and only one resource prefix is supported,\r\n`/rest-api-spec/test`.\r\n\r\ncloses #20240 Tests: Clean up rest test file handling >>> 1"
1479,"Clarify documentation of json processor with a config, a sample doc, and the processed result ingest-node.asciidoc - Clarify json processor >>> 1"
1480,[TEST] ensures REST tests wait for cluster state updates to finish\r\nprocessing before moving to the next test Wait for cluster to become quiescent between REST tests >>> 1
1481,"An important use case for our users is deploying our clients inside of applications containers like Wildfly. Sometimes, we make changes that unintentionally break this use case. We need to know before we ship a release that we have broken such use cases. As Wildfly is one of the bigger application containers, this commit starts by adding an integration test that deploys an application using the transport client to Wildfly and ensures that all is well. Future work can add similar integration tests for the low-level and high-level REST clients.\r\n\r\nCloses #24050\r\n Add Wildfly integration test >>> 1"
1482,"Most of these settings should always be pulled from the repository\r\nsettings. A couple were leftover that should be moved to client\r\nsettings. The path style access setting should be removed altogether.\r\nThis commit adds deprecations for all of these existing settings, as\r\nwell as adding new client specific settings for max retries and\r\nthrottling.\r\n\r\nrelates #24143 S3 Repository: Deprecate remaining `repositories.s3.*` settings >>> 1"
1483,"This commit merges the `Percentile` interface with the `InternalPercentile`\r\nclass, so we don't need to maintain both. Merge Percentile interface with InternalPercentile class >>> 1"
1484,"The `maxUnsafeAutoIdTimestamp` timestamp is a safety marker guaranteeing that no retried-indexing operation with a higher auto gen id timestamp was process by the engine. This allows us to safely process documents without checking if they were seen before.\r\n\r\nCurrently this property is maintained in memory and is handed off from the primary to any replica during the recovery process.\r\n\r\nThis PR takes a more natural approach and stores it in the lucene commit, using the same semantics (no retry op with a higher time stamp is part of this commit). This means that the knowledge is transferred during the file copy and also means that we don't need to worry about crazy situations where an original append only request arrives at the engine after a retry was processed *and* the engine was restarted.\r\n\r\nOnce this is in 5.x, I will submit a follow up PR to remove this part of the recovery logic. Engine: store maxUnsafeAutoIdTimestamp in commit >>> 1"
1485,"Code refactoring that extracts the executor functionality from `ClusterService` and makes it a reusable component that can be tested in isolation. This will allow this component to be used in a follow-up separating `ClusterService` into `MasterService` and `ClusterApplierService`, which will both make use of the executor components. Extract batch executor out of cluster service >>> 1"
1486,"Some aggregations (like `Min`, `Max` etc) use a wrong `DocValueFormat` in\r\ntests (like `IP` or `GeoHash`). We should not test aggregations that expect\r\na numeric value with a DocValueFormat like IP. Such wrong DocValueFormat\r\ncan also prevent the aggregation to be rendered as ToXContent, and this\r\nwill be an issue for the High Level Rest Client tests which expect to be\r\nable to parse back aggregations. [Test] Use appropriate DocValueFormats in Aggregations tests >>> 1"
1487,"In some cases, there is sensitive data that may need to be serialized as part of a request and\r\nSecureString is a good fit for char based data. This commit adds serialization code to SecureString\r\nso that it can be used with StreamInput/StreamOutput without needing custom serialization code in\r\nevery place that it is used. SecureString implements Writeable for serialization >>> 0"
1488,"The `keyed` parameter is supported by the following aggregations:\r\n- [Percentiles Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html)\r\n- [Percentile Ranks Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-rank-aggregation.html)\r\n- [Date Histogram Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html)\r\n- [Date Range Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-daterange-aggregation.html)\r\n- [Geo Distance Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-geodistance-aggregation.html)\r\n- [Histogram Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html)\r\n- [IP Range Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-iprange-aggregation.html)\r\n- [Range Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-range-aggregation.html)\r\n\r\nHowever, in the reference guide, the `keyed` parameter is documented only for Range Aggregation. \r\nThis PR adds the documentations for the rest.<br>\r\nCloses #23731.  Update aggs reference documentation for 'keyed' options >>> 1"
1489,Today when we merge hits we have a hard check to prevent AIOOB exceptions\r\nthat simply skips an expected search hit. This can only happen if there is a\r\nbug in the code which should be turned into a hard exception or an assertion\r\ntriggered. This change adds an assertion an removes the lenient check for the\r\nfetched hits.\r\n Remove leniency when merging fetched hits in a search response phase >>> 1
1490,"Note: pull request against feature branch\r\n\r\nNow the Percentile interface has been merged with the InternalPercentile class in core (#24154) the AbstractParsedPercentiles should use it.\r\n    \r\nThis commit also changes InternalPercentilesRanksTestCase so that it now tests the iterator obtained from parsed percentiles ranks aggregations.\r\n    \r\nAdding this new test raised an issue in the iterators where key and value are ""swapped"" in internal implementations when building the iterators (see InternalTDigestPercentileRanks.Iter constructor that accepts the `keys` as the first parameter named `values`, each key being mapped to the `value` field of Percentile class). This is because percentiles ranks aggs inverts percentiles/values compared to the percentiles aggs.\r\n AbstractParsedPercentiles should use Percentile class >>> 1"
1491,"Ubuntu 12.04 will be EOL on April 28, 2017, I think we can remove it for all 5.x and master packaging tests. Remove Ubuntu 12.04 >>> 1"
1492,Similar to #24085 this adds parsing for InternalSimpleValue and InternalDerivative. This will be needed for \r\nthe high level rest client. PR is against the current feature branch for aggregation parsing. Add parsing for InternalSimpleValue and InternalDerivative >>> 1
1493,Related to #22278  [Test] Add unit tests for InternalHDRPercentilesTests >>> 1
1494,"`script_stack` is super useful when debugging Painless scripts\r\nbecause it skips all the ""weird"" stuff involved that obfuscates\r\nwhere the actual error is. It skips Painless's internals and\r\ncall site bootstrapping.\r\n\r\nIt works fine, but it didn't have many tests. This converts a\r\ntest that we had for line numbers into a test for the\r\n`script_stack`. The line numbers test was an indirect test\r\nfor `script_stack`.\r\n Painless: more testing for script_stack >>> 1"
1495,This was broken in the recent refactoring to add dependsOn directly to\r\ncluster configuration. Build: Fix plugin integ test to depend on bundling plugin >>> 1
1496,This commit adds the primary term to the doc write response.\r\n\r\nRelates #10708\r\n Add primary term to doc write response >>> 1
1497,"Workday recently open-sourced our internal Scala wrapper for the Elasticsearch REST API. We plan to continue maintaining the library and use it in our products. Thought it would be a good idea to link it under the Community Contributed Clients in case anyone else is interested in using it!\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n - **✓** Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n - **✓** Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n - **[N/A]** If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n - **[N/A]** If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n - **[N/A]** If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n - **[N/A]** If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n Doc Addition: add link to newly open-sourced Scala REST client ""escalar"" >>> 1"
1498,"Add option ""enable_position_increments"" with default value true.\r\nIf option is set to false, indexed value is the number of tokens\r\n(not position increments count)\r\n\r\nFix #23227 token_count type : add an option to count tokens (fix #23227) >>> 1"
1499,"This leniency was left in after plugin installer refactoring for 2.0\r\nbecause some tests still relied on it. However, the need for this\r\nleniency no longer exists. Plugins: Remove leniency for missing plugins dir >>> 1"
1500,Similar to #24085 this adds parsing for InternalBucketMetricValue. This will be needed for\r\nthe high level rest client. I also added a new BucketMetricValue interface since InternalBucketMetricValue didn't implement an interface yet so the keys() method wouldn't be accessible on the client side without casting to a concrete implementation.\r\nPR is against the current feature branch for aggregation parsing.\r\n Add parsing for InternalBucketMetricValue >>> 1
1501,Note: pull request against feature branch\r\n\r\nThis pull request adds the parsing methods for `InternalHDRPercentiles` and `InternalTDigestPercentiles` aggregations. Parsing code is straightforward because it uses the ParsedPercentiles class created in #23974 for percentiles ranks aggregations.\r\n\r\nMost of the changes are in test classes where code has been mutualized between classes. Add parsing methods for Percentiles aggregations >>> 1
1502,Some of the base methods that don't have to do with reduce phase and serialization can be moved to the base class which is no longer an interface. This will be reusable by the high level REST client further on the road. Also it simplify things as having an interface with a single implementor is not that helpful. Make Aggregations an abstract class rather than an interface >>> 1
1503,"Unlike other implementations of InternalNumericMetricsAggregation.SingleValue,\r\nthe InternalBucketMetricValue aggregation currently doesn't implement a\r\nspecialized interface that exposes the `keys()` method. This change adds this so\r\nthat clients can access the keys via the interface.\r\n\r\n Add BucketMetricValue interface >>> 1"
1504,"Adds a new ""icu_collation"" field type that exposes Lucene's\r\nICUCollationDocValuesField.  ICUCollationDocValuesField is the replacement\r\nfor ICUCollationKeyFilter which has been deprecated since Lucene 5. Add ICUCollationFieldMapper >>> 1"
1505,"This commit removes the deprecated cloud.aws.* settings. It also removes\r\nbackcompat for specifying `discovery.type: ec2`, and unused aws signer\r\ncode which was removed in a previous PR. Ec2 Discovery: Cleanup deprecated settings >>> 1"
1506,"This commit adds a call to jstack to see where each node is stuck when\r\nstarting up, if a timeout occurs. This also decreases the timeout back\r\nto 30 seconds. Build: Add jstack output when starting integ test cluster if timeout occurs >>> 1"
1507,"These are already removed in 6.0, we should warn if they are set to anything\r\nother than ""elasticsearch"" in the initialization files.\r\n\r\nRelates to #23989\r\n Add deprecation warnings for $ES_USER and $ES_GROUP >>> 1"
1508,"Plumbs the Definition through the CompilerSettings rather than on its own. This seems a bit more natural to me, though I'm not 100% sure about it.\r\n\r\nAlso fixes some weirdness leftover from the Definition migration. Still doesn't add support for more than a single definition. Painless: Plumb the definition through CompilerSettings >>> 0"
1509,A confusing thing that can happen when configuring Log4j is that extraneous whitespace throws off its configuration parsing yet the error messages that arise give no indication that this is the problem. This commit adds a note to the docs.\r\n Add note to docs on whitespace in Log4j settings >>> 1
1510,"Since we plan on removing types, `indexRandom` should not introduce new types.\r\nThis commit refactors `indexRandom` to reuse existing types. ESIntegTestCase.indexRandom should not introduce types. >>> 1"
1511,"it is changed to add TransportAddress rather than InetSocketTransportAddress\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n TransportClient doc typo >>> 1"
1512,"This pull request aligns the percentiles values returned by the `iterator()` method with what is returned by the `percentile(double percent)` method in the `InternalHDRPercentile` class.\r\n\r\nThe `InternalHDRPercentile.percentile(double percent)` has some logic when no values have been recorded in the `DoubleHistogram` state:\r\n\r\n```\r\npublic double percentile(double percent) {\r\n         if (state.getTotalCount() == 0) {\r\n             return Double.NaN;\r\n         }\r\n         return state.getValueAtPercentile(percent);\r\n}\r\n```\r\n\r\nbut when a `Percentile` object it instantiated in the `next()` method of the iterator it does not use the same logic:\r\n```\r\n final Percentile next = new Percentile(percents[i], state.getValueAtPercentile(percents[i]));\r\n```\r\n\r\nSo the value returned by `hdr.percentile(0.99)` can be `Double.NaN` but when accessed using the iterator returns the value returned for the same percentile is `0`. Align behavior HDR percentiles iterator with percentile() method >>> 1"
1513,I mixed up things in #24183 when adding the parse() method in  `InternalAggregationTestCase`.\r\n\r\nThis pull request fixes `InternalAggregationTestCase` so that it always checks that the internal aggregation and the parsed aggregation always produce the same XContent (using `assertToXContentEquivalent()`) even when the original internal aggregation has been shuffled.\r\n  [Test] Always check the XContent equivalent when parsing aggregations >>> 1
1514,The addition of the normalization feature on keywords slowed down the parsing\r\nof large `terms` queries since all terms now have to go through normalization.\r\nHowever this can be avoided in the default case that the analyzer is a\r\n`keyword` analyzer since all that normalization will do is a UTF8 conversion.\r\nUsing `Analyzer.normalize` for that is a bit overkill and could be skipped. Speed up parsing of large `terms` queries. >>> 1
1515,Currently we don't test for count = 0 which will make a difference when adding\r\ntests for parsing for the high level rest client. Also min/max/sum should also\r\nbe tested with negative values and on a larger range and we could use a \r\nrandomized numeric format.\r\n Tests: Extend InternalStatsTests >>> 1
1516,"Currently any `query_string` query that use a wildcard field with no matching field is rewritten with the `_all` field.\r\n\r\nFor instance:\r\n````\r\n#creating test doc\r\nPUT testing/t/1\r\n{\r\n  ""test"": {\r\n    ""field_one"": ""hello"",\r\n    ""field_two"": ""world""\r\n  }\r\n}\r\n#searching abc.* (does not exist) -> hit\r\nGET testing/t/_search\r\n{\r\n  ""query"": {\r\n    ""query_string"": {\r\n      ""fields"": [\r\n        ""abc.*""\r\n      ],\r\n      ""query"": ""hello""\r\n    }\r\n  }\r\n}\r\n````\r\n\r\nThis bug first appeared in 5.0 after the query refactoring and impacts only users that use `_all` as default field.\r\nIndices created in 6.x will not have this problem since `_all` is deactivated in this version.\r\n\r\nThis change fixes this problem by ignoring terms without an explicit field if the requested multi fields are not empty. Query string default field >>> 1"
1517,"The `MultiBucketsAggregation.Bucket` interface extends `Writeable`, forcing\r\nall implementation classes to implement writeTo(). This commit removes\r\nthe Writeable from the interface and move it down to the InternalBucket\r\nimplementation.\r\n MultiBucketsAggregation.Bucket should not extend Writeable >>> 1"
1518,"Lucene 6.5.1 is likely going to be released too late for it to be included\r\nin Elasticsearch 5.4. Yet we are interested in one bug fix it contains:\r\nhttps://issues.apache.org/jira/browse/LUCENE-7749, which allows the query\r\ncache to work with `IndexOrDocValuesQuery`, the new query that we use for range\r\nqueries. Fork LRUQueryCache from Lucene to work around LUCENE-7749 >>> 1"
1519,Otherwise the range improvements that we did on range queries would not work.\r\nThis is similar to https://issues.apache.org/jira/browse/LUCENE-7749. IndicesQueryCache should delegate the scorerSupplier method. >>> 1
1520,"The endpoints `_open` and `_close` require at least one index to be specified as the target for the requested action. Under the default setting of ` allow_no_indices` (`false`) `_open` allows the use of a wildcard resolving to a currently closed index. If the wildcard does not match any such index, the `index_not_found_exception` is thrown. `_close` has the same behavior.\r\n\r\nHowever setting `allow_no_indices=true` both `_open` and `_close` would still throw an exception if there is no matching target index. In other words `allow_no_indices` is ignored in this context.\r\n\r\nThis PR addresses the above inconsistency.\r\n \r\nFixes #24031 Open and close index to honour allow_no_indices option >>> 1"
1521,"This changes the way we register ""pre-built"" token filters so that\r\nplugins can declare them and starts to move all of the ""pre-built""\r\ntoken filters out of core. It doesn't finish the job because doing\r\nso would make the change unreviewably large. So this PR includes\r\na shim that keeps the ""old"" way of registering ""pre-built"" token\r\nfilters around.\r\n\r\nThe Lowercase token filter is special because there is a ""special""\r\ninteraction between it and the lowercase tokenizer. I'm not sure\r\nexactly what to do about it so for now I'm leaving it alone with\r\nthe intent of figuring out what to do with it in a followup.\r\n\r\nThis is a part of #23658 Allow plugins to build ""pre-configured"" token filters >>> 1"
1522,"Note: this pull request is against a feature branch.\r\n\r\nThis pull request adds the logic to parse `InternalDateHistogram` and `InternalHistogram` aggregations. To do that, it introduces a `ParsedMultiBucketAggregation` that implements the `MultiBucketsAggregation` from core. This class provides a base `ParsedBucket` that can be extended by parsed implementations to fit their specific needs.\r\n\r\nFor now, the parsing logic for aggregations and buckets reside in the `ParsedHistogram` and `ParsedDateHistogram` implementations. Some code could be shared but it makes everything harder to read and understand (I'm still looking at how to improve this). \r\n\r\nThe `ParsedHistogram.ParsedBucket` and `ParsedDateHistogram.ParsedBucket` are able to parse sub aggregations. They also handle the parsing logic when aggregations and buckets are keyed/not keyed.\r\n\r\nIt also introduces a `InternalMultiBucketAggregationTestCase` that takes care of verifying the aggregations and multiple buckets. It has a `assertMultiBucketsAggregation` that can checks the buckets in order or not.\r\n\r\nSimilarly to the existing `InternalSingleBucketAggregationTestCase`, the `InternalMultiBucketAggregationTestCase` randomly creates multi bucket aggregations that have sub aggregations of the same type (ie during tests, InternalDateHistogram can only have buckets with aggregations of type InternalDateHistogram). This makes things easier when checking the aggregations contained in a bucket - it uses a recursive call to `assertMultiBucketsAggregation`. Add parsing methods for InternalDateHistogram and InternalHistogram >>> 1"
1523,These methods already exist for ObjectParser. This change adds them to ConstructingObjectParser so they are available when parsing to an object that doesn't have a zero-arg constructor Adds declareNamedObjects methods to ConstructingObjectParser >>> 1
1524,"The plugin cli currently resides inside the elasticsearch jar. This\r\ncommit moves it into a plugin-cli jar. This is change alone is a no-op;\r\nit does not change anything about what is loaded at runtime. But it will\r\nallow easier testing (with fixtures in the future to test ES or maven\r\ninstallation), as well as eventually not loading these classes when\r\nstarting elasticsearch.  Build: Move plugin cli and tests to distribution tool >>> 1"
1525,Similar to #24085 this adds parsing for InternalStats. This will be needed for the high level rest client.\r\nPR is against the current feature branch for aggregation parsing. Add parsing for InternalStats >>> 1
1526,"This pull request adds a XContentParserUtils.parseTypedKeysObject() method that can be used to parse named XContent objects identified by a field name containing a type identifier, a delimiter and the name of the object to parse.\r\n\r\nRelated to #22965\r\n Add utility method to parse named XContent objects with typed prefix >>> 1"
1527,"Changes in #24102 exposed the following oddity: `PrioritizedEsThreadPoolExecutor.getPending()` can return `Pending` entries where `pending.task == null`.\r\n\r\nThis can happen for example when tasks are added to the pending list while they are in the clean up phase, i.e. `TieBreakingPrioritizedRunnable#runAndClean` has run already, but `afterExecute` has not removed the task yet.\r\n\r\nInstead of safeguarding consumers of the API (as was done before #24102) I think that we should not count these tasks as pending at all.\r\n\r\nTest failures: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+java9-periodic/2235/consoleFull\r\n\r\n```\r\nERROR   0.76s J2 | SharedClusterSnapshotRestoreIT.testBatchingShardUpdateTask <<< FAILURES!\r\n   > Throwable #1: java.lang.NullPointerException\r\n   > \tat org.elasticsearch.cluster.service.ClusterService.lambda$pendingTasks$2(ClusterService.java:491)\r\n   > \tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)\r\n   > \tat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\r\n   > \tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\r\n   > \tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\r\n   > \tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\r\n   > \tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n   > \tat java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:511)\r\n   > \tat org.elasticsearch.cluster.service.ClusterService.pendingTasks(ClusterService.java:495)\r\n   > \tat org.elasticsearch.action.admin.cluster.tasks.TransportPendingClusterTasksAction.masterOperation(TransportPendingClusterTasksAction.java:68)\r\n   > \tat org.elasticsearch.action.admin.cluster.tasks.TransportPendingClusterTasksAction.masterOperation(TransportPendingClusterTasksAction.java:38)\r\n   > \tat org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:87)\r\n   > \tat  ...\r\n``` Don't expose cleaned-up tasks as pending in PrioritizedEsThreadPoolExecutor >>> 1"
1528,Today we might promote a primary and recover from store where after translog\r\nrecovery the local checkpoint is still behind the maximum sequence ID seen.\r\nTo fill the holes in the sequence ID history this PR adds a utility method\r\nthat fills up all missing sequence IDs up to the maximum seen sequence ID\r\nwith no-ops.\r\n\r\nRelates to #10708\r\nI still work on a test for store recovery to ensure it's called but I think it's ready for review. Fill missing sequence IDs up to max sequence ID when recovering from store >>> 1
1529,"With #24149 , it is now stored in the Lucene commit and is implicitly transferred in the file phase of the recovery.  Peer Recovery: remove maxUnsafeAutoIdTimestamp hand off >>> 1"
1530,Relates to #22278 [Test] Add unit tests for HDR/TDigest PercentilesAggregators >>> 1
1531,Allow passing a single scroll id as string instead of array.\r\n\r\nClose #24233  Allow passing single scrollID in clear scroll API body >>> 1
1532,"Allow the `Context` to be used in the builder function used within ConstructingObjectParser.\r\nThis facilitates scenarios where a constructor argument comes from a URL parameter, or from document id.\r\n Pass Context to ConstructingObjectParser's function >>> 1"
1533,"At present providing unknown context in a suggestion context query does not provide informative error message.\r\nConsider below mapping and correspongding query\r\n\r\n**Mapping**\r\n\r\n```\r\n{\r\n    ""mappings"": {\r\n        ""products"" : {\r\n            ""properties"" : {\r\n                ""suggest"" : {\r\n                    ""type"" : ""completion"",\r\n                    ""contexts"": [\r\n                        { \r\n                            ""name"": ""product_type"",\r\n                            ""type"": ""category"",\r\n                            ""path"": ""cat""\r\n                        },\r\n                        { \r\n                            ""name"": ""product_color"",\r\n                            ""type"": ""category"",\r\n                            ""path"": ""cat""\r\n                        },\r\n                        { \r\n                            ""name"": ""product_size"",\r\n                            ""type"": ""category"",\r\n                            ""path"": ""cat""\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n**Query**\r\n\r\n```\r\n{\r\n    ""suggest"": {\r\n        ""products_suggestion"" : {\r\n            ""prefix"" : ""sho"",\r\n            ""completion"" : {\r\n                ""field"" : ""suggest"",\r\n                ""size"": 10,\r\n                ""contexts"": {\r\n                    ""product_type"": ""casual"",\r\n                    ""product_brand"": ""reebok""\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nExecuting the above query will show error message as below\r\n\r\n""Unknown context name[product_brand], must be one of 3""\r\n\r\nThis error message is not very informative as it only convey the number of context supported. However printing all the accepted contexts will be more informative, like below:\r\n\r\n""Unknown context name[product_brand], must be one of [product_size, product_color, product_type]""\r\n\r\nThis pull request fixes the issue mentioned. Provide informative error message in case of unknown suggestion context. >>> 1"
1534,"Separates cluster state publishing from applying cluster states:\r\n\r\n- ClusterService is split into two classes MasterService and ClusterApplierService. MasterService  has the responsibility to calculate cluster state updates for actions that want to change the cluster state (create index, update shard routing table, etc.). ClusterApplierService has the responsibility to apply cluster states that have been successfully published and invokes the cluster state appliers and listeners.\r\n- ClusterApplierService keeps track of the last applied state, but MasterService is stateless and uses the last cluster state that is provided by the discovery module to calculate the next prospective state. The ClusterService class is still kept around, which now just delegates actions to ClusterApplierService and MasterService.\r\n- The discovery implementation is now responsible for managing the last cluster state that is used by the consensus layer and the master service. It also exposes the initial cluster state which is used by the ClusterApplierService. The discovery implementation is also responsible for adding the right cluster-level blocks to the initial state.\r\n- NoneDiscovery has been renamed to TribeDiscovery as it is exclusively used by TribeService. It adds the tribe blocks to the initial state.\r\n- ZenDiscovery is synchronized on state changes to the last cluster state that is used by the consensus layer and the master service, and does not submit cluster state update tasks anymore to make changes to the disco state (except when becoming master).\r\n\r\nControl flow for cluster state updates is now as follows:\r\n- State updates are sent to MasterService\r\n- MasterService gets the latest committed cluster state from the discovery implementation and calculates the next cluster state to publish\r\n- MasterService submits the new prospective cluster state to the discovery implementation for publishing\r\n- Discovery implementation publishes cluster states to all nodes and, once the state is committed, asks the ClusterApplierService to apply the newly committed state.\r\n- ClusterApplierService applies state to local node. Separate publishing from applying cluster states >>> 1"
1535,When parsing StoredSearchScript we were adding a Content type option that was forbidden (by a check that threw an exception) by the parser thats used to parse the template when we read it from the cluster state. This was stopping Elastisearch from starting after stored search templates had been added.\r\n\r\nThis change no longer adds the content type option to the StoredScriptSource object when parsing from the put search template request.  This is safe because the StoredScriptSource content is always JSON when its stored in the cluster state since we do a conversion to JSON before this point.\r\n\r\nAlso removes the check for the content type in the options when parsing StoredScriptSource so users who already have stored scripts can start Elasticsearch.\r\n\r\nCloses #24227\r\n No longer add illegal content type option to stored search templates >>> 1
1536,"Today when removing a plugin, we attempt to move the plugin directory to a temporary directory and then delete that directory from the filesystem. We do this to avoid a plugin being in a half-removed state. We previously tried an atomic move, and fell back to a non-atomic move if that failed. Atomic moves can fail on union filesystems when the plugin directory is not in the top layer of the filesystem. Interestingly, the regular move can fail as well. This is because when the JDK is executing such a move, it first tries to rename the source directory to the target directory and if this fails with EXDEV (as in the case of an atomic move failing), it falls back to copying the source to the target, and then attempts to rmdir the source. The bug here is that the JDK never deleted the contents of the source so the rmdir will always fail (except in the case of an empty directory).\r\n\r\nGiven all this silliness, we were inspired to find a different strategy. The strategy is simple. We will add a marker file to the plugin directory that indicates the plugin is in a state of removal. This file will be the last file out the door during removal. If this file exists during startup, we fail startup.\r\n\r\nCloses #24231\r\n  Use a marker file when removing a plugin >>> 1"
1537,Before #22488 when an index couldn't be created during a `_bulk`\r\noperation we'd do all the *other* actions and return the index\r\ncreation error on each failing action. In #22488 we accidentally\r\nchanged it so that we now reject the entire bulk request if a single\r\naction cannot create an index that it must create to run. This\r\nreverts to the old behavior while still keeping the nicer\r\nerror messages. Instead of failing the entire request we now only\r\nfail the portions of the request that can't work because the index\r\ndoesn't exist.\r\n\r\nCloses #24028 Fix _bulk response when it can't create an index >>> 1
1538,"In #24251 we fix an issue with stored search templates that\r\nthis test would have discovered: stored search templates cause\r\nthe node to refuse to start. Technically a ""restart"" test would\r\nhave caught this as well and would have caught it more quickly.\r\nBut we already *have* an upgrade test and we don't have restart tests.\r\nAnd testing this on upgrade is a good thing too.\r\n Test search templates during rolling upgrade test >>> 1"
1539,"For the Windows service, JAVA_HOME should be set to the path to the JDK. We should make this clear in the docs to help users avoid frustrating startup problems.\r\n\r\nRelates #24187\r\n Add note to docs regarding JAVA_HOME on Windows >>> 1"
1540,Another step down the road to dropping the\r\nlucene-analyzers-common dependency from core.\r\n\r\nNote that this removes some tests that no longer compile from\r\ncore. I played around with adding them to the analysis-common\r\nmodule where they would compile but we already test these in\r\nthe tests generated from the example usage in the documentation.\r\n\r\nI'm not super happy with the way that `requriesAnalysisSettings`\r\nworks with regards to plugins. I think it'd be fairly bug-prone\r\nfor plugin authors to use. But I'm making it visible as is for\r\nnow and I'll rethink later.\r\n\r\nA part of #23658 Move char filters into analysis-common >>> 1
1541,"The unwrap method was leftover from support javascript and python. Since\r\nthose languages are removed in 6.0, this commit removes the unwrap\r\nfeature from scripts. Scripts: Remove unwrap method from executable scripts >>> 1"
1542,"ScriptService has two executable methods, one which takes a\r\nCompiledScript, which is similar to search, and one that takes a raw\r\nScript and both compiles and returns an ExecutableScript for it. The\r\nlatter is not needed, and the call sites which used one or the other\r\nwere mixed. This commit removes the extra executable method in favor of\r\ncallers first calling compile, then executable. Scripts: Remove unnecessary executable shortcut >>> 1"
1543,"This commit fixes an issue when deleting the plugin directory while executing the remove plugin command. Namely, we take out a file descriptor on the plugin directory to traverse its contents to obtain the list of files to delete. We leaked this file descriptor. On Unix-based filesystems, this is not a problem, deleting the plugin directory deletes the plugin directory. On Windows though, a delete is not executed until the last file descriptor is closed. Since we leaked this file descriptor, the plugin was not actually deleted. This led to test failures that tried to cleanup left behind temporary directories but these test failures were just exposing this bug. This commit fixes this issue by ensuring that we close the file descriptor to the plugin directory when we are finished with it.\r\n\r\nRelates #24252\r\n \r\n Fix delete of plugin directory on remove plugin >>> 1"
1544,This includes the infrastructure the test class uses (AbstractSerializingTestCase) Backports StoredScriptSourceTests >>> 1
1545,ESIntegTestCase#pluginList was removed in ES 5.0. We are using Arrays.asList instead.\r\n\r\n[Source: Plugin changes in 5.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_plugins.html#_testing_custom_plugins) Replace deprecated pluginList with Arrays.asList >>> 1
1546,This commit fixes the hash code for AliasFilter as the previous implementation was neglecting to take into consideration the fact that the aliases field is an array and thus a deep hash code of it should be computed rather than a shallow hash code on the reference.\r\n\r\nRelates #23994\r\n Fix hash code for AliasFilter >>> 1
1547,"If the user explicitly configured path.data to include default.path.data, then we should not fail the node if we find indices in default.path.data. This commit addresses this.\r\n\r\nCloses #24283\r\n Check for default.path.data included in path.data >>> 1"
1548,Relates to #22278 Added unittests for InternalGeoCentroid >>> 1
1549,"There was a bug in the calculation of the shards that a snapshot must\r\nwait on, due to their relocating or initializing, before the snapshot\r\ncan proceed safely to snapshot the shard data.  In this bug, an\r\nincorrect key was used to look up the index of the waiting shards,\r\nresulting in the fact that each index would have at most one shard in\r\nthe waiting state causing the snapshot to pause.  This could be\r\nproblematic if there are more than one shard in the relocating or\r\ninitializing state, which would result in a snapshot prematurely\r\nstarting because it thinks its only waiting on one relocating or\r\ninitializing shard (when in fact there could be more than one).  While\r\nnot a common case and likely rare in practice, it is still problematic.\r\n\r\nThis commit fixes the issue by ensuring the correct key is used to look\r\nup the waiting indices map as it is being built up, so the list of\r\nwaiting shards for each index (those shards that are relocating or\r\ninitializing) are aggregated for a given index instead of overwritten. Fixes maintaining the shards a snapshot is waiting on >>> 1"
1550,"The `count` value in the stats aggregation represents a simple doc count\r\nthat shouldn't require a formatted version. We didn't render an ""as_string""\r\nversion for count in the rest response, so the method should also be\r\nremoved in favour of just using String.valueOf(getCount()) if a string\r\nversion of the count is needed.\r\n\r\nCloses #24287\r\n\r\n Remove getCountAsString() from InternalStats and Stats interface >>> 1"
1551,"We don't render an ""count_as_string"" in the rest response,\r\nso the method should also be removed in favour of just using\r\nString.valueOf(getCount()) if a string version of the count is needed.\r\n\r\n Deprecate Stats#getCountAsString >>> 1"
1552,This commit adds a compileTemplate method to the ScriptService.\r\nEventually this will be used to easily cutover all consumers to a new\r\nTemplateService.\r\n\r\nrelates #16314 Templates: Add compileTemplate method to ScriptService for template consumers >>> 1
1553,We document that painless can load stored fields but it can't\r\nbecause the classes that make that work aren't whitelisted.\r\n Allow painless to load stored fields >>> 1
1554,Corrects the ScriptedMetricAggregator so that the script can have\r\naccess to scores during the map stage.\r\n Allow scripted metric agg to access `_score` >>> 1
1555,"Teaches the naming conventions check to fail the build when it finds\r\ntests in `src/main/java`. Tests in `src/main/java` aren't run and that\r\nis bad because tests that don't run provide a false sense of\r\nsecurity.\r\n\r\nEdit: As it stands now this actually creates a new similar task to the\r\n`namingConventions` task, `namingConventionsMain` that checks\r\nthe main classes of the `buildSrc` and `test/framework` projects,\r\nlooking for tests. This checking isn't done by the same task. Teach naming conventions check to look for tests in src/main >>> 1"
1556,`tests.enable_mock_modules` is a documented but unrespected / unused\r\noption to disable all mock modules / pluings during test runs. This\r\nwill basically site-step mock assertions like check-index on shard closing.\r\nThis can speed up test-execution dramatically on nodes with slow disks etc.\r\n\r\nRelates to #24304 Add support for `tests.enable_mock_modules` to ESIntegTestCase >>> 1
1557,This commit introduces a nio based tcp transport into framework for\r\ntesting.\r\n\r\nCurrently Elasticsearch uses a simple blocking tcp transport for\r\ntesting purposes (MockTcpTransport). This diverges from production\r\nwhere our current transport (netty) is non-blocking.\r\n\r\nThe point of this commit is to introduce a testing variant that more\r\nclosely matches the behavior of production instances. Introduce NioTransport into framework for testing >>> 1
1558,Similar to #24085 this adds parsing for InternalExtendedStats.\r\nPR is against the current feature branch for aggregation parsing.\r\n\r\nRelates to #24239. Add parsing for InternalExtendedStats >>> 1
1559,I propose this change to clarify documentation and to verify my\r\nunderstanding of the implications of minimum_master_nodes and the\r\nnumber of actual master nodes.\r\n\r\n Add note on risk of split brains to zen.asciidoc >>> 1
1560,related to #16671 Added validation for upsert request >>> 1
1561,"StreamInput has methods such as readVInt that perform sanity checks on the data using assertions,\r\nwhich will catch bad data in tests but provide no safety when running as a node without assertions\r\nenabled. The use of assertions also make testing with invalid data difficult since we would need\r\nto handle assertion errors in the code using the stream input and errors like this should not be\r\nsomething we try to catch. This commit changes StreamInput to throw an IOException instead of\r\nusing an assertion and also fixes a bug where we created an AssertionError and never did\r\nanything with it. StreamInput throws exceptions instead of using assertions >>> 1"
1562,This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThis PR is about the regular expressions used in the codebase. Only 2 instances were found where performance optimizations could be recommended. These are both regexes using the `|` symbol to achieve a logical OR comparison. This can be done much more efficiently with a character class.\r\n\r\nWhat is a character class: http://www.regular-expressions.info/charclass.html\r\n\r\nExplanation as to why it is faster: http://stackoverflow.com/a/26141949\r\n\r\nI made some test cases demonstrating the results are identical.\r\n\r\nVersion.java: https://ideone.com/hMHGh5\r\n\r\nDeprecationLogger.java: https://ideone.com/j9LRU7\r\n\r\n===========\r\n1. Contributor agreement: Has been signed\r\n2. Contributor guidelines: I have read them.\r\n3. Gradle check: build successful.\r\n4. PR is against master\r\n5. The PR is not OS specific.\r\n6. I'm not part of a class. Regex upgrades >>> 1
1563,"This PR extends the functionality of the existing testing fixture code in our gradle build source. Previously, testing fixtures were locked into being executed by an Ant task. In order to take advantage of the existing enhanced Vagrant logging support in the build code, I have split the existing Fixture object into an interface (`Fixture.groovy`) and an implementation (`AntFixture.groovy`), and added a new implementation (`VagrantFixture.groovy`) that is based on the existing `VagrantCommand` task.\r\n\r\nThis is related to #23439 in that the integration tests will need to spin up a Kerberos KDC based in a virtual machine run by Vagrant. Add Vagrant based testing fixture >>> 1"
1564,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThese are what appear to be actual bugs. In order:\r\n\r\n1. Arrays printed without Arrays.toString(). Which results in the actual String being something like `[B@2f4a...` instead of the actual contents of the array.\r\n\r\n2. Code that is not used at all. There doesn't appear to be any point in the code of these files that actually make use of the constructs being set up. In any way. So I'm assuming they were once used and not cleaned up completely when code was moved around.\r\n\r\n3. I'm assuming the `logger.equals(msg)` bit it a typo. Judging by its surrounding methods, it should be `logger.error(msg)`.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Code bugs >>> 0"
1565,"When an index is shrunk using the shrink APIs, the shrink operation adds\r\nsome internal index settings to the shrink index, for example\r\n`index.shrink.source.name|uuid` to denote the source index, as well as\r\n`index.routing.allocation.initial_recovery._id` to denote the node on\r\nwhich all shards for the source index resided when the shrunken index\r\nwas created.  However, this presents a problem when taking a snapshot of\r\nthe shrunken index and restoring it to a cluster where the initial\r\nrecovery node is not present, or restoring to the same cluster where the\r\ninitial recovery node is offline or decommissioned.  The restore\r\noperation fails to allocate the shard in the shrunken index to a node\r\nwhen the initial recovery node is not present, and a restore type of\r\nrecovery will *not* go through the PrimaryShardAllocator, meaning that\r\nit will not have the chance to force allocate the primary to a node in\r\nthe cluster.  Rather, restore initiated shard allocation goes through\r\nthe BalancedShardAllocator which does not attempt to force allocate a\r\nprimary.\r\n\r\nThis commit fixes the aforementioned problem by not requiring allocation\r\nto occur on the initial recovery node when the recovery type is a\r\nrestore of a snapshot.  This commit also ensures that the internal\r\nshrink index settings are recognized and not archived (which can trip an\r\nassertion in the restore scenario).\r\n\r\nCloses #24257 Fixes restore of a shrunken index when initial recovery node is gone >>> 1"
1566,The bats tests are descructive and must be run as root. This is a\r\nhorrible combination on any sane system but perfectly fine to do\r\nin a VM. This change modifies the tests so they revuse to start\r\nunless they are in an environment with an `/etc/is_vagrant_vm`\r\nfile. The Vagrantfile creates it on startup.\r\n\r\nCloses #24137\r\n Make bats tests refuse to start on non-VMs >>> 1
1567,"The one argument ctor for `Script` creates a script with the\r\ndefault language but most usages of are for testing and either\r\ndon't care about the language or are for use with\r\n`MockScriptEngine`. This replaces most usages of the one argument\r\nctor on `Script` with calls to `ESTestCase#mockScript` to make\r\nit clear that the tests don't need the default scripting language.\r\n\r\nI've also factored out some copy and pasted script generation\r\ncode into a single place. I would have had to change that code\r\nto use `mockScript` anyway, so it was easier to perform the\r\nrefactor.\r\n\r\nRelates to #16314 Remove most usages of 1-arg Script ctor >>> 1"
1568,"In case of a Cross Cluster Search, the coordinating node should split the original indices per cluster, and send over to each cluster only its own set of original indices, rather than set taken from the original search request which contains all the indices.\r\n\r\nIn fact, each remote cluster should not be aware of the indices belonging to other remote clusters. Cross Cluster Search: propagate original indices per cluster >>> 1"
1569,"After #24312 , this should be the last of the `NumericMetricsAggregation.MultiValue` aggregations that needs a parsing method for the high level rest client. Add parsing for InternalPercentilesBucket >>> 1"
1570,This commit adds support for plugins having a platform specific variant.\r\nIt also adds unit tests for all official and maven urls.\r\n Plugins: Add support for platform specific plugins >>> 1
1571,Currently `InternalPercentilesBucket#percentile()` relies on the percent array passed in to be in sorted order. This changes the aggregation to store an internal lookup table that is constructed from the percent/percentiles arrays passed in that can be used to look up the percentile values.\r\n\r\nCloses #24331 InternalPercentilesBucket should not rely on ordered percents array >>> 1
1572,"This adds the `index.mapping.single_type` setting, which enforces that indices\r\nhave at most one type when it is true. The default value is true for 6.0+ indices\r\nand false for old indices.\r\n\r\nMost of the change is about fixing tests that created multiple types to either\r\ncreate a single `_type` instead or set `index.mapping.single_type` to `false`\r\n(eg. for parent/child tests, which still require multiple types).\r\n\r\nRelates #15613 Only allow one type on 6.0 indices >>> 1"
1573,"We can leak disrupted threads here since we never wait for them to complete after freeing them from their loops. This commit addresses this by joining on disrupted threads, and addresses fallout from trying to join here.\r\n\r\n Avoid leaks in Long GC disruption tests >>> 1"
1574,"Rewrites the description of the `bool` query's `should`\r\nclauses so it is (hopefully) more clear what the defaults\r\nfor `minimum_should_match` are.\r\n\r\nThere is still an `[IMPORTANT]` section about `minimum_should_match`\r\nin a filter context. I think it is worth keeping because it is, well,\r\nimportant.\r\n\r\nCloses #23831\r\n Rewrite description of `bool`'s `should` >>> 1"
1575,This code removes a few lines of dead code from SnapshotsService. Looks like a forgotten remnant of a past implementation.\r\n\r\n\r\nThis PR is related to PR #24321 as requested by @jasontedor \r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Removal of dead code from SnapshotsService >>> 1
1576,"This code removes a few lines of dead code from ScriptedMetricAggregationBuilder. Just completely dead code, it adds things to a Set that is then not used in any way.\r\n\r\nThis PR is related to PR #24321 as requested by @jasontedor \r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Removal of dead code in ScriptedMetricAggregationBuilder >>> 1"
1577,"These are variant of the InternalStats and InternalExtendedStats. Most of the existing code can be shared,\r\nso this PR only rearranges some common code to be accessible by the new subclasses.\r\n\r\nBased on #24284 which should go in first.\r\nPR is against feature branch. Add parsing for InternalStatsBucket and InternalExtendedStatsBucket >>> 1"
1578,"The tribe service can take a while to initialize, depending on how many\r\ncluster it needs to connect to. This change moves writing the ports file\r\nused by tests to before the tribe service is started. Test: Write node ports file before starting tribe service >>> 1"
1579,We've had `QueryDSLDocumentationTests` for a while but it had a very\r\nhopeful comment at the top about how we want to make sure that the\r\nexample in the query-dsl docs match up with the test but we never\r\nhad anything that made *sure* that they did. This changes that!\r\n\r\nNow the examples from the query-dsl docs are all built from the\r\n`QueryDSLDocumentationTests`. All except for the percolator example\r\nbecause that is hard to do as it stands now.\r\n\r\nTo make this easier this change moves `QueryDSLDocumentationTests`\r\nfrom core and into the high level rest client. This is useful for\r\ntwo reasons:\r\n1. We expect the high level rest client to be able to use the builders.\r\n2. The code that builds that docs doesn't check out all of\r\nElasticsearch. It only checks out certain directories. Since we're\r\nalready including snippets from that directory we don't have to\r\nmake any changes to that process.\r\n\r\nCloses #24320\r\n Build the java query DSL api docs from a test >>> 1
1580,"This very small PR is related to issue #24226\r\n\r\nI came across this method that had a seemingly pointless variable created that is then never used and then returned.\r\n\r\nI had Eclipse check the points of use, there are 2:\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L381\r\n\r\nand\r\n\r\nhttps://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L553\r\n\r\nSo since the return is both not related to the actual tasks performed by the method and not actually picked up, I turned this method into a `void`, like most the other `parse...` methods already are.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. The parseObject method in DocumentParse can be void. >>> 1"
1581,This makes it possible for the recovery source to verify that it is talking to the shard it thinks it is talking to.\r\n\r\nCloses #24167 Provide target allocation id as part of start recovery request >>> 1
1582,Changes the snapshot status read exception from the (misleading)\r\nIndexShardRestoreFailedException to the generic SnapshotException\r\n\r\nCloses #24225 Change snapshot status error to use generic SnapshotException >>> 1
1583,Documents that the `bool` query's `disable_coord` option only\r\napplies to the `classic` similarity.\r\n\r\nCloses #24267\r\n Document that disable_coord only applies to classic similarity >>> 1
1584,"Just some minor tweaks.  Was walking aggregation trees but found that some getters were not available... and I couldn't see a particular reason why they shouldn't be.\r\n\r\n- Add getter for the filter in a FilterAgg\r\n- Add getters for subaggs / pipelines in base AggregationBuilder\r\n- Getters for DateHisto `interval` and ~~`order`~~ `offset` should return a long, not double\r\n\r\n Agg builder accessibility fixes >>> 1"
1585,Aaand a few more before I move onto something else for the day :)\r\n\r\nRelated to #18160\r\n CONSOLEify some more Indices APIs >>> 1
1586,"Mostly just adding `// NOCONSOLE` where appropriate, but also adds a response + test for movavg pipeline.\r\n\r\nRelated to #18160 CONSOLEify Pipeline Aggregations docs >>> 1"
1587,Adds console and testing snippets to the Stats Agg docs :)\r\n\r\nRelated to #18160 CONSOLEify Stats Aggregation docs >>> 1
1588,"Correct a typo in the exception message:\r\n\r\n  ""Failed to created node environment""\r\n Fix typo in Node creation exception message >>> 1"
1589,This adds parsing for InternalGeoBounds.\r\nPR is against the current feature branch for aggregation parsing. Add parsing for InternalGeoBounds >>> 1
1590,"This adds parsing to the InternalGeoCentroid aggregation. One problem I encountered here is that we don't render the `count` parameter that is available through the `count()` method in the GeoCentroid interface to REST (see #24366). After discussing this it looks like we should add this parameter to the REST output of the aggregation. I did this in this PR already but I think I will also open a separate PR to do this in master already. \r\nI'm not sure we can do anything in 5.x if we backport, we will need to work with a dummy constant for the `count` value there since it is not available via REST. \r\n Add parsing for InternalGeoCentroid >>> 1"
1591,"Currently we don't write the count value to the `geo_centroid` aggregation rest response, but it is provided via the java api and the `count()` method in the `GeoCentroid` interface. As discussed in #24366, we should add this parameter to the rest output and provide it via the `getProperty()` method.\r\nThis change adds rendering toXContent, adapts unit and integration tests and the response example in the documentation. I'm not sure if adding a parameter to REST requires other updates (like entry into migration docs, rest specs etc...) Add `count` to rest output of `geo_centroid` >>> 1"
1592,"Eclipse doesn't allow extra semicolons after an import statement:\r\n```\r\nimport foo.Bar;;   // <-- syntax error!\r\n```\r\n\r\nHere is the Eclipse bug:\r\nhttps://bugs.eclipse.org/bugs/show_bug.cgi?id=425140\r\nwhich the Eclipse folks closed as ""the spec doesn't allow these\r\nsemicolons so why should we?"" Which is fair. Here is the bug\r\nagainst javac for allowing them:\r\nhttps://bugs.openjdk.java.net/browse/JDK-8027682\r\nwhich hasn't been touched since 2013 without explanation. There\r\nis, however, a rather educations mailing list thread:\r\nhttp://mail.openjdk.java.net/pipermail/compiler-dev/2013-August/006956.html\r\n\r\nwhich contains gems like, ""In general, it is better/simpler to\r\nchange javac to conform to the spec. (Except when it is not.)""\r\n\r\nI suspect the reason this hasn't been fixed is:\r\n```\r\nFWIW, if we change javac such that the set of programs accepted by javac\r\nis changed, we have an process (currently Oracle internal) to get\r\napproval for such a change.   So, we would not simply change javac on a\r\nwhim to meet the spec; we would at least have other eyes looking at the\r\nbehavioral change to determine if it is ""acceptable"".\r\n```\r\nfrom http://mail.openjdk.java.net/pipermail/compiler-dev/2013-August/006973.html\r\n Fix compilation in Eclipse >>> 1"
1593,This change makes `_type` behave pretty much like `_index` when\r\n`index.mapping.single_type` is true. Do not index `_type` when there is at most one type. >>> 1
1594,"The UpgraderPlugin adds two additional extension points called during cluster upgrade and when old indices are introduced into the cluster state during initial recovery, restore from a snapshot or as a dangling index. One extension point allows plugin to update old templates and another extension points allows to update/check index metadata.\r\n\r\nThis change should simplify upgrade process for plugins that are using special indices to store their data, and should prevent situations when, for example, an accidental restore of an obsolete index would break the plugin.\r\n Allow plugins to upgrade templates and index metadata on startup >>> 1"
1595,"Today we go to heroic lengths to workaround bugs in the JDK or around issues like BSD jails to get information about the underlying file store. For example, we went to lengths to work around a JDK bug where the file store returned would incorrectly report whether or not a path is writable in certain situations in Windows operating systems. Another bug prevented getting file store information on Windows on a virtual drive on Windows. We no longer need to work around these bugs, we could simply try to write to disk and let an I/O exception arise if we could not write to the disk or take advantage of the fact that these bugs are fixed in recent releases of the JDK (e.g., the file store bug is fixed since 8u72). Additionally, we collected information about all file stores on the system which meant that if the user had a stale NFS mount, Elasticsearch could hang and fail on startup if that mount point was not available. Finally, we collected information through Lucene about whether or not a disk was a spinning disk versus an SSD, information that we do not need since we assume SSDs by default. This commit takes into consideration that we simply do not need this heroic effort, we do not need information about all file stores, and we do not need information about whether or not a disk spins to greatly simplfy file store handling.\r\n\r\nCloses #24390 Simplify file store >>> 1"
1596,"After a replica shard finishes recovery, it will be marked as active and its local checkpoint will be considered in the calculation of the global checkpoint on the primary. If there were operations in flight during recovery, when the replica is activated its local checkpoint could be lagging behind the global checkpoint on the primary. This means that when the replica shard is activated, we can end up in a situtaion where a global checkpoint update would want to move the global checkpoint backwards, violating an invariant of the system. This only arises if a background global checkpoint sync executes, which today is only a scheduled operation and might be delayed until the in-flight operations complete and the replica catches up to the primary. Yet, we are going to move to inlining global checkpoints which will cause this issue to be more likely to manifest. Additionally, the global checkpoint on the replica, which is the local knowledge on the replica updated under the mandate of the primary, could be higher than the local checkpoint on the replica, again violating an invariant of the system. This commit addresses these issues by blocking global checkpoint on the primary when a replica shard is finalizing recovery. While we have blocked global checkpoint advancement, recovery on the replica shard will not be considered complete until its local checkpoint advances to the blocked global checkpoint.\r\n\r\nRelates #10708\r\n Block global checkpoint advances when recovering >>> 1"
1597,"We are back to having a 140-column limit. While at some point we will apply auto-formatting tools to the code base, that is a down-the-road thing and adjusting the suppressions now shaves minutes off the build time.\r\n Adjust checkstyle suppressions to 140-column limit >>> 1"
1598,"Add info about the base image used and the github repo of elasticsearch-docker.\r\n\r\nClarify that setting `memlock=-1:-1` is only a requirement when `bootstrap_memory_lock=true` and the alternatives we document elsewhere in docs for disabling swap are valid for Docker as well.\r\n\r\nAdditionally, with latest versions of docker-ce shipping with unlimited (or high enough) defaults for `nofile` and `nproc`, clarify that explicitly setting those per ES container is not required, unless they are not defined in the Docker daemon.\r\n\r\nFinally simplify production `docker-compose.yml` example by removing unneeded options. One such option is `cap_add: IPC_LOCK` which seems to be not required anymore in combination with `memlock=-1:-1`. Update production notes in docs for Docker >>> 1"
1599,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThis PR focusses on lists/maps that get instantiated with the default constructor, only have them be filled up with `addAll`/`putAll` the very next line. Most List/Map classes have a constructor where they accept an existing List/Map.\r\n\r\nThis avoids creating a list with default size, only to have it expand immediately afterward.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. In case of addAll/putAll to List/Map, use constructor that accepts an existing List/Map instead. >>> 1"
1600,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThere is a single file with a large diff in this PR. I'm going to assume that Intellij did this in order to enforce the `.editorconfig` settings, which it picks up on automatically.\r\n\r\nThe file in question has a lot of trailing whitespace, which has now been removed, It seems this is not picked up by checkstyle. So if this isn't an issue, I'll revert those changes and focus only on the string concatenation.\r\n\r\n\r\nAs the title states, this PR is about code that does `field + """"` in order to get a string representation of said field. This was only necessary before Java 1.5 if I recall correctly.\r\n\r\nAlso, situations where primitive types are concatenated with an empty string have been replaced since this is not as performant as calling `String.valueOf()` on the field.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Concattenating empty strings for the purpose of getting a string representation has poor performance. Replacing where necessary. >>> 1"
1601,This commit upgrades the Netty dependency from version 4.1.9.Final to version 4.1.10.Final.\r\n Upgrade Netty to 4.1.10.Final >>> 1
1602,"Netty uses the number of processors for sizing various resources (e.g., thread pools, buffer pools, etc.). However, it uses the runtime number of available processors which might not match the configured number of processors as set in Elasticsearch to limit the number of threads (e.g., in Docker containers). A new feature was added to Netty that enables configuring the number of processors Netty should see for sizing this various resources. This commit takes advantage of this feature to set this number of available processors to be equal to the configured number of processors set in Elasticsearch.\r\n\r\nRelates netty/netty#6224 Set available processors for Netty >>> 1"
1603,`IndicesAliasesRequest#concreteAliases` has to do with how the transport action may or may not resolve wildcards expressions to aliases names. It is only needed in `TransportIndicesAliasesAction` and for this reason it should be a private method in it rather than part of a request class which is also part of the Java API and later in the high level REST client. Move IndicesAliasesRequest#concreteAliases to TransportIndicesAliasesAction >>> 1
1604,"In #22267, we introduced the notion of incompatible snapshots in a\r\nrepository, and they were stored in a root-level blob named\r\n`incompatible-snapshots`.  If there were no incompatible snapshots in\r\nthe repository, then there was no `incompatible-snapshots` blob.\r\n\r\nHowever, this causes a problem for some cloud-based repositories,\r\nbecause if the blob does not exist, the cloud-based repositories may\r\nattempt to keep retrying the read of a non-existent blob with\r\nexpontential backoff until giving up.  This causes performance issues\r\n(and potential timeouts) on snapshot operations because getting the\r\n`incompatible-snapshots` is part of getting the repository data (see\r\nRepositoryData#getRepositoryData()).\r\n\r\nThis commit fixes the issue by creating an empty\r\n`incompatible-snapshots` blob in the repository if one does not exist. Ensure every repository has an incompatible-snapshots blob >>> 1"
1605,"This change makes the request builder code-path same as `Client#execute`. The request builder used to return a `ListenableActionFuture` when calling execute, which allows to associate listeners with the returned future. For async execution though it is recommended to use the `execute` method that accepts an `ActionListener`, like users would do when using `Client#execute`.\r\n\r\nRelates to #24412\r\nRelates to #9201 Java api: ActionRequestBuilder#execute to return a PlainActionFuture >>> 1"
1606,"This adds `-XX:-OmitStackTraceInFastThrow` to the JVM arguments\r\nwhich *should* prevent the JVM from omitting stack traces on\r\ncommon exception sites. Even though these sites are common, we'd\r\nstill like the exceptions to debug them.\r\n\r\nThis also adds the flag when running tests and adapts some tests\r\nthat had workarounds for the absense of the flag.\r\n\r\nCloses #24376\r\n Try to convince the JVM not to lose stacktraces >>> 1"
1607,"RemoteClusterService is an internal service that should not necessarily be exposed\r\nto plugins or other parts of the system. Yet, for cluster name parsing for instance\r\nit is crucial to reuse some code that is used for the RemoteClusterService. This\r\nchange extracts a base class that allows to share the settings related code as well\r\nas cluster settings updates to `search.remote.*` to be observed by other services.\r\n Extract a common base class to allow services to listen to remote cluster config updates >>> 1"
1608,This change upgrade elasticsearch to Lucene 6.5.1 and removes the fork of LRUQueryCache needed to fix a bug present in Lucene 6.5.0. Upgrade to Lucene 6.5.1 >>> 1
1609,"Open/Close index api have allow_no_indices set to false by default, while delete index has it set to true. The flag controls whether a wildcard expression that matches no indices will be ignored or an error will be thrown instead. This commit aligns open/close default behaviour to that of delete index.\r\n\r\nThis is a followup to #24031. Also related to #24341. Open/Close index api to allow_no_indices by default >>> 1"
1610,TransportService and RemoteClusterService are closely coupled already today\r\nand to simplify remote cluster integration down the road it can be a direct\r\ndependency of TransportService. This change moves RemoteClusterService into\r\nTransportService with the goal to make it a hidden implementation detail\r\nof TransportService in followup changes.\r\n Move RemoteClusterService into TransportService >>> 1
1611,"This is a follow-up to #24317, which did the hard work but was merged in such a\r\nway that it exposes the setting while still allowing indices to have multiple\r\ntypes by default in order to give time to people who test against master to\r\nadd this setting to their index settings. Enforce at most one type. >>> 1"
1612,Currently the only implementation of `ListenableActionFuture` requires\r\ndispatching listener execution to a thread pool. This commit renames\r\nthat variant to `DispatchingListenableActionFuture` and converts\r\n`AbstractListenableActionFuture` to be a variant that does not require\r\ndispatching. That class is now named `PlainListenableActionFuture`.\r\nBoth variants are created from static methods methods on \r\n`PlainListenableActionFuture`. Add non-dispatching listenable action future >>> 1
1613,"Unlike significant_terms this aggregation doesn't require fielddata=true to work on text fields, instead re-analyzing matching document contents. It is  recommended to be used with a parent `sampler` agg to limit expense of tokenizing docs. It can take an optional `filter_duplicate_text`:true setting to avoid stats skew from repeated sections of text in search results (a common problem with many document types).\r\n\r\nToken streams are sourced either from re-analyzing `_source` fields or loading from `term_vectors` if available. When duplicate-sequence filtering is required on token streams each token is hashed and moduloed by 256 to give a single byte which is added into DuplicateByteSequenceSpotter. This class maintains an efficient trie structure to detect sequences of 6 or more bytes/tokens that have been seen before in-sequence.\r\n\r\nCloses #23674 SignificantText aggregation - like significant_terms, but for text >>> 1"
1614,Async shard fetching only uses the node id to correlate responses to requests. This can lead to a situation where a response from an earlier request is mistaken as response from a new request when a node is restarted. This commit adds unique round ids to correlate responses to requests.\r\n\r\nRelates to #24007 Discard stale node responses from async shard fetching >>> 1
1615,"We often want the JVM arguments used for a running instance of Elasticsearch. It sure would be nice if these came as part of the nodes API, or any API that includes JVM info. This commit causes these arguments to be displayed.\r\n Show JVM arguments >>> 1"
1616,"`SearchResponseSections` is the common part extracted from `InternalSearchResponse` that can be shared between high level REST and elasticsearch. The only bits left out are around serialization which is not supported. This way it can accept `Aggregations` as a constructor argument, without requiring `InternalAggregations`, as the high level REST client uses its own objects for aggs parsing rather than internal ones.\r\n\r\nThis change also makes Aggregations implement ToXContent, and Aggregation extend ToXContent. Especially the latter is suboptimal but the best solution that allows to share as much code as possible between core and the client, that doesn't require messing with generics and making the api complicated. Also it doesn't have downsides as all of the current implementations of Aggregation do implement ToXContent already.\r\n\r\nThe idea is that with this change we can go back to #22533, add `fromXContent` to `SearchResponse`, where we will be able to return a search response created by passing in an instance of `SearchResponseSections` rather than `InternalSearchResponse`. `SearchResponseSections` holds `Aggregations` rather than `InternalAggregations`, which allows the high level REST client not to use `InternalAggregation` instances for aggs parsing, rather `ParsedAggregation` Introduce SearchResponseSections base class >>> 1"
1617,"Today we only lookup nodes by their ID but never by the (clusterAlias, nodeId) tuple.\r\nThis could in theory lead to lookups on the wrong cluster if there are 2 clusters\r\nwith a node that has the same ID. It's very unlikely to happen but we now can clearly\r\ndisambiguate between clusters and their nodes. Preserve cluster alias throughout search execution to lookup nodes by cluster and ID >>> 1"
1618,These settings are deprecated in 5.5. This change removes them for 6.0.\r\n Remove deprecated S3 settings >>> 1
1619,The `DiscoveryNodesProvider` class provides an unnecessary abstraction and is just used in conjunction with the existing `PingContextProvider` class. Remove DiscoveryNodesProvider >>> 1
1620,"Reuses #21907 and rewired multi search template api to delegate to multi search api, that the `max_concurrent_searches` parameter can just be pushed down to the multi search api instead of duplicating multi concurrent search logic in multi search template api.\r\n\r\nPR for #20912 Add max concurrent searches to multi template search >>> 1"
1621,This commit adds support for indexing and searching the new `ip_range` field type. Both IPv4 and IPv6 formats are supported. Tests and docs are updated. Add new ip_range field type >>> 0
1622,"This is a WIP PR for adding parsing to all implementations of SingleBucketAggregation. They are mostly similar internally, so I did them all in one PR because the changes in the individual parsed aggregation implementations are minimal and they also share common testing infrastruture.\r\n\r\nSince this requires parsing of inner aggregations that are not rendered inside their own json object structure I had to base this on #24219 which is open and it is not sure if we are going to use it in the current form, so I leave this PR as work in progress for discussion. It shows that the approach taken in #24219 solves the parsing problem at least for the single bucket aggs. There are a few minor TODOs in the code around extending testing and randomization but I leave them for later when we know which way we are taking. WIP: Add parsing single bucket >>> 0"
1623,"With Fedora-25 available since 2016-11-22, it's time to switch to the corresponding Vagrant box for the packaging tests as well. Tests: Switch to Fedora-25 for packaging tests >>> 1"
1624,To support kibana this commit adds an internal optimization\r\nto support the cross cluster syntax for indices on the `_field_caps`\r\nAPI.\r\n\r\nCloses #24334\r\n Add cross cluster support to `_field_caps` >>> 1
1625,"This commit revises the issue template, hoping to make it clearer that following the guidelines is a really good thing to do.\r\n Revise issue template >>> 1"
1626,"In pre-release versions of Elasticsearch 5.0.0, users were subject to log messages of the form ""your platform does not.*reliably.*potential system instability"". This is because we disable Netty from being unsafe, and Netty throws up this scary info-level message when unsafe is unavailable, even if it was unavailable because the user requested that it be unavailabe. Users were rightly confused, and concerned. So, we contributed a guard to Netty to prevent this log message from showing up when unsafe was explicitly disabled. This guard shipped with all versions of Netty that shipped starting with Elasticsearch 5.0.0. Unfortunately, this guard was lost in an unrelated refactoring and now with the 4.1.10.Final upgrade, users will again see this message. This commit is a hack around this until we can get a fix upstream again.\r\n\r\nRelates netty/netty#5624, netty/netty#6568\r\n You had one job Netty logging guard >>> 1"
1627,Implements the common enum serialization/deserialization pattern for enumeration on the StreamInput/StreamOutput.\r\n Add StreamInput.readEnum and StreamOutput.writeEnum >>> 1
1628,Closes #23435 [DOCS] Fixes the documentation on leading forward slashes in the base_path of S3 repositories >>> 1
1629,"Currently, the get snapshots API (e.g. /_snapshot/{repositoryName}/_all)\r\nprovides information about snapshots in the repository, including the\r\nsnapshot state, number of shards snapshotted, failures, etc.  In order\r\nto provide information about each snapshot in the repository, the call\r\nmust read the snapshot metadata blob (`snap-{snapshot_uuid}.dat`) for\r\nevery snapshot.  In cloud-based repositories, this can be expensive,\r\nboth from a cost and performance perspective.  Sometimes, all the user\r\nwants is to retrieve all the names/uuids of each snapshot, and the\r\nindices that went into each snapshot, without any of the other status\r\ninformation about the snapshot.  This minimal information can be\r\nretrieved from the repository index blob (`index-N`) without needing to\r\nread each snapshot metadata blob.\r\n\r\nThis commit enhances the get snapshots API with an optional `verbose`\r\nparameter.  If `verbose` is set to false on the request, then the get\r\nsnapshots API will only retrieve the minimal information about each\r\nsnapshot (the name, uuid, and indices in the snapshot), and only read\r\nthis information from the repository index blob, thereby giving users\r\nthe option to retrieve the snapshots in a repository in a more\r\ncost-effective and efficient manner.\r\n\r\nCloses #24288 Enhances get snapshots API to allow retrieving repository index only >>> 1"
1630,"Reformatted the ISSUE TEMPLATE to use shorter sentences, better spacing, easier layout. Rewrote the github issue template to be shorter and more likely to be read >>> 1"
1631,Changes the scope of the AllocationService dependency injection hack so that it is at least contained to the AllocationService and does not leak into the Discovery world. Limit scope of AllocationService dependency injection hack >>> 1
1632,`_search_shards` API today only returns aliases names if there is an alias\r\nfilter associated with one of them. Now it can be useful to see which aliases\r\nhave been expanded for an index given the index expressions. This change also includes non-filtering aliases even without a filtering alias being present.\r\n Include all aliases including non-filtering in  `_search_shards` response >>> 1
1633,The `-XX:-OmitStackTraceInFastThrow` flag is only required by Painless's\r\ntests so we'll only set it there. This is much simpler.\r\n Move flag to painless tests >>> 1
1634,We need to update the node definitions for the new ML functionality [DOCS] Add ML node to node.asciidoc >>> 1
1635,"We start the test JVMs with various options. There are two that we should remove, they do not make sense.\r\n - we require Java 8 yet there was a conditional build option for Java 7\r\n - we do not set MaxDirectMemorySize in our default JVM options, we should not in the test JVMs either Remove obsolete JVM options from build >>> 1"
1636,This change will expand the shard level request to the actual concrete index or to the aliases\r\nthat expanded to the concrete index to ensure shard level requests won't see wildcard expressions\r\nas their original indices Expand cross cluster search indices for search requests to the concrete index or to it's aliases >>> 1
1637,"This commit changes the `Terms.Bucket` abstract class to an interface, so that it's easier for the Java High Level Rest Client to provide its own implementation.\r\n\r\nIn its current state, the Terms.Bucket abstract class inherits from InternalMultiBucketAggregation.InternalBucket which forces subclasses to implement `Writeable` and to expose a public `getProperty()` method that relies on InternalAggregation. This two points make it difficult for the Java High Level Rest Client to implement the Terms and Terms.Bucket correctly. This is also different from other MultiBucketsAggregation like Range which are pure interfaces.\r\n\r\nSadly, changing Terms.Bucket to an interface caused a method clash for the\r\n`getBuckets()` method in InternalTerms because:\r\n - InternalTerms implements Terms which declared a `List<Terms.Bucket> getBuckets()` method\r\n - InternalTerms extends InternalMultiBucketAggregation which declares a  `List<? extends InternalBucket> getBuckets()` method\r\nand both overrides the MultiBucketsAggregation `List<? extends Bucket> getBuckets()` method.\r\n\r\nThere was no clashes before this change because Terms.Bucket extends  InternalBucket and conformed to both declarations. With Terms.Bucket now an interface, this commit changes the getBuckets() method in the Terms interface so that it now returns `List<? extends Bucket>` instead of `List<Terms.Bucket>`. I didn't see a better way to do this.\r\n\r\nThis is a breaking change in the Java API but it's a straightforward change and the Terms multi bucket aggregation interface is also more coherent with the other Range, Histogram, Filters, AdjacencyMatrix etc that all return a `List<? extends Bucket>`. Make Terms.Bucket an interface rather than an abstract class >>> 1"
1638,If a field caps request contains a field name that doesn't exist in all indices\r\nthe response will be partial and we hide an NPE. The NPE is now fixed but we still\r\nhave the problem that we don't pass on errors on the shard level to the user. This will\r\nbe fixed in a followup.\r\n Fix NPE if field caps request has a field that exists not in all indices >>> 1
1639,just a reminder to reenable the tests once snapshots are available. [TEST] Reenable disabled tests for _field_caps and _search_shards >>> 1
1640,Ensure that getRepositoryData() is only called once during a list snapshots operation. Fixes #24509. Fix inefficient (worst case exponential) loading of snapshot repository >>> 1
1641,Minor change in the shorthand form text. Update completion-suggest.asciidoc >>> 1
1642,"If a node in version >= 5.3 acts as a coordinating node during a scroll request that targets a single shard, the scroll may return the same documents over and over iff the targeted shard is hosted by a node with a version <= 5.3.\r\nNodes in this version will advance the scroll only if the search_type has been set to `query_and_fetch` though this search type has been removed in 5.3.\r\nThis change handles this situation by adding the removed `search_type` in the request that targets a node in version <= 5.3. Fix single shard scroll within a cluster with nodes in version `>= 5.3` and `<= 5.3` >>> 1"
1643,"Now that indices have a single type by default, we can move to the next step\r\nand identify documents using their `_id` rather than the `_uid`.\r\n\r\nOne notable change in this commit is that I made deletions implicitly create\r\ntypes. This helps with the live version map in the case that documents are\r\ndeleted before the first type is introduced. Otherwise there would be no way\r\nto differenciate `DELETE index/foo/1` followed by `PUT index/foo/1` from\r\n`DELETE index/bar/1` followed by `PUT index/foo/1`, even though those are\r\ndifferent if versioning is involved. Identify documents by their `_id`. >>> 1"
1644,"Today we rely on background syncs to relay the global checkpoint under the mandate of the primary to its replicas. This means that the global checkpoint on a replica can lag far behind the primary. The commit moves to inlining global checkpoints with replication requests. When a replication operation is performed, the primary will send the latest global checkpoint inline with the replica requests. This keeps the replicas closer in-sync with the primary.\r\n\r\nHowever, consider a replication request that is not followed by another replication request for an indefinite period of time. When the replicas respond to the primary with their local checkpoint, the primary will advance its global checkpoint. During this indefinite period of time, the replicas will not be notified of the advanced global checkpoint. This necessitates a need for another sync. To achieve this, we perform a global checkpoint sync when a shard falls idle.\r\n\r\nRelates #10708\r\n Inline global checkpoints >>> 1"
1645,"Template script engines (mustache, the only one) currently return a\r\nBytesReference that users must know is utf8 encoded. This commit\r\nmodifies all callers and mustache to have the template engine return\r\nString. This is much simpler, and does not require decoding in order to\r\nuse (for example, in ingest). Scripts: Convert template script engines to return String instead of BytesReference >>> 1"
1646,"This starts breaking up the `UpdateHelper.prepare` method so that each piece can\r\nbe individually unit tested. No actual functionality has changed.\r\n\r\nNote however, that I did add a TODO about `ctx.op` leniency, which I'd love to\r\nremove as a separate PR if desired. Refactor UpdateHelper into unit-testable pieces >>> 1"
1647,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nI'm busy adding list sizes when they can be reliably determined for use in the constructor.\r\n\r\nI made this part 1 because there's quite a lot of these to go over in comparison to the amount that will be submitted. There are another 600-ish results to go over, but many will be class-level variables for which no reliable prediction can be done.\r\n\r\nDepending on the choice of the reviewer(s), I can either make another PR for part 2 or just add it here.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Part 1 of reviewing all List constructors and adding a starting size >>> 1"
1648,"This PR introduces a subproject in `test/fixtures` that contains a Vagrant file used for standing up a KRB5 KDC (Kerberos), includes helper scripts for provisioning principals, a few changes to the HDFS Fixture to allow it to interface with the KDC, as well as a new suite of integration tests for the HDFS Repository plugin.\r\n\r\nThe HDFS Repository plugin senses if the local environment can support the HDFS Fixture (Windows is generally a restricted environment). If it can use the fixture generally, it tests if Vagrant is installed with a compatible version to determine if the secure test fixtures should be enabled. If the secure tests are enabled, then we create a Kerberos KDC fixture, tasks for adding the required principals, and an HDFS fixture configured for security. A new integration test task is also configured to use the KDC and secure HDFS fixture and to run a testing suite that uses authentication. At the end of the secure integration test the fixtures are torn down. Introduce Kerberos Test Fixture for Repository HDFS Security Tests >>> 1"
1649,"Pinging @jasontedor as requested. Related to PR #24321 \r\n\r\nThis PR involves an array being printed with the default toString, which will result in seeing stuff like `[B@123oiub...`\r\n\r\nThere's a toString method in `java.util.Arrays` for the exact purpose of printing the actual content of an array, so that's what this PR introduces to that code.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Some error messages use Object#toString to print arrays. >>> 1"
1650,"I stumbled on this code today and I hated it; I wrote it. I did not like that you could not tell at a glance whether or not the method parameters were correct. This commit fixes it. TimeValue#parseTimeValue author is bad, feels bad >>> 1"
1651,nan Document work-around for jar hell in idea_rt.jar file >>> 1
1652,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nA field may be static if it is declared final, and is initialized with a constant, bringing a small performance gain.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Make class level fields static where applicable >>> 1"
1653,"When installing plugin permissions, we try to set the permissions on all installed files ourselves because a umask from the user could violate everything needed to get the permissions right. Sadly, directories were not handled correctly at all and so we were still left with broken installations with umasks like 0077. This commit fixes this issue, adds a thorough unit test for the situation, and most importantly, adds a test that sets the umask before installing the plugin.\r\n\r\nCloses #24480 Fix plugin installation permissions >>> 1"
1654,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThis PR focusses on places in the code where a keySet() is used to loop over a `Map`. This is inefficient when both the key and the value of the are required at the same time. When using a keySet(), getting the value that corresponds to the key requires that you perform a `map.get(key)`. This construct finds the value by looping over all the keys until it finds the index of the key that was specified, and then returns the value with the same index.\r\n\r\nIn other words: it has to iterate the map every time you ask it for a value. That's up to twice the work per key/value pair.\r\n\r\nThe entrySet() method returns both key and value in the same construct, removing the need to start another search for the value.\r\n\r\nCheckstyle acted up about the changes, I had to keep lines under 140 characters, so that's why there are some newlines in 2 of the files.\r\n\r\n===========\r\n\r\n1 .Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Use entrySet to loop over a map instead of keySet >>> 0"
1655,"This is a backport-veresion of the update to 6.0 that defaults this new behavior: #23174.\r\n\r\nI'd like to merge this in first, then remove the setting in master as a follow-up\r\n\r\nPreviously, Mustache would call `toString` on the `_ingest.timestamp`\r\nfield and return a date format that did not match Elasticsearch's\r\ndefaults for date-mapping parsing. The new ZonedDateTime class in Java 8\r\nhappens to do format itself in the same way ES is expecting.\r\n\r\nFixes #23168.\r\n\r\nThis new fix can be found in the form of a cluster setting called\r\n`ingest.new_date_format`. By default, in 5.x, the existing behavior\r\nwill remain the same. One will set this property to `true` in order to\r\ntake advantage of this update for ingest-pipeline convenience. add option for _ingest.timestamp to use new ZonedDateTime (5.x backport) >>> 1"
1656,"Today when opening the engine we skip gaps in the history, advancing the local checkpoint until it is equal to the maximum sequence number contained in the commit. This allows history to advance, but it leaves gaps. A previous change filled these gaps when recovering from store, but since we were skipping the gaps while opening the engine, this change had no effect. This commit removes the gap skipping when opening the engine allowing the gap filling to do its job.\r\n\r\nRelates #10708, relates #24238\r\n Remove gap skipping when opening engine >>> 1"
1657,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\nHi, this is a mirror PR which removed some duplicate imports. And also I suggest we remove all unused imports but it's a big change. Happy to hear your comments. Thanks.\r\n Remove duplicate imports >>> 0"
1658,"In order to make MockLogAppender (utility to test logging) available outside\r\nof es-core move MockLogAppender from core tests to test framework. As\r\nthe package name does not change, no further changes required. Move MockLogAppender to elasticsearch test >>> 1"
1659,This pull request adds parsing methods for Long/Double/String terms aggregations Add parsing for String/Long/Double Terms aggregations >>> 1
1660,"This commit fixes a bug in the cache expire after access implementation. The bug is this: if you construct a cache with an expire after access of T, put a key, and then touch the key at some time t > T, the act of getting the key would update the access time for the entry before checking if the entry was expired. There are situations in which expire after access would be honored (e.g., if the cache needs to prune the LRU list to keep the cache under a certain weight, or a manual refresh was called) but this behavior is otherwise broken. Fix cache expire after access >>> 1"
1661,"When multiple bootstrap checks fail, it's not clear where one error message begins and the next error message ends. This commit numbers the bootstrap check error messages so they are easier to read.\r\n Improve bootstrap checks error messages >>> 1"
1662,"Fix for issue #24422\r\n\r\nNote that I modified a test in `DocumentParserTests` as it was using the field ""_ttl"" to check the rejection of metadata fields which is still in `META_FIELDS`, however not in `mapperRegistry.getMetadataMapperParsers` which seems correct to me given this [link](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-fields.html)\r\n\r\nOne last thing is I did not remove the `META_FIELDS` usage as for instance it used in [GetField](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/get/GetField.java#L68) and I am not sure how to grab correctly a reference to a `MapperService`.\r\n\r\n Modifying metadata checking in document parser to use mapperRegistry  >>> 0"
1663,File scripts will be removed in 6.0. This commit adds a deprecation\r\nwarning for 5.5 when the first file script is loaded.\r\n\r\nrelates #21798 Scripting: Deprecate file scripts >>> 1
1664,"AggregationsTests#testFromXContent verifies that parsing of aggregations works by combining multiple aggs at the same level, and also adding sub-aggregations to multi bucket and single bucket aggs, up to a maximum depth of 5. [TEST] Add test for Aggregations#fromXContent >>> 1"
1665,"Adds tests for reindex-from-remote for the latest 2.4, 1.7, and\r\n0.90 releases. 2.4 and 1.7 are fairly popular versions but 0.90\r\nis a point of pride.\r\n\r\nThis fixes any issues those tests revealed.\r\n\r\nCloses #23828\r\nCloses #24520 Add qa module that tests reindex-from-remote against pre-5.0 versions of Elasticsearch >>> 1"
1666,This is an alternative approach to #24386 that doesn't rely on extending ObjectParsers capabilities and uses manual parsing instead. Add parsing for single bucket aggregations >>> 1
1667,"File scripts have 2 related settings: the path of file scripts, and\r\nwhether they can be dynamically reloaded. This commit deprecates those\r\nsettings.\r\n\r\nrelates #21798 Scripting: Deprecate file script settings >>> 1"
1668,As the title says.  Will be replaced with the settings defined in #24532 in the next PR. Deprecate Fine Grain Settings for Scripts >>> 1
1669,There are now three public static method to build instances of\r\nPreConfiguredTokenFilter and the ctor is private. I chose static\r\nmethods instead of constructors because those allow us to change\r\nout the implementation returned if we so desire.\r\n\r\nAlso moves over two more pre-configured token filters. There still\r\na bunch to go!\r\n\r\nRelates to #23658 Make PreConfiguredTokenFilter harder to misuse >>> 1
1670,"This commit renames ScriptEngineService to ScriptEngine.  It is often\r\nconfusing because we have the ScriptService, and then\r\nScriptEngineService implementations, but the latter are not services as\r\nwe see in other places in elasticsearch. Scripting: Remove ""service"" from ScriptEngine interface name >>> 1"
1671,Also moved InternalAggregationTestCase to test-framework module in order to make use of it from other modules.\r\n\r\nRelates to #22278 Added unit tests for InternalMatrixStats >>> 1
1672,Related to #22278 [Test] Add unit tests for Range aggregations >>> 1
1673,"This commit terminates any controller processes plugins might have after\r\nthe node has been closed.  This gives the plugins a chance to shut down their\r\ncontrollers gracefully.\r\n\r\nPreviously there was a race condition where controller processes could be shut\r\ndown gracefully and terminated by two threads running in parallel, leading to\r\nnon-deterministic outcomes.\r\n\r\nAdditionally, controller processes that failed to shut down gracefully were\r\nnot forcibly terminated when running as a Windows service; there was a reliance\r\non the plugin to shut down its controller gracefully in this situation.\r\nThis commit also fixes this problem. Avoid race when shutting down controller processes >>> 1"
1674,This allows other plugins to use a client to call the functionality\r\nthat is in the core modules without duplicating the logic.\r\nPlugins can now safely send the request and response classes via the\r\nclient even if the requests are executed locally. All relevant classes\r\nare loaded by the core classloader such that plugins can share them. Move DeleteByQuery and Reindex requests into core >>> 1
1675,"Previously query weight was created for each search hit that needed to compute inner hits,\r\n with this change the weight of the inner hit query is computed once for all search hits.\r\n\r\nDepending on how expensive it is to rewrite and create the weight for the `nested`  query's (or `has_child` and `has_parent`) inner query, this optimization can yield quite a nice improvement in the overall query time. For example the rewrite and weight creation of range queries is cheap, while for bool with many term based clauses, phrase query or phrase prefix query this is not, and when these queries will benefit from this change. \r\n\r\nPR for #23917 Reuse inner hit query weight >>> 1"
1676,Relates to #19390 Remove deprecated template query >>> 1
1677,This PR adds the parsing logic for the InternalGeoHashGrid aggregation. Add parsing method to GeoHashGrid aggregation >>> 1
1678,"This pull request adds the parsing logic to InternalRange, InternalDateRange and InternalGeoDistance aggregations. Add parsing methods to Range aggregations >>> 1"
1679,"It was using the wrong version, which can cause errors like\r\n\r\n```\r\n  1> java.security.AccessControlException: access denied (""java.net.SocketPermission"" ""[0:0:0:0:0:0:0:1]:34221"" ""connect,resolve"")\r\n  1> \tat java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) ~[?:1.8.0_111]\r\n  1> \tat java.security.AccessController.checkPermission(AccessController.java:884) ~[?:1.8.0_111]\r\n  1> \tat java.lang.SecurityManager.checkPermission(SecurityManager.java:549) ~[?:1.8.0_111]\r\n  1> \tat java.lang.SecurityManager.checkConnect(SecurityManager.java:1051) ~[?:1.8.0_111]\r\n  1> \tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:625) ~[?:?]\r\n  1> \tat org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processSessionRequests(DefaultConnectingIOReactor.java:273) ~[httpcore-nio-4.4.5.jar:4.4.5]\r\n  1> \tat org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:139) ~[httpcore-nio-4.4.5.jar:4.4.5]\r\n  1> \tat org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:348) ~[httpcore-nio-4.4.5.jar:4.4.5]\r\n  1> \tat org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:192) ~[httpasyncclient-4.1.2.jar:4.1.2]\r\n1> at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64) ~[httpasyncclient-4.1.2.jar:4.1.2]\r\n```\r\n\r\nWhen running tests\r\n Fix SocketPermission in test framework for alpha2 bump >>> 1"
1680,Close #24581  Deprecate tribe service >>> 1
1681,"This commit documents how to write a `ScriptEngine` in order to use\r\nexpert internal apis, such as using Lucene directly to find index term\r\nstatistics. These documents prepare the way to remove both native\r\nscripts and IndexLookup.\r\n\r\nThe example java code is actually compiled and tested under a new gradle\r\nsubproject for example plugins. This change does not yet breakup\r\njvm-example into the new examples dir, which should be done separately.\r\n\r\nrelates #19359\r\nrelates #19966 Scripting: Replace advanced and native scripts with ScriptEngine docs >>> 1"
1682,Machine permission can be updated without deleting instances. Machine Permission update on Google Compute >>> 1
1683,"This was introduced in #24460: the constructor of `Translog.Delete` that takes\r\na `StreamInput` does not set the type and id. To make it a bit more robust, I\r\nmade fields final so that forgetting to set them would make the compiler\r\ncomplain.\r\n\r\nTagging as a `non-issue` since the bug is not released. `type` and `id` are lost upon serialization of `Translog.Delete`. >>> 1"
1684,"This change allows plugins to use the test class.  The only material, and potentially controversial, change is that `build()` had to be made public instead of protected... but the sole user of that API\r\n(`AbstractAggregatorBuilder`) already exposed it as public so I think this is ok.\r\n\r\nEverything else is just automated formatting and some unused import cleanup.\r\n\r\n/cc @colings86 @martijnvg  Move AggregatorTestCase to test framework >>> 0"
1685,"Specifying s3 access and secret keys inside repository settings are not\r\nsecure. However, until there is a way to dynamically update secure\r\nsettings, this is the only way to dynamically add repositories with\r\ncredentials that are not known at node startup time. This commit adds\r\nback `access_key` and `secret_key` s3 repository settings, but protects\r\nit with a required system property `allow_insecure_settings`. S3 Repository: Add back repository level credentials >>> 1"
1686,Support multiple named inner hits on a field collapsing request.  This change is backward compatible. Support Multiple Inner Hits on a Field Collapse Request >>> 0
1687,We allow non-dynamic settings to be updated on closed indices but we don't\r\ncheck if the updated settings can be used to open/create the index.\r\nThis can lead to unrecoverable state where the settings are updated but the index\r\ncannot be reopened since the settings are not valid. Trying to update the invalid settings\r\nis also not possible since the update will fail to validate the current settings.\r\nThis change adds the validation of the updated settings for closed indices and make sure that the new settings do not prevent the reopen of the index.\r\n\r\nFixes #23787 Validates updated settings on closed indices >>> 1
1688,"Flake ids organize bytes in such a way that ids are ordered. However, we do not\r\nneed that property and could reorganize bytes in an order that would better suit\r\nLucene's terms dict instead.\r\n\r\nSome synthetic tests suggest that this change decreases the disk footprint of\r\nthe `_id` field by about 50% in many cases (see `UUIDTests.testCompression`).\r\nFor instance, when simulating the indexing of 10M docs at a rate of 10k docs\r\nper second, the current uid generator used 20.2 bytes per document on average,\r\nwhile this new generator which only puts bytes in a different order uses 9.6\r\nbytes per document on average.\r\n\r\nWe had already explored this idea in #18209 but the attempt to share long common\r\nprefixes had had a bad impact on indexing speed. This time I have been more\r\ncareful about putting discriminant bytes early in the `_id` in a way that\r\npreserves indexing speed on par with today, while still allowing for better\r\ncompression. Optimize the order of bytes in uuids for better compression. >>> 1"
1689,"This PR allows Hotspot's escape analysis to work correctly, preventing us from creating many instances stressing GC.\r\n\r\nI was talking with @forax two days ago on the Jax 2017 Conference (where we had a panel discussion about Java 9). We discussed about Hotspot internals and the `LambdaMetaFactory`'s implementation details. He was very impressed about the work of @jdconrad and @rmuir (and my work) on implementing painless. FYI: He suggest that we should have a blog post about painless in regards to lambdas, but he was also impressed about the fact that we also use Java 9's indified string concats (as first scripting language in the wild). He also liked our PIC, MIC implementations. I will open a separate PR, because he suggested to invert the PIC guardWithTest chain to have the first receiver on top of all guards (as the first seen type is likely the one you see most often).\r\n\r\nAlso, the `MethodHandle#asType()` invokedynamic was in Rémi's original implementation of `LambdaMetaFactory` (and in reality it should do the same). So Rémi thinks that the recent changes in LambdaMetaFactory of Java 9 are buggy! We should really open a bug report with a test case: **@jdconrad your turn!!! Please do this!** The OpenJDK LambdaMetaFactory instead implements all the conversions for performance reasons for the case where you have a lambda only called once (happens very often in Java code). This does not affect us, as our lambdas are called on every document of an index :-) So the asType trick by @jdconrad is great and this is how it should look like - Congrats!\r\n\r\nHe was also able to explain to me, why the OpenJDK meta factory creates a static factory for the capturing lambdas - this is the reason for this PR. Of course, the current code is correct, and in an ideal world it should be like that, BUT: Hotspot is currently unable (even in Java 9) to look through a MethodHandles.findConstructor() method handle and allow escape analysis to work. Because our lambdas have only final fields and each instance does not allow to escape any shit from it, it can optimize away the object creation. But the code in hotspot only works if it sees a real chain of newinstance bytecode folowed up by invokespecial. For the method handle this is not the case, breaking the escape analysis and then it creates a new lambda capturing instance on heap! With the static factory this works, as the bytecode matches the pattern.\r\n\r\nThis PR will fix this by readding the static factory. I reused the same code that I used for newinvokedynamic, so we have no code duplication. I also removed a lot of ""<init>"" hardcoded strings. Optimize instance creation in LambdaBootstrap >>> 1"
1690,"This commit removes file scripts, which were deprecated in 5.5.\r\n\r\ncloses #21798 Scripting: Remove file scripts >>> 1"
1691,If the request asks for the `_source` stored field then don't\r\nduplicate it when forcing the `_source` parameter to onto the\r\nrequest for reindex-from-remote from versions before 1.0.\r\n\r\nCloses #24628\r\n Reindex: don't duplicate _source parameter >>> 1
1692,This change adds a rest test for sliced scroll that checks that the hashing does not use a random seed for the partitioning. Add rest test for sliced scroll >>> 1
1693,"`LegacyGeoPointField` was using the wrong decoding for min/max prefix coded GeoPoint Terms. This PR applies the correct decoding. Note that the min/max values, though, are likely useless anyway since they map to a low resolution morton encoded version of the point; not something that is really of value for field stats.\r\n\r\ncloses #24275  Fix legacy GeoPointField decoding in FieldStats >>> 1"
1694,"- Removes `clusterState`, `getInitialClusterState` and `getMinimumMasterNodes` methods from `Discovery` interface.\r\n- Sets PingContextProvider in ZenPing constructor\r\n- Renames `state` in ZenDiscovery to `committedState` Simplify Discovery interface >>> 1"
1695,"With the current implementation, `SniffNodesSampler` might close the\r\ncurrent connection right after a request is sent but before the response\r\nis correctly handled. This causes to timeouts in the transport client\r\nwhen the sniffing is activated in all versions since #22828.\r\n\r\ncloses #24575\r\ncloses #24557 SniffNodesSampler should close connection after handling responses >>> 1"
1696,"Some of the CI boxes for Elasticsearch have inconsistently configured environments when it comes to Vagrant. Vagrant requires $HOME to be set, as well as VirtualBox to be installed. This PR checks those two things are present before enabling tests with VagrantFixtures in the HDFS Repo (only place vagrant fixtures are used right now). Sense for VirtualBox and $HOME when deciding to turn on vagrant testing. >>> 1"
1697,Relates to #22278 [Tests] Add unit tests for InternalFilters >>> 1
1698,According to https://github.com/elastic/elasticsearch/pull/23767/files/7b70704ab45eb3ba98e0c31ce4eb8e65dd963381#r109564828 `getTook()` methods should be removed from `BulkResponse` and `SearchResponse` as there is an alternative `getTookInMillis()` which can be used.\r\n\r\nEdit : use `getTook()`  instead of `getTookInMillis()` Removing unneeded getTookInMillis method >>> 1
1699,"This change adds a new module named parent-join.\r\nThe goal of this module is to provide a replacement for the _parent field but as a first step this change only moves the has_child, has_parent queries and the children aggregation to this module.\r\nThese queries and aggregations are no longer in core but they are deployed by default as a module.\r\n\r\nRelates #20257 Add parent-join module >>> 1"
1700,Closes #24515 Deprecated use of + in index expressions >>> 1
1701,This keeps failing the build so I am temporarily disabling it\r\nuntil #24636 gets merged.\r\n\r\n\r\n [TEST] Temporarily disable the secure fixture for hdfs tests >>> 1
1702,Today we prune transport handlers in TransportService when a node is disconnected.\r\nThis can cause connections to starve in the TransportService if the connection is\r\nopened as a short living connection ie. without sharing the connection to a node\r\nvia registering in the transport itself. This change now moves to pruning based\r\non the connections cache key to ensure we notify handlers as soon as the connection\r\nis closed for all connections not just for registered connections.\r\n\r\nRelates to #24632\r\nRelates to #24575\r\nRelates to #24557 Notify onConnectionClosed rather than onNodeDisconnect to prune transport handlers >>> 1
1703,"Currently a `delete document` request against a non-existing index actually **creates** this index.\r\n\r\nWith this change the `delete document` no longer creates the previously non-existing index and throws an `index_not_found` exception instead.\r\n\r\nHowever as discussed in https://github.com/elastic/elasticsearch/pull/15451#issuecomment-165772026, if an external version is explicitly used, the current behavior is preserved and the index is still created and the document is marked for deletion.\r\n\r\nFixes #15425  If the index does not exist, delete document will not auto create it >>> 1"
1704,This adds parsing to the InternalFilters aggregation. Add parsing for InternalFilters aggregation >>> 1
1705,This commit upgrades the Netty dependency from 4.1.10.Final to 4.1.11.Final. Upgrade to Netty 4.1.11.Final >>> 1
1706,"Netty removed a logging guarded we added to prevent a scary logging message. We added a hack to work around this. They've added the guard back, so we can remove the hack now.\r\n\r\nRelates #24469, relates netty/netty#5624, netty/netty#6568, netty/netty#6696 Remove Netty logging hack >>> 1"
1707,This commit adds a new method to the SearchOperationListener that allows implementers to validate\r\nthe SearchContext immediately after it is retrieved from the active contexts. The listener may\r\nthrow an exception if it deems the SearchContext is not valid and that the use of the context\r\nshould be terminated. Allow SearchOperationListeners to validate a search context >>> 1
1708,This PR adds `ignore_malformed` support to `geo_shape` field types to skip malformed geoJson fields. Tests are updated.\r\n\r\ncloses #23747 Add ignore_malformed to geo_shape fields >>> 1
1709,This commit renames all rest test files to use the .yml extension\r\ninstead of .yaml. This way the extension used within all of\r\nelasticsearch for yaml is consistent.\r\n\r\nsee https://github.com/elastic/elasticsearch/pull/24633#issuecomment-301096471 Tests: Change rest test extension from .yaml to .yml >>> 1
1710,Added a warning to align reader expectations and understanding with our best practises of running 3+ nodes in production.\r\n\r\nhttps://discuss.elastic.co/t/docker-two-node-cluser-use/85442 was the catalyst. Update docker.asciidoc >>> 0
1711,"This PR is related to issue #24226\r\n\r\nThis is part 2, follow-up to part 1, which can be found here: #24439\r\n\r\nI'd like to point out that `Iterable<T>` doesn't have a `size()` method.\r\n\r\nAnd these are the remaining PRs I still have open: #24533 and #24340\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Part 2 of reviewing all list constructors and adding a size where possible. >>> 0"
1712,This converts the deprecation added for 5.5 from .yml to .yaml. Note\r\nthat this deprecation has not been released yet.\r\n\r\nrelates #19391\r\n Settings: Update settings deprecation from yml to yaml >>> 1
1713,"This commit removes the deprecated support for .yaml and .json files. If\r\nthe files still exist, the node will fail to start, indicating the file\r\nmust be converted or renamed.\r\n\r\ncloses #19391 Settings: Remove support for yaml and json config files >>> 1"
1714,"The disruption tests sit in a single test suite which causes these tests to be single-threaded. We can split this test suite into multiple suites (logically, of course) enabling them to be run in parallel reducing the total run time of all integration tests in core. This commit splits the discovery with service disruptions test suite into three suites\r\n - master disruptions\r\n - discovery disruptions\r\n - cluster disruptions\r\n\r\nThe last one could probably be better named, it is meant to represent performing actions in the cluster (indexing, failing a shard, etc.) while a disruption is taking place.\r\n\r\n Split disruption test suite >>> 1"
1715,This commit fixes inconsistent terms aggregation order by ensuring the order contains a tie breaker.\r\nIf needed a tie-breaker (_term asc) is added by using a compound order.\r\n\r\nCloses #23613 for the 5.X branch (backport from #22343).\r\n Fix inconsistent terms aggregation order for 5.x >>> 1
1716,Changing back the InternalSampler type constant that was changed with #24564.\r\nThis PR is against the feature branch used for adding parsing to the aggregations.\r\n Revert changing the InternalSampler type constant >>> 1
1717,This commit changes `SignificantTerms.Bucket` so that it is an interface and not an abstract class anymore. It is more coherent with the others aggregations and it will be easier for the Java High Level Rest Client to provide its own implementation of SignificantTerms and SignificantTerms.Bucket.\r\n\r\nEdit: this PR is similar to what has been done in  #24492 for terms Make SignificantTerms.Bucket an interface rather than an abstract class >>> 1
1718,"After merging in some changes in master, this improves some of the registry setup in InternalAggregationTestCase. Small improvement in InternalAggregationTestCase test setup  >>> 1"
1719,"We generally accept string values when a boolean is expected. We've been doing that in our parsing code, but we missed that bit when moving parsing code to ObjectParser, which throws an error instead. This commit makes ObjectParser parse also string values into booleans. It throws an error in case the value is not `true` or `false`.\r\n\r\nCloses #21802 Make ObjectParser support string to boolean conversion >>> 1"
1720, The `toXContent` methods in String and Long significant terms aggregations and buckets are very similar. They can be factored out in the `InternalSignificantTerms` class an `InternalMappedSignificantTerms` class.\r\n  Share XContent rendering code in significant terms aggregations >>> 1
1721,"When retrieving documents to extract terms from as part of a more like this query, the _routing value can be set, yet it gets lost. That leads to not being able to retrieve the documents, hence more_like_this used to return no matches all the time.\r\n\r\nCloses #23699 Pass over _routing value with more_like_this items to be retrieved >>> 1"
1722,We had a hack in setting up permissions for tests to support testing the lang-python plugin. We also had a hack to prevent Log4j from loading a shaded version of Jansi provided by Jython. This plugin has been removed so these hacks are no longer necessary.\r\n\r\nRelates #20334\r\n Remove Jython hacks >>> 1
1723,"Range queries with now based date ranges were previously not allowed,\r\nbut since #23921 these queries were allowed. This change should really\r\nfix range queries with now based date ranges.\r\n Fix range queries with date range based on current time in percolator queries. >>> 1"
1724,This fix is only necessary for 5.4 and 5.x branches.\r\n\r\nPR for #24485 For legacy indices rewrite percolator query upon percolation time >>> 1
1725,"This is almost exclusively for docs test which frequently match the\r\nentire response. This allow something like:\r\n```\r\n  - set: {nodes.$master.http.publish_address: host}\r\n  - match:\r\n      $body:\r\n        {\r\n          ""nodes"": {\r\n            $host: {\r\n              ... stuff in here ...\r\n            }\r\n          }\r\n        }\r\n```\r\n\r\nThis should make it possible for the docs tests to work with\r\nunpredictable keys.\r\n Allow unstashing values into keys >>> 1"
1726,This class is also needed for plugins to use reindex functionality.\r\n\r\nRelates to #24578\r\n Move ReindexAction class to core >>> 1
1727,Adds allowed_types and allowed_contexts as new security settings for scripts described in detail in the issue #24532.\r\n\r\nCloses #24532 Add New Security Script Settings >>> 1
1728,"This moves the releasing logic to the base class, so that individual test cases don't need to worry about releasing the aggregators.  It's not a big deal for individual aggs, but once tests start using sub-aggs, it can become tricky to free (without double-freeing) all the aggregators.\r\n\r\n/cc @colings86  Automatically close releasables after aggregator tests >>> 1"
1729,relates #19391\r\nrelates #24633 Docs: Add migration note about .yaml and .json removal >>> 1
1730,Today when an index is `read-only` the index is also blocked from\r\nbeing deleted which sometimes is undesired since in-order to make\r\nchanges to a cluster indices must be deleted to free up space. This is\r\na likely scenario in a hosted environment when disk-space is limited to switch\r\nindices read-only but allow deletions to free up space.\r\n Add a cluster block that allows to delete indices that are read-only >>> 1
1731,"This commit adds gcs credential settings to the elasticsearch keystore.\r\nThe setting name follows the same pattern as the s3 client settings,\r\nbeginning with `gcs.client.`, followed by the client name, and then the\r\nsetting name, in this case, `credentials_file`. Using the legacy service\r\nfile setting is also deprecated. GCS Repository: Add secure storage of credentials >>> 1"
1732,"This commit adds a deprecation warning if `_index` is used in scripts.\r\nIt is emitted each time a script is invoked, but not per document. There\r\nis no test because constructing a LeafIndexLookup is quite difficult,\r\nbut the deprecation warning does show up in IndexLookupIT, there is just\r\nno way to assert warnings in integ tests.\r\n\r\nrelates #19359 Scripting: Deprecate index lookup >>> 1"
1733,Adding a unit test to InternalAdjecencyMatrix that extends the shared InternalAggregationTestCase that we use for testing aggregations. \r\nRelates to #22278 [Tests] Add unit test for InternalAdjecencyMatrix aggregation >>> 1
1734,This pull request adds parsing methods for the `SignificantStringTerms` and `SignificantLongTerms` aggregations. Add parsing to Significant Terms aggregations >>> 1
1735,This adds parsing and related tests to the InternalAdjacencyMatrix aggregation.\r\nBased on #24698 which should go into master first.\r\n Add parsing for InternalAdjacencyMatrix aggregation >>> 1
1736,"In scripts (at least some of the languages), the terms dictionary and\npostings can be access with the special _index variable. This is for\nvery advanced use cases which want to do their own scoring. The problem\nis segment level statistics must be recomputed for every document.\nAdditionally, this is not friendly to the terms index caching as the\norder of looking up terms should be controlled by lucene.\n\nThis change removes _index from scripts. Anyone using it can and should\ninstead write a Similarity plugin, which is explicitly designed to allow\ndoing the calculations needed for a relevance score.\n\ncloses #19359\n Remove script access to term statistics >>> 1"
1737,This method is not used and not tested. But it existence forces implementations of the interface to implement it even if it's unused. Remove the unused SignificantTerms.compareTerm() method >>> 1
1738,The IP validator doesn't expect a null value for a setting that causes NPEs\r\nif a user tries to reset a setting that uses this validator.\r\n\r\nCloses #24709 Allow resetting settings that use an IP validator >>> 1
1739,This pull request adds the parsing logic for `InternalBinaryRange` aggregation. Add parsing method for binary range aggregation >>> 1
1740,Native scripts are no longer documented and instead using a ScriptEngine\r\nis recommended. This change adds a deprecation warning for removal in\r\n6.0.\r\n\r\nrelates #19966 Scripting: Deprecate native scripts >>> 1
1741,Moves the remaining preconfigured token figured into the analysis-common module. There were a couple of tests in core that depended on the pre-configured token filters so I had to touch them:\r\n\r\n* `GetTermVectorsCheckDocFreqIT` depended on `type_as_payload` but didn't do anything important with it. I dropped the dependency. Then I moved the test to a single node test case because we're trying to cut down on the number of `ESIntegTestCase` subclasses.\r\n* `AbstractTermVectorsTestCase` and its subclasses depended on `type_as_payload`. I dropped their usage of the token filter and added an integration test for the termvectors API that uses `type_as_payload` to the `analysis-common` module.\r\n* `AnalysisModuleTests` expected a few pre-configured token filtes be registered by default. They aren't any more so I dropped this assertion. We assert that the `CommonAnalysisPlugin` registers these pre-built token filters in `CommonAnalysisFactoryTests`\r\n* `SearchQueryIT` and `SuggestSearchIT` had tests that depended on the specific behavior of the token filters so I moved the tests to integration tests in `analysis-common`. Move remaining pre-configured token filters into analysis-common >>> 1
1742,Today the `_field_caps` API doesn't implement its request serialization\r\ncorrectly since indices and indices options are not serialized at all.\r\nThis will likely break with all transport clients etc. and if this request\r\nmust be send across the network. This commit fixes this and adds correct\r\nhandling if we have only remote indices to prevent the inclusion of\r\nall local indices. Fix `_field_caps` serialization in order to support cross cluster search >>> 1
1743,"Shared settings were added intially to allow the few common settings\r\nnames across aws plugins. However, in 6.0 these settings have been\r\nremoved. The last use was in netty, but since 6.0 also has the netty 3\r\nmodules removed, there is no longer a need for the shared property. This\r\ncommit removes the shared setting property. Settings: Remove shared setting property >>> 1"
1744,"SearchResponse#fromXContent allows to parse a search response, including search hits, aggregations, suggestions and profile results. Only the aggs that we can parse today are supported (which means all of them but a couple that are left to support). SearchResponseTests reuses the existing test infra to randomize aggregations, suggestions and profile response.\r\n\r\nThis PR contains some of the changes proposed with #22533 and add extensive tests for it based on the existing test infra for parsing responses.\r\n\r\nRelates to #23331 Add fromXContent method to SearchResponse >>> 1"
1745,Approaching the release of 6.0 we need to sort out the usage of\r\n`Version#minimumCompatibilityVersion` which was still set to 5.0.0.\r\nNow this change moves it to the latest released version of 5.x (5.4 at this point)\r\nto ensure we are compatible with the latest minor of the previous major. This change\r\nalso removes all the `_UNRELEASED` from the versions that where released and drops versions\r\nthat were never released and are not expected to be released (bugfixes in minors that are not\r\nthe latest in the previous major).\r\n\r\nI will open separate PRs for 5.x Fix Version based BWC and set correct minCompatVersion >>> 1
1746,"variable assignment needs to be quoted to correctly handle the scenario where the batch file path contains parentheses, for example, such as unzipping to a directory under `C:\Program Files (x86)\`.\r\n\r\nIf variable assignment is not quoted, the the following error is exhibited\r\n\r\n```\r\n<path after parentheses>\..\config\jvm.options was unexpected at this time.\r\n```\r\n\r\nIs is not sufficient to just quote `%~dp0\..\config\jvm.options` because `%ES_JVM_OPTIONS%` will then contain quotes and thus be double quoted when performing the subsequent `findstr` operation, and in addition, the quotes cannot be removed from `""%ES_JVM_OPTIONS%""` in the `findstr` operation because they are needed to handle the case where the value is set from an environment variable (which may contain parentheses).\r\n\r\nQuoting the whole assignment handles the case where the value assigned contains parentheses correctly.\r\n\r\nFixes #24712  Handle parentheses in batch file path >>> 1"
1747,This change skips the expand search phase entirely when there is no search hits in the response.\r\n\r\nFixes #24672 Fix ExpandSearchPhase when response contains no hits >>> 1
1748,This pull request adds the parsing logic for the InternalTopHits aggregation. Add parsing method for Top Hits aggregation >>> 1
1749,Native scripts have been replaced in documentation by implementing\r\na ScriptEngine and they were deprecated in 5.5.0. This commit\r\nremoves the native script infrastructure for 6.0.\r\n\r\ncloses #19966\r\n Scripting: Remove native scripts >>> 1
1750,"This adds parsing of the `scripted_metric` aggregation that we need for the high level java rest client.\r\nWhile the parsing code itself is straight forward, it wasn't completely clear which kind of values we\r\nshould support to parse back from xContent. The [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-scripted-metric-aggregation.html#_allowed_return_types) mentions:      primitive types, String, Map, Array (containing elements of only the types listed here) so that's what is parsed and tested now. It might be possible that other kinds of values also currently work when using the transport client (e.g. StreamOutput#writeGenericValue() also supports GeoPoint, so that might work via transport but it gets rendered as a json object). I included one test to cover these cases with the example of GeoPoint as well.\r\n Add parsing for InternalScriptedMetric aggregation >>> 1"
1751,We've switched to supporting only `yml` files but anyone who didn't\r\nnotice will commit a `yaml` file which won't be executed\r\nwhich is bad because it is easy not to notice. The test to catch this is\r\nsimple enough that I think it is worth adding just to warn folks about\r\ntheir mistake.\r\n Fail rest tests on yaml files >>> 1
1752,This change removes the field data specialization needed for the parent field and replaces it with\r\na simple DocValuesIndexFieldData. The underlying global ordinals are retrieved via a new function called IndexOrdinalsFieldData#getOrdinalMap.\r\nThe children aggregation is also modified to use a simple WithOrdinals value source rather than the deleted WithOrdinals.Parent.\r\n\r\nRelates #20257 Removes parent child fielddata specialization >>> 1
1753,A user reported uneven balancing of load on nodes handling search requests from Kibana which supplies a session ID in a routing preference. Each shardId was selecting the same node for a given session ID from the list of allocations (in their case 2 data nodes with all primaries on one node and replicas on the other).\r\nThis change counteracts the tendency to opt for the same node given the same user-supplied preference by incorporating shard ID in the hash of the preference key. This will help randomise node choices across shards.\r\n\r\nCloses #24642 Search: Fairer balancing when routing searches by session ID >>> 1
1754,We were improperly testing that it was a `ConcreteShardRequest` instead of a\r\n`ConcreteReplicaRequest`. This adds that change and also ensures that the\r\ncheckpoint is retrievable from the request.\r\n [TEST] Fix TransportReplicationActionTests.testRetryOnReplica for replica request >>> 1
1755,"This commit expands the logic for version extraction from Version.java\r\nto include a list of all versions for backcompat purposes. The tests\r\nusing bwcVersion are converted to use this list, but those tests\r\n(rolling upgrade and backwards-5.0) are still not randomized; that will\r\nhappen in another followup.\r\n Build: Extract all ES versions into gradle properties >>> 1"
1756,"Removes all fine grained settings for script types, contexts, and engines.\r\n\r\nScript settings will now be specified as defined in #24532. Remove Deprecated Script Settings >>> 1"
1757,Now that we generate the versions list from Versions.java we can\r\ndrop the list of versions maintained for vagrant testing. One nice\r\nthing that the vagrant testing did was to check if the list of\r\nversions was out of date. This moves that test to the `core`\r\nproject. Remove vagrant testing versions >>> 1
1758,"Allows plugins to register pre-configured tokenizers. Much of the decisions are the same as those in #24223, #24572, and #24223. This only migrates the `lowercase` tokenizer but I figure that is a good start because it proves out the features. Allow plugins to register pre-configured tokenizers >>> 1"
1759,"This commit changes the rolling upgrade test to create a set of rest\r\ntest tasks per wire compat version. The most recent wire compat version\r\nis always tested with the `integTest` task, and all versions can be\r\ntested with `bwcTest`. Test: Convert rolling upgrade test to have task per wire compat version >>> 1"
1760,This change removes all `_UNRELEASED` from versions that are actually\r\nreleased. We will add code to enforce this in followups. Remove `_UNRELEASED` from Version constants for released versions >>> 1
1761,"PR's I still have open: #24656 and #24340 and #24533\r\n\r\nThis very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThis PR revolves around places in the code where introducing a `StringBuilder` might make the construction of a `String` easier to follow and also, maybe avoid a case where the compiler's very safe way of introducing `StringBuilder` instead of String might not always be optimal for performance. Though it isn't much.\r\n\r\nMost of my understanding of the subject comes from this article: http://www.pellegrino.link/2015/08/22/string-concatenation-with-java-8.html\r\n\r\n===========\r\n\r\n1 .Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Replacing String concattenation with StringBuilder >>> 1"
1762,The packaging scripts still referenced the scripts directory which is now removed.\r\n\r\n remove remaining references to scripts directory >>> 1
1763,This PR adds the parsing logic for the `InternalMatrixStats` aggregation. Add parsing method for Matrix Stats >>> 1
1764,"This PR moves some functionality from PublishClusterStateAction to ZenDiscovery, which allows each class to focus on it's core competencies:\r\n\r\n- PendingStatesQueue is now solely managed by ZenDiscovery (no shared access by both PublishClusterStateAction and ZenDiscovery)\r\n- Validation logic is handled exclusively by ZenDiscovery Clear responsibilities for PublishClusterStateAction and ZenDiscovery >>> 1"
1765,"This commit removes the ability to specify the google credential json\r\nfile on disk, which is deprecated in 5.5.0.\r\n GCS Repository: Remove specifying credential file on disk >>> 1"
1766,\r\nrelates to #24758 Build: Fix plugin installation for integ test to have unique configuration name >>> 1
1767,This commit adds the ability to store and retireve data that should be associated with a\r\nScrollContext. Additionally the ScrollContext was made final as we should only have a single\r\nimplementation of this concept.\r\n Add the ability to store objects with a ScrollContext >>> 1
1768,This PR upgrades master to a current Lucene snapshot with commit id `a0aef2f`.\r\n Upgrade to lucene-7.0.0-snapshot-a0aef2f >>> 1
1769,"This commit renames the backwards-5.0 qa test to mixed-cluster and\r\ncreates a test within the project per wire compat version. Like with\r\nrolling upgrade tests, the integTest task will run against the most\r\nrecent version, while all versions will be tested with the bwcTest task.\r\n Test: Make mixed cluster bwc test per wire compat version >>> 1"
1770,"This very small PR is related to issue #24226 it is small exactly to keep it readable.\r\n\r\nThis PR tackles places in the code where `tail recursion` occurs. Which is the method calling itself again in a certain step. Which might sometimes only be one step deep, could be hundreds of steps deep.\r\n\r\nAll these changes were automatic, done by Intellij. The system being used is to actually replace it with a while/true loop, where the point in the code where the recursive call used to happen now gets the relevant variables updated and then a `continue` occurs. Which is effectively the same, except no method call happens.\r\n\r\nI can appreciate this might be controversial. Some people find while/true constructs dangerous/hard to read. I personally have no issue with them.\r\n\r\nI'd like to hear what the opinion on this is, I can take it back down if it's not wanted.\r\n\r\nNote: I did `gradle check`, build successful, nothing failed.\r\n\r\n===========\r\n\r\n1. Contributor agreement: Has been signed\r\n\r\n2. Contributor guidelines: I have read them.\r\n\r\n3. Gradle check: build successful.\r\n\r\n4. PR is against master\r\n\r\n5. The PR is not OS specific.\r\n\r\n6. I'm not part of a class. Replacing tail recursion with while/true >>> 0"
1771,"Today a replica learns of a new primary term via a cluster state update and there is not a clean transition between the older primary term and the newer primary term. This commit modifies this situation so that:\r\n - a replica shard learns of a new primary term via replication operations executed under the mandate of the new primary\r\n - when a replica shard learns of a new primary term, it blocks operations on older terms from reaching the engine, with a clear transition point between the operations on the older term and the operations on the newer term\r\n\r\nThis work paves the way for a primary/replica sync on primary promotion. Future work will also ensure a clean transition point on a promoted primary, and prepare a replica shard for a sync with the promoted primary.\r\n\r\nRelates #10708\r\n\r\n Block older operations on primary term transition >>> 1"
1772,"This commit adds a `doc_count` field to the response body of Matrix Stats aggregation. It exposes the number of documents involved in  the computation of statistics, a value that can already be retrieved using\r\nthe method `MatrixStats.getDocCount()` in the Java API.\r\n\r\nThis information will be useful for the Java High Level Rest Client in order to provide the same level of information as what a user can get using the Transport Client (see https://github.com/elastic/elasticsearch/pull/24746#discussion_r117244017).\r\n Add document count to Matrix Stats aggregation response >>> 1"
1773,"Today when we get a metadata snapshot from the index shard we ensure\r\nthat if there is no engine started on the shard that we lock the index\r\nwriter before we go and fetch the store metadata. Yet, if we concurrently\r\nrecover that shard, recovery finalization might fail since it can't acquire\r\nthe IW lock on the directory. This is mainly due to the wrong order of acquiring\r\nthe IW lock and the metadata lock. Fetching store metadata without a started engine\r\nshould block on the metadata lock in Store.java but since IndexShard locks the writer\r\nfirst we get into a failed recovery dance especially in test. In production\r\nthis is less of an issue since we rarely get into this situation if at all.\r\n\r\nCloses #24481\r\n Obey lock order if working with store to get metadata snapshots >>> 1"
1774,Note: PR against a feature branch\r\n\r\nThis commit removes some //norelease tags and cleans up remaining @Before. Remove //norelease and cleans up some aggregations tests >>> 1
1775,"Now the Java High Level Rest Client has tests to parse all aggregations, this test is not needed anymore. We have better tests like  AggregationsTests and sub classes of InternalAggregationTestCase.\r\n\r\nThis PR is against the feature branch so that the test will be removed at the same time the new tests are added to the master branch.\r\n\r\nRelated to #23965 [Test] Remove ParsedAggregationTests >>> 1"
1776,"This commit moves the handling of nested and parent/child inner hits to specialized classes that can be defined outside of ES core.\r\nInnerHitBuilderContext is now used by the parent query (nested or hasChild, ...) to build the sub context from the InnerHitBuilder definition.\r\nBWC is also ensured so that nodes in previous versions can still send/receive inner hits to/from this version.`\r\n\r\nRelates #20257 Add the ability to define custom inner hit sub context builder >>> 1"
1777,"The Netty recycler is nothing but trouble, so let us disable this by default in the client too.\r\n\r\nRelates #22452, relates #24721\r\n Disable the Netty recycler in the client >>> 1"
1778,"The method should rather advance one token and only then require a START_OBJECT as the current token. This allows to parse given a parser that's at the beginning of the response, where the initial/current token is null. SearchResponse#fromXContent to not require START_OBJECT as current token >>> 1"
1779,"SearchSourceBuilder#toXContent prints out a complete object, hence it should implement ToXContentObject Mark SearchSourceBuilder as ToXContentObject >>> 1"
1780,"Before this change we used the name of the constant to check if a version was released or not. Versions ending in `_UNRELEASED` were considered unreleased and other versions were considered released. The trouble with this is that we'd often forget to switch a version from unreleased to released. That is a problem because unreleased versions don't get backwards compatibility tests. This changes the logic so that the released versions are inferred from the current version using the following rules:\r\n1. If the current version is a bug fix release all other versions are released.\r\n2. Otherwise, the version constant before the current version and before all the alphas is considered unreleased.\r\n3. If that unreleased version is itself a bug fix release then that is the only unreleased version.\r\n4. Otherwise, the version before it is unreleased.\r\n\r\nSome examples:\r\n* Right now in master the current version is 6.0.0-alpha2 which means that the last non-alpha in the list is unreleased. That version is 5.5.0. Because that is a `.0` release the version before that is also considered unreleased. So in master 5.5.0 and 5.4.1 are both considered unreleased.\r\n* Right now in 5.x the current version is 5.5.0 which means that that last non-alpha in the list is unreleased. That version is 5.4.1. It isn't a `.0` release, so the only unreleased version is 5.4.1.\r\n* Right now in 5.4 the current version is 5.4.1. Since that isn't a `.0` release there are no unreleased versions.\r\n\r\nThis logic is mirrored in both VersionUtils.java and in build.gradle so that gradle can run the appropriate backwards compatibility tests, some of which are already written.\r\n\r\nA secondary goal with removing the `_UNRELEASED` suffix is to one day be able to use a script to add the new version constant.\r\n\r\nIt is also important to note that gradle already has a test that verifies that the list of released versions is correct according to maven central. This means that we won't be able to forget to add a version constant.\r\n\r\nCloses #24768 Remove the need for _UNRELEASED suffix in versions >>> 1"
1781,"This PR is currently based on #24795 and #24794 . It will go in after them. It is also opened against the feature/client_aggs_parsing as it is based on the work done on the branch, but it will go directly to master once the branch is merged. Add search method to high level REST client >>> 1"
1782,"The proposed API doesn't work.\r\nInstead, the one suggested in the Tasks API page works.\r\nLike GET _tasks/node_id:task_id works.\r\nSimilarly for cancelling and re-throttling\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n GET _tasks/taskId:1 doesn't work to get task info >>> 0"
1783,Moar docs. Docs: More search speed advices. >>> 1
1784,This commit changes the name of the method that updates the cluster \r\nstate with index aliases to make it more meaningful and makes the \r\nmethod public for use in tests. Makes the index alias cluster state updates method public for testing >>> 0
1785,"Adds a special setting value of ""none"" for ""script.allowed_types"" and ""script.allowed_contexts"" to specify no types/contexts are allowed, respectively. Add Ability to Specify No Types/Contexts Allowed For Scripts >>> 1"
1786,"ScriptEngine implementations have an overridable method to indicate they\r\nare safe to use as inline scripts. Since groovy was removed fro 6.0,\r\nthere are no longer any implementations which used the default false\r\nvalue. Furthermore, the value was not actually read anywhere. This\r\ncommit removes the method. The ScriptEngineRegistry was also no longer\r\nnecessary as it only was used to build a map from language to engine.\r\n Scripting: Remove ""inline script enabled"" on script engines >>> 1"
1787,This commit is a simple cleanup to remove an unnecessary extra method on\r\nScriptService which was only used in 3 places. There is now only one\r\nsearch method. Scripting: Simplify search method on script service >>> 1
1788,This change upgrades the analysis-icu plugin to use the latest icu4j (56.1). Upgrade icu4j to latest version >>> 1
1789,"Given that both InternalAggregation and ParsedAggregation have this method, it makes sense to move it to the interface they both implement. Move getType to Aggregation interface >>> 1"
1790,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\nFixes #24735 \r\n\r\nThis pull request is created against branch 5.x since modified class is marked as deprecated and removed from master already.\r\n Thread falls into infinite loop when processing Indices query >>> 1"
1791,"Adds a ""magic"" key to the yaml testing stash mostly for use with\r\ndocumentation tests. When unstashing an object, `$_path` is the\r\npath into the current position in the object you are unstashing.\r\nThis means that in docs tests you can use\r\n`// TESTRESPONSEs/somevalue/$body.${_path}/` to mean ""replace\r\n`somevalue` with whatever is the response in the same position.""\r\n\r\nCompare how you must carefully mock out all the numbers in the profile\r\nresponse without this change:\r\n```\r\n// TESTRESPONSE[s/""id"": ""\[2aE02wS1R8q_QFnYu6vDVQ\]\[twitter\]\[1\]""/""id"": $body.profile.shards.0.id/]\r\n// TESTRESPONSE[s/""rewrite_time"": 51443/""rewrite_time"": $body.profile.shards.0.searches.0.rewrite_time/]\r\n// TESTRESPONSE[s/""score"": 51306/""score"": $body.profile.shards.0.searches.0.query.0.breakdown.score/]\r\n// TESTRESPONSE[s/""time_in_nanos"": ""1873811""/""time_in_nanos"": $body.profile.shards.0.searches.0.query.0.time_in_nanos/]\r\n// TESTRESPONSE[s/""build_scorer"": 2935582/""build_scorer"": $body.profile.shards.0.searches.0.query.0.breakdown.build_scorer/]\r\n// TESTRESPONSE[s/""create_weight"": 919297/""create_weight"": $body.profile.shards.0.searches.0.query.0.breakdown.create_weight/]\r\n// TESTRESPONSE[s/""next_doc"": 53876/""next_doc"": $body.profile.shards.0.searches.0.query.0.breakdown.next_doc/]\r\n// TESTRESPONSE[s/""time_in_nanos"": ""391943""/""time_in_nanos"": $body.profile.shards.0.searches.0.query.0.children.0.time_in_nanos/]\r\n// TESTRESPONSE[s/""score"": 28776/""score"": $body.profile.shards.0.searches.0.query.0.children.0.breakdown.score/]\r\n// TESTRESPONSE[s/""build_scorer"": 784451/""build_scorer"": $body.profile.shards.0.searches.0.query.0.children.0.breakdown.build_scorer/]\r\n// TESTRESPONSE[s/""create_weight"": 1669564/""create_weight"": $body.profile.shards.0.searches.0.query.0.children.0.breakdown.create_weight/]\r\n// TESTRESPONSE[s/""next_doc"": 10111/""next_doc"": $body.profile.shards.0.searches.0.query.0.children.0.breakdown.next_doc/]\r\n// TESTRESPONSE[s/""time_in_nanos"": ""210682""/""time_in_nanos"": $body.profile.shards.0.searches.0.query.0.children.1.time_in_nanos/]\r\n// TESTRESPONSE[s/""score"": 4552/""score"": $body.profile.shards.0.searches.0.query.0.children.1.breakdown.score/]\r\n// TESTRESPONSE[s/""build_scorer"": 42602/""build_scorer"": $body.profile.shards.0.searches.0.query.0.children.1.breakdown.build_scorer/]\r\n// TESTRESPONSE[s/""create_weight"": 89323/""create_weight"": $body.profile.shards.0.searches.0.query.0.children.1.breakdown.create_weight/]\r\n// TESTRESPONSE[s/""next_doc"": 2852/""next_doc"": $body.profile.shards.0.searches.0.query.0.children.1.breakdown.next_doc/]\r\n// TESTRESPONSE[s/""time_in_nanos"": ""304311""/""time_in_nanos"": $body.profile.shards.0.searches.0.collector.0.time_in_nanos/]\r\n// TESTRESPONSE[s/""time_in_nanos"": ""32273""/""time_in_nanos"": $body.profile.shards.0.searches.0.collector.0.children.0.time_in_nanos/]\r\n```\r\n\r\nTo how you can cavalierly mock all the numbers at once with this change:\r\n```\r\n// TESTRESPONSE[s/(?<=["" ])\d+(\.\d+)?/$body.$_path/]\r\n```\r\n Add magic $_path stash key to docs tests >>> 1"
1792,"With #24779 in place, we can now guaranteed that a single translog generation file will never have a sequence number conflict that needs to be resolved by looking at primary terms. These conflicts can a occur when a replica contains an operation which isn't part of the history of a newly promoted primary. That primary can then assign a different operation to the same slot and replicate it to the replica.\r\n\r\nPS. Knowing that each generation file is conflict free will simplifying repairing these conflicts when we read from the translog.\r\n\r\nPPS. This PR also fixes some bugs in the piping of primary terms in the bulk shard action. These bugs are a result of the legacy of IndexRequest/DeleteRequest being a ReplicationRequest. We need to change that as a follow up.\r\n\r\nRelates to #10708  Guarantee that translog generations are seqNo conflict free >>> 1"
1793,"This will be useful for the high level client to add support for the matrix stats aggregation, as we will ship with this jar by default like we do for parent-join-client which is aligned with distributing core with the modules already included.\r\n\r\nRelates to #24796 Build: add client jar for aggs-matrix-stats >>> 1"
1794,This PR fixes the `RangeFieldMapper` and `RangeQueryBuilder` to pass the correct relation to the `RangeQueryBuilder` when performing a range query over range fields.\r\n\r\ncloses #24744  Fix RangeFieldMapper rangeQuery to properly handle relations >>> 1
1795,For comparing actual and parsed object equality for the response parsing we\r\ncurrently rely heavily on comparing the original xContent and the output of the\r\nparsed object. Currently we only have cryptic error messages if this comparison\r\nfails which are hard to read also because we recursively compare lists and maps\r\nof the xContent structures we compare.\r\n\r\nThis commits adds better error messages for these failures by e.g. keeping track\r\nof the path to where differences in the two xContent structures occur and\r\nprinting out this information on failure. [Tests] Improve error message for failed xContentEquivalent() tests >>> 1
1796,"As we work towards contexts implying the return type of compilation, we\r\nfirst need ScriptContext to not be an enum. This commit removes the\r\nStandard enum and Plugin subclass of ScriptContext.\r\n Scripting: Simplify ScriptContext >>> 1"
1797,Correct the names of the settings script.allowed_types and script.allowed_contexts. Fix Script Settings Names >>> 1
1798,"Today in the code base we have lots of ugly code blocks like:\r\n\r\n```java\r\n  boolean assertionsEnabled = false;\r\n  assert assertionsEnabled = true;\r\n  if (assertionsEnabled) {\r\n    // something\r\n  }\r\n```\r\n\r\nThese are a nuisance. Instead, we can do this in exactly one place and replace these blocks with\r\n\r\n```java\r\n  if (Assertions.ENABLED) {\r\n    // something\r\n  }\r\n```\r\n\r\nThe cool thing here is that since this is a static final field, the JIT can optimize away the check at runtime if assertions are disabled.\r\n\r\n Add assertions enabled helper >>> 1"
1799,"This change cleans up some missed TODOs for content type detection on the source of put mapping and put index template requests. In 5.3.0 and newer versions, the source is always JSON so the content type detection is not needed. The TODOs were missed after the change was backported to 5.3.\r\n\r\nRelates #24798 Put mapping and index template requests do not need content type detection for 5.3.0+ >>> 1"
1800,"After releasing 5.3.2, the 5.3.3 version constant was created.  However,\r\nthis causes issues for the rolling upgrade tests, which expect to have\r\nall older versions artifacts published and no point releases created off\r\nof the older versions (older meaning more than one version behind the\r\ncurrent version).  This commit removes the 5.3.3 version constant,\r\nassuming we will not need it anywhere. Removes the 5.3.3 version constant >>> 1"
1801,Relates to #22278 Add unit tests for MatrixStatsAggregator >>> 1
1802,* Enable doc values for range fields by default.\r\n* Store ranges in a binary format that support multi field fields.\r\n* Added BinaryDocValuesRangeQuery that can query ranges that have been encoded into a binary doc values field.\r\n* Wrap range queries on a range field in IndexOrDocValuesQuery query.\r\n\r\nCloses #24314 Query range fields by doc values when they are expected to be more efficient than points >>> 1
1803,"This commit adds `GET|PUT /{index}/_doc/{id}` and relatives to\r\nindex, bulk and get. This is a step towards removing types from elasticsearch.\r\nThis change adds a default `doc` type as the type for requests against typeless\r\nAPIs. In the future we might improve this by dyncamically detecting the type\r\nin the mapping if there is one already.\r\nThis chance also doesn't cut over existing rest tests to ensure tests work for\r\nour BWC tests. Once this is backported to 5.5 we can cut over rest tests to use\r\nthe typeless API as well as adding deprecation headers to the typed APIs.\r\n\r\nRelates to #24592\r\nRelates to #15613\r\n\r\n Add typeless APIs to `Get` `Index` and `Bulk` >>> 0"
1804,"This PR is the backport of #24824 for 5.x. It required a separate PR because in the original feature branch we relied on the aggs test infra that has been recently expanded on master only as part of #22278. Some test classes were missing and are now introduced as part of this PR, yet only the pieces that are needed to test the aggs parsing code rather than all the work done on master. For instance `InternalAggregationTestCase` is added but it extends `ESTestCase` rather than `AbstractWireSerializingTestCase`. The reduce and serialization of aggs is still tested only on master while This PR introduces the `testFromXContent` test method only, which tests the new parsing code.\r\n\r\nThe following are all the PRs that were merged to the [feature/client_aggs_parsing](https://github.com/elastic/elasticsearch/tree/feature/client_aggs_parsing) branch:\r\n\r\n- Add ParsedAggregation as base Aggregation impl for high level client (#23965)\r\n- Adding ParsedCardinality (#23973)\r\n- Add parsing for percentiles ranks (#23974)\r\n- Add parsing to some single value aggregations (#24085)\r\n- AbstractParsedPercentiles should use Percentile class (#24160)\r\n- Add parsing for InternalSimpleValue and InternalDerivative (#24162)\r\n- Add parsing for InternalBucketMetricValue (#24182)\r\n- Add parsing methods for Percentiles aggregations (#24183)\r\n- [Test] Always check the XContent equivalent when parsing aggregations (#24208)\r\n- Add parsing methods for InternalDateHistogram and InternalHistogram (#24213)\r\n- Add parsing for InternalStats (#24239)\r\n- Add parsing for InternalExtendedStats (#24284)\r\n- Add parsing for InternalStatsBucket and InternalExtendedStatsBucket (#24312)\r\n- Add parsing for InternalPercentilesBucket (#24330)\r\n- Add parsing for InternalGeoBounds (#24365)\r\n- Add parsing for InternalGeoCentroid (#24371)\r\n- Introduce SearchResponseSections base class (#24442)\r\n- Add parsing for String/Long/Double Terms aggregations (#24521)\r\n- [TEST] Add test for Aggregations#fromXContent (#24524)\r\n- Add parsing for single bucket aggregations  (#24564)\r\n- Add parsing methods to Range aggregations (#24583)\r\n- Add parsing method to GeoHashGrid aggregation (#24589)\r\n- Add parsing for InternalFilters aggregation (#24648)\r\n- Revert changing the InternalSampler type constant (#24667)\r\n- Small improvement in InternalAggregationTestCase test setup (#24675)\r\n- Add parsing to Significant Terms aggregations (#24682)\r\n- Add parsing for InternalAdjacencyMatrix aggregation (#24700)\r\n- Add parsing method for binary range aggregation (#24706)\r\n- Add parsing method for Top Hits aggregation (#24717)\r\n- Add fromXContent method to SearchResponse (#24720)\r\n- Add parsing for InternalScriptedMetric aggregation (#24738)\r\n- Add parsing method for Matrix Stats (#24746)\r\n- Remove //norelease and cleans up some aggregations tests (#24789)\r\n- [Test] Remove ParsedAggregationTests (#24791)\r\n- SearchResponse#fromXContent to not require START_OBJECT as current token (#24794)\r\n- Move getType to Aggregation interface (#24822)\r\n\r\n\r\nIn addition to these, the following PRs have been partially backported too:\r\n\r\n- Improve unit test coverage of aggs (#22668)\r\n- Adds tests for cardinality and filter aggregations (#23826)\r\n- [Test] Use appropriate DocValueFormats in Aggregations tests (#24155)\r\n- Remove getCountAsString() from InternalStats and Stats interface (#24291)\r\n- Added unit tests for InternalMatrixStats (#24559)\r\n Backport aggs parsers for high level REST Client >>> 1"
1805,A tiny leftover that I just found in the ParsedGeoBounds class. This should use the constants in InternalGeoBounds to have\r\nless magic strings floating around.\r\n Use ParseField constants in ParsedGeoBounds >>> 1
1806,"Since groovy was removed, we no longer have any ScriptEngines with\r\nresources to release. We may want to keep the option open for a script\r\nengine to close resources, but this would not be common. This commit\r\nadds a default implementation to ScriptEngine for `close()` to reduce\r\nthe boiler plate that must be added for a ScriptEngine implementation.\r\n Scripting: Add default implementation of close() for ScriptEngine >>> 1"
1807,"This commit cleans up tests which currently use custom script engine\r\nimplementations, converting them to use a MockScriptEngine with script\r\nfunctions provided by the tests. It also creates a common set of metric\r\nscripts which were copied across a couple metric agg tests.\r\n Test: Convert test script engine impls to use MockScriptEngine >>> 1"
1808,If a cluster disconnects and comes back up we should ensure that\r\nwe connected to the cluster before we fire the requests.\r\n\r\nCloses #24763 Ensure remote cluster is connected before fetching `_field_caps` >>> 1
1809,Fixes #24606. Fix link to perl docs >>> 1
1810,This is a relict from the TTL functionality that has been removed.\r\n\r\nRelates to #21670 Remove AlreadyExpiredException >>> 1
1811,"Some packaging tests depend on snapshot versions of packaging distributions yet the build does not use a repository that includes such distributions. While we could add such a repository, a better strategy is to follow our approach for other BWC tests where we depend on a locally-compiled archive distribution. This commit adds a local compilation of packaging artifacts and substitutes these anywhere that we would otherwise depend on a snapshot of these artifacts.\r\n Add BWC packaging distributions >>> 1"
1812,nan fix bug of weight computation >>> 1
1813,"This is a spin off for https://github.com/elastic/elasticsearch/pull/24398.\r\n\r\nThis commit refactors the query phase in order to be able\r\nto automatically detect queries that can be early terminated.\r\nIf the index sort matches the query sort, the top docs collection is early terminated\r\non each segment and the computing of the total number of hits that match the query is delegated\r\nto a simple TotalHitCountCollector.\r\nThis change also adds a new parameter to the search request called `track_total_hits`.\r\nIt indicates if the total number of hits that match the query should be tracked.\r\nIf false, queries sorted by the index sort will not try to compute this information\r\nand will limit the collection to the first N documents per segment.\r\nAggregations are not impacted and will continue to see every document\r\neven when the index sort matches the query sort and `track_total_hits` is false.\r\n\r\nRelates #6720 Automatically early terminate search query based on index sorting >>> 1"
1814,"This commit adds a new `bg_count` field to the REST response of `SignificantTerms` aggregations. Similarly to the `bg_count ` that already exists in significant terms buckets, this new `bg_count` field is set at the aggregation level and is populated with the superset size value.\r\n\r\nThe addition of this field allows the High Level REST client to provide implementations of `SignificantTerms`and `SignificantTerms.Bucket` with the exact same behavior as the internal implementations. Before this pull request, a significant term aggregation didn't know about the superset size at all. Same thing for the aggregation's buckets that throw unsupported operation exceptions because the aggregation's superset size and subset size were unkown. This PR fixes that and adds support for both fields that are now populated at parsing time.\r\n\r\nNote that the subset size field at the bucket could have been implemented before but I didn't know much about how to do that. Thanks to @markharwood it is now supported.\r\n\r\nThere's a bit of history around these fields (see https://github.com/elastic/elasticsearch/pull/5146#r9862823) and the superset size information was not added to the aggregation at the first place. I think the main argument was that it could be retrieved at query time using a Global aggregation. This argument is still valid, but adding this new field provides a better support of Significant Terms aggregation in the High Level REST Client and it might also be useful in the future for a new chart type in Kibana. Add superset size to Significant Term REST response >>> 1"
1815,"This commit modifies the compile method of ScriptService to be context\r\naware. The ScriptContext is now a generic class which contains both the\r\ninstance type and compiled type for a script. Instance type may be\r\nstateful (for example, pre loading field information for the index a\r\nscript will execute on, like in expressions), while the compiled type is\r\nstateless and used to construct instance type instances. This change is\r\nonly a first step to cutover ScriptService to the new paradigm. It only\r\nconverts callers to the script service, and has a small shim to wrap\r\ncompilation from the script engines to support the current two fixed\r\ninstance types, SearchScript and ExecutableScript.\r\n Add instance and compiled classes to script contexts >>> 1"
1816,"Related to #24745\r\n\r\n@jasontedor as we discussed, this has the following three properties:\r\n\r\n1. Passes before inlining checkpoint change, I backported the test to 5.x to\r\n   make sure it passes.\r\n\r\n2. Fails after inlining checkpoint change, I undid your fix for the Shard ->\r\n   Replica request and it does indeed fail.\r\n\r\n3. Passes after your one-line change ConcreteShardRequest -> ConcreteReplicaRequest,\r\n   this does now pass, now that the correct class is used for the request (fixed\r\n   as part of 657686cefb736999b2231b4caee318da14e9462a)\r\n [TEST] Add test for retrying replica operations with real network >>> 1"
1817,"This commit changes the compile method of ScriptEngine to be generic in\r\nthe same way it is on ScriptService. This moves the shim of handling the\r\ntwo existing context classes into each script engine, so that each\r\nengine can be worked on independently to convert to real handling of\r\ncontexts. Make ScriptEngine.compile generic on the script context >>> 1"
1818,"When developing the new ScriptContext, the compiled type was original\r\ngeneric, so that the instance type was also necessary. However, since\r\nCompiledType is all that is used by the compile method signature, we\r\nactually don't need the instance type to be generic. This commit removes\r\nthe InstanceType, and finds the Class for it through reflection on the\r\nCompiledType method. Scripting: Remove unnecessary generic type from ScriptContext >>> 1"
1819,"These tests spin up two nodes of an older version of Elasticsearch,\r\ncreate some stuff, shut down the nodes, start the current version,\r\nand verify that the created stuff works.\r\n\r\nYou can run `gradle qa:full-cluster-restart:check` to run these\r\ntests against the head of the previous branch of Elasticsearch\r\n(5.x for master, 5.4 for 5.x, etc) or you can run\r\n`gradle qa:full-cluster-restart:bwcTest` to run this test against\r\nall ""index compatible"" versions, one after the other. For master\r\nthis is every released version in the 5.x.y version *and* the tip\r\nof the 5.x branch.\r\n\r\nI'd love to add more to these tests in the future but these\r\ncurrently just cover the functionality of the `create_bwc_index.py`\r\nscript and start to cover the assertions in the\r\n`OldIndexBackwardsCompatibilityIT` test.\r\n Begin replacing static index tests with full restart tests >>> 1"
1820,"Drops `TokenizerFactory#name`, replacing it with\r\n`CustomAnalyzer#getTokenizerName` which is much better targeted at\r\nits single use case inside the analysis API.\r\n\r\nDrops a test that I would have had to refactor which is duplicated by\r\n`AnalysisModuleTests`.\r\n\r\nTo keep this change from blowing up in size I've left two mostly\r\nmechanical changes to be done in followups:\r\n1. `TokenizerFactory` can now be entirely dropped and replaced with\r\n`Supplier<Tokenizer>`.\r\n2. `AbstractTokenizerFactory`'s ctor still takes a `String` parameter\r\nwhere the name once was.\r\n Drop name from TokenizerFactory >>> 1"
1821,It may not be clear to users that the Ingest ScriptProcessor context object `ctx` can \r\nmanipulate document metadata like `_index` and `_type`.\r\n\r\nThis PR adds such an example add docs example for Ingest scripts manipulating document metadata >>> 1
1822,"This message broken in recent refactoring, this commit also adds\r\nbasic unittests to ensure we maintain the correct version.\r\n Fix error message if an incompatible node connects >>> 1"
1823,"This commit fixes the error message to escape the dollar sign for\r\nreferencing a literal `$HADOOP_HOME`, which caused an error while trying\r\nto generate an error.\r\n\r\ncloses #24878 Build: Fix hadoop integ test error on windows >>> 1"
1824,"When transitive dependencies are disable for a dependency, gradle adds a\r\nwildcard exclusion to the generated pom. However, some external tools\r\nlike ivy have bugs with wildcards. This commit adds back the explicit\r\ngeneration of transitive excludes, and removes the gradle generated\r\nexclusions element from the pom.\r\n\r\ncloses #24490\r\n Build: Add back explicit exclusions and remove gradle exclusions >>> 1"
1825,"This is a simple refactoring to move the context definitions into the\r\ntype that they use. While we have multiple context names for the same\r\nclass at the moment, this will eventually become one ScriptContext per\r\ninstance type, so the pattern of a static member on the interface called\r\nCONTEXT can be used. This commit also moves the consolidated list of\r\ncontexts provided by core ES into ScriptModule.\r\n Scripting: Move context definitions to instance type classes >>> 1"
1826,"In #24605, logic was implemented to ensure that completed snapshots were\r\nproperly removed from the cluster state upon a change in master nodes.\r\nThis commit removes redundant logic that also attempted to clean up\r\ncompleted snapshots from the cluster state on master election, but only\r\ncovered a limited case that was remedied in #24605.\r\n\r\nThis commit also adds a test to ensure cleaning up of completed\r\nsnapshots at the right moment in time when a master election happens\r\nbefore finalizing a snapshot, as well as adds a check to handle the case\r\nwhere the old master and new master could attempt to finalize the\r\nsnapshot and write the same blob to the repository simultaneously. Consolidates the logic for cleaning up snapshots on master election >>> 1"
1827,This commit adds collection of all contexts to the parameters of\r\ngetScriptEngine. This will allow script engines like painless to\r\nprecache extra information about the contexts.\r\n Make contexts available to ScriptEngine construction >>> 1
1828,"This commit renames the concept of the ""compiled type"" to a ""factory\r\ntype"", along with all implementations of this class to be named Factory.\r\nThis brings it inline with the classes purpose.\r\n Scripting: Rename CompiledType to FactoryType in ScriptContext >>> 1"
1829,"If the bucket already exists, due to non-overlapping series or missing data, the MovAvg creates a merged bucket with the existing aggs + the new prediction.  This fixes a small bug where the doc_count was not being set correctly.\r\n\r\nRelates to #24327 Correctly set doc_count when MovAvg ""predicts"" values on existing buckets >>> 1"
1830,"Splits TranslogRecoveryPerformer into three parts:\r\n- the translog operation to engine operation converter\r\n- the operation perfomer (that indexes the operation into the engine)\r\n- the translog statistics (for which there is already RecoveryState.Translog)\r\n\r\nThis makes it possible for peer recovery to use the same IndexShard interface as bulk shard requests (i.e. Engine operations instead of Translog operations). It also pushes the ""fail on bad mapping"" logic outside of IndexShard. Future pull requests could unify the BulkShard and peer recovery path even more. Remove TranslogRecoveryPerformer >>> 1"
1831,"ClearScrollRequest can be created from a request body, but it doesn't support the opposite, meaning printing out its content to an XContentBuilder. This is useful to the high level REST client and allows for better testing of what we parse.\r\n\r\nMoved parsing method from RestClearScrollAction to ClearScrollRequest so that fromXContent and toXContent sit close to each other. Added unit tests to verify that body parameters override query_string parameters when both present (there is already a yaml test for this but unit test is even better) ClearScrollRequest to implement ToXContentObject >>> 1"
1832,"`IndexRequest` and `DeleteRequest` inherit from `ReplicationRequest`, which\r\nallows getting and setting the primary term for a particular operation. However,\r\nin this case, either request will be wrapped into a `BulkItemRequest` wrapper,\r\nwhich is where the primary term and shardId should actually be set. We\r\n(dangerously) use the request object to store some internal state about the\r\nrequest.\r\n\r\nThis PR makes that internal state for just the Index and Delete request throw an\r\nexception, so that it is clear that it should never be set or retrieved from the\r\ninner object. Originally we intended to have separate objects\r\n`LegacyIndexRequest` and `LegacyDeleteRequest` for removing that functionality,\r\nhowever the wrappers required for client compatibility would make the complexity\r\ntrade-off not worth it. We plan to revisit the removal of this internal state\r\nafter the transport client has been removed.\r\n Prevent Index & Delete request primaryTerm getter/setter, setShardId setter >>> 1"
1833,"Removes the `distribution:bwc` project in favor of\r\n`distribution:bwc-release-snapshot` and\r\n`distribution:bwc-stable-snapshot`.\r\n`distribution:bwc-release-snapshot` builds a snapshot of the\r\nlatest release branch (5.4 now) if needed for backwards\r\ncompatibility. `distribution:bwc-stable-snapshot` builds a\r\nsnapshot of the latest stable branch (5.x now) if needed for\r\nbackwards compatibility.\r\n\r\n\r\n\r\nI edited the description to match what was actually committed. This was the old description for posterity:\r\n\r\nAdds the `:distribution:bwc-stable` project which builds the previous\r\nstable branch when `:distribution:bwc` is building an unreleased\r\nbranch. When `:distribution:bwc` builds a released branch then\r\n`:distribution:bwc-stable` is an empty build, not used or depended\r\non by anything.\r\n\r\nRequires #24798\r\n Rework bwc snapshot projects to build up to two bwc versions >>> 1"
1834,In #23093 we made a change so that total bytes for a filesystem would not be a\r\nnegative value when the total bytes were > Long.MAX_VALUE.\r\n\r\nThis fixes #24453 which had a related issue where `available` and `free` bytes\r\ncould also be so large that they were negative. These will now return\r\n`Long.MAX_VALUE` for the bytes if the JDK returns a negative value.\r\n Adjust available and free bytes to be non-negative on huge FSes >>> 1
1835,"In order to support script engines creating arbitrary instance\r\nclasses from script contexts, we need there to be a single level of\r\nscript construction. Currently search scripts have 2 levels. First you\r\nconstruct a SearchScript using the factory, then you would call\r\ngetLeafSearchScript to create instances of a completely different type,\r\nLeafSearchScript. Having script engines implement this middle tier would\r\nbe difficult to do in a generic way.\r\n\r\nThis commit makes SearchScript an abstract class and adds a `forSegment`\r\nmethod which duplicates the script and binds it to the given segment.\r\nThis means search scripts returned by a search script factory\r\nare not runnable, and clones must be creating which are bound to a segment.\r\n\r\nrelates #20426 Scripting: Conslidate SearchScript and LeafSearchScript >>> 0"
1836,"The Lucene version constants for 5.4.1 and 5.5.0 are wrong, they are listed as 6.5.0 instead of 6.5.1. This commit fixes these issues, and adds a test to ensure that this does not happen again.\r\n Verify Lucene version constants >>> 1"
1837,The `took` time computed for search requests does not take in account the expand search phase.\r\nThis change delays the computation to after the expand phase finishes.\r\n\r\nRelates #24900 Compute the took time of the query after the expand phase of field collapsing >>> 1
1838,"SearchScrollRequest can be created from a request body, but it doesn't support the opposite, meaning printing out its content to an XContentBuilder. This is useful to the high level REST client and allows for better testing of what we parse.\r\n\r\nMoved parsing method from RestSearchScrollAction to SearchScrollRequest so that fromXContent and toXContent sit close to each other. Added unit tests to verify that body parameters override query_string parameters when both present (there is already a yaml test for this but unit test is even better)\r\n\r\nRelates to #3889 SearchScrollRequest to implement ToXContentObject >>> 1"
1839,ClearScrollResponse can print out its content into an XContentBuilder as it implements ToXContentObject. This PR add a fromXContent method to it so that we are able to recreate the response object when parsing the response back. This will be used in the high level REST client. Add fromXContent method to ClearScrollResponse >>> 1
1840,The `IndexDeletionPolicy` is currently instantiated by `IndexShard` and is then passed through to the engine as a parameter. That's a shame as it is really just an implementation detail and the engine already has a method to acquire a commit.\r\n\r\nThis is preparing for a follow up PR that will we connect the index deletion policy with a new translog deletion policy.\r\n\r\nRelates to #10708  Move the IndexDeletionPolicy to be engine internal >>> 1
1841,"This commit introduces a clean transition from the old primary term to the new primary term when a replica is promoted primary. To accomplish this, we delay all operations before incrementing the primary term. The delay is guaranteed to be in place before we increment the term, and then all operations that are delayed are executed after the delay is removed which asynchronously happens on another thread. This thread does not progress until in-flight operations that were executing are completed, and after these operations drain, the delayed operations re-acquire permits and are executed.\r\n\r\nRelates #10708\r\n Introduce clean transition on primary promotion >>> 1"
1842,"Today if the primary throws an exception while handling the replica response (e.g., because it is already closed while updating the local checkpoint for the replica), or because of a bug that causes an exception to be thrown in the replica operation listener, this exception is caught by the underlying transport handler plumbing and is translated into a response handler failure transport exception that is passed to the onFailure method of the replica operation listener. This causes the primary to turn around and fail the replica which is a disastrous and incorrect outcome as there's nothing wrong with the replica, it is the primary that is broken and deserves a paddlin'. This commit handles this situation by failing the primary.\r\n\r\nCloses #24935 Handle primary failure handling replica response >>> 1"
1843,This is a follow-up to #23941. Currently there are a number of\r\ncomplexities related to compression. The raw `DeflaterOutputStream` must\r\nbe closed prior to sending bytes to ensure that EOS bytes are written.\r\nBut the underlying `ReleasableBytesStreamOutput` cannot be closed until\r\nthe bytes are sent to ensure that the bytes are not reused.\r\n\r\nRight now we have three different stream references hanging around in\r\n`TCPTransport` to handle this complexity. This commit introduces\r\n`CompressibleBytesOutputStream` to be one stream implemenation that will\r\nbehave properly with or without compression enabled.\r\n Add CompressibleBytesOutputStream for compression >>> 1
1844,"The order in which double values are added in Java can give different results,\r\nso in testing the sum and sumOfSquares we need to allow some delta for testing\r\nequality. The difference can be larger for large sum values, so we should\r\naccount for this by making the delta in the assertion depend on the values\r\nmagnitude.\r\n\r\nCloses #24931 [Tests] Harden InternalExtendedStatsTests >>> 1"
1845,"We default to 0 replicas in the rolling restart scenario already to ensure\r\nwe test against worst case. Yet, this adds a dummy index to ensure we also\r\nrecover and index with replicas just fine.\r\n Add a dummy_index to upgrade tests to ensure we recover fine with replicas >>> 1"
1846,The high level REST client supports _search but not _search/scroll yet. This PR adds support for it. Integration tests will be added to SearchIT once clear scroll is also supported.\r\n\r\nRelates to #23331 Add search scroll method to high level REST client >>> 1
1847,This change fixes the script field sort when the returned type is a number.\r\n\r\nCloses #24940 Fix script field sort returning Double.MAX_VALUE for all documents >>> 1
1848,"DateProcessor's DateFormat UNIX format parser resulted in\r\na floating point rounding error when parsing certain stringed\r\nepoch times. Now Double.parseDouble is used, preserving the\r\nintended input. Fix floating-point error when DateProcessor parses UNIX >>> 1"
1849,This commit adds support in ParsedMatrixStats for parsing the doc_count\r\nfield now it has been added in #24776. Add doc_count to ParsedMatrixStats >>> 1
1850,"Currently, the decisions regarding which translog generation files to delete are hard coded in the interaction between the `InternalEngine` and the `Translog` classes. This PR extracts it to a dedicated class called `TranslogDeletionPolicy`, for two main reasons:\r\n\r\n1) Simplicity - the code is easier to read and understand (no more two phase commit on the translog, the Engine can just commit and the translog will respond)\r\n2) Preparing for future plans to extend the logic we need - i.e., retain multiple lucene commit and also introduce a size based retention logic, allowing people to always keep a certain amount of translog files around. The latter is useful to increase the chance of an ops based recovery.\r\n Introducing a translog deletion policy >>> 1"
1851,"Covers GlobalOrdinalsSignificantTermsAggregator, GlobalOrdinalsSignificantTermsAggregator.WithHash, SignificantLongTermsAggregator and SignificantStringTermsAggregator\r\n\r\nRemoved integration test\r\n\r\nRelates #22278\r\n Added unit test coverage for SignificantTerms >>> 1"
1852,`terms` aggregations at the root level use the `global_ordinals` execution hint by default.\r\nWhen all sub-aggregators can be run in `breadth_first` mode the collected buckets for these sub-aggs are dense (remapped after the initial pruning).\r\nBut if a sub-aggregator is not deferrable and needs to collect all buckets before pruning we don't remap global ords and the aggregator needs to deal with sparse buckets.\r\nMost (if not all) aggregators expect dense buckets and uses this information to allocate memories.\r\nThis change forces the remap of the global ordinals but only when there is at least one sub-aggregator that cannot be deferred.\r\n\r\nRelates #24788 Terms aggregation should remap global ordinal buckets when a sub-aggregator is used to sort the terms >>> 1
1853,"Currently, entire classpath is in exception message, which is not very helpful:\r\n```\r\nCaused by: java.lang.IllegalStateException: jar hell!\r\nduplicate jar on classpath: /Applications/IntelliJ IDEA.app/Contents/lib/...\r\n```\r\n\r\nAfter the change, only problematic jar should be included in the message.\r\n IllegalStateException: Only duplicated jar instead of classpath >>> 1"
1854,Relates to #24939 Port OldIndexBackwardsCompatibilityIT#assertBasicSearchWorks over to full cluster restart qa module >>> 1
1855,and removed terms agg java integ tests that were replaced by unit tests.\r\n\r\nRelates to #22278\r\n\r\n Add more unit tests for terms aggregation >>> 1
1856,"Move `keyword_marker`, `trim`, `snowball` and `porter_stemmer` tokenfilter factories from core to common-analysis module.\r\n\r\nRelates to #23658 Move several token filters to common-analysis module >>> 1"
1857,"This makes profiling classes acquire a timer up-front that can be then reused\r\nacross all calls, in order to save bound checks for methods that are called in\r\ntight loops. Eliminate array access in tight loops when profiling is enabled. >>> 1"
1858,"adds `exclude_keys` option, just like Logstash allows. Where, if included, a field will not show up in the resulting document. Attempts to preserve exact behavior.\r\n\r\nCloses #23856 add `exclude_keys` option to KeyValueProcessor >>> 1"
1859,PR for #24958 Stop using the `mapping.single_type` setting in percolator tests. >>> 1
1860,"`AggregationsTests.createTestInstance()` is used to generate a random aggregation that can contain multiple sub aggregations. The number of sub aggregation is limited (from 0 to 4) but sub aggregations can also have sub aggregations. This aggregation ""tree"" is limited in depth (only 5 levels of inner aggregations are allowed) but it can still get very big and cause OOM errors. This is mostly due to the way multi bucket aggregation are generated: each multi bucket aggregation is created with many buckets (up to 10 for histograms) and each bucket will be created with the same set of sub aggregations... It basically multiplies the number of aggregation for each bucket at each level of the aggregation tree, and it blows when printing out the XContent.\r\n\r\nThis pull request changes the InternalMultiBucketAggregationTestCase class so that it now has a `maxNumberOfBuckets` attribute with a default value of 25. When executed, up to 25 buckets can be generated by InternalMultiBucketAggregationTestCase sub classes. In specific cases like `AggregationsTests` and `SearchResponseTests`, where many aggregations are combined, the `maxNumberOfBuckets` is lowered down to 3. \r\n\r\nCloses #24891 [Test] Reduce number of buckets in SearchResponseTests and AggregationsTests >>> 1"
1861,"When a primary is promoted, it could have gaps in its history due to concurrency and in-flight operations when it was serving as a replica. This commit fills the gaps in the history of the promoted shard after all operations from the previous term have drained, and future operations are blocked. This commit does not handle replicating the no-ops that fill the gaps to any remaining replicas, that is the responsibility of the primary/replica sync that we are laying the ground work for.\r\n\r\nRelates #10708\r\n Fill gaps on primary promotion >>> 1"
1862,It needs to override the `normalize` method. PatternAnalyzer should lowercase wildcard queries when `lowercase` is true. >>> 1
1863,This commit sets the number of processes in the systemd unit file for Elasticsearch to meet the bootstrap checks.\r\n\r\nRelates #20874\r\n Set number of processes in systemd unit file >>> 1
1864,"In previous work, we refactored the delay mechanism in index shard operation permits to allow for async delaying of acquisition. This refactoring made explicit when permit acquisition is disabled whereas previously we were relying on an implicit condition, namely that all permits were acquired by the thread trying to delay acquisition. When using the implicit mechanism, we tried to acquire a permit and if this failed, we returned a null releasable as an indication that our operation should be queued. Yet, now we know when we are delayed and we should not even try to acquire a permit. If we try to acquire a permit and one is not available, we know that we are not delayed, and so acquisition should be successful. If it is not successful, something is deeply wrong. This commit takes advantage of this refactoring to simplify the internal implementation.\r\n\r\nRelates #24925\r\n\r\n Clarify acquiring index shard permit >>> 1"
1865,As the title says.  Allows for easier management of compilation of individual interfaces on a per script context basis. Make Painless Compiler Use an Instance Per Context >>> 1
1866,"ScriptContexts currently understand a FactoryType that can produce\r\ninstances of the script InstanceType. However, for search scripts, this\r\ndoes not work as we have the concept of LeafSearchScript that is created\r\nper lucene segment. This commit effectively renames the existing\r\nSearchScript class into SearchScript.LeafFactory, which is a new,\r\noptional, class that can be defined within a ScriptContext.\r\nLeafSearchScript is effectively renamed back into SearchScript. This\r\nchange allows the model of stateless factory -> stateful factory ->\r\nscript instance to continue, but in a generic way that any script\r\ncontext may take advantage of.\r\n\r\nrelates #20426\r\n Add StatefulFactoryType as optional intermediate factory in script contexts >>> 1"
1867,"A continuation of #24869, this drops the `name()` method from\r\nall remaining analysis component factories, moving name members\r\nto `CustomAnalayzer` in service of the `_analyze` action.\r\n\r\nLike #24869 I've kept a `String` in the method signature of the\r\nctor of some abstract components so that back these analysis\r\ncomponent factories. It is *much* more convenient to remove all\r\nthe names at once in a followup. That change will be large but\r\npurely mechanical.\r\n Drop name from all analysis component factories >>> 0"
1868,"This metric is not used in the ES codebase at all. It's also not as likely to be\r\nused since it relies on a periodic ""tick"", which we don't currently use. Remove unused MeterMetric and specialized EWMA >>> 1"
1869,max_doc condition for index rollover should use document count only from primary shards \r\n\r\nFixes #24217\r\n Rollover max docs should only count primaries >>> 1
1870,"Be able to update child type mapping without specifying it's `_parent` field, because it should be considered as unchanged if it is not explicitly specified.\r\nIn this case, the merged mapper's `parentType` field should be `null`, so we can just merge it to keep the original one instead of throwing an exception of ""trying to change it to 'null' ""\r\n\r\nClose #23381  keep _parent field while updating child type mapping >>> 1"
1871,Closes #24129\r\n Fix context suggester to read values from keyword type field >>> 1
1872,"This change adds a new field mapper named ParentJoinFieldMapper. This mapper is a replacement for the ParentFieldMapper but instead of using the types in the mapping\r\nit uses an internal field to materialize parent/child relation within a single index.\r\nThis change also adds a fetch sub phase that automatically retrieves the join name (parent or child name) and the parent id for child documents in the response hit fields.\r\nThe compatibility with `has_parent`, `has_child` queries and `children` agg will be added in a follow up.\r\n\r\nRelates #20257 Introduce ParentJoinFieldMapper, a field mapper that creates parent/child relation within documents of the same index >>> 1"
1873,"Support normalizer param and custom normalizer with char_filter/filter param.\r\n\r\nIn this PR, I didn't change a response.\r\nIf user send a request with keyword field name or normalizer name, analyze api display a response with tokenizer that is KeywordTokenizer.\r\nShould we change a response format for normalizer?\r\n\r\nCloses #23347 [Analysis] Support normalizer in request param >>> 1"
1874,Currently global ordinals are documented under `fielddata`. It moves them to\r\ntheir own file since they also work with doc values and fielddata is on the way\r\nout.\r\n\r\nCloses #23101 Reorganize docs of global ordinals. >>> 1
1875,"Today there is a lot of code duplication and different handling of errors\r\nin the two different scroll modes. Yet, it's not clear if we keep both of\r\nthem but this simplification will help to further refactor this code to also\r\nadd cross cluster search capabilities.\r\n\r\nThis refactoring also fixes bugs when shards failed due to the node dropped out of the cluster in between scroll requests and failures during the fetch phase of the scroll. Both places where simply ignoring the failure and logging to debug. This can cause issues like #16555\r\n\r\n Extract a common base class for scroll executions >>> 1"
1876,"By default, the remove plugin CLI command preserves configuration files. This is so that if a user is upgrading the plugin (which is done by first removing the old version and then installing the new version) they do not lose their configuration file. Yet, there are circumstances where preserving the configuration file is not desired. This commit adds a purge option to the remove plugin CLI command.\r\n\r\n Add purge option to remove plugin CLI >>> 1"
1877,This adds a basic unit test for the PathHierarchyTokenizerFactory and checks some analysis outputs.\r\n [Tests] Add unit test for PathHierarchyTokenizerFactory >>> 1
1878,"This is the first step towards adaptive replica selection (#24915). This PR\r\ntracks the execution time, also known as the ""service time"" of a task in the\r\nthreadpool. The `QueueResizingEsThreadPoolExecutor` then stores a moving average\r\nof these task times which can be retrieved from the executor.\r\n\r\nCurrently there is no functionality using the EWMA yet (other than tests), this\r\nis only a bite-sized building block so that it's easier to review.\r\n\r\n[1]: EWMA = Exponentially Weighted Moving Average\r\n Track EWMA[1] of task execution time in search threadpool executor >>> 1"
1879,"The nodes usage API has 2 main endpoints\r\n\r\n/_nodes/usage and /_nodes/{nodeIds}/usage return the usage statistics\r\nfor all nodes and the specified node(s) respectively.\r\n\r\nAt the moment only one type of usage statistics is available, the REST\r\nactions usage. This records the number of times each REST action class is\r\ncalled and when the nodes usage api is called will return a map of rest\r\naction class name to long representing the number of times each of the action\r\nclasses has been called.\r\n\r\nStill to do:\r\n\r\n* [x] Create usage service to store usage statistics\r\n* [x] Record usage in REST layer\r\n* [x] Add Transport Actions\r\n* [x] Add REST Actions\r\n* [x] Tests\r\n* [x] Documentation\r\n* [ ] Check for references to statistics and replace with ""feature usage""\r\n Adds nodes usage API to monitor usages of actions >>> 1"
1880,This commit provides the TransportRequest that caused the retrieval of a search context to the\r\nSearchOperationListener#validateSearchContext method so that implementers have access to the\r\nrequest. Provide the TransportRequest during validation of a search context >>> 1
1881,This is related to #24927. There was a small possibility that a test\r\nwas attempting to compress a stream with zero bytes. This was causing\r\na failure.\r\n\r\nThis test now requires at least one byte. Fix broken build from stream with zero bytes >>> 1
1882,openSUSE-13 has reached [EOL](https://en.opensuse.org/Lifetime).\r\n\r\nReplace openSUSE-13 with openSUSE-42 (Leap) for packaging tests and\r\nupdate docs.\r\n Tests: Switch to openSUSE 42 (Leap) for packaging tests >>> 1
1883,Close #24969  Update tika version to 1.15 >>> 1
1884,The error message was confusing because it doesn't include unreleased versions like CURRENT. Improve verifyVersions error message >>> 1
1885,"This commit adds a new `branchConsistency` task which will run in CI\r\nonce a day, instead of on every commit. This allows `verifyVersions` to\r\nnot break immediately once a new version is released in maven.\r\n Build: Move verifyVersions to new branchConsistency task >>> 1"
1886,It looks like many unnecessary files remain in the core test resources directory. This pull request spring cleans them. [Test] Remove unused test resources from core >>> 1
1887,\r\nCloses #24836 [DOCS] Clarify connections and gateway nodes selection in cross cluster search docs >>> 1
1888,"Previously, when allocating bytes for a BigArray, the array was created\r\n(or attempted to be created) and only then would the array be checked\r\nfor the amount of RAM used to see if the circuit breaker should trip.\r\n\r\nThis is problematic because for very large arrays, if creating or\r\nresizing the array, it is possible to attempt to create/resize and get\r\nan OOM error before the circuit breaker trips, because the allocation\r\nhappens before checking with the circuit breaker.\r\n\r\nThis commit ensures that the circuit breaker is checked before all big\r\narray allocations (note, this does not effect the array allocations that\r\nare less than 16kb which use the [Type]ArrayWrapper classes found in\r\nBigArrays.java).  If such an allocation or resizing would cause the\r\ncircuit breaker to trip, then the breaker trips before attempting to\r\nallocate and potentially running into an OOM error from the JVM.\r\n\r\nCloses #24790 Checks the circuit breaker before allocating bytes for a new big array >>> 1"
1889,"This commit adds an optional `context` url parameter to the put stored\r\nscript request. When a context is specified, the script is compiled\r\nagainst that context before storing, as a validation the script will\r\nwork when used in that context.\r\n Scripting: Add optional context parameter to put stored script requests >>> 1"
1890,Note: Code will be cleaned up in an overhaul later.  This is simply to get progress towards working customizable script contexts.\r\n\r\nUpdate Painless to use ScriptContexts with the following rules:\r\n    *  All public methods starting with get will be added as local variables\r\n    to the execute method.\r\n    * The execute method on a ScriptContext must be both public and\r\n    abstract.  This method will be implemented by the Painless compiler.\r\n    * A static list of parameter names for the execute method must be\r\n    provided since the names will be eliminated at runtime.\r\n    * The uses$ methods will still be implemented as before.\r\n    * A single constructor may be provided by the ScriptContext.  This\r\n    constructor will be overridden by the Painless compiler to include the\r\n    exact same arguments.  This allows instances of a Painless script to\r\n    potentially contain state.  If a constructor is not provided it is\r\n    assumed the default constructor with no arguments will be used. Update Painless to Use New Script Contexts >>> 1
1891,"This change ensures that there is a single parent-join field defined per mapping.\r\nThe verification is done through the addition of a special field mapper (MetaJoinFieldMapper) with a unique name (_parent_join) that is registered to the mapping service\r\nwhen the first parent-join field is defined. If a new parent-join is added, this field mapper will clash with the new one and the update will fail.\r\nThis change also simplifies the parent join fetch sub phase by retrieving the parent-join field without iterating on all fields in the mapping. Disallow multiple parent-join fields per mapping >>> 1"
1892,"This removes the `accumulateExceptions()` method (and its usage) from `TransportNodesAction` and `TransportTasksAction`, forcing both transport actions to always accumulate exceptions.\r\n\r\nWithout this change, some transport actions, like `TransportNodesStatsAction` would respond in very unexpected ways by returning no response due to some failure, but instead of returning an error the response would simply be empty: no response and no error.\r\n\r\nThis results in a very trappy response structure where users can check for an error, then attempt to blindly use the response when no error is returned.\r\n\r\nSecond part of #23099. Always Accumulate Transport Exceptions >>> 1"
1893,This change allows `eager_global_ordinals` to be set globally for the `parent-join` field.\r\nDefaults to `true`. Add the ability to set eager_global_ordinals in the new parent-join field >>> 1
1894,We can hit an already closed exception when filling the gaps after blocking operations when updating the primary term on a promoted replica shard. We should catch this and suppress it as it is an expected outcome instead of letting it bubble up which leads to trying to fail the shard which throws yet another already closed exception.\r\n\r\nRelates #24925\r\n Handle already closed while filling gaps >>> 1
1895,"and at the same time maintaining support for the `_parent` meta field type.\r\n\r\nNotes:\r\n* The has_child and has_parent query builder tests still need to be changed to use join field mapper. We can do that once we port inner hits over to join field type. (these tests randomily generate queries with inner hits)\r\n* I had to make some classes / constructor public for testing. If we really don't want that then we should move all classes into a single `o.e.join` package.\r\n\r\nRelates to #20257\r\n Change `has_child`, `has_parent` queries and `childen` aggregation to work with the new join field type >>> 1"
1896,"This commit changes the `RestHighLevelClient` class so that it now accepts a list of plugin classes as a constructor parameter. Similarly to the `PreBuiltTransportClient`, the RestHighLevelClient uses the PluginService to load the plugins and then extracts their aggregations and suggesters specs. The aggregation specs have been changed in core in order to add a `addResultParser` method that allows to reference the parsing methods of an aggregation. This information is use in the RestHighLevelClient\r\n to register additional NamedXContent entries to parse back custom aggregations/suggesters.\r\n Allow RestHighLevelClient to use plugins >>> 0"
1897,"This change removes the `postings` highlighter. This highlighter has been removed from Lucene master (7.x) because it behaves\r\nexactly like the `unified` highlighter when index_options is set to `offsets`:\r\nhttps://issues.apache.org/jira/browse/LUCENE-7815\r\n\r\nIt also makes the `unified` highlighter the default choice for highlighting a field (if `type` is not provided).\r\nThe strategy used internally by this highlighter remain the same as before, it checks `term_vectors` first, then `postings` and ultimately it re-analyzes the text.\r\nThis change also rewrites the docs so that the options that the `unified` highlighter cannot handle are clearly marked as such.\r\nThere are few features that the `unified` highlighter is not able to handle which is why the other highlighters (`plain` and `fvh`) are still available.\r\nI'll open separate issues for these features and we'll deprecate the `fvh` and `plain` highlighters when full support for these features have been added to the `unified`. Remove the postings highlighter and make unified the default highlighter choice >>> 1"
1898,"This removes the parsing of things like `GET /idx/_aliases,_mappings`, instead,\r\na user must choose between retriving all index metadata with `GET /idx`, or only\r\na specific form such as `GET /idx/_settings`.\r\n\r\nRelates to (and is a prerequisite of) #24437\r\n Remove comma-separated feature parsing for GetIndicesAction >>> 1"
1899,"This catches `AlreadyClosedException` during `stats` calls to avoid failing a `_nodes/stats` request because of the ignorable, concurrent index closure.\r\n\r\nPart of #23099 _nodes/stats should not fail due to concurrent AlreadyClosedException >>> 1"
1900,Fixes the plumbing so plugins can register char filters and moves\r\nthe `html_strip` char filter into analysis-common.\r\n\r\nRelates to #23658\r\n Plugins can register pre-configured char filters >>> 1
1901,"Pins the random testing seed at build start rather than letting\r\nit vary with every randomized testing invocation. This is useful\r\nfor projects where random decisions in one randomized testing run\r\ncan effect the outcome of a second randomized testing run such as\r\nthe full cluster restart tests.\r\n\r\nThe goal isn't for tests to be able to assume that random decision\r\nwill be the same in both tests. It is more to make sure that the\r\nseed printed when a test fails reproduces the appropriate random\r\ndecisions. And pinning the seed at startup should do just that.\r\n\r\nThis works by taking the key passed as a system property if one\r\nis passed, otherwise picking a random long and getting it into\r\nappropriate key format. The build just calls\r\n`new Random().nextLong()` to get the seed while randomized testing\r\nuses a Murmur3 hash of `System.nanoTime`.\r\n Pin the random seed at start of the build >>> 1"
1902,This adds an option to `ClusterConfiguration` to preserve the\r\n`shared` directory when starting up a new cluster and switches\r\nthe `qa:full-cluster-restart` tests to use it rather than\r\ndisable the clean shared task.\r\n\r\nRelates to #24846\r\n Allow preserving shared dir for full-cluster-restart tests >>> 1
1903,"Both gradle and java code attempt to infer whether or not each\r\nversion in Version.java is released or not and whether or not it is\r\nwire compatible or index compatible. It is super important that\r\nthey infer the same things about each version. If they disagree\r\nwe might accidentally not be testing backwards compatibility for\r\nsome version.\r\n\r\nThis adds a test to make sure that they agree, modulo known and\r\naccepted differences (mostly around alphas). It also changes the\r\nminimum wire compatible version from the released 5.4.0 to the\r\nunreleased 5.5.0 as that lines up with the gradle logic.\r\n\r\nRelates to #24798\r\n Test that gradle and Java version types match >>> 1"
1904,This commit creates TemplateScript and associated classes so that\r\ntemplates no longer need a special ScriptService.compileTemplate method.\r\nThe execute() method is equivalent to the old run() method.\r\n\r\nrelates #20426 Scripting: Convert CompiledTemplate to a ScriptContext >>> 1
1905,"This is the deprecation logging and documentation for 5.x related to #24723. It\r\nlogs when a user does a request like:\r\n\r\n```\r\nGET /index/_alias,_mapping\r\n```\r\n Add deprecation logging for comma-separated feature parsing >>> 1"
1906,This completes the support for scroll in the high level REST client. An integration test is also added which was taken from ScrollIT and migrated from Java api code. Add support for clear scroll to high level REST client >>> 1
1907,This splits `executeUpdateRequest` into separate parts and adds some unit tests\r\nfor the behavior in it. The actual behavior has not been changed. Refactor TransportShardBulkAction.executeUpdateRequest and add tests >>> 1
1908,`GeoDistance` enum ordinals from 5.3+ are not backwards compatible with earlier client versions.. This PR adds backward compatibilty support in `GeoDistance` serialization.\r\n\r\ncloses #24816  Fix GeoDistance Ordinal for BWC >>> 1
1909,This change skips rest tests that use multiple types if the cluster\r\nis a pure 6.x cluster. This allows all indices to be created with a version\r\nless than 6.0 and that means we can safely use the `mapping.single_type` setting.\r\n\r\nRelates to #24961\r\n\r\n Skip rest tests that use mutiple types in pure 6.x clusters >>> 1
1910,Is it a way to solve refresh stats tracking by passing `refreshMetric` into `get` function ? And it just keeps the refresh number count as before.\r\n\r\nRelated to #24806  Add refresh stats tracking for realtime get >>> 1
1911,The set of previously-seen tokens in a doc was allocated per-JSON-field string value rather than once per JSON document meaning the number of docs containing a term could be over-counted leading to exceptions from the checks in significance heuristics. Added unit test for this scenario\r\n\r\nCloses #25029 Aggregations bug: Significant_text fails on arrays of text. >>> 1
1912,Remove processor is now able to handle an array. The parameter name is still `field`. Should we rename it to `fields` ? And deprecates the name `field` in `5.x` branch? \r\n\r\nClose #24622  Allow removing multiple fields in ingest processor >>> 1
1913,"I have noticed that any time an inline action would be declared in the list of processors, an error will be thrown as a Map is expected in the _ReadProcessor_ method of ConfigurationUtils. As those errors happen at runtime I added a check in ConfigurationUtils that throws an error if the value is not a map. I added a unit test as well to confirm the behaviour is as expected.\r\n\r\nEdit: referencing the issue #23824  Better handling of inline scripts in ingest #23824 >>> 0"
1914,"This commit fixes a bug in retrieving a sub Settings object for a given\r\nprefix with secure settings. Before this commit the returned Settings\r\nwould be filtered by the prefix, but the found setting names would not\r\nhave the prefix removed.\r\n\r\nNote that nothing was yet using the prefix behavior for secure settings, so I marked this as a non-issue.\r\n Settings: Fix secure settings by prefix >>> 1"
1915,This PR enables Ingest plugins to leverage processor-scoped REST\r\nendpoints. First of which being the Grok endpoint that retrieves\r\nGrok Patterns for users to retrieve all the built-in patterns.\r\nExample usage: Kibana Grok Autocomplete!\r\n\r\nCloses #24725 Add Ingest-Processor specific Rest Endpoints & Add Grok endpoint >>> 1
1916,"Unknown patterns used to silently be ignored. This was a problem because users did not know they were providing an invalid pattern name, and maybe thought the rest of their regexes were invalid.\r\n\r\nFixes #22831. fix grok's pattern parsing to validate pattern names in expression >>> 1"
1917,Modifies randomVersionBetween so that it works with unreleased\r\nversions. This should make switching a version from unreleased\r\nto released much simpler. Make randomVersionBetween work with unreleased versions >>> 1
1918,"The test infrastructure method `ensureStableCluster` with local flag set to true internally issues a cluster health request. Cluster health requests with local flag set to true are trappy, however, as they will happily the right number of nodes even if the node is not part of an actual cluster but has a NO_MASTER_BLOCK up (see https://github.com/elastic/elasticsearch/issues/24457#issuecomment-298882617).\r\n\r\nThis commit removes the ""local"" parameter from `ensureStableCluster` and changes the few callers that were using the flag. [TEST] Never use local=true in ensureStableCluster method >>> 0"
1919,This change moves the parent_id query to the parent-join module and handles the case when only the parent-join field can\r\nbe declared on an index (index with single type on).\r\nIf single type is off it uses the legacy parent join field mapper and switch to the new one otherwise (default in 6).\r\n\r\nRelates #20257 Move parent_id query to the parent-join module >>> 1
1920,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)? Yes\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)? Yes Tiny fix for a word in the doc >>> 1"
1921,The `postings` highlighter is deprecated in Lucene and will be replaced by the `unified`.\r\nThis change adds a deprecation warning for removal in 6.0.\r\n\r\nRelates https://github.com/elastic/elasticsearch/pull/25028\r\n Postings highlighter deprecation >>> 1
1922,"During package install on systemd-based systems, some sysctl settings\r\nshould be set (e.g. vm.max_map_count).\r\n\r\nIn some environments, changing sysctl settings plainly does not work;\r\npreviously a global environment variable named ES_SKIP_SET_KERNEL_PARAMETERS\r\nwas introduced to skip calling sysctl, but this causes trouble for:\r\n * configuration management systems, which usually cannot apply an env\r\n   var when running a package manager\r\n * package upgrades, which will not have the env var set any more, and\r\n   thus leaving the package management system in a bad state (possibly\r\n   half-way upgraded, can be very hard to recover)\r\n\r\nThis removes the env var again and instead of calling systemd-sysctl manually,\r\ntells systemd to restart the wrapper unit - which itself can be masked by\r\nsystem administrators or management tools if it is known that sysctl does\r\nnot work in a given environment.\r\nThe restart is not silent on systems in their default configuration, but\r\nis ignored if the unit is masked.\r\n\r\nRelated: #21899, elastic/puppet-elasticsearch#806\r\n Honor masking of systemd-sysctl.service >>> 1"
1923,"When we open a translog, we rely on the `translog.ckp` file to tell us what the maximum generation file should be and on the information stored in the last lucene commit to know the first file we need to recover. This requires coordination and is currently subject to a race condition: if a node dies after a lucene commit is made but before we remove the translog generations that were unneeded by it, the next time we open the translog we will ignore those files and never delete them (I have added tests for this).\r\n\r\nThis PR changes the approach to have the translog store both of those numbers in the `translog.ckp`. This means it's more self contained and easier to control. \r\n\r\nThis change also decouples the translog recovery logic from the specific commit we're opening. This prepares the ground to fully utilize the deletion policy introduced in #24950 and store more translog data that's needed for Lucene, keep multiple lucene commits around and be free to recover from any of them. Translog file recovery should not rely on lucene commits >>> 1"
1924,Fix NPE in token_count datatype with null value (#24928) token_count datatype should handle null value >>> 1
1925,"Previously the HEAD and GET aliases endpoints were misaligned in behavior. The HEAD verb would 404 if any aliases are missing while the GET verb would not if any aliases existed. When HEAD was aligned with GET, this broke the previous usage of HEAD to serve as an existence check for aliases. It is the behavior of GET that is problematic here though, if any alias is missing the request should 404. This commit addresses this by modifying the behavior of GET to behave in this way. This fixes the behavior for HEAD to also 404 when aliases are missing.\r\n\r\nCloses #24644\r\n GET aliases should 404 if aliases are missing >>> 1"
1926,This commit fixes the group methods of Settings to properly include\r\ngrouped secure settings. Previously the secure settings were included\r\nbut without the group prefix being removed.\r\n\r\ncloses #25069 Settings: Fix setting groups to include secure settings >>> 1
1927,nan Bumping version to v6.0.0-alpha3 >>> 1
1928,at the same time maintaining support for the `_parent` meta field type.\r\n\r\nRelates to #20257 Changed inner_hits to work with the new join field type and >>> 1
1929,"We use a callback in recovery land during primary relocation to ensure the relocation target is on at least the same version as the relocation source. This callback is typed as a `Callback<Long>` which is an unnecessary custom type (we can use `Consumer<T>` or the appropriate primitive callbacks). Here, we can use `LongConsumer`.\r\n Modify cluster state callback in recovery land >>> 1"
1930,Those plugins don't replace the discovery logic but rather only provide a custom unicast host provider for their respective platforms. in 5.1 we introduced the  `discovery.zen.hosts_provider` setting to better reflect it. This PR removes BWC code in those plugins as it is not needed anymore\r\n\r\nFixes #24543\r\n\r\n Remove `discovery.type` BWC layer from the EC2/Azure/GCE plugins >>> 1
1931,nan Add version 5.6 to versions >>> 1
1932,The PR takes a different approach to solve #24806 than currently implemented via #25052. The `refreshMetric` that IndexShard maintains is updated using the refresh listeners infrastructure in lucene. This means that we truly count all refreshes that lucene makes and not have to worry about each individual caller (like `IndexShard@refresh` and `Engine#get()`) Update `IndexShard#refreshMetric` via a `ReferenceManager.RefreshListener` >>> 1
1933,We have a callback interface that is not needed because it is effectively the same as java.util.function.Consumer. This commit removes it.\r\n\r\nCloses #25078\r\n Remove unnecessary callback interface >>> 1
1934,nan  Bumping version to v5.6.0 >>> 1
1935,Ingest was using it's own wrapper around TemplateScripts and the ScriptService.\r\nThis commit removes that abstraction remove Ingest's Internal Template Service >>> 1
1936,"This PR backports the high level REST client supports the following apis: ping, info, index, bulk, get, delete, update, search, search scroll and clear scroll. Also, the BulkProcessor has been updated so that it can be used with the high level client as well.\r\n\r\nHere is a list of PRs that have already  been pushed to master and are part of this PR that is targeted to 5.x only:\r\n- Add REST high level client gradle submodule and first simple method (#22371)\r\n- Add get/exists method to RestHighLevelClient (#22680)\r\n- Add Index API to High Level Rest Client (#23040)\r\n- Add delete API to the High Level Rest Client (#23187)\r\n- Add UpdateRequest support to High Level Rest client (#23266)\r\n- Add BulkRequest support to High Level Rest client (#23312)\r\n- Add support for named xcontent parsers to high level REST client (#23328)\r\n- Add info method to High Level Rest client (#23350)\r\n- Add first High level client documentation (#23351)\r\n- Decouple BulkProcessor from client implementation (#23373)\r\n- Add search method to high level REST client (#24796)\r\n- Add search scroll method to high level REST client (#24938)\r\n- Add support for clear scroll to high level REST client (#25038) Backport high level REST client >>> 1"
1937,"This commit removes wrapper methods on QueryShardContext used to compile\r\nscripts. Instead, the script service is made accessible in the context,\r\nand calls to compile can be made directly. This will ease transition to\r\neach of those location becoming their own context, since they would no\r\nlonger be able to expect the same script class type.\r\n\r\n Scripting: Remove unnecessary intermediate script compilation methods on QueryShardContext >>> 1"
1938,You can read about the full changelog at\r\nhttps://lucene.apache.org/core/6_6_0/changes/Changes.html. Upgrade to Lucene 6.6.0. >>> 1
1939,The High Level REST Client can use Java's SPI to load named XContent parsers implementations provided by plugins or modules.\r\n\r\nThis pull request is an alternate solution for #25024 after [a suggestion](https://github.com/elastic/elasticsearch/pull/25024#issuecomment-305829768) from @rjernst and requires #25097 to be merged first. Use SPI in High Level Rest Client to load XContent parsers >>> 1
1940,"This commit moves the `assumeFalse()` calls that implement test skipping\r\nand blacklisting out of the `@Before` method of `ESClientYamlSuiteTestCase`.\r\nThe problem with having them in the `@Before` method is that if an\r\nassumption triggers then the `@Before` methods of classes that extend\r\n`ESClientYamlSuiteTestCase` will not run, but their `@After` methods will.\r\nThis can lead to inconsistencies that cause assertions in the `@After`\r\nmethods and fail the test even though it was skipped/blacklisted.\r\n\r\nInstead the `assumeFalse()` calls are now at the beginning of the `test()`\r\nmethod, which runs after all `@Before` methods (including those in classes\r\nthat extend `ESClientYamlSuiteTestCase`) have completed.  The only side\r\neffect is that overridden `test()` methods in classes that extend\r\n`ESClientYamlSuiteTestCase` which call `super.test()` and also do other things\r\nmust now be designed not to consume any `InternalAssumptionViolatedException`\r\nthat may be thrown by the `super.test()` call. [TEST] Move test skip/blacklist assumptions out of @Before method >>> 1"
1941,The unified highlighter rewrites MultiPhrasePrefixQuery to SpanNearQuer even when there is a single term in the phrase.\r\nThough SpanNearQuery throws an exception when the number of clauses is less than 2.\r\nThis change returns a simple PrefixQuery when there is a single term and builds the SpanNearQuery otherwise.\r\n\r\nRelates #25088 Higlighters: Fix MultiPhrasePrefixQuery rewriting >>> 1
1942,"This PR is a back port for the typeless parent-join in 5.x\r\nThe new field mapper is active only if `index.mapping.single_type` is true, otherwise the old parent/child can still be used.\r\n\r\n Typeless parent child backport >>> 1"
1943,It looks like eclipse blows up when you take a method reference to\r\nvarargs method in a finally method:\r\nhttps://bugs.eclipse.org/bugs/show_bug.cgi?id=517951\r\n Fix compilation in eclipse >>> 1
1944,This pull request adds a test that tests and demonstrates how `RestHighLevelClient` can be extended to support custom requests and responses. [Test] Add test for custom requests in High Level Rest Client >>> 1
1945,"The `scorerSupplier` API allows to give a hint to queries in order to let them\r\nknow that they will be consumed in a random-access fashion. We should use this\r\nfor aggregations, function_score and matched queries. Leverage scorerSupplier when applicable. >>> 1"
1946,Uses bytecode generation in Painless to create a factory that generates script instances based on a specific context. Generate Painless Factory for Creating Script Instances >>> 1
1947,"Previously in #24723 we changed the `_alias` API to not go through the\r\n`RestGetIndicesAction` endpoint, instead creating a `RestGetAliasesAction` that\r\ndid the same thing.\r\n\r\nThis changes the formatting so that it matches the old formatting of the\r\nendpoint, before:\r\n\r\n```\r\nGET /test-1/_alias\r\n\r\n{ }\r\n```\r\n\r\nAnd after this change:\r\n\r\n```\r\nGET /test-1/_alias\r\n\r\n{\r\n  ""test-1"": {\r\n    ""aliases"": {}\r\n  }\r\n}\r\n```\r\n\r\nThis is related to #25090 Return index name and empty map for /{index}/_alias with no aliases >>> 1"
1948,To complete the cross cluster search capabilities for all search types and\r\nfunction this change adds cross cluster search support for scroll searches.\r\n Add Cross Cluster Search support for scroll searches >>> 1
1949,Downstream users of out network intercept infrastructure need this information which is\r\nhidden due to member and class visibility. Add helper methods to TransportActionProxy to identify proxy actions and requests >>> 1
1950,This change extracts the main logic from `TransportClearScrollAction`\r\ninto a new class `ClearScrollController` and adds a corresponding unittest.\r\n\r\nRelates to #25094 Break out clear scroll logic from TransportClearScrollAction >>> 1
1951,"This PR modifies `query_string`, `simple_query_string` and `multi_match` queries to always use a DisjunctionMaxQuery when a disjunction over multiple fields is built. The tiebreaker is set to `1` in order to behave like the boolean query in terms of scoring. \r\nThe removal of the coord factor in Lucene 7 made this change mandatory to correctly handle `minimum_should_match`. \r\nSee https://github.com/elastic/elasticsearch/issues/23966#issuecomment-293578652\r\n\r\nCloses #23966 Always use DisjunctionMaxQuery to build cross fields disjunction >>> 1"
1952,"NOOPs are incorrectly written use the current primary term of the replica, which can be higher than the primary term of the original noop that was replicated. Use correct primary term for replicating NOOPs >>> 1"
1953,In order to add scroll support for cross cluster search we need\r\nto resolve the nodes encoded in the scroll ID to send requests to the\r\ncorresponding nodes. This change adds the low level connection infrastructure\r\nthat also ensures that connections are re-established if the cluster is\r\ndisconnected due to a network failure or restarts.\r\n\r\nRelates to #25094\r\n Add remote cluster infrastructure to fetch discovery nodes. >>> 1
1954,Binary script doc values should make a deep copy of the BytesRef before populating it in the values array. Before this change in case of array fields all the field values in a script would be pointing to the last element in the array.\r\n\r\nAlso cleaned up the BinaryDVFieldDataTests:\r\n* Use junit assertions instead of hamcrest\r\n* Use BytesRef directly instead of byte[]\r\n\r\nThis bug only exists in 6.0.0 alpha1 and alpha2 releases. Fix binary script doc values for multi valued fields >>> 1
1955,The FVH fails with an NPE when a match phrase prefix is rewritten in an empty phrase query.\r\nThis change makes sure that the multi match query rewrites to a MatchNoDocsQuery (instead of an empty phrase query) when there is\r\na single term and that term does not expand to any term in the index.\r\n\r\nFixes #25088 Fix Fast Vector Highlighter NPE on match phrase prefix >>> 1
1956,"The target of a primary relocation is not aware of the state of the replication group. In particular, it is not tracking in-sync and initializing shards and their checkpoints. This means that after the target shard is started, its knowledge of the replication group could differ from that of the relocation source. In particular, this differing view can lead to it computing a global checkpoint that moves backwards after it becomes aware of the state of the entire replication group. This commit addresses this issue by transferring a primary context during relocation handoff.\r\n\r\nRelates #10708, relates #25355\r\n\r\n Introduce primary context >>> 1"
1957,When parsing resonses we should be ignoring any new unknown fields or inner objects in most cases to be forward compatible with changes in core on the\r\nclient side. This change adds test for this for Suggestions and its various subclasses to check if we are able to ignore new fields and objects in the xContent. [Tests] Check Suggestion parsers robustness for new fields >>> 1
1958,When parsing resonses we should be ignoring any new unknown fields or inner objects in most cases to be forward compatible with changes in core on the\r\nclient side. This change adds test for this for QueryProfileShardResult and nested substructures and changes the parsing code where necessary to be able to\r\nignore new fields and objects in the xContent.\r\n [Tests] Check QueryProfileShardResult parser robustness for new fields >>> 1
1959,This commit adds a new `NamedXContentProvider` interface to core that can be implemented by plugins in order to provide a list of named XContent parsers implementations to external applications (like the High Level REST Client) using the Java Service Provider Interface.\r\n\r\nThis pull request uses Java's SPI as a alternate mechanism to `PluginService` in order to provide named XContent parsers to the High Level REST Client as suggested by @rjernst  in https://github.com/elastic/elasticsearch/pull/25024#issuecomment-305829768.\r\n\r\nFor now services files are static but they could be generated by Gradle. Use SPI in High Level Rest Client to load XContent parsers >>> 0
1960,"This commit adds back ""id"" as the key within a script to specify a\r\nstored script (which with file scripts now gone is no longer ambiguous).\r\nIt also adds ""source"" as a replacement for ""code"". This is in an attempt\r\nto normalize how scripts are specified across both put stored scripts and script usages, including search template requests. Scripting: Change keys for inline/stored scripts to source/id >>> 1"
1961,"Previously this would output:\r\n\r\n```\r\nGET /test-1/_mappings\r\n\r\n{ }\r\n```\r\n\r\nAnd after this change:\r\n\r\n```\r\nGET /test-1/_mappings\r\n\r\n{\r\n  ""test-1"": {\r\n    ""mappings"": {}\r\n  }\r\n}\r\n```\r\n\r\nTo bring parity back to the REST output after #24723.\r\n\r\nRelates to #25090\r\n Include empty mappings in GET /{index}/_mappings requests >>> 1"
1962,"When we disabled `_all` by default for indices created in 6.0, we missed adding\r\na layer that would handle the situation where `_all` was not enabled in 5.x and\r\nthen the cluster was updated to 6.0, this means that when the cluster was\r\nupdated the `_all` field would be disabled for 5.x indices and field values\r\nwould not be added to the `_all` field.\r\n\r\nThis adds a compatibility layer for 5.x indices where we treat the default\r\nenabled value for the `_all` field to be `true` if unset on 5.x indices.\r\n\r\nResolves #25068\r\n Correctly enable _all for older 5.x indices >>> 1"
1963,"The Log4j dependency is separated into two artifacts, the API and the core implementation. This is to enable replacing Log4j on the backend through the SLF4J bridge with another logging implementation. For this reason, the dependencies are marked as optional. This causes confusion amongst users as to use the bridge, the API should be non-optional since it is needed for the bridge to function correctly. While they could pull it into their application directly, it would be clearer if we simply marked this depdendency as non-optional. Note that this does not mean that users have to use Log4j for logging in their application, so we are not marking core as required, it only clarifies what they need to be able to plug in a different logging implementation.\r\n\r\nRelates #22671\r\n Mark Log4j API dependency as non-optional >>> 1"
1964,"We introduced a new API for ranges in order to be able to decide whether points\r\nor doc values would be more appropriate to execute a query, but since\r\n`ProfileWeight` does not implement this API, the optimization is disabled when\r\nprofiling is enabled. Make sure range queries are correctly profiled. >>> 1"
1965,"This adds a `moving_fn` pipeline aggregation which executes a function on the values in a sliding window.  For example, you can find the minimum value within a window of time, which then slides forward to find the next min, etc etc.\r\n\r\nIt supports four pre-built functions:\r\n\r\n- Min\r\n- Max\r\n- Sum\r\n- Median\r\n\r\nIt also supports user-defined scripts for custom functionality.\r\n\r\nThe PR looks larger than it really is for two reasons: first, the `movavg` package was renamed to `moving` so that it could accommodate the new agg.  Second, some of the methods of `MovAvgModel` were moved into a new `MovModel` class (and `MovAvgModel` now extends that).  That way simple functions like min can extend `MovModel` without extra cruft like predictions.\r\n\r\nAlso cleaned up some generics in movavg.... we only use Doubles, no need for generic numeric support.\r\n\r\nCloses #17607, although we may want to add some more functions in the future as mentioned in the issue (moving std dev, etc). Add MovingFunction pipeline aggregation >>> 0"
1966,Implements https://github.com/elastic/elasticsearch/issues/23044\r\n\r\nBased on https://github.com/elastic/elasticsearch/pull/23061 which is closed\r\n\r\nTODO\r\n - [x] Need to define version for this feature (for serialization) Create index request should return the index name >>> 1
1967,"This is a follow up to PR #19846 and the deprecation portion of #22914. Since Lucene optimizes Sinnot's haversine distance computation, there is no need to carry optional distance calculations. Therefore the distance_type parameter and GeoDistance class is deprecated.\r\n Deprecate distance_type parameter in GeoDistanceQueryBuilder >>> 0"
1968,In a similar way as https://github.com/elastic/elasticsearch/pull/25132 or #25130 this change extends the testing of SearchShardFailure to make sure we don't fail on additional fields the parser doesn't know yet.\r\n [Test] Extend test for parsing SearchShardFailure >>> 1
1969,This change extends the tests and parsing of SearchResponse to make sure we can skip additional fields the parser doesn't know for forward compatibility reasons. [Test] Extending parsing checks for SearchResponse >>> 1
1970,"just added a note that you can hit the server directly from the bowser no curl needed. Helps us poor little windows computer users.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n small update for us windows users >>> 0"
1971,"Sorted scroll search can use early termination when the index sort matches the scroll search sort.\r\nThe optimization can be done after the first query (which still needs to collect all documents)\r\nby applying a query that only matches documents that are greater than the last doc retrieved in the previous request.\r\nSince the index is sorted, retrieving the list of documents that are greater than the last doc\r\nonly requires a binary search on each segment.\r\nThis change introduces this new query called `SortedSearchAfterDocQuery` and apply it when possible.\r\nScrolls with this optimization will search all documents on the first request and then will early terminate each segment\r\nafter $size doc for any subsequent requests.\r\n\r\nRelates #6720 Speed up sorted scroll when the index sort matches the search sort >>> 1"
1972,"When `index.mapping.single_type` is `true` the `_uid` field is not used and instead `_id` field is used.\r\nPrior to this change nested documents would in this case still use the `_uid` field to mark to what root\r\ndocument they belong to. In case of deleting documents this could lead to only the root  Lucene document\r\nto be deleted and not the nested Lucene documents. This broke the docid block ordering the block join\r\nrelies on in order to work correctly and thus causing the `nested` query, `nested` aggregation, nested sorting\r\nand nested inner hits to either fail or yield incorrect results.\r\n\r\nThis bug only manifests in 6.0.0-ALPHA2 release and snaphots (5.5.0-SNAPSHOT, 5.6.0-SNAPSHOT, 6.0.0-SNAPSHOT). In case of a single type the _id field should be added to the nested document instead of _uid field >>> 1"
1973,"This PR extends the TranslogDeletionPolicy to allow keeping the translog files longer than what is needed for recovery from lucene. Specifically, we allow specifying the total size of the files and their maximum age (i.e., keep up to 512MB but no longer than 3 hours). This will allow making ops based recoveries more common. \r\n\r\nTo achieve the above the translog files are extended to contain a timestamp, indicating when they were created.\r\n\r\nNote that the default size and age still set to 0, maintaining current behavior. This is needed as the other components in the system are not yet ready for a longer translog retention. I will adapt those in follow up PRs.\r\n\r\nRelates to #10708  Introduce translog size and age based retention policies >>> 1"
1974,"When the cluster state is updated with Shard Started entries, it simply adds ""shard-started"" as the source of the change.\r\n\r\nThis adds the index name and shard ID so that we can see who/what is spamming the changes when the index creation step has already left the cluster state.\r\n\r\nFor example, while debugging this, it would have been helpful to know what shards these were because both indices only had a single shard, but there are three `shard-started` messages:\r\n\r\n```\r\n   > {time_in_queue=670ms, time_in_queue_millis=670, source=create-index [index1], cause [auto(bulk api)], executing=true, priority=URGENT, insert_order=209}\r\n   > {time_in_queue=647ms, time_in_queue_millis=647, source=create-index [index2], cause [auto(bulk api)], executing=false, priority=URGENT, insert_order=210}\r\n   > {time_in_queue=647ms, time_in_queue_millis=647, source=create-index [index2], cause [auto(bulk api)], executing=false, priority=URGENT, insert_order=211}\r\n   > {time_in_queue=641ms, time_in_queue_millis=641, source=create-index [index2], cause [auto(bulk api)], executing=false, priority=URGENT, insert_order=212}\r\n   > {time_in_queue=632ms, time_in_queue_millis=632, source=create-index-template [templateA], cause [api], executing=false, priority=URGENT, insert_order=214}\r\n   > {time_in_queue=632ms, time_in_queue_millis=632, source=create-index-template [templateB], cause [api], executing=false, priority=URGENT, insert_order=216}\r\n   > {time_in_queue=631ms, time_in_queue_millis=631, source=create-index-template [templateC], cause [api], executing=false, priority=URGENT, insert_order=218}\r\n   > {time_in_queue=632ms, time_in_queue_millis=632, source=create-index [index2], cause [auto(bulk api)], executing=false, priority=URGENT, insert_order=213}\r\n   > {time_in_queue=631ms, time_in_queue_millis=631, source=create-index-template [templateD], cause [api], executing=false, priority=URGENT, insert_order=219}\r\n   > {time_in_queue=632ms, time_in_queue_millis=632, source=create-index-template [templateE], cause [api], executing=false, priority=URGENT, insert_order=215}\r\n   > {time_in_queue=630ms, time_in_queue_millis=630, source=create-index-template [templateF], cause [api], executing=false, priority=URGENT, insert_order=220}\r\n   > {time_in_queue=631ms, time_in_queue_millis=631, source=create-index-template [templateG], cause [api], executing=false, priority=URGENT, insert_order=217}\r\n   > {time_in_queue=629ms, time_in_queue_millis=629, source=create-index-template [templateH], cause [api], executing=false, priority=URGENT, insert_order=221}\r\n   > {time_in_queue=98ms, time_in_queue_millis=98, source=shard-started, executing=false, priority=URGENT, insert_order=222}\r\n   > {time_in_queue=46ms, time_in_queue_millis=46, source=shard-started, executing=false, priority=URGENT, insert_order=223}\r\n   > {time_in_queue=10ms, time_in_queue_millis=10, source=shard-started, executing=false, priority=URGENT, insert_order=224}\r\n``` ""shard started"" should show index and shard ID >>> 1"
1975,"This exposes Lucene's new Simple Pattern Tokenizer and Simple Pattern Split Tokenizer, which use a restricted subset of regular expressions for faster tokenization. They're annotated as experimental in Lucene and are documented as such here.\r\n\r\nFor #23363\r\n Expose simplepattern and simplepatternsplit tokenizers >>> 1"
1976,Today if a channel gets closed due to a disconnect we notify the response\r\nhandler that the connection is closed and the node is disconnected. Unfortunately\r\nthis is not a complete solution since it only works for published connections.\r\nConnections that are unpublished ie. for discovery can indefinitely hang since we\r\nnever invoke their handers when we get a failure while a user is waiting for\r\nthe response. This change adds connection tracking to TcpTransport that ensures\r\nwe are notifying the corresponding connection if there is a failure on a channel.\r\n Ensure pending transport handlers are invoked for all channel failures >>> 1
1977,This commit changes the task type of the checkoutBwcBranch task to Exec from LoggedExec so that the output of the checkout command is shown. This enables us to see the SHA used for the checkout which can be useful when debugging a BWC break.\r\n\r\n Log checkout so SHA is known >>> 1
1978,"This modifies a method Mark added to the AggregatorBase that allows aggregations\r\nto add additional memory tracking for datastructures used during execution. If\r\nan aggregation would like to reclaim circuit breaker reserved bytes by adding a\r\nnegative number, `addWithoutBreaking` should be used instead of\r\n`addEstimateBytesAndMaybeBreak`.\r\n\r\nResolves #24511\r\n Tweak AggregatorBase.addRequestCircuitBreakerBytes >>> 1"
1979,Add important note for Azure Storage account types\r\n\r\nRelates #20844 Supported Azure Storage account types >>> 1
1980,"Today when an exception is thrown handling a HEAD request, the body is swallowed before the channel has a chance to see it. Yet, the channel is where we compute the content length that would be returned as a header in the response. This is a violation of the HTTP specification. This commit addresses the issue. To address this issue, we remove the special handling in bytes rest response for HEAD requests when an exception is thrown. Instead, we let the upstream channel handle the special case, as we already do today for the non-exceptional case.\r\n\r\nRelates #21125\r\n Fix handling of exceptions thrown on HEAD requests >>> 1"
1981,This pull request adds the infrastructure necessary to reference X-Pack content.\r\n\r\nIt uses a layout similar to what was worked out in the Kibana Reference for  https://github.com/elastic/kibana/pull/12062\r\n\r\nIt also adds some attribute definitions that are specific to X-Pack and changes the name of the attribute for the X-Pack Reference URL so that it does not cause confusion.  [DOC] Add X-Pack links to Elasticsearch Reference >>> 1
1982,- add `script` field to pipelines as a replacement to executing processors\r\n- introduce a specific IngestContext script context for executing within pipelines add ability to run scripts directly (without processors) within Ingest Pipelines >>> 0
1983,"When attempting to obtain the node lock, if an exception is thrown it is not logged. This makes debugging difficult. This commit causes such an exception to be logged.\r\n Do not swallow node lock failed exception >>> 1"
1984,"Duplicate data paths already fail to work because we would attempt to take out a node lock on the directory a second time which will fail after the first lock attempt succeeds. However, how this failure manifests is not apparent at all and is quite difficult to debug. Instead, we should explicitly reject duplicate data paths to make the failure cause more obvious.\r\n\r\n Explicitly reject duplicate data paths >>> 1"
1985,"Port support for commercial GeoIP2 databases from Logstash, City and Country for now. If there is interest, I'll submit ISP database in separate pull request. Port support for commercial GeoIP2 databases from Logstash. >>> 1"
1986,"It was only in the list because it was required for `randomVersionBetween` but now (#25042) it isn't. Since `CURRENT` isn't actually released, we should stop confusing ourselves by saying that it is. Remove CURRENT from the list of released versions >>> 1"
1987,"This pull request is related to https://github.com/elastic/elasticsearch/pull/25164\r\n\r\nIt adds index-shared.asciidoc and index-all.asciidoc to 5.4 and earlier releases, so that index-all.asciidoc can be used in the docs/conf.yaml file. Note that in 5.4 and earlier releases, there is currently no difference between the contents of index.asciidoc and index-all.asciidoc.  This is because there are currently no plans to integrate X-Pack-specific content in those releases. [DOCS] Add index-all.asciidoc for build purposes >>> 1"
1988,Extract the snapshot/restore full cluster restart tests from the translog full cluster restart tests. That way they are easier to read.\r\n\r\nCloses #25203 Extract the snapshot/restore full cluster restart tests from the translog full cluster restart tests >>> 1
1989,The secure repository-hdfs tests fail on JDK 9 because some Hadoop code reaches into sun.security.krb5. This commit adds the necessary flags to open the java.security.jgss module. Note that these flags are actually needed at runtime as well when using secure repository-hdfs. For now we will punt on how best to help users obtain this when running on JDK 9 with this plugin.\r\n Fix secure repository-hdfs tests on JDK 9 >>> 1
1990,because it is no longer used. Remove PrefixAnalyzer >>> 1
1991,Relates to #24939 Port more OldIndexBackwardsCompatibilityIT tests to full cluster restart qa tests >>> 1
1992,"This commit adds a setting to change the request timeout for the rest client. This is useful as the\r\ndefault timeout is 30s, which is also the same default for calls like cluster health. If both are\r\nthe same then the response from the cluster health api will not be received as the client usually\r\ntimes out first making test failures harder to debug.\r\n\r\nRelates #25185 Test: add setting to change request timeout for rest client >>> 1"
1993,Use Apache commons IO to copy streams in repository S3 plugin to avoid SecurityException. A plugin is only allowed to use its own jars when performing privileged operations. The S3 client might open a new Socket on close(). #25192\r\n\r\n\r\n Fix SecurityException in repository-s3 plugin  >>> 0
1994,"This commit removes the cleanPath method, in favor of using java's\r\nPath.normalize(). Internal: Remove Strings.cleanPath >>> 1"
1995,"This prevents possible race conditions between the Elasticsearch JVM and\r\nplugin native controller processes that can cause the Elasticsearch shutdown\r\nto hang.  The problem can happen when the JVM and the controller process\r\nreceive a SIGTERM at almost the same time.\r\n\r\n(There's an assumption here that Elasticsearch will continue to use other\r\nmechanisms to kill native controller processes.) When stopping via systemd only kill the JVM, not its control group >>> 1"
1996,This commit adds a note to the docs to clarify that only some settings\r\ncan be used with the keystore. Docs: Add note about which secure settings are valid >>> 1
1997,"The following token filters were moved: `edge_ngram`, `ngram`, `uppercase`, `lowercase`, `length`, `flatten_graph` and `unique`.\r\n\r\nRelates to #23658\r\n Move more token filters to analysis-common module >>> 1"
1998,The ternary logic in `prefixCodedToGeoPoint` was reversed such that numericEncoded GeoPoints were using the decoding logic for GeoCoded points and vice versa. This PR fixes that boneheaded bug.\r\n\r\ncloses #24275\r\n Fix GeoPoint FieldStats ternary logic bug >>> 1
1999,Added a few specific numeric date math examples in place of the text ones. Create api-conventions.asciidoc >>> 1
2000,"Some more minor tweaks to make agg builders more consistent.\r\n\r\n- getMetadata for all aggs\r\n- various getters on TermsAggBuilder (without ""get"" prefix to maintain convention)\r\n- Also makes InternalSum's ctor public, to follow suit of other metrics (min/max/avg/etc)\r\n\r\n@cbuescher would you mind taking a look at this?  Want to make sure I'm not breaking anything related to the java client project. :)\r\n Add more missing AggregationBuilder getters >>> 1"
2001,"Move installation documentation for Windows with the .zip archive into the zip and tar installation documentation, and clearly indicate any differences for installing on macOS/Linux and Windows.\r\n Add MSI installation to documentation >>> 1"
2002,"This is a follow-up of #17916 from @jbertouch's work brought up to date with the latest master, here's a subset of original description:\r\n\r\n-----\r\nFirst pass at closing #15335. The new behavior for the `RestController#executeHandler` method is as follows:\r\n- For a request to a **valid endpoint** with an **unsupported HTTP method** -> Return a _405 HTTP error_ including allowed methods for the endpoint in the **Allow** HTTP header. Refer to [HTTP/1.1 - 10.4.6 - 405 Method Not Allowed](https://tools.ietf.org/html/rfc2616#section-10.4.6).\r\n- For an OPTIONS HTTP method request to a **valid endpoint** -> Return a _200 HTTP response_ including allowed methods for the endpoint in the **Allow** HTTP header. Refer to [HTTP/1.1 - 9.2 - Options](https://tools.ietf.org/html/rfc2616#section-9.2).\r\n----\r\n\r\nIt allows things like the following:\r\n\r\n```\r\n~ λ curl -v -XPOST 'localhost:9200/my_index/_settings'\r\n*   Trying 127.0.0.1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> POST /my_index/_settings HTTP/1.1\r\n> Host: localhost:9200\r\n> User-Agent: curl/7.51.0\r\n> Accept: */*\r\n> \r\n< HTTP/1.1 405 Method Not Allowed\r\n< Allow: PUT,GET\r\n< content-type: application/json; charset=UTF-8\r\n< content-length: 134\r\n< \r\n{\r\n  ""error"" : ""Incorrect HTTP method for uri [/my_index/_settings] and method [POST], allowed: [PUT, GET]"",\r\n  ""status"" : 405\r\n}\r\n* Curl_http_done: called premature == 0\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\nWhere you can see the `HTTP/1.1 405 Method Not Allowed` response and `Allow: GET` header.\r\n\r\nIt also adds support for `OPTIONS`:\r\n\r\n```\r\n~ λ curl -v -XOPTIONS x:9200/_cluster/settings\r\n*   Trying 127.0.0.1...\r\n* TCP_NODELAY set\r\n* Connected to x (127.0.0.1) port 9200 (#0)\r\n> OPTIONS /_cluster/settings HTTP/1.1\r\n> Host: x:9200\r\n> User-Agent: curl/7.51.0\r\n> Accept: */*\r\n> \r\n< HTTP/1.1 200 OK\r\n< Allow: GET,PUT\r\n< content-type: text/plain; charset=UTF-8\r\n< content-length: 0\r\n< \r\n* Curl_http_done: called premature == 0\r\n* Connection #0 to host x left intact\r\n``` Improve REST error handling when endpoint does not support HTTP verb, add OPTIONS support >>> 1"
2003,This PR backports https://issues.apache.org/jira/browse/LUCENE-7833 to 5.4.\r\n\r\nCloses #24647 Fix the `max` score mode. >>> 1
2004,It was brought up in #24987 that the low level REST client doesn't url encode the endpoint. This PR adds unit tests for this behaviour.\r\n\r\nRelates to #24987  [TEST] test that low level REST client leaves path untouched >>> 1
2005,nan Migration docs for #25080 >>> 1
2006,"This change adds tests for the aggregation parsing that try to simulate that we can parse existing aggregations in a forward compatible way in the future, ignoring potential newly added fields or substructures to the xContent response. \r\n\r\nMarked as WIP because expect some discussions around how we want to handle parsing inner objects where we now expect only inner aggregations, e.g. in cases like this:\r\n\r\n```\r\n{\r\n\t""filter#myFilterAgg"": {\r\n                ""meta"": {},\r\n\t\t""doc_count"": 524513125,\r\n\t\t""max#myMaxAgg"": {\r\n\t\t\t""meta"": {},\r\n\t\t\t""value"": 0.14740193438495786\r\n\t\t},\t\t\r\n\t\t""someNewInnerObject"": {\r\n\t\t\t""foo"": ""bar""\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nCurrently we would throw an Exception because we only parse the meta data objects and treat all other objects as if they were named inner aggregations. We need to decide if adding any other inner object in the future is likely and how we should parse this in clients that don't understand this yet. The current solution in this PR is to have a ""lenient"" flag `parseTypedKeysObject()` and skip over things that don't contain a type delimiter.\r\n\r\nThe next question would be what we do to handle future aggregation types, e.g. the response is:\r\n\r\n```\r\n{\r\n\t""filter#myFilterAgg"": {\r\n                ""meta"": {},\r\n\t\t""doc_count"": 524513125,\r\n\t\t""max#myMaxAgg"": {\r\n\t\t\t""meta"": {},\r\n\t\t\t""value"": 0.14740193438495786\r\n\t\t},\t\t\r\n\t\t""futureAggType#myName"": {\r\n\t\t\t""foo"": ""bar""\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nIf a client who doesn't undestand this new type yet is receiving above response, we currently will throw an exception. Maybe we should also be lenient in this case, but it would at least be good to notify the user that there are some parts missing from the response object that couldn't be parsed. I don't know if we have decided on logging etc... for such cases. [Tests] Check that parsing aggregations works in a forward compatible way >>> 1"
2007,"In #25201, a setting was added to allow setting the retry timeout for the rest client under the\r\nimpression that this would allow requests to go longer than 30s. However, there is also a socket\r\ntimeout that needs to be set to greater than 30s, which this change adds a setting for. Test: allow setting socket timeout for rest client >>> 1"
2008,"This snapshot has faster range queries on range fields (LUCENE-7828), more\r\naccurate norms (LUCENE-7730) and the ability to use fake term frequencies\r\n(LUCENE-7854).\r\n Upgrade to lucene-7.0.0-snapshot-92b1783. >>> 1"
2009,"This was a BWC layer where we expicitly set the `search_type` to\r\n""query_and_fetch"" when a single node is queried on pre-5.3 nodes. Since 6.0 no\r\nlonger needs to be compatible with 5.3 nodes, this can be removed.\r\n Remove QUERY_AND_FETCH BWC for pre-5.3.0 nodes >>> 1"
2010,"The `document_type` parameter is no longer required to be specified,\r\nbecause by default from 6.0 only a single type is allowed. (`index.mapping.single_type` defaults to `true`) Deprecate percolate query's document_type parameter. >>> 1"
2011,"If a cluster is configured with an HDFS repository and a node is started, that node must be able to\r\nreach HDFS, or else when it attempts to add the repository from the cluster state at start up it will\r\nfail to connect and the repository will be left in an inconsistent state. Adding a blurb in the docs to\r\noutline the expected availability for HDFS when using the repository plugin. [DOCS] Clarify expected availability of HDFS for the HDFS Repository >>> 1"
2012,"This commit removes the global caching of the field query and replaces it with\r\na caching per field. Each field can use a different `highlight_query` and the rewriting of\r\nsome queries (prefix, automaton, ...) depends on the targeted field so the query used for highlighting\r\nmust be unique per field.\r\nThere might be a small performance penalty when highlighting multiple fields since the query needs to be rewritten\r\nonce per field with this change.\r\n\r\nFixes #25171 FastVectorHighlighter should not cache the field query globally >>> 1"
2013,"This commit adds the docs for the new parent-join field.\r\nIt explains how to define, index and query this new field.\r\n\r\nRelates #20257\r\n Add documentation for the new parent-join field >>> 1"
2014,Removes the `assemble` task from projects that are not published.\r\nThis should speed up `gradle assemble` by skipping projects that\r\ndon't need to be built. Which is useful because `gradle assemble`\r\nis how we cut releases.\r\n Remove assemble task when not used for publishing >>> 1
2015,Use this attribute when specifying the location of included tests. \r\n\r\nI also updated the existing test paths.\r\n\r\n@nik9000 added you as a reviewer since you're the only one who's using test includes so far. Mostly just want you to know I'm adding the attribute. \r\n [DOCS] Defined es-test-dir in index.asciidoc.  >>> 1
2016,"This commit adds a gradle project, set inside the root build.gradle,\r\nwhich controls all our bwc tests. This allows for seamless (ie no errant\r\nCI failures) backporting of bwc behavior.\r\n Build: Add master flag for disabling bwc tests >>> 1"
2017,As the title says. Support Script Context Stateful Factory in Painless >>> 1
2018,UnicodeSetFilter was only allowed in the icu_folding token filter.\nIt seems useful to expose this setting in icu_normalizer token filter and char filter.\n\nCloses #20820\n [analysis-icu] Allow setting unicodeSetFilter >>> 1
2019,"This commit renames the needsScores method so as to make it\r\nautomatically generatable, based on the name of the `_score` variable\r\nwhich is available in search scripts. It also adds documentation to\r\nScriptContext to explain the naming and signature of such methods. Scripting: Rename SearchScript.needsScores to needs_score >>> 1"
2020,"#24133 added the ability to specify a `targetField` in SortProcessor. This results in some interesting behavior that was missed in the review.\r\nThis processor sorts in-place, so there is a side-effect in both the original field and the target field.\r\nAnother bug was that the targetField was not being set if the list being sorted was fewer than two elements.\r\n\r\nThe new behavior works like this: If targetField and fieldName are not the same, we copy the list.\r\n\r\nto reproduce original test failure:\r\n\r\n```\r\ngradle :modules:ingest-common:test -Dtests.seed=812BA08DE627020C -Dtests.class=org.elasticsearch.ingest.common.SortProcessorTests -Dtests.method=""testSortWithTargetField"" -Dtests.security.manager=true -Dtests.jvm.argline=""-XX:+AggressiveOpts"" -Dtests.locale=bg -Dtests.timezone=America/Pangnirtung\r\n``` Sort Processor does not have proper behavior with targetField >>> 1"
2021,"At index time Elasticsearch needs to look up the version associated with the\n`_uid` of the document that is being indexed, which is often the bottleneck for\nindexing.\n\nWhile reviewing the output of the `jfr` telemetry from a Rally benchmark, I saw\nthat significant time was spent in `ConcurrentHashMap#get` and `ThreadLocal#get`.\nThe reason is that we cache lookup objects per thread and segment, and for every\nindexed document, we first need to look up the cache associated with this\nsegment (`ConcurrentHashMap#get`) and then get a state that is local to the\ncurrent thread (`ThreadLocal#get`). So if you are indexing N documents per\nsecond and have S segments, both these methods will be called N*S times per\nsecond.\n\nThis commit changes version lookup to use a cache per index reader rather than\nper segment. While this makes cache entries live for less long, we now only need\nto do one call to `ConcurrentHashMap#get` and `ThreadLocal#get` per indexed\ndocument.\n\nNOTE: I had to remove `IndexReader#getCombinedCoreAndDeletesKey` from the\nforbidden APIs in order to make it work.\n\nHere are some screenshots of the hot methods as reported when indexing 100M empty documents with rally using 1 shard, 0 replicas and otherwise default settings.\nBefore:\n![before](https://cloud.githubusercontent.com/assets/299848/17476212/209ba2d4-5d5f-11e6-86aa-f4f56baa2dec.png)\nAfter:\n![after](https://cloud.githubusercontent.com/assets/299848/17476215/259f873c-5d5f-11e6-80d1-74af84034f40.png)\n`ThreadLocal#get` is no longer among the hottest methods after the change, and while `ConcurrentHashMap#get` is still there, its main callers are now the live version map and the circuit breaker, not the state we maintain for version lookups in the index.\n\nHere is the report from `rally compare` on two races that have been run without telemetry (so that it does not affect the benchmark results). The speedup looks real.\n\n```\n                             Metric    Baseline    Contender                Diff\n-----------------------------------  ----------  -----------  ------------------\n   Min Indexing Throughput [docs/s]       67415        73665  +6250.00000\nMedian Indexing Throughput [docs/s]       73608      80540.3  +6932.33333\n   Max Indexing Throughput [docs/s]       75151        81799  +6648.00000\n                Indexing time [min]     113.854      94.9646    -18.88890\n                   Merge time [min]     15.0901      11.0337     -4.05640\n                 Refresh time [min]     1.52247      1.57633     +0.05387\n                   Flush time [min]    0.901483     0.874733     -0.02675\n          Merge throttle time [min]     5.95868       2.5531     -3.40558\n       Median CPU usage (index) [%]     566.462      568.981     +2.51969\n             Total Young Gen GC [s]     113.929      116.784     +2.85500\n               Total Old Gen GC [s]      18.633       19.979     +1.34600\n                    Index size [GB]     1.88654      1.87796     -0.00858\n               Totally written [GB]     20.4271      19.9763     -0.45073\n                      Segment count          22           34    +12.00000\n```\n Speed up PK lookups at index time. >>> 1"
2022,"Moves the `keyword` tokenizer to the analysis-common module. The `keyword` tokenizer is special because it is used by `CustomNormalizerProvider` so I pulled it out into its own PR. To get the move to work I've reworked the lookup from static to one using the `AnalysisRegistry`. This seems safe enough.\r\n\r\nPart of #23658. Move pre-configured ""keyword"" tokenizer to the analysis-common module >>> 1"
2023,nan Fix documentation for percentiles bucket aggregation >>> 1
2024,The Lucene library uses 59.1 so we should use the same.\r\n\r\nCloses #21425 Upgrade icu4j for the ICU analysis plugin to 59.1 >>> 1
2025,We use assertBusy in many places where the underlying code throw exceptions. Currently we need to wrap those exceptions in a RuntimeException which is ugly.\r\n\r\n move assertBusy to use CheckException >>> 1
2026,"This commit puts the parent/child definition in an inner section named ""relations"".\r\nMapping for the parent-join will look like this:\r\n\r\n```\r\n""join_field"": {\r\n  ""type"": ""join""\r\n  ""relations"":\r\n    ""parent"": ""child""\r\n  }\r\n}\r\n``` Add a section named `relations` in the ParentJoinFieldMapper >>> 1"
2027,This assertion was failing due to lack of IndexWriterConfig setup. Actually the assertion was from a copy/paste of terms agg tests and is irrelevant to this test and so was removed.\r\n\r\nCloses #25245\r\n Test fix - removed superfluous assertion in SignificantTextAggregatorTests >>> 1
2028,It adds notes about:\r\n - how preference can help optimize cache usage\r\n - the fact that too many replicas can hurt search performance due to lower\r\n   utilization of the filesystem cache\r\n - how index sorting can improve _source compression\r\n - how always putting fields in the same order in documents can improve _source\r\n   compression\r\n More advices around search speed and disk usage. >>> 1
2029,"This commit changes the parsing logic of DocWriteResponse, ReplicationResponse and GetResult so that it skips any unknown additional fields (for forward compatibility reasons). This affects the IndexResponse, UpdateResponse, DeleteResponse and GetResponse objects. [Test] Extend parsing checks for DocWriteResponses >>> 1"
2030,"This commit does two things:\r\n  1. Adds logging at the DEBUG level for when the index-N blob is\r\n  updated.\r\n  2. When attempting to delete a snapshot, if the snapshot was not found\r\n  in the repository data, an exception is now thrown instead of silently\r\n  ignoring the lack of presence of the snapshot in the repository data.\r\n Improves snapshot logging and snapshot deletion error handling >>> 1"
2031,"In #24379 we added ability to upgrade templates on full cluster startup. This PR invokes the same update procedure also when a new node first joins the cluster allowing to update templates on a rolling cluster restart as well.\r\n\r\nCloses #24680\r\n\r\n@abeyad, @spinscale could you take a look to make sure this will work for you? TemplateUpgraders should be called during rolling restart >>> 1"
2032,Today TcpTransport is the de-facto base class for transport implementations.\r\nThe need for all the callbacks we have in TransportServiceAdaptor are not necessary\r\nanymore since we can simply have the logic inside the base class itself. This change\r\nmoves the stats metrics directly into TcpTransport removing the need for low level\r\nbytes send / received callbacks.\r\n Move TransportStats accounting into TcpTransport >>> 1
2033,Add needs methods for specific variables to Painless script context factories. Add Needs Methods to Painless Script Context Factories >>> 1
2034,"This commit adds a note to the migration docs regarding a change in behavior for HEAD /index used an index existence check versus GET /index.\r\n\r\nRelates #21125, relates #23112, relates #25056, relates #25258\r\n Add note regarding checking existence of indices >>> 1"
2035,"I'm still trying to hunt down rare failures in the cancelation tests\r\nfor reindex and friends. Here is the latest:\r\nhttps://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+5.x+multijob-unix-compatibility/os=ubuntu/876/console\r\n\r\nIt doesn't show much, other than that one of the tasks didn't kill\r\nitself when asked to cancel.\r\n\r\nSo I'm going a bit crazy with debug logging so that the next time this\r\ncomes up I can trace exactly what happened.\r\n\r\nAdditionally, this tweaks the logic around how rethrottles were\r\nperformed around cancel. Previously we set the `requestsPerSecond`\r\nto `0` when we cancelled the task. That was the ""old way"" to set them\r\nto inifity which was the intent. This switches that from `0` to\r\n`Float.MAX_VALUE` which is the ""new way"" to set the `requestsPerSecond`\r\nto infinity. I don't know that this is much better, but it feels better.\r\n\r\nI also cleaned up many of the comments that were ill-formatted or\r\ninaccurate.\r\n Tweak reindex cancel logic and add many debug logs >>> 1"
2036,This is intended to keep parity of behavior between Logstash and Elasticsearch\r\n\r\nhere was the original PR: https://github.com/logstash-plugins/logstash-filter-date/pull/4\r\n\r\nTODO:\r\n- add proper parity tests using the smoke test framework\r\n- add more edge-case test scenarios [WIP] update DateFormat to use same year heuristic as Logstash >>> 0
2037,Today we maintain a map of open connections in order to close them when\r\na low level channel gets closed or handles a failure. We also spawn a thread due to some\r\ntricky concurrency issues especially with respect to netty since they listener might\r\nbe called on a transport / boss thread. Executions on those threads must not be blocking\r\nsince otherwise we will likely deadlock the event processing which adds to the\r\ncomplexity of the concurrency model in this class.\r\n\r\nThis change associates the connection with the close callback that every channel invokes\r\nonce it's closed which allows us to remove the connections map. A relaxed non-blocking\r\nconcurrency model in the connection close listener allows cleaning up connected nodes without\r\nblocking on any lock. Simplify connection closing and cleanups in TcpTransport >>> 1
2038,"Indexing or deleting documents through the `IndexShard` interface is quite complex and error-prone. It requires multiple calls, e.g. first `prepareIndexOnPrimary`, then do some checks if mapping updates have occurred, then do the actual indexing using `index(...)` etc. Currently each consumer of the interface (local recovery, peer recovery, replication) has additional custom checks built around it to deal with mapping updates, some of which are even inconsistent (cf. https://github.com/elastic/elasticsearch/pull/24858#discussion_r120304497). This PR aims at reducing the complexity by exposing a simpler interface on IndexShard. There are no more `prepare***` methods and the mapping complexity is also hidden, but still giving callers a possibility to implement custom logic to deal with mapping updates. Simplify IndexShard indexing / deletion methods >>> 1"
2039,"With #23997 we have introduced a new internal index option that allows to resolve index expressions only against concrete indices while ignoring aliases. Such index option was applied to IndicesAliasesRequest, so that the index part of alias actions would only be resolved against concrete indices.\r\n\r\nSame is done in this commit with delete index request. Deleting aliases has always been confusing as some users expect it to only remove the alias from the index (which has its own specific API). Even worse, in case of filtered aliases, deleting an alias may leave users with the expectation that only the documents that match the filter are deleted, which was never the case. To address all this confusion, delete index api works now only against concrete indices. Wildcard expressions will be only resolved against concrete indices, as if aliases didn't exist. If one tries to delete against an alias, an `IndexNotFoundException` will be thrown regardless of whether the alias exists or not, as a concrete index with such a name doesn't exist.\r\n\r\nCloses #2318 Delete index API to work only against concrete indices >>> 1"
2040,"Expand `/_cat/nodes` with already present information about\r\navailable disk space `diskAvail` (alias: `d`, `disk`) by:\r\n\r\n* `diskTotal` (alias `dt`): total disk space\r\n* `diskUsed` (alias `du`): used disk space (`diskTotal - diskAvail`)\r\n* `diskUsedPercent` (alias `dup`): used disk space percentage\r\n\r\nNote: The available disk space is the number of bytes available\r\nto the node's Java virtual machine. The size might by smaller\r\nthan the real one. That means the used disk space (percentage)\r\nis larger.\r\n\r\nThe value of `diskAvail` is backtracked to:\r\n```\r\norg.elasticsearch.monitor.fs.FsProbe:\r\n\r\n129:    public static FsInfo.Path getFSInfo(NodePath nodePath) throws IOException {\r\n130:        FsInfo.Path fsPath = new FsInfo.Path();\r\n131:        fsPath.path = nodePath.path.toAbsolutePath().toString();\r\n132:\r\n133:        // NOTE: we use already cached (on node startup) FileStore and spins\r\n134:        // since recomputing these once per second (default) could be costly,\r\n135:        // and they should not change:\r\n136:        fsPath.total = nodePath.fileStore.getTotalSpace();\r\n137:        fsPath.free = nodePath.fileStore.getUnallocatedSpace();\r\n138:        fsPath.available = nodePath.fileStore.getUsableSpace();\r\n139:        fsPath.type = nodePath.fileStore.type();\r\n140:        fsPath.mount = nodePath.fileStore.toString();\r\n141:        fsPath.spins = nodePath.spins;\r\n142:        return fsPath;\r\n143:    }\r\n```\r\nwhere `nodePath.fileStore` is of type `java.nio.file.FileStore`\r\n\r\nCloses #21679. expand `/_cat/nodes` to return information about hard drive >>> 1"
2041,nan make identificaiton of closed indices more clear >>> 0
2042,\r\nRelates to #24515 Remove (deprecated) support for '+' in index expressions >>> 1
2043,Also tweaked the qa module's gradle file to actually run bwc tests against all index compat versions when executing `gradle bwcTest`.\r\n\r\nRelates to #24939\r\n Port the remaining old indices search tests to full cluster restart qa module >>> 1
2044,It caught me offguard yesterday that our executors won't always\r\nreject when the ThreadPool is shutdown.\r\n\r\n Javadoc: ThreadPool doesn't reject while shutdown >>> 1
2045,Following https://github.com/elastic/elasticsearch/pull/25257#discussion_r122423604 I created a simple unit test for the XContentParserUtilsTests.parseStoredFieldsValue() method. It looks like it does not need to handle null values (because they are not printed out by MappedFieldType). [Test] Add unit test for XContentParserUtilsTests.parseStoredFieldsValue >>> 1
2046,It's trivial to get `NullPointerException`s when using this class if as the default constructor does not initialise the members. \r\n\r\n Initialise empty lists in BaseTaskResponse constructor >>> 1
2047,Changed names to be snake case for consistency\r\n\r\nFor #25159 Rename simple pattern tokenizers >>> 1
2048,This commit fixes a typo in the KeyStoreCli class. The add-file command was incorrectly set to use\r\nthe AddStringKeyStoreCommand instead of the AddFileKeyStoreCommand. Keystore CLI should use the AddFileKeyStoreCommand for files >>> 1
2049,"In tests, we sometimes create a random directory service and as part of that the IndexSettings get\r\nbuilt again. When we build them again, we need to make sure we do not set the secure settings on\r\nthe new IndexMetaData object that gets created as the node settings already have the secure\r\nsettings and the index settings and node settings will be combined. If both have secure settings,\r\nthe settings builder will throw an AlreadySetException. Test: do not copy secure settings when creating random directory service >>> 1"
2050,In MockFSDirectory we should use the actual indexes settings to build\r\na new IndexMetaData settings object instead of the node settings.\r\n\r\nRelates to #25297 Use IndexMetaData settings as a basis for new index settings >>> 1
2051,"This changes the replica selection to prefer to return replicas on the highest\r\nversion when choosing a replacement to promote when the primary shard fails.\r\n\r\nConsider this situation:\r\n\r\n- A replica on a 5.6 node\r\n- Another replica on a 6.0 node\r\n- The primary on a 6.0 node\r\n\r\nThe primary shard is sending sequence numbers to the replica on the 6.0 node and\r\nskipping sending them for the 5.6 node. Now assume that the primary shard fails\r\nand (prior to this change) the replica on 5.6 node gets promoted to primary, it\r\nnow has no knowledge of sequence numbers and the replica on the 6.0 node will be\r\nexpecting sequence numbers but will never receive them.\r\n\r\nRelates to #10708\r\n Promote replica on the highest version node >>> 1"
2052,"Today when an index is shrunk, the primary terms for its shards start from one. Yet, this is a problem as the index will already contain assigned sequence numbers across primary terms. To ensure document-level sequence number semantics, the primary terms of the target shards must start from the maximum of all the shards in the source index. This commit causes this to be the case.\r\n\r\nRelates #10708\r\n\r\n Initialize primary term for shrunk indices >>> 1"
2053,Adds a task that streams all operations from the primary's global checkpoint to all shards.\r\n\r\nRelates to #10708 Live primary-replica resync (no rollback) >>> 1
2054,This pull request aims to use the method `baseSettings` already present in the class. The method returns  same settings as those which were created in tests.\r\n Refactor node tests settings >>> 1
2055,This setting is supposed to ease index upgrades as it allows you\r\nto check for a new setting called `index.internal.version` which\r\nin turn can be configured in the index settings and has to be set\r\nto '6' in order to be valid. IndexMetaData: Introduce internal format index setting >>> 1
2056,"#25147  added the translog deletion policy but didn't enable it by default. This PR enables a default retention of 512MB (same maximum size of the current translog) and an age of 12 hours (i.e., after 12 hours all translog files will be deleted). This increases to chance to have an ops based recovery, even if the primary flushed or the replica was offline for a few hours.\r\n\r\nIn order to see which parts of the translog are committed into lucene the translog stats are extended to include information about uncommitted operations.\r\n\r\nViews now include all translog ops and guarantee, as before, that those will not go away. Snapshotting a view allows to filter out generations that are not relevant based on a specific sequence number.\r\n\r\nI still have to write some docs and add a migration note, but I think we can start reviewing.\r\n\r\nRelates to #10708  Enable a long translog retention policy by default >>> 1"
2057,"These commits produce a working Elasticsearch binary and get the full test suite to pass.  It has not been tested in any production capacity.  It depends on elastic/jna-build#1, which adds `aarch64` support to our custom JNA jar.\r\n\r\nDevelopment system is currently an ARMv8, 64-core, **Huawei Hi1616** Type 2a2 from our friends at [Packet](https://www.packet.net).\r\n\r\n```\r\n% bin/elasticsearch\r\n[...]\r\n[2017-06-20T14:20:35,064][INFO ][o.e.n.Node   ] version[6.0.0-alpha3-SNAPSHOT], pid[52835], build[64aebe7/2017-06-19T19:06:06.582Z], OS[Linux/4.9.0/aarch64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/1.8.0_131/25.131-b11]\r\n[...]\r\n% curl -s localhost:9200/_nodes/os | jq "".nodes[].os""\r\n{\r\n  ""refresh_interval_in_millis"": 1000,\r\n  ""name"": ""Linux"",\r\n  ""arch"": ""aarch64"",\r\n  ""version"": ""4.9.0"",\r\n  ""available_processors"": 64,\r\n  ""allocated_processors"": 64\r\n}\r\n``` Preliminary support for ARM >>> 1"
2058,Moved SocketAccess.doPrivileged up the stack to DefaultS3OutputStream in repository-S3 plugin to avoid SecurityException by Streams.copy(). A plugin is only allowed to use its own jars when performing privileged operations. The S3 client might open a new Socket on close(). #25192\r\n Avoid SecurityException in repository-S3 on DefaultS3OutputStream.flush() >>> 1
2059,"MockTransportServices allows us to simulate network disruptions in our testing infra. Sadly it wasn't updated to the state of the art in Transport land. This PR brings it up to speed. Specifically:\r\n\r\n1) Opening a connection is now also blocked (before only node connections were blocked)\r\n2) Simplifies things using the latest connection based notification between TcpTransport and TransportService for when a disconnect happens.\r\n3) By 2, it fixes a race condition where we may fail to respond to a sent request when it is sent concurrently with the closing of a connection. The old code relied on a node based bridge between tcp transport and transport service. Sadly, the following doesn't work any more:\r\n\r\n```\r\n   if (transport.nodeConnected(node)) {\r\n            // this a connected node, disconnecting from it will be up the exception\r\n            transport.disconnectFromNode(node); <-- this may now be a noop and it doesn't mean that the transport service was notified of the disconnect between the nodeConnected check and here.\r\n   } else {\r\n            throw new ConnectTransportException(node, reason, e);\r\n   }\r\n```\r\n\r\nCloses #25338 Update MockTransportService to the age of Transport.Connection >>> 1"
2060,nan Fix settings serialization to not serialize secure settings or not take the total size into account >>> 1
2061,"Add logging for when these commands complete sucessfully, with\r\na warning about data loss. Previously they would only log\r\nanything when there was an error.\r\n\r\nFor #22821\r\n\r\nI hunted around a while for a good way to get a logger into these classes in a non-static context, but it seemed like it would have changed too much in the type hierarchy.\r\n\r\nI didn't find an obvious place to test for this either, which makes sense since it's just logging. Warn logging for allocate empty/stale primary commands >>> 0"
2062,If secure settings are closed after the node has been constructed\r\nno key-store access is permitted. We should also try to be as close as possible\r\nto the real behavior if we mock secure settings. This change also adds\r\nthe same behavior as bootstrap has to InternalTestCluster to ensure we fail\r\nif we try to read from secure settings after the node has been constructed.\r\n Ensure we never read from a closed MockSecureSettings object >>> 1
2063,Ports all of RepositoryUpgradabilityIT to qa:full-cluster-restart and ports as much of RestoreBackwardsCompatIT as possible into qa:full-cluster-restart. Port most snapshot/restore static bwc tests to qa:full-cluster-restart >>> 1
2064,"<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n Removed duplicated documentation line (merge issue, perhaps) >>> 1"
2065,"1) Correct a typo that refers to the 'graph_synonyms' filter by the name of the analyzer ('search_synonyms').\r\n2) Explicitly specify a 'tokenizer' setting in the 'synonym_graph' filter configuration to make it clear that this setting is allowed, which is not obvious from the documentation.\r\n\r\n<!--\r\nThank you for your interest in and contributing to Elasticsearch! There\r\nare a few simple things to check before submitting your pull request\r\nthat can help with the review process. You should delete these items\r\nfrom your submission, but they are here to help bring them to your\r\nattention.\r\n-->\r\n\r\n- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS that we support](https://www.elastic.co/support/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n Update synonym-graph-tokenfilter.asciidoc >>> 1"
2066,This change cleans up core tests to not use `index.mapping.single_type=false`\r\nbut instead where applicable use a single type or markt the index as created\r\nwith a pre 6.x version.\r\n\r\nRelates to #24961\r\n\r\n Remove `index.mapping.single_type=false` from core/tests >>> 1
2067,Since `BooleanQuery` has special rewrite rules in case some clauses are\r\ninstances of `MatchAllDocsQuery` this can help simplify queries. I think this\r\nchange will be especially helpful when #22640 is implemented. Rewrite `exists` queries to a `match_all` query when all documents match. >>> 0
2068,"Bringing together shards in a shrunken index means that we need to address the start of history for the shrunken index. The problem here is that sequence numbers before the maximum of the maximum sequence numbers on the source shards can collide in the target shards in the shrunken index. To address this, we set the maximum sequence number and the local checkpoint on the target shards to this maximum of the maximum sequence numbers. This enables correct document-level semantics for documents indexed before the shrink, and history on the shrunken index will effectively start from here.\r\n\r\nRelates #10708\r\n\r\n Initialize sequence numbers on a shrunken index >>> 1"
2069,Remove redundant and not resettable (fails on retries) check-summing. Checksums are calculated and compared by the S3 client already. #25269\r\n\r\n Remove redundant and broken MD5 checksum from repository-s3 >>> 1
2070,"Due to limitations with CreateProcessW on Windows (ultimately used by ProcessBuilder) with respect to maximum path lengths, we need to get the short path name for any native controllers before trying to start them in case the absolute path exceeds the maximum path length. This commit uses JNA to invoke the necessary Windows API for this to start the native controller using the short path.\r\n\r\nTo be precise about the limitation here, the MSDN docs for CreateProcessW say for the command line parameter:\r\n\r\n>The command line to be executed. The maximum length of this string is 32,768 characters, including the Unicode terminating null character. If lpApplicationName is NULL, the module name portionof lpCommandLine is limited to MAX_PATH characters.\r\n\r\nThis is exactly how the Windows implementation of Process in the JDK invokes CreateProcessW: with the executable name (lpApplicationName) set to NULL.\r\n\r\n Get short path name for native controllers >>> 1"
2071,"Most notable changes:\r\n - better update concurrency: LUCENE-7868\r\n - TopDocs.totalHits is now a long: LUCENE-7872\r\n - QueryBuilder does not remove the boolean query around multi-term synonyms:\r\n   LUCENE-7878\r\n - removal of Fields: LUCENE-7500\r\n\r\nFor the `TopDocs.totalHits` change, this PR relies on the fact that the encoding\r\nof vInts and vLongs are compatible: you can write and read with any of them as\r\nlong as the value can be represented by a positive int. Upgrade to lucene-7.0.0-snapshot-ad2cb77. >>> 1"
2072,Disable date field changing in mapping.\r\n\r\nI also modified a little `DocumentMapperMergeTests` in format.\r\n\r\nCloses #25271 Disable date field mapping changing >>> 1
0,"A draft of 1.0 announcement, any edits / additions?\n@IvanSanchez @yohanboniface @perliedman @danzel \n Leaflet 1.0 announcement blog post >>> 1"
1,nan Fix webpack using valid image file for default icon path (#4849) >>> 1
2,"This moves a lot of files around, cleaning things up and keeping the files for each tutorial separate.\n\nI think this is important if we're going to have more tutorials, as there are seemingly random geojson files and image files laying around.\n\nBesides, this uses more templates around, including a `tutorial_link` in `examples.md` - this makes it easier to add more than one example at the same time and not have merge conflicts. The current way of keeping URLs at the bottom can lead to lots of confusion and broken links.\n\nI also added redirects for the current tutorials, in case anyone is linking to them externally.\n Clean up docs/examples - one directory per tutorial, more templating. >>> 1"
3,"This an in-progress PR that switches Leaflet to use ES6 modules system and Rollup for bundling. Closes #3229. It's A LOT of tedious work going through all the code, but it looks like it's going to be a major improvement.\r\n- Dependencies will be imported explicitly in each file instead of depending on global namespaces, making the code easier to maintain and reason about.\r\n- Things like `L.Util.bind` will turn into just `bind`, which means pretty much all references to Leaflet functions/classes will be properly minified, reducing the bundle size significantly (in theory).\r\n- I love the syntax, it just comes along very naturally.\r\n- We'll switch to a rollup-watch-based debug workflow, eliminating the `document.write` hack.\r\n- People who use ES6 modules or Rollup in their apps will be able to benefit from tree shaking (import stuff they need directly and discard all the rest).\r\n\r\ncc @IvanSanchez @patrickarlt @olanod @tmcw @perliedman @yohanboniface @hyperknot \r\n\r\nStatus:\r\n- [x] src/**\r\n- [x] `publish.sh` script\r\n- [x] Manually check `pointer` event handling hacks\r\n- [x] Manually check `touch` event handling hacks\r\n- [x] Cleanup old build script & custom builds system\r\n- [x] Move unrunnable `L.Path` tests from `suites/layer/vector/PathSpec.js` to somewhere else\r\n- [ ] Tweak build script to output file sizes\r\n- [x] Rename output files from `leaflet-rollup-src.js` to `leaflet-src.js`\r\n- [x] Wait until https://github.com/rollup/rollup/pull/1069 is fixed (IE8 is broken until then)\r\n- [x] Rollup config/plugin to replace `L.version` to a string including revision ES6 modules & Rollup >>> 1"
4,"Chrome renders half pixels when doing CSS transforms, which can make the tooltips blurry when the tooltip has an uneven hight or width.\n\n`Math.round()` could also be added to the `layer/Tooltip.js` directly in the [_setPosition](https://github.com/Leaflet/Leaflet/blob/master/src/layer/Tooltip.js#L124), but I prefer handling exact numbers within the code and only rounding when rendering.\n Fixes #4813: Added rounding of position to translate3d >>> 0"
5,Replaced awkward text with easier to understand version.\n Better description >>> 0
6,"I believe that having type definitions for Leaflet's public interface would be very helpful. Writing the definition over the past 2 weeks surfaced a number of inconsistencies in interfaces, docs, tests and debug scripts leading to 45 pull requests. So not only is it useful for people using Typescript, but it adds another check for correctness in the project. The definition file is based off of https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/leaflet, but fully re-written for 1.0.\n\nThe tests file, besides testing every single interface, also includes all of the code in debug/ which I think is a good sampling of edge cases and code in the wild.\n\nSince most of the interfaces and types are defined in the docstrings, I worked on a tool to auto-generate the definition file from the docs. This worked ok, but not well enough to be automated, so that's not included.\n\nThe reason I think this should live in the Leaflet repo (besies the DefinitelyTyped repo), is to keep it in sync with the current master version and each release. DefinitelyTyped would always have the latest stable. Immutable.js takes this approach: https://github.com/facebook/immutable-js/tree/master/type-definitions\n\nTests are run strictly with --noImplicitAny. Added a command for running the tests in the Jakefile (`jake typescript`), not sure if `typescript` should be added to the devDependencies.\n Typescript definition file and tests >>> 0"
7,"Adds clarification in the documentation that specifies that the`project` and `unproject` methods cannot take in arrays, but only real `L.point` instances.\n\nThis is my first time trying to add a fix to an open-source project, so I hope I got the workflow down okay. I have used Leaflet in the past, but never actually touched this specific method. I hope I didn't misunderstand the Issue.\n\nCloses #4817 \n Fix doc: add clarification for projection; closes #4817 >>> 1"
8,"This is just a WIP, which applies the @mourner-style ES6fications (#4989) to: \n- Marker\n- Icon\n- DivIcon\n- DomUtil\n- Draggable\n- GridLayer\n- TileLayer\n- Path\n- Popup (was not fully ES6ified)\n- All map interaction handlers\n\nNote this targets the `rollup2` branch.\n WIP: ES6-ify more stuff >>> 1"
9,"nan Finish drags when a L.Draggable is removed, fix #5063. >>> 1"
10,"Closes #4683 \r\n\r\nI wasn't sure how to set fill opacity since DomUtils only has setOpacity, which only seems to apply to stroke opacity.\r\n\r\nAlso amended test cases to match new option Added opacity to CircleMarker >>> 0"
11,"This is an alternative fix for #5073. @IvanSanchez's fix is in #5074. This is minor details, but it seemed a bit easier to me to avoid any state for fixing this.\r\n\r\nI've tested both approaches with the example provided in #5073 and they both do their job. Always create L.Control.Layers as collapsed; expand if collapse option not set >>> 1"
12,"This implements a few basic fixes I suggested in https://github.com/Leaflet/Leaflet/issues/3210#issuecomment-258918073.\r\n\r\nThis should do 2 things:\r\n\r\n1. Add the standards based way to force screen readers to ignore images with `role=""presenation""` in addition to an empty `alt` tag.\r\n2. Add proper ARIA role and label to Zoom buttons. This makes VoiceOver read them as ""Zoom in - button"" as opposed to ""+ - link"". I an not sure if this will improve JAWs as well but it should. A11y fixes from #3210 >>> 1"
13,"### Issue clarification \r\n@sisou https://github.com/Leaflet/Leaflet/issues/4813#issuecomment-241047544\r\n> http://playground-leaflet.rhcloud.com/yol/edit?html,css,output\r\n> \r\n> One of the two tooltips will be blurry, because the height of the tooltip is an uneven number. That uneven height is then halved in the calculation for the tooltip position.\r\n> \r\n\r\n### Solution\r\nAdd  [round](http://leafletjs.com/reference-1.0.0.html#point-l-point) parameter to L.point calling for tooltip [_setPosition](https://github.com/Leaflet/Leaflet/blob/master/src/layer/Tooltip.js#L124) to avoid half values\r\n Fix #4813 by adding round to L.point in Tooltip._setPosition >>> 1"
14,"As reported in #5091, if overlapping layers are clicked with canvas renderer, each of the overlapping layers get a click event. This PR makes sure only the topmost layer gets the click. Fix canvas firing click event on multiple overlapping layer on canvas >>> 1"
15,nan Fixed broken realtive links >>> 1
16,"An alternative to #4710, this one with unit tests.\n Add a 'sortLayers' option to L.Control.Layers >>> 1"
17,"This makes a lot of changes to how canvas is handled. Primarily:\r\n\r\n* Adds canvas support for `bringToFront` and `bringToBack` (close #974)\r\n* Fixes canvas sometimes not redrawing properly (close #5093)\r\n* Fixes canvas calling `_project` on a layer that is no longer on the map (close #5097)\r\n* Fixes canvas not rendering layers which moves significantly (close #5114)\r\n* Theoretically improves performance when redrawing layers on canvas, by using `clearRect` instead of drawing all layers in the paint region twice\r\n\r\nThis makes some pretty significant changes to how canvas is handled internally, so a thorough review would be good. Canvas improvements >>> 1"
18,See: http://leafletjs.com/plugins.html#basemap-formats Change a Markdown link to raw HTML in docs/plugins.md to fix rendering >>> 1
19,Closes #5125 Fixed docstring >>> 1
20,nan Remove unnecessary gendered text in documentation >>> 1
21,"osm.org isn't an officially supported endpoint, doesn't have SSL certificates etc. - it's just a convenience for typing into your browser URL bar Use openstreetmap.org, not osm.org >>> 1"
22,Leaflet.Routing.Amap is routing plugin from AMap service Add new plugin for routing  >>> 1
23,1.0.1 -> 1.0.2 Update L.version to 1.0.2 >>> 1
24,Fixes regression caused by #5087. Close #5126. Only collapse layer control if collapsed: true >>> 1
25,"When `inside` is true, the code shall use `Math.max` instead of `Math.min` when comparing the horizontal and vertical scale factors between the bbox and the map viewport size. Fix max/min calculation for getBoundsZoom (fixes #5136) >>> 1"
26,To me it appears that the property bounds is expected in the rest of the code to be 'Bounds' and not 'LatLngBounds'. All the included projections pass 'Bounds'. Giving the area where the projection is valid in latlng's did not work for me. Update bounds property in Projection.leafdoc >>> 1
27,Note that the new unit tests fail without the fix. Sanity check in LatLngBounds.contains (fixes #5132) >>> 1
28,"If I some plugin include leaflet to dependencies and I have local version leaflet, then I have two version - leaflet from plugin and my. Change dependencies to peerDependencies >>> 1"
29,related to https://github.com/Leaflet/Leaflet/issues/4968\n #4968 Fix path for data URLS >>> 0
30,This resolves #5118  Issue #5118 - Documentation for L.Map.FitBounds is incorrect >>> 0
31,"This fixes uncaught exception when checking whether a popup isOpen(), when the layer has no popup bound to it. Fixes Layer.isPopupOpen when no popup is bound >>> 1"
32,Pull request for bug #5145  Vertical lines while moving over canvas elements >>> 1
33,"Following #5154, added mention about using `bounds` on Grid Layer when `noWrap` is used (i.e. set to `true`), in order to prevent requesting tiles outside the main world (CRS limits).\r\n\r\nLooks like behaviour changed compared to Leaflet 0.7, where the CRS limits were still effective.\r\n\r\nIn order to restore that behaviour, application developer must now specify `bounds`.\r\n\r\nEspecially visible with CartoDB tiles which do serve tiles outside the main world. docstring: grid layer use bounds with noWrap >>> 1"
34,Fixes #5150 Add missing method documentation for ImageOverlay >>> 1
35,nan Correct latLng ordering in examples >>> 1
36,"`typeListeners.count` was removed in an earlier update to event handling, but was still being referenced and producing an error when incremented.\r\n\r\nFixes #5159  Remove incrementation of uninitialized typeListeners attribute 'count' >>> 1"
37,`Map#containerPointToLatLng` returns a `LatLng` and not a `Point`. This PR fixes the documentation for that method. Thanks! Fixes documentation for Map#containerPointToLatLng >>> 1
38,"Apparently @perliedman introduced a very-hard-to-spot typo in the `touch-action` CSS rules in #4552, and nobody noticed until #5180. :disappointed:  Fix typo in touch-action CSS rules >>> 1"
39,"This PR aborts the handling of `PointerEvent`s in two cases:\r\n\r\n* In `DomEvent.DoubleTap`, do not check for double taps if the event is a pointer event of type `mouse` when not in Edge.\r\n  This gets rid of the synthetic doubleclicks in Chrome >=55. Chrome still needs that code so that Leaflet fires `dblblick`s when using a finger.\r\n  This also means that mouse `dblclick`s in Chrome >=55 are handled by the OS **only**, and touch `dblclick`s are handled by the Leaflet code.\r\n\r\n* In `DomEvent.Pointer.js`, skip handling the event if the `pointerType` is a `mouse`. There is an extra check for IE (IE11???) which uses the `e.MSPOINTER_TYPE_MOUSE` constant, which doesn't exist in Chrome. That's what's causing the spurious touch events.\r\n\r\nThis should fix #5180, hopefully. Testing this PR would help. Be more picky about pointer events (blind attempt to fix #5180) >>> 1"
40,nan Prevent infinite loop on failing errorTileUrls (fixes #5176) >>> 1
41,It was 404ing. docs: fix quick start link in mobile tutorial >>> 1
42,nan Add leaflet-responsive-popup to plugins.md >>> 1
43,Create new bounds from projected coordinates rather than subtract in getBoundsZoom to avoid negative scale. Fixes #5195 Avoid getBoundsZoom return Infinity when using CRS with flipped axis >>> 1
44,"I get this exception when tried to open & close a marker popup immediately:\r\nTypeError: Cannot read property 'attributionControl' of null\r\n    at NewClass._layerAdd (http://localhost:8100/build/main.js:20926:39)\r\n    at NewClass.whenReady (http://localhost:8100/build/main.js:20566:13)\r\n    at NewClass.addLayer (http://localhost:8100/build/main.js:20982:8)\r\n    at NewClass.openPopup (http://localhost:8100/build/main.js:24186:15)\r\n    at NewClass.openPopup (http://localhost:8100/build/main.js:24292:14)\r\n    at NewClass._openPopup (http://localhost:8100/build/main.js:24367:9)\r\n    at NewClass.fire (http://localhost:8100/build/main.js:17552:11)\r\n    at NewClass._fireDOMEvent (http://localhost:8100/build/main.js:20541:15)\r\n    at NewClass._handleDOMEvent (http://localhost:8100/build/main.js:20500:8)\r\n    at HTMLDivElement.handler (http://localhost:8100/build/main.js:21164:14)\r\n\r\nIt runs the following code:\r\n```\r\n_layerAdd: function (e) {\r\n var map = e.target;\r\n\r\n // check in case layer gets added and then removed before the map is ready\r\n if (!map.hasLayer(this)) { return; }\r\n\r\n this._map = map;\r\n this._zoomAnimated = map._zoomAnimated;\r\n\r\n if (this.getEvents) {\r\n  var events = this.getEvents();\r\n  map.on(events, this);\r\n  this.once('remove', function () {\r\n   map.off(events, this);\r\n  }, this);\r\n }\r\n\r\n this.onAdd(map);\r\n\r\n if (this.getAttribution && this._map.attributionControl) {\r\n  this._map.attributionControl.addAttribution(this.getAttribution());\r\n }\r\n\r\n this.fire('add');\r\n map.fire('layeradd', {layer: this});\r\n}\r\n```\r\n\r\nbut the line:  this.onAdd(map); can make this._map be undefind, like in:\r\n```\r\n...\r\nlayer._map = layer._mapToAdd = null;\r\n```\r\n\r\nso I replaced it with map instead which must be defined because it is used afterwards as well. BUGFIX - Cannot read property 'attributionControl' of null >>> 1"
45,"Per #4823 \r\n\r\nIf `clientWidth` or `clientHeight` are undefined (as might be the case in an environment like jsdom), `L.Point` will return a point with `undefined` or `NaN` coordinates. Since `L.Point` always expects a number to be passed in as an argument, clientWidth and clientHeight can fallback to 0 in case they are undefined.\r\n\r\nFor further background, check the issue linked above. adds graceful fallback to clientWidth & clinetHeight in getSize >>> 1"
46,nan Include changes from #5054 into VML code (fixes #5213) >>> 1
47,Leaflet.Viewpoint extends L.CircleMarker and represents marker with multiple directions. Useful to show photos taken from one point.  Add Leaflet.Viewpoint to plugins list >>> 1
48,"Based on my observations and on the feedback given by @dtapuska, I now think that the right behaviour is to rely on native `dblclick`s whenever we're on something other than Edge **or** the pointer type is a mouse.\r\n\r\nThe rationale is that Edge fires native `dblclick`s when using a mouse, but not when using a touchscreen. Chrome always fires native `dblclick`s. IEMobile (not edge) always fires native `dblclick`s.\r\n\r\nThis should fix #5180 (or at least make the code behave better in more browsers) Blind attempt to fix Edge dblclicks (side effects of #5185) >>> 1"
49,"During a `viewreset`, a `moveend` is also fired, which both cause `Canvas` to redraw. This is of course unnecessary and bad for performance.\r\n\r\nThis PR addresses this by listening to the `viewprereset` event, and postpones any calls to `_updatePaths` until the view reset finishes, so that it's only called once.\r\n\r\nI'm not too fond of how I actually implemented this, so any feedback on how it could be improved is welcome.\r\n\r\nAnd, oh, this _also_ fixes #5170, making it the fifth PR that fixes this issue, if I counted correctly. Avoid multiple canvas updatePaths/redraws during viewreset >>> 1"
50,"This is an alternative to #5164 that should also fix #5149.\r\n\r\nAfter looking at @DiogoMCampos 's PR, I think that it would be better to just wrap the center of the bounds. This would ensure that the shifted bounds overlaps as much of the CRS as possible in every scenario.\r\n\r\nAlso, I think that having this as a separate method would help in other scenarios - namely, preventing lines/polygons from wrapping across the antimeridian.\r\n\r\nI'll whip up a few tests if I can find a bit of time.\r\n Implement wrapLatLngBounds (to fix #5149) >>> 1"
51,"With a bit of luck this will fix #5180.\r\n\r\nI've copy-pasted this patch into a playground here: https://playground-leaflet.rhcloud.com/kunur/1/edit?html,output - will appreciate testing. cc @perliedman @hyperknot @davetimmins  timlohnes \r\n\r\nThere is also #5267, with a different approach Another blind attemp to work around dblclicks on Edge >>> 1"
52,"Ping @mourner @perliedman @yohanboniface @hyperknot - Please check if something is missing, and amend the post as needed. Blog post for v1.0.3 release >>> 1"
53,nan Update API reference (and links) to 1.0.3. >>> 1
54,Fixes #5277. Stop scroll propagation in L.Layers.Control in chrome>55 >>> 1
55,fulfilled my duty:\r\nhttps://github.com/Leaflet/Leaflet/issues/5278 added transformation factory >>> 1
56,"Fix for #5293 \r\nNot 100% sure this is the right way to do this, but it is probably the easiest ;)\r\n\r\nAn issue we have now is that interacting with the dragging object while the marker is not on the map is not really valid as the dragging handler depends on the icon, which doesn't exist while not on the map.\r\n\r\nThis is probably a breaking change, although I don't expect it to actually affect many users. Remove marker.dragging when not on the map >>> 1"
57,Fixes #5292 Fix a few uses of children that should be child in comments >>> 1
58,"Fixes #5263.\r\n\r\nThis scrubs all the DOM elements which would be become detached elements after a `map.remove()` call. It's necessary to both `L.DomUtil.remove()` the element and `delete` every reference to them, even some implicit closures that we made for convenience.\r\n\r\nAlso adds a new way to call `L.DomEvent.off()`, as to scrub all (known) native event handlers.\r\n Scrubbing of detached DOM elements, prevents memory leaks >>> 1"
59,"Currently, the ImageOverlay Layer type does not support z-index like the LayerGroup / GridLayer,\r\n\r\nThis adds similar functionality to GridLayer.\r\n\r\nJust after finishing this I have realized that I can ensure I have the correct z-index for all layers by ensuring that each individual layer is in its own pane.\r\n\r\nI thought I would share as other's may find this functionality usefull. Added z-index support to ImageOverlay >>> 0"
60,"""autoClose"" documentation was actually about ""closeOnClick"" and ""autoClose"" documentation was missing. Fix popup ""autoClose"" and ""closeOnClick"" options documentation, fixes #5040 >>> 1"
61,Removed unneeded type attribute in <script> form in .html files.\r\n\r\ntype attribute is not needed in HTML5 cos its default scripting language in html Removed type attribute in HTML5 files >>> 0
62,"This fix isn't working as of now ,  Needs some improvements .\r\nIssue : https://github.com/Leaflet/Leaflet/issues/5281 Error handler in ImageOverlay for 404 links >>> 1"
63,"fixes #5302 Add CSS for -webkit-tap-highlight-color, fixes #5302 >>> 1"
64,"We used some old, incomplete logic to disable click propagation from zoom control buttons, switch to use `L.DomEvent.disableClickPropagation` instead. Should fix the #5308, except for the things that relate to controls in other plugins (that we can't do much about). Disable click propagation on zoom control buttons >>> 1"
65,"nan Pull min/maxNativeZoom from TileLayer into GridLayer, as per #5316. >>> 1"
66,This allows to streamline the import/export in the main file.\n\nThis PR incorporates the suggestions from https://github.com/Leaflet/Leaflet/pull/4989#pullrequestreview-18713630. Add index.js files to subdirectories/namespace >>> 1
67,"Currently, `SVG.create()` and `SVG.pointsToPath()` are unavailable when imported as a module. Provide missing SVG static methods. >>> 1"
68,"Fixes #5234 by reverting 1eae1719bb51cef00382b882ec368c5238c873e7.\r\n\r\nAs a bonus, some unit tests for keyboard handler. Do not stop keypress on escape if no popup is open >>> 1"
69,"New year, new token! cc @mourner  Rotate access tokens >>> 1"
70,Adding my leaftlet plugin to the plugin list plugins: Add Leaflet.Zoominfo >>> 1
71,Fixes #5341 \r\nThe resulting source map points back to the original files too :+1: Have uglifyjs spit out a sourcemap for leaflet.js too. >>> 1
72,Looking over at the documentation I noticed that `lastId` was an available property however it wasn't exported.\r\n\r\nhttp://leafletjs.com/reference-1.0.3.html#util-lastid Export lastId in Util >>> 1
73,"The `expand()` method was called only when expanding the Layers Control through user action.\r\nIn the case of option `collapsed: false`, no event listener is attached (no user action expected to expand), therefore the control height is no longer adjusted compared to map container's height, whereas the only time it is done is at initialization, when the control is not yet inserted into the DOM, hence it does not have an actual height to check against.\r\nTherefore added a hook on `addTo()` in order to run `expand()` AFTER the control has been inserted into the DOM.\r\n\r\nThe same issue happens when later adding more base layers / overlays to the Layers Control: if not collapsed, we should run again the height check (e.g. through the `expand()` method) to make sure we make it scrollable if necessary.\r\nTherefore called `expand()` after each `_addLayer()`.\r\n\r\nActually checking first if the control is on map and if option `collapsed: false` in order to prevent calling `expand()` for nothing.\r\n\r\nNote about test spec suites: unlike most other tests, had to actually insert the map container into the DOM (i.e. `document.body`) for these tests to be useful, otherwise the height remains at 0.\r\nThis may lead to memory leak and tests hanging if done on too many tests (see Leaflet.markercluster tests issue, e.g. Leaflet/Leaflet.markercluster#577) Fix(#5328): Layers Control can now become scrollable even if collapsed: false >>> 1"
74,"Related to #5349. It seems that the docstrings for `L.Util.lastId` was lost in the big rollupJS commit, [over here](https://github.com/Leaflet/Leaflet/commit/703ae02aa8cbd0b87be5b01e77754b83ad732267?w=1#diff-4b892233603abc3c8825cfb5905b7453L59). Docstrings: recover lost docstring for L.Util.lastId >>> 1"
75,I used `translate` property for webkit browsers and also added `will-change` property for Chrome which doesn't cause white lines while panning/zooming.\r\nhttps://github.com/Leaflet/Leaflet/issues/3575 Fix white lines in Webkit >>> 0
76,"When a LayerGroup contains another LayerGroup .toGeoJSON returns an invalid GeoJSON object as it will return nested FeatureCollection objects.\r\n\r\nThis fix squashes nested FeatureCollections by concatenating their features array with the parent FeatureCollection's features array.\r\n\r\nThe GeoJSON spec states that a FeatureCollection's features property contains an array of Feature objects.\r\n\r\nhttp://geojson.org/geojson-spec.html#feature-collection-objects\r\n> The value corresponding to ""features"" is an array. Each element in the array is a feature object as defined above.\r\n\r\nCurrently LayerGroup.toGeoJSON will return a FeatureCollection where the features array may contain one or more FeatureCollection objects.\r\n\r\nExample:\r\nhttps://playground-leaflet.rhcloud.com/pin/2/edit?html,console fix invalid GeoJSON produced by nested LayerGroups >>> 1"
77,Flatten toolbar inner border radius to match grey outline.  fixes #5360 Update toolbar inner border radius >>> 1
78,This addresses #5358 by including `L.Mixin.Events` in the exported namespace again. It also adds a deprecation warning in the log. Include L.Mixin.Events again; add deprecation notice >>> 1
79,"Looking at #5350, I found out that `Map.Drag` stops an ongoing `flyTo` animation once a pointer down event is received, and not when the user actually starts dragging the map. This leaves the possibility that the user only taps the map, without starting to drag. In this situation, the animation will be aborted, but no map movement will be registered by `Map.Drag`, so no `moveend` event will be fired, neither for the `flyTo` animation or the drag.\r\n\r\nThis in turn becomes a problem since it's reasonable to assume a `flyTo` call will be succeeded by a `moveend` at some point, either when the animation finishes or when it's aborted - clearly the map has moved by then.\r\n\r\nI looked through other usages of `Map._stop`, and it appears it is otherwise only used where a `moveend` will be fired, so `Map.Drag` is an exception, which is not guaranteed to fire a `moveend`.\r\n\r\nAn alternative to this fix would be to make `Map._stop` fire a `moveend` if an animation is interrupted - this would also make sense to me, but at the same time seems to be a potentially more intrusive fix: we would sometimes fire multiple `moveend` events where we today only fire one, potentially causing problems for existing apps. Stop map on drag start instead of pointer down >>> 1"
80,"Use ""yarn global add"" instead of ""yarn install -g"". Because ""yarn install"" is used to install all dependencies for a project [1]. ""yarn global""  install packages globally on your operating system [2].\r\n\r\n[1] https://yarnpkg.com/en/docs/cli/install\r\n[2] https://yarnpkg.com/en/docs/cli/global Update CONTRIBUTING.md: Use ""yarn global add"" >>> 1"
81,"Part of #4239.\r\n\r\n~~~I started wanting to write about the `zoomSnap`/`zoomDelta` options, but got carried away by explaining basic stuff in more depth.~~~\r\n\r\nDone:\r\n- [x] Cheerful overview of web mercator\r\n- [x] Images about tiles at zoom 0, 1, 2\r\n- [x] Something about `setZoom()`\r\n- [x] Something about `fitBounds()`\r\n- [x] Something about `zoomSnap` and `zoomDelta`\r\n\r\n~~~Left for another tutorial later:~~~\r\n- [ ] ~~~Something about `minZoom`/`maxZoom`~~~\r\n- [ ] ~~~Something about `maxNativeZoom` on `TileLayer`s~~~\r\n- [ ] ~~~Something about `maxBounds` option~~~\r\n\r\n WIP: Tutorial about zooming. >>> 1"
82,"Fixes #5401. Calling the renderer's `_initContainer()` before the renderer is added to the map has no ill effects.\r\n\r\nHaven't tried with a `Canvas` renderer, but I expect no problems, as the DOM is not involved in that case. Ensure renderer's container is init'ed when a path is added to map >>> 1"
83,nan makes gender neutral >>> 1
84,Added event for when loading image fails on ImageOverlay layer and added missing documentation for load event Added event for when loading image fails + documentation >>> 0
85,"Fix #5373.\r\n\r\n- Remove references to removed file ""../../build/deps.js""\r\n- Update leaflet-include.js to point to ""../dist/leaflet-src.js""\r\n- Update watch to use the same destination file as rollup (dist/leaflet-src.js)\r\n- Define getRandomLatLng where used Fix debug examples after rollup >>> 1"
86,Added event for when loading image fails on ImageOverlay layer and added missing documentation for load event Added event for when loading image fails + documentation >>> 1
87,This adds support for z-index on Image overlay.\r\n\r\nIt is a updated and merged #5226\r\n\r\nI have also refactored the tests as per @perliedman 's comments on #5416\r\n Added z-index support to ImageOverlay >>> 1
88,Can be seen as an extension of the fixes done in #4214 / #4213\r\nFixes #5421\r\n\r\n Control.Layers: Only add layer events to layers when we are on the map >>> 1
89,nan add jsdoc for DomUtil.TRANSITION_END >>> 1
90,"The markup renderer will still literally render html tags inside ""pre"" and ""code"" unless they're escaped. The result was that the example code would be rendered as:\r\n```\r\nvar baseMaps = {\r\n\t""Grayscale"": grayscale,\r\n\t""Streets"": streets\r\n};\r\n```\r\nrather than\r\n```\r\nvar baseMaps = {\r\n\t""<span style='color: gray'>Grayscale</span>"": grayscale,\r\n\t""Streets"": streets\r\n};\r\n```\r\nbecause the ""Grayscale"" text would be literally styled as a span with gray color, and then recolored by the syntax highlighter.\r\n\r\nEscaping the span tags fixes this. Escape html elements inside code sample >>> 1"
91,"I was looking at some other mapping APIs and became jealous of their ability to display videos. So, I added a tiny bit of functionality to the built-in `ImageOverlay`.\n\nOn top of that, I created a tutorial for that. I want to try and see if a ""one feature, one tutorial"" policy makes sense.\n Add new class L.VideoOverlay >>> 1"
92,"The codes first execute ""element.tabIndex""(show that element is not\r\nNULL), then execute ""!element"" in the condition of if stmt(check whether\r\nthe element is NULL or not). It is a contradiction.\r\nSince the element must not be NULL(otherwise the execution of\r\n""element.tabIndex"" would be wrong) when the while stmt finishes, the\r\nnext if stmt doesn't need to check the element like ""!element"" again.\r\nChecking the element.style is already enough. So remove the ""!element"". fix the coding contradiction in the function of preventOutline. >>> 1"
93,"Noticed the homepage says 2015 in the copyright notice, so I updated it to 2017. Change copyright year from 2015 to 2017 >>> 1"
94,"The missing tiles after fast zoom in/out seems to be a race condition. \r\n\r\nWhen the tiles of the level have been loaded before they are re-used. But it can occur that this tile is not marked as active (maybe the opacity animation wasn't completed before), and this removes the current tile at the purging function.\r\n\r\nForcing the current tile to active solves this problem. On update set current tiles active to avoid pruning (#5381) >>> 1"
95,Vertical center(middle) and horizontal center(center) positions are added to controls New control positions (middle & center) >>> 0
96,This adds the other secondary headers in the Contributing document to the Table of Contents at the top. I have kept the depth at what it was - this just adjusts the oversight of other sections. Add other h2 headers to ToC >>> 1
97,nan Added the quatree plugin >>> 1
98,As mentioned in #5118 the method setZoom needed to fix the docs.\r\nChecked the code and it should be optional. Issue #5118 Fix documentation for method setZoom >>> 1
99,so tabbing through the page make the link(in the popup) to the first entry focussed before the 'x' for close Web accessibility >>> 1
100,Addresses problem where Internet Explorer re-fires mouseover if the element is re-appended to the DOM.\r\n\r\nFixes #4050. Fix using bringToFront/bringToBack in mouse listeners for Internet Explorer >>> 1
101,Should fix #5466. Add subresource integrity information & scripts >>> 1
102,I found global L usage in code.\r\nWas it kept for some purpose or just forgot to change? Remove global L usage >>> 1
103,Fixes #5455  Fix __super__ by not copying it from the parent when subclassing >>> 1
104,This make dashArray working properly for circles.\r\n\r\nfix #5182 Canvas: call ctx.setLineDash in _fillStroke >>> 1
105,Make L.Mixin.Events a simple object instead of a prototype so it doesn't have a 'constructor' property. \r\nFixes #5451  Remove constructor property from L.Mixin.Events >>> 1
106,leaflet-map-builder plugin added in Frameworks and build systems section leaflet-map-builder plugin added >>> 1
107,"The deferred call to `_resetState` can interrupt the next box zoom if\r\nthe user initiates it before the timeout fires. This causes the mouse\r\nmove handler to create a second box zoom element, orphaning the first\r\none and leaving it in the DOM.\r\n\r\nIt is possible to reproduce this issue in Chrome by using the Timeline\r\npanel's CPU throttle (20x) and box zooming in rapid succession. Fix box zoom race condition >>> 1"
108,"As recommended in PR #5237. Example usage:\r\n\r\n    var marker = new L.Marker([10.123456, 20.123456, 30.123456]);\r\n    marker.toGeoJSON();    // Works without changes\r\n    marker.toGeoJSON(3);   // Restricts all numbers to three significant digits Add precision parameter to all toGeoJSON functions >>> 1"
109,Fixes #5477 Sanity check to prevent loading tiles when bounds are `Infinity` >>> 1
110,nan Better sanity checks for avoiding loading infinite tiles >>> 1
111,"Here isn't owned by Nokia anymore: furthermore, the former URL linked to a 404 page. Correct Here Maps URL >>> 1"
112,Should fix #4538 as reported by @HarelM Take devicePixelRatio into account for scrollwheel zoom in win10+chrome >>> 1
113,"This is just a subset of changes from #5290.\r\n\r\nMotivation is @mourner's comments in https://github.com/Leaflet/Leaflet/pull/5290#issuecomment-286386260 , about the tile gap not being a core issue. This PR just allows plugin authors to run some code whenever a `GridLayer`  creates or deletes a tile zoom-level container, and whenever a tile becomes fully opaque. That, in turn, will allow much cleaner code for [L.TileLayer.NoGap](https://github.com/Leaflet/Leaflet.TileLayer.NoGap/blob/master/L.TileLayer.NoGap.js), since right now it reimplements the whole tile pruning logic. Add hook points to allow for a proper NoGap plugin >>> 1"
114,"As suggested in https://github.com/Leaflet/Leaflet/issues/5475, this PR adds `getTopLeft()` and `getBottomRight()` methods to `Bounds` to retrieve the 2 missing corners (besides `getBottomLeft()` and `getTopRight()` already existing).\r\n\r\nHaving now the 4 corners make `Bounds` more in line with `LatLngBounds`.\r\n\r\nAlso added 4 tests (for each of the corner retrieval methods).\r\n\r\nCloses https://github.com/Leaflet/Leaflet/issues/5475 together with the previous PR https://github.com/Leaflet/Leaflet/pull/5487 Feat(Bounds): add new methods for 2 missing corners >>> 1"
115,"This replaces the `reference.html` file with a symlink to `reference-1.0.0.html` and updates a couple of links around.\r\n\r\nSupersedes #4972, because merge conflicts. Symlink reference.html to reference-1.0.3.html >>> 1"
116,Added the leaflet-geopackage plugin to the Overlay data formats section leaflet-geopackage plugin added >>> 1
117,nan Replace reference.html symlink with redirect  >>> 1
118,Fixes #5497 Fix handline Polylines with empty array of LatLngs with canvas renderer >>> 1
119,"`examples/quick-start.html` redirects to a 404, `examples/quick-start/` seems to be the correct link. Fixing quick start link in docs root >>> 1"
120,"I ended up fixing this in `Leaflet.js`, but there might be some Rollup trickery that would be a cleaner solution. I'm not familiar enough with Rollup to find it, in that case. Always export window.L; fixes #5489 >>> 1"
121,nan Update banner copyright to include 2017 >>> 1
122,"as suggested in issue #5475, and for consistency with https://github.com/Leaflet/Leaflet/pull/5059 (for `LatLngBounds`).\r\n\r\nThe actual relative position of the given corners do not matter for the factory / constructor current functionality: it just determines the min/max coordinate values. Docs(Bounds): change topLeft/bottomRight to corner1/2 >>> 1"
123,"Fixes #5153.\r\n\r\n~~Lacking unit tests right now, should be able to whip up one.~~ There's a unit test, and I've made sure that it fails without the fix. Handle edge case of empty bounds on _getBoundsCenterZoom >>> 1"
124,"Derived from conversation in #4882, this turns the (hidden) `nonBubblingEvents` layer option into a new documented (boolean) option, `bubblingEvents`, with explicit different defaults for markers and paths.\n\nThis should keep behaviour the same as now, while documenting why markers don't bubble mouse events to the map (by default).\n\ncc @yohanboniface \n Turn nonBubblingEvents into a documented boolean option >>> 1"
125,"Addresses #5499 by:\r\n\r\n* removing the general transformation of enter `keypress` into `click`\r\n* adding a `keypress` listener for markers with bound popups\r\n\r\nPossible downside: popups or other `click` handlers that added without `bindPopup` will no longer fire when enter is pressed on a focused marker.\r\n\r\nI first tried to fix this in a more general way, but it ends up more of a hack, where we can still end up with a `click` event without mouse coordinates and `latlng`, exactly like #5499 again. Don't turn enter keypress into clicks on map >>> 1"
126,leaflet-ruler plugin added. Update plugins.md >>> 1
127,nan ce >>> 1
128,"add image randdering option,  imageoverlayer can zoom using interpolation algorithm: nearest-neighbor Add imageOverlayer image rendering option >>> 0"
129,I have added the option to pass a DOM element to divIcon.\r\nThis will append the element in the div created for the icon.\r\nIf both html and element are passed. The html option will be set. Div icon accept node element as option >>> 1
130,Does not totally fit in this subsection but I did not want to create a new section for this. Add Easymap to plugin list >>> 1
131,:haircut_man: (because i have nothing better to do on a friday night) dont new up layerGroup in live sample >>> 1
132,"I've gone through the commits since 1.0.3, tried to group them reasonably and hope I didn't mess up too much. Changelog for 1.1.0 >>> 0"
133,"As this filters out custom inputs created from property names (supposedly using innerHTML), they still will become unusable whenever _update() is fired on control.\r\nI believe that this is a different issue coming from design of a module. Fix for issue #5116 >>> 0"
134,I have created a plugin that overlays UK Ordinance Survey 1km grid squares. This grid appears on OS Landranger and Explorer maps.  Add UK Ordinance Survey grid squares plugin >>> 1
135,Rebased from #5537 Fix for issue #5116 >>> 1
136,I can't figure out a good way to add a regression test for this. Any help on that would be appreciated. Fixes #5534 by adding a check for null. >>> 1
137,"Fixed grammar & capitalization errors on line #3803 to ""not required by all Leaflet users"" from ""not required by all of Leaflet users"" and to ""JavaScript"" from ""javascript"". Update plugins.md >>> 1"
138,I noticed that the minus sign in the zoom control was probably just a normal hyphen as it was narrow. The proper minus character is the exact same with as the plus character and has consistent vertical alignment. More info here: https://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes\r\n\r\nAfter replacing the hyphen with the minus from Wikipedia I checked a zoomed in screenshot of it in GIMP. The minus sign was wider than the plus because the font size had been enlarged - presumably because someone thought it looked too small in comparison. Now they can be the same font size which saves a bit of CSS too. Use minus character instead of hyphen in the zoom control >>> 1
139,We'd like to add a new plugin listing to the LeafletJS docs.\r\n\r\nMore background info and links:\r\n\r\n- plugin repo: https://github.com/jwasilgeo/Leaflet.Canvas-Flowmap-Layer\r\n\r\n- blog post announcement: https://cerebellumaps.wordpress.com/2017/04/20/flow-mapping-with-leaflet/\r\n\r\nThanks! Added Leaflet.Canvas-Flowmap-Layer to plugins list >>> 1
140,"Hopefully it reads easier now.\r\n\r\nThis has been prompted by https://gis.stackexchange.com/questions/244788/map-ids-to-add-mapbox-basemaps-to-leaflet-or-openlayers - apparently mapbox's ""map IDs"" are not as obvious as they used to be. Tutorials: rewrite the paragraph about the mapbox map IDs >>> 1"
141,Plugins: add SuperMap iClient Plugins: add SuperMap iClient >>> 0
142,"so that it is more explicit how Map's options may get automatically computed from its Grid/Tile layers, and what is the exact effect of these options on Grid/Tile layers.\r\n\r\nThe explanations for Map `minZoom` and `maxZoom` options are taken from https://github.com/Leaflet/Leaflet/pull/4643#r66759552\r\n\r\nReplaces / closes https://github.com/Leaflet/Leaflet/pull/4643\r\nFixes https://github.com/Leaflet/Leaflet/issues/4034\r\n\r\nNote: the ""By default the entire map."" sentence on Grid Layer `minZoom` option looks misplaced. It might have been intended for the above `bounds` option. But in that case, I am not sure the wording is appropriate? I would have said that if not set, there is no horizontal limit to Tile Loading… which is what the current docstrings says anyway, by describing the opposite situation. Docstrings(Map/Grid+TileLayer): improve minZoom and maxZoom explanations >>> 1"
143,"While reviewing #5515, it occured to me that a much more general approach would be to allow users to add any CSS class to an `ImageOverlay` using the `className` option, instead of adding custom options for details like rendering options.\r\n\r\nThis also has the advantage that it works exactly like a lot of other classes, for example `GridLayer`, which already have the `className` option. Add className option for ImageOverlay >>> 1"
0,"I've come up with a partial solution to making the JSON macro more robust when handling recursive types. Rather than making a pull request, I wanted to discuss my findings here first, because I don't know how well they will be received.\n\nSuppose we have: \n\n```\ncase class Foo(id: Long, value: Either[String, Foo])\n```\n\nand some:\n\n```\nimplicit def eitherReads[A : Reads, B : Reads]: Reads[Either[A, B]] = ???\n```\n\nThe following macro call will fail from a type-mismatch error:\n\n```\nimplicit val reads = Json.reads[Foo]\n```\n\nAfter some digging, I found that if the macro determines that the structure is recursive (and not using a nullable call), it will eventually land [here](https://github.com/playframework/playframework/blob/f7e17a7880972b2e6a955e71a7035ae0f7903681/framework/src/play-json/src/main/scala/play/api/libs/json/JsMacroImpl.scala#L192-L200). This means that even if the [implicit found earlier in the macro](https://github.com/playframework/playframework/blob/f7e17a7880972b2e6a955e71a7035ae0f7903681/framework/src/play-json/src/main/scala/play/api/libs/json/JsMacroImpl.scala#L151) has type `Reads[Either[String, Foo]]`, the macro will insert the recursive call of type `Reads[Foo]` in the [`else` branch](https://github.com/playframework/playframework/blob/f7e17a7880972b2e6a955e71a7035ae0f7903681/framework/src/play-json/src/main/scala/play/api/libs/json/JsMacroImpl.scala#L200).\n\nOne idea I had was inspecting the implicit tree that was resolved, and carefully splicing in the call to `this.lazyStuff`. I see only a few possibilities for the structure of the implicit tree:\n1.  An implicit method that likely has it's own implicit parameters, to handle generics. Such as `Reads.seq`, or the above `eitherReads`, etc.\n2.  An implicit that can be typechecked to `N[A]` (an exact recursive call to the ident the macro result will be assigned to).\n3.  Some other static tree, such as an implicit val, or object, which requires no recursion and can be left alone.\n\nSo I wrote a method to handle these cases:\n\n```\ndef recursiveReplace(impl: Tree): List[Tree] = impl match {\n  case q""$obj.$impldef[..$tparams](..$implparams)"" => {\n    val replaced = implparams.map(p => recursiveReplace(p))\n    q""$obj.$impldef[..$tparams](..$replaced)""\n  }\n  case obj if (c.typecheck(obj).tpe <:< natag.tpe) => q""this.lazyStuff""\n  case tree => tree\n}\n```\n\nThis will inspect a implicitly resolved tree and attempt to more carefully splice in the recursive calls. If the tree is a (1), it will extract the parameter trees and process them recursively with `recursiveReplace`. If it is a (2), it is replaced with the recursive call to the macro result `q""this.lazyStuff""`. If it is a (3) (for the moment), the tree is left alone.\n\nI wrote a few tests for `Either` and replaced [this call](https://github.com/playframework/playframework/blob/f7e17a7880972b2e6a955e71a7035ae0f7903681/framework/src/play-json/src/main/scala/play/api/libs/json/JsMacroImpl.scala#L200) with a call to `recursiveReplace(impl)`. Theoretically, `recursiveReplace` should also work for the manually handled `Set`, `List`, `Map` and `Seq` helpers, so I tried removing them and the tests failed (rather, didn't compile). It was then I realized the fix wouldn't work without a type annotation on the macro result, and why the aforementioned types are being handled specially.\n\nOne [already existing test](https://github.com/playframework/playframework/blob/f7e17a7880972b2e6a955e71a7035ae0f7903681/framework/src/play-json/src/test/scala/play/api/libs/json/JsonExtensionSpec.scala#L416-L426) for example contains:\n\n```\nimplicit val userReads = Json.reads[UserMap]\n```\n\nWhere `UserMap` is: `case class UserMap(name: String, friends: Map[String, UserMap] = Map())`\n\nIt _would_ work with my patch if the test actually read: \n\n```\nimplicit val userReads: Reads[UserMap] = Json.reads[UserMap]\n```\n\nThe problem lies in the implicit resolution. The type checker fails to find the implicit `Reads[Map[...]]` because it can't find the `Reads[UserMap]` that it depends on, because there is no type annotation:\n\n```\n/playframework/framework/src/play-json/src/test/scala/play/api/libs/json/JsonExtensionSpec.scala:465: json.this.Reads.mapReads is not a valid implicit value for play.api.libs.json.Reads[Map[String,play.api.libs.json.UserMap]] because:\n[info] hasMatchingSymbol reported error: No Json deserializer found for type play.api.libs.json.UserMap. Try to implement an implicit Reads or Format for this type.\n[info]       implicit val userReads = Json.reads[UserMap]\n```\n\nSo the ""needed implicit"" is actually an empty tree.\n\nIt seems like the macro can be much more powerful than it currently is, to support any kind of recursive type (well, maybe not any, but many more). The obvious problem with the above solution is that it introduces a breaking change where client code must annotate `Reads`, `Writes` and `Format` generated by the macro (at least in the recursive case).\n\nHowever:\n- It is very rigid to only support `Set`, `List`, `Map` and `Seq`, and `Option` within recursive types, and adding more would make the code far more ugly.\n- Isn't it good practice to explicitly annotate implicit result types anyway?\n- Many user-defined recursive types are not supported by the macro because of this.\n\nOr is needing to annotate the macro result unacceptable?\n Limited support for recursive types within Json macro (reads/writes/format) >>> 1"
1,"Extracts the Spring Data Binder based Forms code from play-java project, and puts it into another play-java-forms project.\n Extracts Java forms >>> 1"
2,Adds 3 tutorials\n Update Tutorials.md >>> 0
3,Adds the REST API guide into the tutorials page.\n [doc] Add REST API link into tutorials. >>> 1
4,"## Purpose\n\nThe assets controller has a few flaws in its current implementation. \nThis PR is supposed to fix those flaws and adds new tests to further harden the\nAssets-Controller implementation\n\nThis commit fixes the following: \n- try to infer the ETAG/Digest from the filename if no digest file exists\n  -> 6f7dc7a4eb8d1b529ab33e9ddbd7ea220fc01308-vendor.js will have ETAG 6f7dc7a4eb8d1b529ab33e9ddbd7ea220fc01308 infered from its name. \n- correctly calculate the GZIP url also if a digested version exists\n  -> previously, if a digest file existed, the incorrect gzip version was used. \n- correctly infer digest from filename \n  -> previously the filename itself was used incorrectly, e.g. main.js digest was itself ""main.js"" instead of """"\n- If digested file is requested and found, serve it instead of barefile\n  -> if we have 6f7dc7a4eb8d1b529ab33e9ddbd7ea220fc01308-main.js serve it instead of main.js\n- if a file with a different digest (e.g. from a previous version of the assets) is requested, check if it is available and serve it eventually\n- if a asset is requested without a digest (e.g. just ""vendor.js""), see if we have a digest for it and serve the digested version with correct ETAG\n Assets-Controller bug fixes (see description): fixes #6144 >>> 0"
5,"Fixes #6506 \n\nUses a try-with-resources to auto-close the ws client after use, and uses the default test server port. Since this is a functional test, it's fine to use a different WSClient than the one in the server.\n [2.5.x] Fix unsafe usage of WS client in tests >>> 1"
6,Adds 3 tutorials\n Update Tutorial.md >>> 1
7,"Actually this reverts back how query strings were handled in previous versions of Play 2.5.x. I guess most of the time the matrix params aren't a good idea and will probably make more problems than they will be helpful.\nI guess to get the Uri Encoding / Decoding right, we should create Docs that explains the why and how (which I will do in https://github.com/playframework/playframework/pull/6524 where we also use a library for uri encoding and decoding), that's the best way forward.\n\nI'm sorry for all people that rely on that, that I got it that wrong and introduced a quite breaking change in 2.5.x mid term.\n\n**Edit:** I guess it's fine to set the mima filter.\n [2.5.x] reverts matrix params in query strings >>> 1"
8,"## Fixes\n\nFixes https://github.com/playframework/playframework/issues/6400\n## Purpose\n\nThis PR adds a Timeout trait which replaces the deprecated Promise.timeout method, and provides an implicit class so we can use `future.withTimeout(100 millis)` in a Play app where we know that actorSystem is in context.\n\nWe have [Futures.timeout](https://playframework.com/documentation/latest/api/java/play/libs/concurrent/Futures.html#timeout-long-java.util.concurrent.TimeUnit-) in the Java API already, so we don't need to add anything on the Java API side.\n## Background Context\n\nTimeout operations are applicable all through Play HTTP requests in the form of `Future[Result]` and `Future[WSResponse]` -- while there are underlying timeouts, it's not well known how to do non-blocking timeouts, and there are multiple instances of applications using `Thread.sleep` or the like.\n## References\n\nhttps://github.com/johanandren/futiles#timeouts---markattafutilestimeouts\n\nhttps://github.com/semberal/semberal.github.io/blob/master/scala-future-timeout-patterns.md\n\nhttp://doc.akka.io/docs/akka/current/scala/futures.html#After\n\nhttps://groups.google.com/d/topic/play-framework/Gv3GfFwZo2k/discussion\n Add Timeout trait to play.api.libs.concurrent >>> 1"
9,"currently the raw-request-uri-header was never set, even after changing the configuration\nsince we construct the initial-settings manually, so it always raised ""Can't get raw request URI. Please set...'\nUnfortunatly the user can't set it by himself\n polished the akka-http backend >>> 1"
10,"Actually this reverts back how query strings were handled in previous versions of Play 2.5.x. I guess most of the time the matrix params aren't a good idea and will probably make more problems than they will be helpful.\nI guess to get the Uri Encoding / Decoding right, we should create Docs that explains the why and how (which I will do in #6524 where we also use a library for uri encoding and decoding), that's the best way forward.\n reverts matrix params in query strings >>> 1"
11,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [x] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes no bug, this is a new feature.\n## Purpose\n\nImplements https://tools.ietf.org/html/rfc2324#section-2.3.2 : HTTP status 418 ""I'm a teapot"".\n## Background Context\n\nI think having a full list of available HTTP status codes is wonderful for developers (myself included).\nI was sad to see this code missing.\n## References\n\nhttps://tools.ietf.org/html/rfc2324#section-2.3.2 \n\n…l/rfc2324#section-2.3.2\n Added HTTP response code 418, according to https://tools.ietf.org/html/rfc2324#section-2.3.2 >>> 1"
12,https://github.com/sbt/sbt/pull/2716\n\n/cc @eed3si9n\n avoid deprecated methods since sbt 0.13.13 >>> 1
13,Fixes https://github.com/playframework/playframework/issues/4695\n\nThis PR changes the Play doc to reflect sbt-native-packager doc.\n\nNeeds backport to 2.5.x docs.\n [doc] Add note about Play PID in Debian deployment >>> 1
14,"Scrubs some more ""object Application"" references in the documentation.\n [doc] Remove object controller references >>> 1"
15,"Upgrades Scalariform to 0.1.8.  See https://github.com/scala-ide/scalariform#usage-within-a-project\n\nThis should fix the parser errors we were getting earlier:\n\n```\n[warn] Scalariform parser error for /home/travis/build/playframework/playframework/framework/src/play-specs2/src/main/scala/play/api/test/Specs.scala: Expected token RBRACKET but got Token(XML_START_OPEN,<,3066,<)\n```\n Upgrade scalariform to 0.1.8 >>> 1"
16,nan [2.5.x] polished the akka-http backend (#6566) >>> 0
17,"# Pull Request Checklist\n- [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nWhat does this PR do?\n## Background Context\n\nWhy did you take this approach?\n## References\n\nAre there any relevant issues / PRs / mailing lists discussions?\n\nIt took me some time to get Eclipse up an running with play integration. These are minor changes which would have helped me.\n Update IDE.md >>> 0"
18,Add possibility to customize error message for built-in constraints in more flexible way.\n Custom error messages for built-in validation constraints >>> 1
19,Removes FakeApplication and other deprecated helpers from `play.test` and `play.api.test`.\n Remove deprecated test helpers >>> 1
20,"Is there  a reason for this behavior?\n\n```\nscala> case class Emtpy()\ndefined class Emtpy\n\nscala> Json.reads[Emtpy]\n<console>:16: error: Apply of object Emtpy has no parameters. Are you using an empty case class?\n              Json.reads[Emtpy]\n                        ^\n```\n\nIn a project I'm working on, this situation is fairly typical. It would be nice if this case produced a `Reads` that matched an empty `JsObject`.\n Json.reads fails for case classes with no parameters >>> 1"
21,"Actually my Commit for Akka-Http missed some Tests and Documentation, this adds it: https://github.com/playframework/playframework/commit/a10f4a77a54fafc9e23afb4ec9072f53e9fa8b5d\n adds documentation around AkkaHttpServer embedded and adds some tests >>> 1"
22,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [x] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n## Fixes\n\nFixes #6419 \n Fix Java Formatters (#6419) >>> 1"
23,"The signature for `Action.call` is outdated [here](https://www.playframework.com/documentation/2.5.x/JavaActionsComposition) saying that call throws `Throwable`, but according to the [javadocs](https://www.playframework.com/documentation/2.5.x/api/java/play/mvc/Action.html#call-play.mvc.Http.Context-), nothing is thrown.\n Docfix - Update JavaActionsComposition - Action.call doesn't throw anymore >>> 1"
24,Pick off some low-hanging fruit with IntelliJ IDEA.\n Remove unused imports >>> 1
25,"actually these adds a wrapper class JFilter which actually works around the different trait encoding\nthat will be introduced by scala 2.12\nand it removes some calls to Int.unbox() in java which aren't necessary since we can just cast these to Integer (which Int.unbox) will do anyway, calling the intValue() isn't needed since this won't be so much overhead here and the jvm can unbox it anyway\nit also adds the newest version of twirl and play-doc\n\nGuess it's fine to merge as is\n upgrades play-doc, twirl and fixes some hiccups while trying to get scala 2.12 to work >>> 1"
26,"Upgrades specs2 and twirl versions to 3.8.5 and Twirl 1.2.0, to be better suited for Scala 2.12, for https://github.com/playframework/playframework/issues/6110\n\nSee https://github.com/etorreborre/specs2/issues/504 and \n Upgrade spec2 and twirl for 2.12 >>> 1"
27,"Adds a description of using Play WS from a main method in the simple case, with no bells or whistles.\n [doc] Add example of using WS from main >>> 1"
28,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n## Purpose\n\nEvolutions doesn't work with query parameter.\n\n> http://localhost:9000/search?q=aaa\n\nWhen url already has query parameters on Evolution, Play can't catch a redirect parameter as a query parameter in apply button.\n\n> http://localhost:9000/search?q=aaa/@evolutions/apply/default?redirect=http%3A%2F%2Flocalhost%3A8484%2Fsearch%3Fq%3Daaa\n\nBecause `?` character is duplicated in redirect url.\n\nMy PR removes the search query from url in apply button for Evolution.\n\n> http://localhost:9000/search/@evolutions/apply/default?redirect=http%3A%2F%2Flocalhost%3A8484%2Fsearch%3Fq%3Daaa\n fix failed evolutions with query parameter >>> 1"
29,"# Pull Request Checklist\n- [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #6272\n## Purpose\n\nThis is a backport of the change in the 2.5.x branch to fix the issue as above\n## Background Context\n\nThis was suggested as part of issue #6592 \n## References\n#6592\n 2.4.x Backport to fix issue #6272 >>> 1"
30,Removes deprecated example from scaladoc.\n Remove Play.current from scaladoc header >>> 1
31,Following https://github.com/playframework/playframework/pull/6255#issuecomment-226993937\n Add javax.inject.Inject to default Twirl constructor >>> 1
32,Removes the fork run based projects.\n\nSee http://eed3si9n.com/sbt-server-reboot -- the new plans for sbt server exclude them.\n Remove fork-run and sbt-remote-control >>> 1
33,"Connects Play apidoc with Akka, Java and Scaladocs.  \n\nThis means that https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.package will have links to the scala-lang site, java.lang.String, Akka, and so forth.\n\nThis is naturally difficult to test, but here's how you run through this:\n\n```\ncd framework\nsbt apiDocs\ncd ../documentation\nsbt run\n```\n\nThe external javadoc links were broken because of a missing "":"", meaning the apiMappings were not applied previously in 2.5.x.  This PR contains the following fix:\n\n```\n      val options = Seq(\n        // Note, this is used by the doc-source-url feature to determine the relative path of a given source file.\n        // If it's not a prefix of a the absolute path of the source file, the absolute path of that file will be put\n        // into the FILE_SOURCE variable below, which is definitely not what we want.\n        // Hence it needs to be the base directory for the build, not the base directory for the play-docs project.\n        ""-sourcepath"", (baseDirectory in ThisBuild).value.getAbsolutePath,\n        ""-doc-source-url"", ""https://github.com/playframework/playframework/tree/"" + sourceTree + ""/framework€{FILE_PATH}.scala"",\n        s""-doc-external-doc:${externalDocsScalacOption}"")\n```\n\nNote that while `sbt apiDocs` looks like `sbt doc`, it's actually targeting the root and doing some additional logic.  The additional links to external javadoc are fiddly and have to be done by hand, as AHC / Guice / EhCache do not have a standard position for their docs.  The work done here is incomplete, but is at least a basis.\n Connect docs to external apidoc where possible >>> 1"
34,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n## Purpose\n\nI add the link to my project of porting the [Handlebars](http://handlebarsjs.com/) templates to Play Framework.  It's based on the [Handlebars.java](https://github.com/jknack/handlebars.java) and add some helpers that works with play, like assets, reverse routing. There is also resolver for the Scala JSON classes. \n## Background Context\n\nI use it in my private project and was wondering that Play does not have such module yet. \n## References\n\n[Documentation](https://github.com/andriykuba/play-handlebars/blob/master/README.md)\n Update ModuleDirectory.md >>> 1"
35,Fixes https://github.com/playframework/playframework/issues/3672\n [doc] Add a paragraph emphasizing that an implicit request must exist in CSRF >>> 1
36,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [x] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n# Helpful things\n\nLet's upgrade sbt-jmh to the latest version (with the latest JMH version 1.14.1) if there's no problems :)\nFrom sbt-jmh version `0.2.10`, we can use Flight Recorder / Java Mission Control[1], which would be nice.\n[1] https://github.com/ktoso/sbt-jmh#using-oracle-flight-recorder\n Upgrade sbt-jmh to 0.2.16 >>> 1"
37,"# Pull Request Checklist\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n\nGenerally spelling fixes shouldn't require a CLA. I'd rather wait to see if someone wants these changes. If not, I'd rather skip the effort (the CLA text seems moderately problematic).\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n\nI'm willing to squash later. It's much easier for me to adapt to requirements of maintainers if I wait until I hear them.\n# Helpful things\n## Purpose\n\nFix spelling errors\n## Background Context\n\nAutomated tool to identify possibly misspelled words;\nManual interaction to select spelling fixes;\nI've intentionally left individual words as distinct commits, they can be squashed once someone is comfortable w/ the changes.\n\nThe vast majority of these changes won't affect builds. Most of the others are string or function name changes to tests which I'm fairly confident won't break anything.\n\nThere were some words that had both en-us and en-gb spellings, the en-us spellings were dominant and thus selected.\n Spelling >>> 1"
38,"actually on netty 4.1 some headers will be sent lowercased,\nhowever our current basic http test client wasn't inlined with the current rfc\nand handled headers in a case sensitive fashion.\n\nThis actually will enable netty-reactive-streams 2.0.0-SNAPSHOT, async-http-client 2.1.0-alpha1 and netty 4.1.5 to run the play test suite.\n changes the basic http client to use a case insensitive tree map >>> 1"
39,Throwables being thrown during initialization of DefaultHttpErrorHandler\nmay suppress prior thrown ones.\n\nThis happens when you e.g. have a broken configuration the can't be initialized (by expecting a (non-optional) environment variable).\n\nDefaultHttpErrorHandlerProvider now provides the DefaultHttpErrorHandler\nas lazy val to prevent this behavior.\n Do not suppress throwables on DefaultHttpErrorHandler initialization >>> 1
40,This required moving a few play-java classes to play. I think we should be able to safely backport to 2.5.x.\n\nThis prevents `play-java` from being pulled in on Scala projects that use the filters (resulting in spring-core and various other dependencies getting pulled in).\n\nThanks to @dbuschman7 for noticing the unnecessary dependency.\n Remove play-java dependency from filters-helpers >>> 1
41,"This creates a `SimpleModule` class that makes it slightly easier to define Play DI modules. It allows the syntax:\n\n``` scala\nclass FooModule extends SimpleModule(\n  bind[Foo].to[FooImpl],\n  bind[Bar].to[BarImpl]\n)\n```\n\nor passing a function:\n\n``` scala\nclass BazModule extends SimpleModule((env, conf) => Seq(\n  bind[Environment] to env,\n  bind[Configuration] to conf\n))\n```\n\nThis is mainly intended to clean up our internal code but it could also be useful to external module developers.\n Create SimpleModule helper for creating Play modules >>> 1"
42,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nUpdated Streaming HTTP responses documentation with  new API  to create a Result object directly and choose an HttpEntity to represent its body.\n## Background Context\n Updated Streaming HTTP responses documentation >>> 1"
43,Backport https://github.com/playframework/playframework/pull/6584 to 2.5.x\n Backport documentation mappings >>> 1
44,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nImprove scala code for compactness and performance.\n## Background Context\n Replaced pattern matching with if-else on boolean for compactness and performance >>> 0"
45,- Mention SBT as the build tool except for the few areas we talk about features exclusive to Activator.\n- Clarify that Activator is just a wrapper for SBT\n- Refer to the sample template downloads being added to the Play website: https://github.com/playframework/playframework.com/pull/97\n De-emphasize activator in docs and refer to sample templates >>> 1
46,"This makes the `executionContext` method of `Action` abstract. Previously we defaulted to Play's default execution context obtained using global state.\n\nThis shouldn't have much effect on our users, since Actions are generally created using `ActionBuilder`.\n Make Action#executionContext abstract >>> 1"
47,This moves the actual encoding of cookies to the server implementation. The list of cookies as well as separate session and flash data structures are stored directly on the request. These are encoded when the request is sent.\n\nThis has two main advantages:\n1. Not having to encode and decode the cookies every time we change a small thing saves us unnecessary work and makes the implementation simpler\n2. Doing the encoding on the server instead of on the result itself makes it easier to remove the global state from results.\n\n@richdougherty Do you think it makes sense to use attributes on the result rather than extra fields here for session and flash?\n Move encoding of cookies from Result to server implementation >>> 1
48,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nFor code performance and compactness.\n## Background Context\n\nWhy did you take this approach?\n## References\n\n```\n #6634 \n```\n Code updation of HttpConfiguration for compactness and performance >>> 1"
49,Fixes #6646\n Clarify guice error message >>> 1
50,"The four templates now work with SNAPSHOT and are built seperately, so we can remove this.\n Remove templates directory >>> 1"
51,nan More clarifications to docs to explain how Play's DI system works >>> 1
52,"To do this I also added a `Server.withRouterFromComponents` that allows you to create a router from a `BuiltInComponents`, but this could potentially be refined.\n Remove many uses of global Action builder >>> 1"
53,"## Purpose\n\nExtract currentApp() from Http.Context, and inject MessagesApi and Langs from outside using JavaContextComponents.\n## Background Context\n\nHttp.Context() contains a `messages()` method, so it must be provided from somewhere in the app.  JavaHandlerComponents are used in JavaAction, so DI can be merged in from that point.\n Factor out global state from Http.Context >>> 1"
54,Moves around a bunch of the assets stuff so everything is injected.\n Remove global state from Assets >>> 1
55,"This PR adds an example and clarifies how the `options` helper actually converts the list/pairs given to it into values/displays for a `select` helper. \n\nPreviously, it wasn't clear to people who weren't comfortable looking at the source code of the helpers that the first value of the pair would become the value of an input and the second would become the text displayed to the user.\n\nSigned the CLA via the github application. \n Update ScalaForms documentation with options example >>> 1"
56,"This PR simply fixes 2 lines in documentation/manual/gettingStarted/Installing.md,\nwhere the path to activator was wrong.\n Fix path to activator in document >>> 1"
57,"## Purpose\n\nRemoves an unneeded JavaContextComponents from the `JavaCompatibleHttpRequestHandler` constructor.\n## Background Context\n\n`JavaContextComponents` is pulled from `JavaHandlerComponents`, so this constructor doesn't need to exist.\n Remove an extraneous JavaContextComponents parameter >>> 1"
58,Fixes #6626\n\nAlso remove several deprecated methods for Java cookies (these have been deprecated since 2.5.0 and are easily rewritten).\n Change CookieBuilder#withMaxAge to accept Duration >>> 1
59,"Fixes: https://github.com/playframework/playframework/issues/5966\r\n\r\nSo, this is going to be fairly long, and is partly inspired by @cb372's post on Twitter:\r\n\r\n> Trying to guide a Play beginner through all the steps needed to add a simple form to a Play 2.4.x app is just embarrassing -- https://twitter.com/cbirchall/status/679359109234339840\r\n\r\nand expanded on at https://www.theguardian.com/info/developer-blog/2015/dec/30/how-to-add-a-form-to-a-play-application and https://github.com/cb372/play-forms-tutorial\r\n\r\nTo get a message out of Play you need:\r\n1. A `MessagesApi` reference\r\n2. A `Lang` reference\r\n3. The key / value to actually call the message.\r\n\r\n```\r\nval message = messagesApi.apply(key)(implicit lang)\r\n```\r\n\r\nThere is a case class `Messages`, which locks down the `Lang` reference, and lets you call just one thing:\r\n\r\n```\r\nval message = messages.apply(key)\r\n```\r\n\r\nBut there's a problem in that `Messages` is a concrete case class, which doesn't just expose `apply` like it should.  It also exposes messagesApi (not great) and extends Product (which means `copy` et al) and it means that delegating / mocking out those methods is basically impossible.  \r\n\r\nThis gets even worse when you look at the form helpers signature:\r\n\r\n```\r\n@(field: play.api.data.Field, args: (Symbol,Any)*)(implicit handler: FieldConstructor, messages: play.api.i18n.Messages)\r\n```\r\n\r\nBecause all the form helpers require the `Messages` case class to be passed in to the template, that means that for any template with a form helper, there must be an implicit Messages in scope.  This means we have to specify an implicit messages in template:\r\n\r\n```\r\n@(form: Form[UserData])(implicit m: Messages)\r\n\r\n@main(""Welcome to Play"") {\r\n  @helper.inputText(form(""name""))\r\n  @helper.inputText(form(""age""))\r\n}\r\n```\r\n\r\nSo if you declare a form:\r\n\r\n```\r\nclass HomeController @Inject() extends Controller {\r\n  import play.api.data.Forms._\r\n  import play.api.data._\r\n\r\n  val form = Form(\r\n    mapping = mapping(\r\n      ""name"" -> text,\r\n      ""age"" -> number\r\n    )(UserData.apply)(UserData.unapply)\r\n  )\r\n\r\n  def index = Action {\r\n    Ok(views.html.index(form))\r\n  }\r\n}\r\n```\r\n\r\nNope: ""could not find implicit value for parameter m: play.api.i18n.Messages"".  Using `(implicit r: Request[_])` doesn't work either, because you don't have a reference to `MessagesApi` from the request.  (Ironically, you can get one from a Java template, because Http.Context will provide one implicitly, but we're talking about Scala.)\r\n\r\nThe canonical way to do it is to use I18NSupport:\r\n\r\n```\r\nclass HomeController @Inject()\r\n  (val messagesApi: play.api.i18n.MessagesApi) \r\n  extends Controller with play.api.i18n.I18nSupport {\r\n\r\n}\r\n```\r\n\r\nAt which point the implicit conversion of request2Messages will kick in, and `@(form: Form[UserData])(implicit m: Messages)` will work.  \r\n\r\nBut we're not done yet.  We want to use CSRF in the form,  \r\n\r\n```\r\n@helper.CSRF.formField\r\n```\r\n\r\nBut when we try this, we get `Cannot find any HTTP Request Header here`.\r\n\r\n which means we have to include the request implicitly as well:\r\n\r\n```\r\n@(form: Form[UserData])(implicit request: RequestHeader, m: Messages)\r\n```\r\n\r\nWhich means for a basic ""hello world"" type form, we have to implement and / or inject `MessagesApi`, `I18nSupport`, `Request[_]` and `Messages` into the controller and template.  \r\n\r\nWhat this PR does instead is:\r\n- Provide Messages functionality through a `Messager` interface (""Messenger"" was considered too confusing)\r\n- Create a `MessagerProvider` interface with a single `messager: Messager` interface.\r\n- Make `Messages` extends `Messager` and `MessageProvider` so existing code will continue to work\r\n- Make the form helpers take `MessagerProvider` instead of `Messages`.\r\n\r\nUsing a MessagerProvider means that we can break out the messagesApi from the controller, and move it into the request or the action:\r\n\r\n```\r\ntrait MessagesRequestHeader extends RequestHeader with MessagerProvider\r\n\r\nabstract class MessagesRequest[A](request: Request[A])\r\n  extends WrappedRequest(request) with MessagesRequestHeader\r\n\r\nclass MessagesAction @Inject()(messagesApi: MessagesApi) extends ActionBuilder[MessagesRequest] {\r\n  override def invokeBlock[A](request: Request[A], block: (MessagesRequest[A]) => Future[Result]) = {\r\n    block(new MessagesRequest[A](request) {\r\n      lazy val messager: Messager = messagesApi.preferred(request)\r\n    })\r\n  }\r\n}\r\n```\r\n\r\nwhich means the following is all that's required in the controller:\r\n\r\n```\r\nclass HomeController @Inject() (action: MessagesAction) extends Controller {\r\n  def index = action { implicit request =>\r\n    Ok(views.html.index(form))\r\n  }\r\n}\r\n```\r\n\r\nAnd this is all that's needed in the template:\r\n\r\n```\r\n@()(implicit mr: MessagesRequestHeader)\r\n```\r\n\r\nTechnically this could be `MessagesProvider`, but it's not great that something that is essentially a partially applied function on `messages.get(lang)(key)` is being exposed as a case class.  Everything still returns `Messages`, so it's only classes that take `Messages` as input that should switch to `Messager`.\r\n Refactor I18n API >>> 1"
60,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [x] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n# Helpful things\n## Purpose\n\nAdds a `Providing` trait to make access to components in functional testing a bit easier.\n## Background Context\n\nUsing `app.injector.instanceOf[Foo]` all the time is not all that fun, and this provides a more direct experience to the end user, who really shouldn't have to care about the injector per se.\n Add Injecting trait to WithApplication >>> 1"
61,Fixes a broken link from the deployment docs to the sbt-native-packager docs regarding default mappings for the java server archetype.\n Fix reference to default mappings in sbt-native-packager docs >>> 1
62,Added the mohiva/swagger-codegen-play-scala module and updated the URLs for Silhouette.\n Update ModuleDirectory.md >>> 1
63,"# Pull Request Checklist\n- [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n\nMostly...\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nWhat does this PR do?\n\nAdds a summary of the capabilities of the Scala JSON package so users know it's able to convert to and from case classes automatically. Also provided an example of doing that.\n## Background Context\n\nWhy did you take this approach?\n\nThe first page of the JSON docs is pretty intimidating for new users. Now it highlights some of the features of the package so users don't get bogged down in the details if they just want to serialise to/from case classes.\n## References\n\nAre there any relevant issues / PRs / mailing lists discussions?\n#6663\n Summarize the JSON package and provide relevant links. >>> 1"
64,"# Pull Request Checklist\n- [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [x] Have you added copyright headers to new files?\n- [x] Have you checked that both Scala and Java APIs are updated?\n- [x] Have you updated the documentation for both Scala and Java sections?\n- [x] Have you added tests for any changed functionality?\n# Helpful things\n## Purpose\n\nThis PR adds the SLF4J Marker API to play.Logger and play.api.Logger interfaces. \n\nIn the Java API, it is a straight port of the SLF4J Logger API.\n\nIn the Scala API, markers are added through a MarkerContext trait, which is added as an implicit parameter to the logger methods, i.e. \n\n``` scala\nimport play.api._\nlogger.info(""some info message"")(MarkerContext(someMarker))\n```\n\nThis opens the door for implicit markers to be passed for logging in several statements, which makes adding context to logging much easier.  In particular, see what you can do with the [Logstash Logback Encoder](https://github.com/logstash/logstash-logback-encoder#event-specific-custom-fields):\n\n``` scala\nimplicit def requestToMarkerContext[A](request: Request[A]): MarkerContext = {\n  import net.logstash.logback.marker.LogstashMarker\n  import net.logstash.logback.marker.Markers._\n\n  val requestMarkers: LogstashMarker = append(""host"", request.host)\n    .and(append(""path"", request.path))\n\n  MarkerContext(requestMarkers)\n}\n\ndef index = Action { request =>  \n  logger.debug(""index: "")(request)\n  Ok(""testing"")\n}\n```\n## Background Context\n\nFirst, because play.api.Logger does not take Marker, at all, so it should be added.  There's a number of teams that use SLF4J because the underlying API is richer.  The static API for play.Logger / play.api.Logger is a distinct issue and is not being touched here.\n\nSecond, By using an implicit context, one of the main problems with logging contextual information can be resolved without using MDC / thread locals.  \n\nThirdly, if we can't get rid of the Logger API, we should make it useful.  This should help with things like [logstash-logback-encoder](https://github.com/logstash/logstash-logback-encoder), for example, although it doesn't touch StructuredArguments:\n\n```\nlogger.info(""My Message {}"", StructuredArguments.keyValue(""key"", ""value""))\n```\n## References\n\nDeprecate Play Logger (note does not deal with Scala Logger): https://github.com/playframework/playframework/issues/1669\n Add Marker support to Logger API >>> 1"
65,"This makes sure the Netty server always uses Akka's ExecutionContext to execute actions (like we did in 2.4.x), rather than using the Netty thread.\n\n/cc @jroper \n Use the Akka ExecutionContext to execute actions >>> 1"
66,"**DISCLAIMER/WARNING:** This uses 2 Commits to preserve the commit history for Build.scala so don't use the `Squash and merge` Feature of Github or any sort of Rebase, else the history will be destroyed!\n\n**Test this carefully**\n\n---\n\nActually I moved `Build.scala` to `build.sbt` via `git mv`, after that I actually fixed the build and created a seperate `BuildSettings.scala` which contains the `BuildSettings` object, unfortunatly the history for that object is lost.\n\nAlso one thing to notice in `BuildSettings` the scalaVersion is hardcoded: https://github.com/playframework/playframework/pull/6675/commits/8b06c87de909e1cc3bcd86430c3a69be2a328de6#diff-fe35f749d4ff761b1776fd1b8bfc3922R196\n\nOne way to fix it would be changing Interplay to have the scalaVersion of PlayProject somewhere as a static object and then be used here.\n\nI actually run the following commands and couldn't find any major regression:\n- sbt publishLocal\n- sbt scripted\n- sbt test inside documentation and framework\n Move build scala to build sbt >>> 1"
67,Fixes https://github.com/playframework/playframework/issues/6110#issuecomment-256843739\n fixes MessagesTest so that the community build won't throw any error >>> 1
68,"# Pull Request Checklist\n- [X] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [X] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [X] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [X] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [X] Have you added copyright headers to new files?\n- [X] Have you checked that both Scala and Java APIs are updated?\n- [X] Have you updated the documentation for both Scala and Java sections?\n- [X] Have you added tests for any changed functionality?\n## Purpose\n\nImprove handling of If-None-Match header:\n- correctly parse a list of etags\n- correctly parse weak etags\n- use etag weak comparison as specified by rfc7232\n If-None-Match caching header improvements. >>> 1"
69,No reason this would deadlock so it can execute on the same thread.\n Use trampoline EC in Action#asJava >>> 1
70,# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #5755\r\n\r\n## Purpose\r\n\r\nRemoves the play-ws ssl config and uses typesafe ssl-config instead.\r\n removes play-ws ssl and uses typesafe ssl-config instead >>> 1
71,#6605 [Doc] Adding 'StreamedBody' example to `forward-body`  [Doc] Adding 'StreamedBody' example to `forward-body` >>> 1
72,"The source code above the text I've changed refers to `controllers.routes.Application` and `controllers.Application`, and not `controllers.admin.routes.Application` and `controllers.admin.Application`.\r\n\r\nI therefore removed the `.admin` - which is otherwise inconsistent with the preceding code.\r\n\r\nIntroducing a different namespace placement of the Application in this paragraph could confuse the reader.\r\n\r\nOne can see the rendering of the code and document here, for review:\r\n\r\nhttps://playframework.com/documentation/2.5.6/ScalaRouting#reverse-routing\r\n\r\nhttps://playframework.com/documentation/2.5.6/JavaRouting#reverse-routing\r\n Remove incorrect occurrence of "".admin."" in HTTP Routing docs >>> 1"
73,"Fixed up reference to ""two years.""  Also some minor grammar corrections I noticed. [Doc] Tidied Philosophy.md >>> 1"
74,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n\r\n fix unchecked assignment problem in sample code >>> 1"
75,"As Play Pagelets is a kind of framework on top of Play, I have added a new section. Add Play Pagelets to the Module directory >>> 0"
76,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nIt adds a tiny bit to the akka documentation clarifying how the child actor gets its name, because in the existing example the `key` parameter is used both as a parameter to the factory as well as the actor's name.\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n\r\n  - this confused me a lot when I saw that the key is passed twice. documentation: clarify how the child actor get its name >>> 1"
77,"Sets up a Play version that works against 2.12.0 by default and does a cross-build to 2.11.8.\r\n\r\nAll the dependencies have been worked through, so if this builds, we should be good. Upgrade to Scala 2.12 >>> 1"
78,"The goal of this change is to remove global state from the cookies,\r\nsession and flash request properties. Before this change these properties were\r\ndefined as lazy values on the RequestHeader trait.\r\n\r\nUnfortunately the lazy values needed to access global state to get\r\ntheir configuration and it was difficult to feed into the\r\nRequestHeader trait without adding configuration as a property of the\r\nRequestHeader object itself.\r\n\r\nThis commit changes the RequestHeader trait so that it stores the\r\ncookie, session and flash objects as attributes of the RequestHeader\r\nin a TypedMap instead of as lazy properties.\r\n\r\nThe attributes are explicitly attached to the RequestHeader using a\r\nnew type of class called a RequestFactory. The RequestFactory is used\r\nto create every RequestHeader that is sent to an application.\r\n\r\nThe RequestFactory participates in dependency injection, so it doesn't\r\nuse any global state to do its work. It manipulates the cookie,\r\nsession and flash objects based on its injected configuration.\r\n\r\nIn order to make it easier to create, transform and manipulate\r\nrequests I simplified their properties a bit more. I made two new\r\nobjects to bundle together sets of related request properties. The\r\nRequestTarget class holds the URI, path and query string. The\r\nRemoteConnection class holds the remote address, HTTPS and certificate\r\ninformation.\r\n\r\nI considered using a filter to attach the attributes to the request.\r\nHowever a filter doesn't work with only a RequestHeader; it expects a\r\nRequest with a body. Also filters are asynchronous; a simple\r\nsynchronous function for creating RequestHeaders is usually more\r\nconvenient.\r\n\r\nSince I was already moving the cookies, session and flash properties\r\ninto attributes, I did the same for the request id and request tags.\r\n\r\nThings to consider in the future:\r\n\r\n1. Moving the ForwardedHeaderHandler logic out of the ModelConversion\r\n   classes and into the default RequestFactory.\r\n2. Making the RequestFactory implementation configurable so that\r\n   users can make their own kinds of requests.\r\n3. Making aspects of the default RequestFactory implementation\r\n   configurable, e.g. not attaching request ids if they're not\r\n   needed.\r\n4. Updating the Result object so that it has attributes, then\r\n   storing the Result's cookie, session and flash objects as\r\n   attributes. Move cookies into attributes; remove cookie global state >>> 1"
79,Fixes #6629.\r\n\r\nI also changed to use Guava's IP address parser instead of the standard Java parser. The Java parser will make DNS lookups for non-IP addresses. I don't think we need DNS lookups so the Guava parser is probably better.\r\n\r\nCc @swoopster-sb. Parse IPv4-mapped IPv6 addresses in forward headers >>> 1
80,I `cherry-picked` the commit of https://github.com/playframework/playframework/pull/6569 from @xuwei-k and fixed all `InputKey` tasks. \r\nAfter that I also fixed all errors and upgraded sbt to `0.13.13` wherever it was necessary. Removed sbt deprecations and upgraded sbt to 0.13.13 >>> 1
81,nan Add Play Pagelets #2 >>> 1
82,"The OpenID class contains static methods, was deprecated in 2.5.x, is not used by anything, and has references to `Play.current`. Remove deprecated OpenID class >>> 1"
83,"# Pull Request Checklist\n- [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nWhat does this PR do?\n## Background Context\n\nWhy did you take this approach?\n## References\n\nAre there any relevant issues / PRs / mailing lists discussions?\n 2.3.x >>> 0"
84,"# Pull Request Checklist\n- [X] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [X] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [X] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [X] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [X] Have you added copyright headers to new files?\n- [X] Have you checked that both Scala and Java APIs are updated?\n- [X] Have you updated the documentation for both Scala and Java sections?\n- [X] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #5193 \n## Purpose\n\nThis patch adds support to conditionally handle requests from non-whitelisted origins as non-CORS requests, ie let them to pass through. The default behavior is not changed, unallowed origins are forbidden. Please note that serving forbidden origins as non-CORS requests doesn't lower the security, given that browsers check the access control headers and block the response anyway.\n## Background Context\n\nNot all requests with an `Origin` header are CORS requests generated by browsers. Other clients can set the `Origin` header, don't send preflight calls, and don't expect to receive CORS access control headers in the answer. A common client of such type is a Cordova application, that always sends the `Origin: file://` header.\n Add a CORS config param to allow pass-through of forbidden origins. >>> 1"
85,@wsargent You added a few things to `ControllerComponents` so I wanted to make them also available through `AbstractController`. I also changed names to make them less likely to conflict with existing methods in the controller. Add methods to AbstractController >>> 1
86,/cc @wsargent  [Backport 2.5.x] Add Timeout trait to play.api.libs.concurrent (#6459) >>> 1
87,"Removes static `play.api.libs.Mimetypes` class that was calling `Play.current`, and moves all the mapping into configuration. [UFR] Add FileMimeTypes and move mime types to configuration >>> 1"
88,I noticed I couldn't find the bindings for the Java versions of these anywhere. I think we inadvertently removed them. Add bindings for Java i18n classes >>> 1
89,"The master build seems to be failing and we don't need to use a custom java installer now that Travis CI has upgraded past the u31 bug...\r\n\r\nhttps://travis-ci.org/playframework/playframework/builds/174834911\r\n\r\n```\r\nUsing worker: worker-linux-docker-d5c5f82d.prod.travis-ci.org:travis-linux-3\r\n\r\nBuild system information\r\nBuild language: scala\r\nBuild group: stable\r\nBuild dist: precise\r\nBuild id: 174834911\r\nJob id: 174834912\r\ntravis-build version: 628c8d2e6\r\nBuild image provisioning date and time\r\nThu Feb  5 15:09:33 UTC 2015\r\nOperating System Details\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 12.04.5 LTS\r\nRelease:\t12.04\r\nCodename:\tprecise\r\nLinux Version\r\n3.13.0-29-generic\r\nCookbooks Version\r\na68419e https://github.com/travis-ci/travis-cookbooks/tree/a68419e\r\nGCC version\r\ngcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\r\nCopyright (C) 2011 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nLLVM version\r\nclang version 3.4 (tags/RELEASE_34/final)\r\nTarget: x86_64-unknown-linux-gnu\r\nThread model: posix\r\nPre-installed Ruby versions\r\nruby-1.9.3-p551\r\nPre-installed Node.js versions\r\nv0.10.36\r\nPre-installed Go versions\r\n1.4.1\r\nRedis version\r\nredis-server 2.8.19\r\nriak version\r\n2.0.2\r\nMongoDB version\r\nMongoDB 2.4.12\r\nCouchDB version\r\ncouchdb 1.6.1\r\nNeo4j version\r\n1.9.4\r\nRabbitMQ Version\r\n3.4.3\r\nElasticSearch version\r\n1.4.0\r\nInstalled Sphinx versions\r\n2.0.10\r\n2.1.9\r\n2.2.6\r\nDefault Sphinx version\r\n2.2.6\r\nInstalled Firefox version\r\nfirefox 31.0esr\r\nPhantomJS version\r\n1.9.8\r\nant -version\r\nApache Ant(TM) version 1.8.2 compiled on December 3 2011\r\nmvn -version\r\nApache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)\r\nMaven home: /usr/local/maven\r\nJava version: 1.7.0_76, vendor: Oracle Corporation\r\nJava home: /usr/lib/jvm/java-7-oracle/jre\r\nDefault locale: en_US, platform encoding: ANSI_X3.4-1968\r\nOS name: ""linux"", version: ""3.13.0-29-generic"", arch: ""amd64"", family: ""unix""\r\n\r\n$ export DEBIAN_FRONTEND=noninteractive\r\nW: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-updates_restricted_binary-amd64_Packages.gz is not what the server reported 19576 20785\r\nW: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-updates_restricted_binary-i386_Packages.gz is not what the server reported 19521 20707\r\nW: Size of file /var/lib/apt/lists/us.archive.ubuntu.com_ubuntu_dists_precise-backports_multiverse_source_Sources.gz is not what the server reported 5886 5888\r\nW: Size of file /var/lib/apt/lists/ppa.launchpad.net_travis-ci_zero-mq_ubuntu_dists_precise_main_binary-amd64_Packages.gz is not what the server reported 832 1195\r\nW: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-amd64_Packages.gz is not what the server reported 33653 36677\r\nW: Size of file /var/lib/apt/lists/ppa.launchpad.net_ubuntugis_ppa_ubuntu_dists_precise_main_binary-i386_Packages.gz is not what the server reported 33699 36733\r\nW: Size of file /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_precise-security_restricted_binary-amd64_Packages.gz is not what the server reported 13782 14904\r\nW: Size of file /var/lib/apt/lists/security.ubuntu.com_ubuntu_dists_precise-security_restricted_binary-i386_Packages.gz is not what the server reported 13751 14885\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nThe following extra packages will be installed:\r\n  libc-bin libc-dev-bin libc6-dev\r\nSuggested packages:\r\n  glibc-doc\r\nThe following packages will be upgraded:\r\n  libc-bin libc-dev-bin libc6 libc6-dev\r\n4 upgraded, 0 newly installed, 0 to remove and 263 not upgraded.\r\nNeed to get 8,840 kB of archives.\r\nAfter this operation, 14.3 kB disk space will be freed.\r\nGet:1 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6-dev amd64 2.15-0ubuntu10.15 [2,943 kB]\r\nGet:2 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-dev-bin amd64 2.15-0ubuntu10.15 [84.7 kB]\r\nGet:3 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc-bin amd64 2.15-0ubuntu10.15 [1,177 kB]\r\nGet:4 http://us.archive.ubuntu.com/ubuntu/ precise-updates/main libc6 amd64 2.15-0ubuntu10.15 [4,636 kB]\r\nFetched 8,840 kB in 0s (27.1 MB/s)\r\nPreconfiguring packages ...\r\n(Reading database ... 72019 files and directories currently installed.)\r\nPreparing to replace libc6-dev 2.15-0ubuntu10.10 (using .../libc6-dev_2.15-0ubuntu10.15_amd64.deb) ...\r\nUnpacking replacement libc6-dev ...\r\nPreparing to replace libc-dev-bin 2.15-0ubuntu10.10 (using .../libc-dev-bin_2.15-0ubuntu10.15_amd64.deb) ...\r\nUnpacking replacement libc-dev-bin ...\r\nPreparing to replace libc-bin 2.15-0ubuntu10.10 (using .../libc-bin_2.15-0ubuntu10.15_amd64.deb) ...\r\nUnpacking replacement libc-bin ...\r\nProcessing triggers for man-db ...\r\nSetting up libc-bin (2.15-0ubuntu10.15) ...\r\n(Reading database ... 72018 files and directories currently installed.)\r\nPreparing to replace libc6 2.15-0ubuntu10.10 (using .../libc6_2.15-0ubuntu10.15_amd64.deb) ...\r\nUnpacking replacement libc6 ...\r\nSetting up libc6 (2.15-0ubuntu10.15) ...\r\nSetting up libc-dev-bin (2.15-0ubuntu10.15) ...\r\nSetting up libc6-dev (2.15-0ubuntu10.15) ...\r\nProcessing triggers for libc-bin ...\r\nldconfig deferred processing now taking place\r\nUpdating sbt\r\n$ git clone --depth=50 --branch=master https://github.com/playframework/playframework.git playframework/playframework\r\nCloning into 'playframework/playframework'...\r\nremote: Counting objects: 4218, done.\r\nremote: Compressing objects: 100% (2667/2667), done.\r\nReceiving objects: 100% (4218/4218), 4.88 MiB | 0 bytes/s, done.\r\nremote: Total 4218 (delta 906), reused 2841 (delta 701), pack-reused 0\r\nResolving deltas: 100% (906/906), done.\r\nChecking connectivity... done.\r\n\r\n$ cd playframework/playframework\r\n$ git checkout -qf 22582318c2d42e30a33267c858451715879e9669\r\nInstalling APT Packages (BETA)\r\n$ export DEBIAN_FRONTEND=noninteractive\r\n$ sudo -E apt-get -yq update &>> ~/apt-get-update.log\r\n\r\n$ sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install oracle-java8-installer\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nSuggested packages:\r\n  binfmt-support visualvm ttf-baekmuk ttf-unfonts ttf-unfonts-core\r\n  ttf-kochi-gothic ttf-sazanami-gothic ttf-kochi-mincho ttf-sazanami-mincho\r\n  ttf-arphic-uming\r\nRecommended packages:\r\n  oracle-java8-set-default\r\nThe following packages will be upgraded:\r\n  oracle-java8-installer\r\n1 upgraded, 0 newly installed, 0 to remove and 262 not upgraded.\r\nNeed to get 23.5 kB of archives.\r\nAfter this operation, 128 kB disk space will be freed.\r\nGet:1 http://ppa.launchpad.net/webupd8team/java/ubuntu/ precise/main oracle-java8-installer all 8u111+8u111arm-1~webupd8~0 [23.5 kB]\r\nFetched 23.5 kB in 0s (101 kB/s)\r\nPreconfiguring packages ...\r\n(Reading database ... 72018 files and directories currently installed.)\r\nPreparing to replace oracle-java8-installer 8u31+8u33arm-1~webupd8~1 (using .../oracle-java8-installer_8u111+8u111arm-1~webupd8~0_all.deb) ...\r\noracle-license-v1-1 license has already been accepted\r\nUnpacking replacement oracle-java8-installer ...\r\nProcessing triggers for shared-mime-info ...\r\nSetting up oracle-java8-installer (8u111+8u111arm-1~webupd8~0) ...\r\nNo /var/cache/oracle-jdk8-installer/wgetrc file found.\r\nCreating /var/cache/oracle-jdk8-installer/wgetrc and\r\nusing default oracle-java8-installer wgetrc settings for it.\r\nDownloading Oracle Java 8...\r\n--2016-11-10 17:09:15--  http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz\r\nResolving download.oracle.com (download.oracle.com)... 104.96.220.152, 104.96.220.162\r\nConnecting to download.oracle.com (download.oracle.com)|104.96.220.152|:80... connected.\r\nHTTP request sent, awaiting response... 302 Moved Temporarily\r\nLocation: https://edelivery.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz [following]\r\n--2016-11-10 17:09:15--  https://edelivery.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz\r\nResolving edelivery.oracle.com (edelivery.oracle.com)... 104.96.237.225, 2600:1408:10:185::2d3e, 2600:1408:10:184::2d3e\r\nConnecting to edelivery.oracle.com (edelivery.oracle.com)|104.96.237.225|:443... connected.\r\nHTTP request sent, awaiting response... 302 Moved Temporarily\r\nLocation: https://www.oracle.com/splash/edelivery/index.html [following]\r\n--2016-11-10 17:09:15--  https://www.oracle.com/splash/edelivery/index.html\r\nResolving www.oracle.com (www.oracle.com)... 104.96.237.225, 2600:1408:10:184::2d3e, 2600:1408:10:185::2d3e\r\nConnecting to www.oracle.com (www.oracle.com)|104.96.237.225|:443... connected.\r\nHTTP request sent, awaiting response... 503 Service Unavailable\r\n2016-11-10 17:09:15 ERROR 503: Service Unavailable.\r\n\r\ndownload failed\r\nOracle JDK 8 is NOT installed.\r\ndpkg: error processing oracle-java8-installer (--configure):\r\n subprocess installed post-installation script returned error exit status 1\r\nErrors were encountered while processing:\r\n oracle-java8-installer\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n\r\napt-get install failed\r\n$ cat ~/apt-get-update.log\r\nGet:1 http://downloads-distro.mongodb.org dist Release.gpg [490 B]\r\nGet:2 http://downloads-distro.mongodb.org dist Release [2,040 B]\r\nGet:3 http://downloads-distro.mongodb.org dist/10gen amd64 Packages [30.9 kB]\r\nGet:4 http://downloads-distro.mongodb.org dist/10gen i386 Packages [30.5 kB]\r\nHit http://us.archive.ubuntu.com precise Release.gpg\r\nGet:5 http://us.archive.ubuntu.com precise-updates Release.gpg [198 B]\r\nGet:6 http://us.archive.ubuntu.com precise-backports Release.gpg [198 B]\r\nHit http://us.archive.ubuntu.com precise Release\r\nGet:7 http://us.archive.ubuntu.com precise-updates Release [55.4 kB]\r\nGet:8 http://us.archive.ubuntu.com precise-backports Release [55.5 kB]\r\nHit http://us.archive.ubuntu.com precise/main Sources\r\nHit http://us.archive.ubuntu.com precise/restricted Sources\r\nHit http://us.archive.ubuntu.com precise/universe Sources\r\nHit http://us.archive.ubuntu.com precise/multiverse Sources\r\nHit http://us.archive.ubuntu.com precise/main amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/restricted amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/universe amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/multiverse amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/main i386 Packages\r\nHit http://us.archive.ubuntu.com precise/restricted i386 Packages\r\nHit http://us.archive.ubuntu.com precise/universe i386 Packages\r\nHit http://us.archive.ubuntu.com precise/multiverse i386 Packages\r\nGet:9 http://us.archive.ubuntu.com precise-updates/main Sources [613 kB]\r\nGet:10 http://us.archive.ubuntu.com precise-updates/restricted Sources [9,183 B]\r\nGet:11 http://us.archive.ubuntu.com precise-updates/universe Sources [163 kB]\r\nGet:12 http://us.archive.ubuntu.com precise-updates/multiverse Sources [11.0 kB]\r\nGet:13 http://us.archive.ubuntu.com precise-updates/main amd64 Packages [1,334 kB]\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nGet:14 http://ppa.launchpad.net precise Release.gpg [316 B]\r\nGet:15 http://ppa.launchpad.net precise Release.gpg [316 B]\r\nGet:16 http://ppa.launchpad.net precise Release.gpg [316 B]\r\nGet:17 http://security.ubuntu.com precise-security Release.gpg [198 B]\r\nGet:18 http://us.archive.ubuntu.com precise-updates/restricted amd64 Packages [19.6 kB]\r\nGet:19 http://us.archive.ubuntu.com precise-updates/universe amd64 Packages [365 kB]\r\nGet:20 http://us.archive.ubuntu.com precise-updates/multiverse amd64 Packages [19.4 kB]\r\nGet:21 http://ppa.launchpad.net precise Release.gpg [316 B]\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nGet:22 http://ppa.launchpad.net precise Release [12.9 kB]\r\nGet:23 http://us.archive.ubuntu.com precise-updates/main i386 Packages [1,425 kB]\r\nGet:24 http://apt.postgresql.org precise-pgdg Release.gpg [819 B]\r\nGet:25 http://ppa.launchpad.net precise Release [12.9 kB]\r\nGet:26 http://us.archive.ubuntu.com precise-updates/restricted i386 Packages [19.5 kB]\r\nGet:27 http://us.archive.ubuntu.com precise-updates/universe i386 Packages [375 kB]\r\nGet:28 http://us.archive.ubuntu.com precise-updates/multiverse i386 Packages [19.6 kB]\r\nGet:29 http://us.archive.ubuntu.com precise-backports/main Sources [6,057 B]\r\nGet:30 http://us.archive.ubuntu.com precise-backports/restricted Sources [40 B]\r\nGet:31 http://us.archive.ubuntu.com precise-backports/universe Sources [52.3 kB]\r\nGet:32 http://us.archive.ubuntu.com precise-backports/multiverse Sources [5,886 B]\r\nGet:33 http://us.archive.ubuntu.com precise-backports/main amd64 Packages [6,727 B]\r\nGet:34 http://us.archive.ubuntu.com precise-backports/restricted amd64 Packages [40 B]\r\nGet:35 http://us.archive.ubuntu.com precise-backports/universe amd64 Packages [57.6 kB]\r\nGet:36 http://us.archive.ubuntu.com precise-backports/multiverse amd64 Packages [5,459 B]\r\nGet:37 http://us.archive.ubuntu.com precise-backports/main i386 Packages [6,719 B]\r\nGet:38 http://us.archive.ubuntu.com precise-backports/restricted i386 Packages [40 B]\r\nGet:39 http://us.archive.ubuntu.com precise-backports/universe i386 Packages [57.4 kB]\r\nGet:40 http://us.archive.ubuntu.com precise-backports/multiverse i386 Packages [5,437 B]\r\nGet:41 http://security.ubuntu.com precise-security Release [55.5 kB]\r\nGet:42 http://ppa.launchpad.net precise Release [12.9 kB]\r\nGet:43 http://ppa.launchpad.net precise Release [13.0 kB]\r\nGet:44 http://apt.postgresql.org precise-pgdg Release [40.1 kB]\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nGet:45 http://security.ubuntu.com precise-security/main Sources [183 kB]\r\nGet:46 http://apt.postgresql.org precise-pgdg/main amd64 Packages [115 kB]\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nGet:47 http://ppa.launchpad.net precise/main amd64 Packages [612 B]\r\nGet:48 http://ppa.launchpad.net precise/main i386 Packages [612 B]\r\nGet:49 http://ppa.launchpad.net precise/main amd64 Packages [832 B]\r\nGet:50 http://ppa.launchpad.net precise/main i386 Packages [828 B]\r\nGet:51 http://ppa.launchpad.net precise/main amd64 Packages [33.7 kB]\r\nGet:52 http://ppa.launchpad.net precise/main i386 Packages [33.7 kB]\r\nGet:53 http://security.ubuntu.com precise-security/restricted Sources [4,548 B]\r\nGet:54 http://security.ubuntu.com precise-security/universe Sources [63.2 kB]\r\nGet:55 http://security.ubuntu.com precise-security/multiverse Sources [2,902 B]\r\nGet:56 http://security.ubuntu.com precise-security/main amd64 Packages [834 kB]\r\nGet:57 http://ppa.launchpad.net precise/main amd64 Packages [3,110 B]\r\nGet:58 http://ppa.launchpad.net precise/main i386 Packages [3,110 B]\r\nGet:59 http://apt.postgresql.org precise-pgdg/main i386 Packages [115 kB]\r\nGet:60 http://security.ubuntu.com precise-security/restricted amd64 Packages [13.8 kB]\r\nGet:61 http://security.ubuntu.com precise-security/universe amd64 Packages [179 kB]\r\nGet:62 http://security.ubuntu.com precise-security/multiverse amd64 Packages [3,215 B]\r\nGet:63 http://security.ubuntu.com precise-security/main i386 Packages [923 kB]\r\nGet:64 http://security.ubuntu.com precise-security/restricted i386 Packages [13.8 kB]\r\nGet:65 http://security.ubuntu.com precise-security/universe i386 Packages [188 kB]\r\nGet:66 http://security.ubuntu.com precise-security/multiverse i386 Packages [3,374 B]\r\nFetched 7,621 kB in 7s (1,035 kB/s)\r\nReading package lists...\r\nHit http://downloads-distro.mongodb.org dist Release.gpg\r\nHit http://downloads-distro.mongodb.org dist Release\r\nHit http://downloads-distro.mongodb.org dist/10gen amd64 Packages\r\nHit http://downloads-distro.mongodb.org dist/10gen i386 Packages\r\nHit http://security.ubuntu.com precise-security Release.gpg\r\nHit http://us.archive.ubuntu.com precise Release.gpg\r\nHit http://us.archive.ubuntu.com precise-updates Release.gpg\r\nHit http://us.archive.ubuntu.com precise-backports Release.gpg\r\nHit http://security.ubuntu.com precise-security Release\r\nHit http://us.archive.ubuntu.com precise Release\r\nHit http://us.archive.ubuntu.com precise-updates Release\r\nHit http://us.archive.ubuntu.com precise-backports Release\r\nHit http://security.ubuntu.com precise-security/main Sources\r\nHit http://us.archive.ubuntu.com precise/main Sources\r\nHit http://us.archive.ubuntu.com precise/restricted Sources\r\nHit http://us.archive.ubuntu.com precise/universe Sources\r\nHit http://us.archive.ubuntu.com precise/multiverse Sources\r\nHit http://us.archive.ubuntu.com precise/main amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/restricted amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/universe amd64 Packages\r\nHit http://security.ubuntu.com precise-security/restricted Sources\r\nHit http://security.ubuntu.com precise-security/universe Sources\r\nHit http://security.ubuntu.com precise-security/multiverse Sources\r\nHit http://security.ubuntu.com precise-security/main amd64 Packages\r\nHit http://security.ubuntu.com precise-security/restricted amd64 Packages\r\nHit http://security.ubuntu.com precise-security/universe amd64 Packages\r\nHit http://security.ubuntu.com precise-security/multiverse amd64 Packages\r\nHit http://security.ubuntu.com precise-security/main i386 Packages\r\nHit http://security.ubuntu.com precise-security/restricted i386 Packages\r\nHit http://security.ubuntu.com precise-security/universe i386 Packages\r\nHit http://us.archive.ubuntu.com precise/multiverse amd64 Packages\r\nHit http://us.archive.ubuntu.com precise/main i386 Packages\r\nHit http://us.archive.ubuntu.com precise/restricted i386 Packages\r\nHit http://us.archive.ubuntu.com precise/universe i386 Packages\r\nHit http://security.ubuntu.com precise-security/multiverse i386 Packages\r\nHit http://us.archive.ubuntu.com precise/multiverse i386 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/main Sources\r\nHit http://us.archive.ubuntu.com precise-updates/restricted Sources\r\nHit http://us.archive.ubuntu.com precise-updates/universe Sources\r\nHit http://us.archive.ubuntu.com precise-updates/multiverse Sources\r\nHit http://us.archive.ubuntu.com precise-updates/main amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/restricted amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/universe amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/multiverse amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/main i386 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/restricted i386 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/universe i386 Packages\r\nHit http://us.archive.ubuntu.com precise-updates/multiverse i386 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/main Sources\r\nHit http://us.archive.ubuntu.com precise-backports/restricted Sources\r\nHit http://us.archive.ubuntu.com precise-backports/universe Sources\r\nHit http://us.archive.ubuntu.com precise-backports/multiverse Sources\r\nHit http://us.archive.ubuntu.com precise-backports/main amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/restricted amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/universe amd64 Packages\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://ppa.launchpad.net precise Release.gpg\r\nHit http://us.archive.ubuntu.com precise-backports/multiverse amd64 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/main i386 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/restricted i386 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/universe i386 Packages\r\nHit http://us.archive.ubuntu.com precise-backports/multiverse i386 Packages\r\nHit http://apt.postgresql.org precise-pgdg Release.gpg\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise Release\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://apt.postgresql.org precise-pgdg Release\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://ppa.launchpad.net precise/main amd64 Packages\r\nHit http://ppa.launchpad.net precise/main i386 Packages\r\nHit http://apt.postgresql.org precise-pgdg/main amd64 Packages\r\nHit http://apt.postgresql.org precise-pgdg/main i386 Packages\r\nReading package lists...\r\n\r\nThe command ""sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install oracle-java8-installer"" failed and exited with 100 during .\r\n\r\nYour build has been stopped.\r\n``` Remove custom Java installer for travis build >>> 1"
90,"```\r\n[error] /Users/cchantep/Projects/playframework/framework/src/play-java-forms/src/main/java/play/data/validation/Constraints.java:72: no suitable method found for collect(java.util.stream.Collector<java.lang.Object,capture#1 of ?,java.util.List<java.lang.Object>>)\r\n[error]     method java.util.stream.Stream.<R>collect(java.util.function.Supplier<R>,java.util.function.BiConsumer<R,? super capture#2 of ?>,java.util.function.BiConsumer<R,R>) is not applicable\r\n[error]       (cannot infer type-variable(s) R\r\n[error]         (actual and formal argument lists differ in length))\r\n[error]     method java.util.stream.Stream.<R,A>collect(java.util.stream.Collector<? super capture#2 of ?,A,R>) is not applicable\r\n[error]       (cannot infer type-variable(s) R,A,capture#3 of ?,T\r\n[error]         (argument mismatch; java.util.stream.Collector<capture#2 of ?,capture#4 of ?,java.util.List<capture#2 of ?>> cannot be converted to java.util.stream.Collector<? super capture#2 of ?,capture#4 of ?,java.util.List<capture#2 of ?>>))\r\n[error] constraints.stream().map(c -> c.getAnnotation()).collect\r\n``` OS X/Java build error >>> 1"
91,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #6707\r\n\r\n## Purpose\r\n\r\nFixes a documentation error that made people use the noop `setContentType` of `play.mvc.Http.Response`\r\n\r\n## Background Context\r\n\r\nCleaning up an issue that had no real priority but gave backlog.\r\n\r\n## References\r\n\r\nhttps://github.com/playframework/playframework/issues/6707 Fix documentation error (usage of noop setContentType) >>> 1"
92,"Actually in `IdleTimeoutSpec.scala` we have had a regex that also checked the SocketException message, however this message might change depending on the system you test. In master we already removed them.\r\n\r\nP.S.: Doc validation still fails but we might Backport https://github.com/playframework/playframework/commit/ce8c083bba163faac9a2dd7a782bd87b57e0baa7 so that it will be fixed or remove the ""bad doc"" references. [2.5.x] fixes the NettyIdleClientTimeoutSpec >>> 1"
93,Fixes ScalaResultsSpec to use the non-deprecated config settings. Remove deprecated config settings from tests >>> 1
94,"# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #6688\r\n\r\n## Purpose\r\n\r\nFixes #6688 adds a includeDocumentation variable to remove the java/scala-doc from the distribution\r\n\r\n## Background Context\r\n\r\nActually to really stop it from generating the documentation would mean to actually set `packageDoc` to new `File(""."")`, see https://github.com/playframework/playframework/pull/6723/files#diff-69b29e5ccdf04d73ef169d932b4e2dbcR20.\r\n\r\nAnother way would be creating a `AutoPlugin` to remove the `mappings` but even that can't remove the `packageDoc` call.\r\n\r\n/cc @muuki88  Fixes #6688 adds a includeDocumentation variable >>> 1"
95,"`Json.reads[Foo]` with all optional parameters will not actually validate that the input is an object. Instead for non-object inputs it will assume all the optional parameters are not provided.\n\nTest case:\n\n``` scala\nimport play.api.libs.json._\n\ncase class Thingy(id: Option[Long], name: Option[String], description: Option[String])\nimplicit val readsThingy = Json.reads[Thingy]\nval emptyFromObject = Json.fromJson(Json.obj())\nval emptyFromString = Json.fromJson(JsString(""foo""))\nval emptyFromBoolean = Json.fromJson(JsBoolean(false))\n```\n\nresults in:\n\n``` scala\nemptyFromObject: play.api.libs.json.JsResult[Thingy] = JsSuccess(Thingy(None,None,None),)\nemptyFromString: play.api.libs.json.JsResult[Thingy] = JsSuccess(Thingy(None,None,None),)\nemptyFromBoolean: play.api.libs.json.JsResult[Thingy] = JsSuccess(Thingy(None,None,None),)\n```\n\nThe last two statements should fail.\n\nNote that this also has the same result when using the functional combinators to construct the Reads.\n Json.reads[Foo] does not validate that the input is an object >>> 1"
96,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n\r\n [Doc] Thread switching scenario corrected >>> 1"
97,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n\r\n Change deprecated 'application.secret' from docs >>> 1"
98,Removes some references to `Play.maybeApplication` in the helper twirl pages. Remove some static play references in helper pages >>> 1
99,"## Purpose\r\n\r\nRemoves Play.privateMaybeApplication from the CSRF specs.  \r\n\r\nRefactors the tests a little bit to make using an implicit application easier by using withActionServer when components are used in routing, adds an inject method, and takes crypto out of the equation.\r\n\r\n [UFR] Remove privateMaybeApplication from CSRF tests >>> 1"
100,nan [2.5.x] uses the new trusty image for travis >>> 1
101,"## Purpose\r\n\r\nRemoves JNDI, relying on jndi.properties https://docs.oracle.com/javase/jndi/tutorial/beyond/env/source.html to set up the factory instance.  \r\n\r\nThis is still global across the JVM, but does not use `Play.privateMaybeApplication`.  \r\n\r\nSetting up an Context bound to an Application is problematic, because of all the tests that pass in a database to WithApplication in tests, etc.  Also, AFAIK there is only one JNDI per classloader -- if we need to set up namespaces, we may have to set up a ""java:/$playapp/DefaultDS"" naming scheme to disambiguate...\r\n [UFR] Remove JNDI singleton >>> 1"
102,"Fixes more javadoc with missing @params, apart from HttpExecution.java and CSRFTokenSigner.java (which are already handled in different PRs).\r\n\r\nTo reproduce -- add a broken javadoc tag like @deprecatedBlah and then run:\r\n\r\n```\r\nsbt apiDocs\r\n```\r\n\r\nYou should see much reduced number of warnings.\r\n\r\n Fix more missing javadoc >>> 1"
103,"The `Execution.internalContext` methods are static, so they all depend on Play.privateMaybeApplication.  They can be replaced with generic thread pool references that don't go through the same pathway. [UFR] Remove `Execution.internalContext` references >>> 1"
104,Simplifies the play tutorials page to point to example projects. [doc] Update tutorials page in 2.5.x >>> 1
105,"## Purpose\r\n\r\nThis PR breaks out the construction of a `DefaultMessagesApi` from configuration using a `javax.inject.Provider`.  It removes a hardcoded reference to HttpConfiguration and allows for other instances of `DefaultMessagesApi` to be passed back with just a Map backing and default arguments instead of the entire environment / configuration etc.\r\n\r\nThis also makes it much easier to supply a MessagesApi object to FakeRequest, etc.\r\n\r\nDocs added: unit testing a form by passing a `DefaultMessagesApi` in. [UFR] Define MessageApi configuration in a provider. >>> 1"
106,"## Purpose\r\n\r\nDeprecates `play.lib.Classpath`.  It's not used anywhere in the codebase, but is public, so this marks it for removal.  Will probably make this package private. [UFR] Deprecate play.libs.Classpath >>> 1"
107,"## Purpose\r\n\r\nDeprecates `HttpExecution.defaultContext`, as it uses Execution.internalContext under the hood, has no usage internally, and can be easily replaced. [UFR] Deprecate HttpExecution.defaultContext >>> 1"
108,"Add more detailed example to Timeout.java, add `@Inject()` to Timeout.scala to provide better examples. [UFR] Add @Inject() to Timeout example >>> 1"
109,"WIP DO NOT MERGE NEEDS DOCS AND FEEDBACK\r\n\r\nAdds an `Injectable` trait that takes an implicit `Application` to resolve the type.  \r\n\r\nThis is different than `Injecting` because it requires an implicit val in scope.  Given that `WithApplication` and `WithServer` declare `implicit val app: Application`, having two of them may be redundant. [WIP] Adds an Injectable trait to PlaySpecification >>> 0"
110,# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #6754\r\n\r\ncurrently the link used https:// however sbt uses github pages which results in a broken certificate. changes sbt download page to use http:// #6754 >>> 1
111,"Hey there,\r\n\r\nI updated links and made some minor fixes here and there. Update Tutorials.md >>> 1"
112,"## Purpose\r\n\r\nRemoves some global references to TemporaryFileCreator, adds a Java API, adds more options to cleaning up temporary files from the filesystem.\r\n\r\nI've also cleaned up the API so that TemporaryFile is a pure trait, and changed the implementation so that a [`FinalizableReferenceQueue`](https://google.github.io/guava/releases/19.0/api/docs/com/google/common/base/FinalizableReferenceQueue.html) and phantom references inside the TemporaryFileCreator are used instead of a `finalize` method inside the TemporaryFile instance.   The essential logic of ""remove on GC"" hasn't changed.\r\n\r\nI think fixing https://github.com/playframework/playframework/issues/5545 means doing something with TemporaryFile that meet the criteria on a scheduled basis, using an Akka actor and scheduler to delete wayward files.  Probably more robust than finalization solution anyway...\r\n\r\n## Background Context\r\n\r\nThere are a few cases that should be addressed here, as TemporaryFile is used internally for maxFileBuffer stuff and so may not be accessible to the client.\r\n\r\nCase 1: The application calls `tempFileCreator.delete(temporaryFile)`.  In this case, the removal is explicit.\r\n\r\nCase 2: There is a call to move the temporary file to another location, using `temporaryFile.moveTo`.  In this case, the file should no longer exist in the old place, but the tempFileCreator didn't remove it.\r\n\r\nCase 3: The TemporaryFile goes out of scope, and at some point GC kicks in and removes the temporary file.  This is possible because there are custom form handling cases using TemporaryFile from user code, and so it could be that it's created, read, but never explicitly deleted.  Here, there is an edge case https://github.com/playframework/playframework/issues/5545 where if the TemporaryFile goes out of scope but for whatever reason a GC does not kick in fast enough, then there can be a gross accumulation of file in the tempfilesystem space.\r\n\r\nNote that this is not the same thing as https://github.com/playframework/playframework/issues/4707 -- in the event of JVM crash / application start, all the temp files are cleaned up when the application comes back online -- but if the temp fs is full, then that poses its own problems.\r\n\r\nCase 3 / https://github.com/playframework/playframework/issues/5545 is addressed by the reaper, which is disabled by default, but which can delete files older than a given date and runs using the akka scheduler.\r\n\r\nAlso see [Potential delete of temporary file before it is used](https://github.com/playframework/playframework/issues/6786)\r\n\r\n [UFR] Refactor global state from TemporaryFileCreator >>> 1"
113,Fixes #6190 for `2.4.x` branch\n Fix displaying runtime exception coming from Twirl template while running in development mode - PR for branch 2.4.x >>> 1
114,the newest version supports Scala 2.12 properly and is capable of\ndetecting a variety of compatibility issues older versions missed upgrade to latest MiMa (0.1.12) >>> 1
115,"Expose the PlayRequestHandler class, and make it simple to override this\r\nin a NettyServer subclass.  This will make it possible to use\r\nlightweight wrappers around the request handler's `handle()` method. Simplify wrapping of PlayRequestHandler. >>> 1"
116,"Hi there,\r\nthis brings Play onto 2.4.14 - it's fully binary compatible otherwise - just extending the materializer is ""gray zone"" so this needed to change.  Update to Akka 2.4.14, materializer is not public API and changed a bit >>> 1"
117,"Actually we need to set `no-java-comments` for 2.12 unfortunatly that does not get applied for play-java since it overrides the scala version. I also made it future proof since 2.12.1 would not include the fix for it. publishLocal fails due to -no-java-comments not getting applied, since we override the scala version >>> 1"
118,"See title.\r\n\r\nThe previous return type was overly ambitious for no real gain.\r\nAll tests in `Play-Netty-Server` still pass. Better return type on method, to avoid cast when overriding handleRead() >>> 1"
119,"Removes play-datacommons, which was previously needed by both play and play-json to provide `ValidationError`. Instead I've decided to create a `JsonValidationError` to represent JSON errors.\r\n\r\nThis is a prerequisite to being able to split off the play-json library into a separate project.\r\n\r\n/cc @wsargent @jroper  [WIP] Remove play-datacommons and create separate JsonValidationError >>> 1"
120,"This PR breaks apart WS functionality in the following ways:\r\n\r\n* Moves the AHC implementation from play-ws to play-ahc-ws.\r\n* Remove play-java-ws and moves the Java classes to play-ws and play-ahc-ws.\r\n* Moves the OpenID classes in the Play-WS package out to a new module play-openid.\r\n\r\nThere is intentionally no code changes (including the ugly Netty dependency in playOpenIdDeps) so they can be tackled in subsequent PRs and the git log will be easier to follow i.e. rename and repackage first, then start tweaking. Break Play WS into modules >>> 1"
121,"Fixes https://github.com/playframework/playframework/issues/6752\r\n\r\nNow that Play WS SSL relies on SSLConfig, we can also point the documentation directly to that codebase. Point WS SSL documentation to Typesafe SSLConfig >>> 1"
122,"# Pull Request Checklist\n- [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [ ] Have you added copyright headers to new files?\n- [ ] Have you checked that both Scala and Java APIs are updated?\n- [ ] Have you updated the documentation for both Scala and Java sections?\n- [ ] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #6272 and #6592\n## Purpose\n\nThis is a backport of the fix for #6272 (Set form value even when form has errors) into the 2.3.x branch. I have also backported to 2.4.x as well in PR #6595 \n## Background Context\n\nAs recommended in #6592\n## References\n#6592 and #6595\n Backport of Issue #6272 to 2.3.x >>> 1"
123,## Fixes\r\n\r\nFixes #6773 \r\n\r\nNote to myself: don't code at night.\r\n\r\n\r\n changes the slice bytes transformer to graph stage #6773 >>> 1
124,Removes the 3.5.x Netty dependency on QueryStringDecoder from Play-OpenId project by moving to the Netty 4.x version.\r\n Remove Netty dependency from play-openid >>> 1
125,Deprecated some methods which should have been deprecated before the 2.5 release already.\nAlso clearified some comments and exceptions.\n\nI want to backport this to 2.5.x so people are aware of the deprecations (they can use alternative methods in 2.5 already anyway) so we can clean up for the 2.6.x release - I have some pull requests with enhancements for the JPA in the queue.\n Remove JPA methods >>> 0
126,"### Play Version (2.5.x / etc)\r\n\r\nPlay 2.5.x\r\n\r\n### API (Scala / Java / Neither / Both)\r\n\r\nScala\r\n\r\n### Operating System (Ubuntu 15.10 / MacOS 10.10 / Windows 10)\r\n\r\n- Any \r\n- Darwin leto.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Sep  1 15:01:16 PDT 2016; root:xnu-3248.60.11~2/RELEASE_X86_64 x86_64\r\n\r\n### JDK (Oracle 1.8.0_72, OpenJDK 1.8.x, Azul Zing)\r\n\r\n- Any\r\n- java version ""1.8.0_25"" \r\n\r\n### Expected Behavior\r\n\r\nAllow to enrich/rewrite an existing JSON writer, with a function given the initial input and the intermediary JSON output.\r\n\r\n```scala\r\nval transformed: OWrites[Foo] = OWrites.transform(writes) { (foo, obj) =>\r\n  obj ++ Json.obj(""hash"" -> foo.hashCode)\r\n}\r\nval foo = Foo(""Lorem"")\r\nval written: JsObject = transformed.writes(foo)\r\n``` New transformations for JSON writers >>> 1"
127,Upgrades master to Scala 2.12.1 Upgrade Scala 2.12.1 >>> 1
128,Fixes #6039 Use correct classloader when loading logger configurator class >>> 1
129,"In case of a body multipart failure, it'd be nice to have the source exception. Add source exception to multipart failure >>> 1"
130,"well sonatype is a maven style repo so the ivy style would be wrong, was probably wrong since the transition from typesafe to sonatype repo. corrected the sonatype repository >>> 1"
131,well the note is also invalid (follow up on https://github.com/playframework/playframework/commit/edf0e24bfc4f8eb2c67446b69c9459a1587bf232) removed the note about the sonatype repository >>> 1
132,# Fixes\r\n\r\n!(isDefined) and isEmpty are equivalent.\r\n\r\n# Purpose\r\n\r\nMake it simple.\r\n\r\n Refactoring play-server - !(isDefined) to isEmpty >>> 1
133,"Well this is a controversial one, however netty allows **every** content-type (even `Content-Type: wsargent/gmethvin`), not just arbitary ones.\r\nThis PR allows actually Setting non RFC like Content-Types.\r\n\r\nWell in Future Play Versions we should probably Parse everything **and** disallow such things, however as seen in the AssetsSpec some people actually need non compliance (a big one is probably adding a charset to json, which is invalid by the specification, but is actually used in many many frameworks/applications)\r\nAlso the parsing should've been done a level higher, so I guess even if we have typed headers we would need to convert them and I guess this is the most straightforwarded way since it tricks `akka-http`. akka-http header should not be parsed to be on par with netty >>> 1"
134,shows how to use a standalone ws client with the configuration from a HOCON file\r\nCLA signed Added sample code to java documentation for custom WS with config >>> 1
135,See [my comments](https://github.com/playframework/playframework/pull/6526/files#r91960286) Remove unnecessary parameter >>> 1
136,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nRemoves a typo:\r\n\r\n`Reads[Option[T]]readNullable` -> `Reads[Option[T]]` Typo in ScalaJsonCombinators.md >>> 1"
137,I think this should have been done in #6559 but was overlooked.\r\n\r\nI tried to run a very simple play java project with current `master` but I got this error when compiling the twirl templates:\r\n\r\n```\r\n[info] Compiling 6 Scala sources and 3 Java sources to play-java-seed/target/scala-2.12/classes...\r\n[error] play-java-seed/app/views/index.scala.html:1: object data is not a member of package play\r\n[error] @()\r\n[error] ^\r\n[error] play-java-seed/app/views/main.scala.html:1: object data is not a member of package play\r\n[error] @*\r\n[error] ^\r\n[error] two errors found\r\n[error] (compile:compileIncremental) Compilation failed\r\n```\r\n\r\nChanging the plugin settings fixed the problem - it should have been like this in first place already I think. Fix PlayJava after extracting Java forms >>> 1
138,"This change makes Play depend on Interplay 1.3.1, which uses the Scala 2.12.1 compiler explicitly.\r\n\r\nThis fixes the javadoc stackoverflow bug and ensures downstream Play projects don't run into odd issues.\r\n\r\nNotably, the sbt-twirl download error in https://github.com/playframework/playframework/issues/6782 was being caused by a misapplied scalaVersion looking for a 2.12 version of sbt-twirl, when all SBT projects should be on 2.10. Upgrade to interplay 1.3.1 >>> 1"
139,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #6792\r\n\r\n## Purpose\r\n\r\nAdds dependency on play.db.DBApi to play.db.jpa.DefaultJPAApi.JPAApiProvider to ensure that data sources are initialised and bound to JNDI before they are accessed.\r\n\r\n## Background Context\r\n\r\nThe implementations of JPAApi and DBApi do not have any dependencies that would ensure the order in which they are both created, so there is no guarantee that the data source is bound to JNDI by HikariCPConnectionPool before it is accessed by DefaultJPAApi. By adding a dependency on play.db.DBApi to play.db.jpa.DefaultJPAApi.JPAApiProvider data sources are always initialised and bound to JNDI before they are accessed.\r\n\r\n## References\r\n Added dependency on play.db.DBApi to play.db.jpa.DefaultJPAApi.JPAApiProvider to ensure that data sources are initialised and bound to JNDI before they are accessed >>> 1"
140,"Replaces #6013\r\n\r\nAs we know to run code within a JPA transaction the preferred way is to call\r\n```java\r\n// inject jpaApi\r\njpaApi.withTransaction(em -> ...)\r\n```\r\nWith this PR an entity manager now **always** gets passed to all the various `withTransaction` methods. The em doesn't get stored in a `ThreadLocal` anymore.\r\n\r\nOnly when using `@Transactional` the entity manager still will be stored in the `Http.Context` so we can access and (re-)use it later from within the controller's action methods. But I did change how this happens. Instead of just accessing `Http.Context.current()` you now have to explicitly pass the `context` in which the em should get stored to `withTransaction` (happens in `TransactionalAction.java`).\r\nEventually, to retrieve the em we now have to pass a context to `JPA.em(ctx)`.\r\n\r\nWith this changes `JPAEntityManagerContext` is becoming ""just"" a simple helper class which just makes it easier to store and read entity managers from a http context.\r\n\r\nIn future we can remove `JPA.em()` (it's deprecated now) in favor of `JPA.em(ctx)` - actually it's just a shortcut for `JPAEntityManagerContext.em(ctx)`.\r\n\r\nThis should make `play-jpa` ready for the removal of the http context threadlocal which I think is targeted for a future play version. (It should work nicely with the changes in #6473 done by @gmethvin)\r\n\r\nOne more thing:\r\nIn `@TransactionalAction` we tell `withTransaction` to keep the em and transaction open so it doesn't get closed immediately but only after the future has completed. I am not entirely sure if this should be managed here or if I should add a `withTransaction` that returns a `CompletionStage<T>` and manage it there. Related to #6489 Refactoring play-jpa - Got rid of ThreadLocal >>> 0"
141,Right now we create a `ValidatorFactory` **each time** we need a validator - which is quite often (for every constraint annotation for each request).\r\nBut we just need a single `ValidatorFactory` per app which delivers us the desired validator(s).\r\n\r\nAlso using `usingContext` isn't ideal here. What we do right now we create a `ValidationFactory` via `buildDefaultValidatorFactory` (which actually is just a shortcut for `.byDefaultProvider().configure().buildValidatorFactory()`) and then re-configure this just created factory via `usingContext()`. With my change we now immediately create the factory we want. \r\n\r\nThis change will save some CPU cycles and heap space :laughing:  Create ValidatorFactory just once >>> 1
142,"# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #6801\r\n\r\n## Background Context\r\n\r\nwell the only sane way to have a backportable way (followup on #6802)\r\n\r\n## References\r\n\r\nAlso we actually flipped `new QueryStringDecoder` and `new URI` since the netty `QueryStringDecoder` now works differently (https://github.com/netty/netty/commit/b548579a3d72da91df7be7fdc73263c28196517d). With this it restore's the behavior of 2.3.x, 2.4.x and early 2.5.x\r\nActually I created a forward ticket for 2.6 to create a valid URI parser, see:\r\nhttps://github.com/playframework/playframework/issues/6802\r\nsince our master branch uses a `java.net.URI` field which I needed to make lazy to make my tests work.\r\n\r\nNeeds backport (sadly not cherry-pickable due to diverging files)\r\n\r\n parses the path and the query string from our uri (#6801) >>> 1"
143,## Purpose\r\n\r\nJavaConversions was deprecated in Scala 2.12. This PR replaces it with JavaConverters. Use JavaConverters instead of deprecated JavaConversions >>> 1
144,Fixes the case where sbt plugin projects were not found because they were looking for 2.12 versions.\r\n\r\n```\r\n[warn]     :: com.typesafe.sbt#sbt-twirl;1.3.0: not found\r\n[warn]     :: com.typesafe.sbt#sbt-native-packager;1.1.1: not found\r\n[warn]     :: com.typesafe.sbt#sbt-web;1.3.0: not found\r\n[warn]     :: com.typesafe.sbt#sbt-js-engine;1.1.3: not found\r\n```\r\n\r\nInterplay 1.3.2 contains https://github.com/playframework/interplay/pull/22\r\n Update to interplay 1.3.2 >>> 0
145,"## Fixes\r\n\r\nFixes #6694 and invalidates #6753.\r\n\r\n## Purpose\r\n\r\nRemove the global state access made at Cookies, Flash, Sessions and related classes.\r\n\r\n## References\r\n\r\n#6753  PR was made agains branch 2.3.x, so maybe we can backport part of this PR to branch 2.5. Basically, these two new configurations:\r\n\r\n1. play.http.session.path: defaults to play.http.context\r\n2. play.http.flash.path: defaults to play.http.context Remove global state for Cookies, Sessions and Flash >>> 1"
146,All future PRs to play-json should be made at https://github.com/playframework/play-json. Remove play-json and move to https://github.com/playframework/play-json >>> 1
147,Fixes #6811  Add documentation for Scala Configuration and ConfigLoader >>> 1
148,This eliminates the global state in `EssentialAction` by running the `Result::asScala` in the same thread. Use trampoline ExecutionContext in Java EssentialAction >>> 1
149,Also add a big scary warning. Deprecate StaticRoutesGenerator >>> 1
150,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nThis filter redirects http to https based on the `X-Forwarded-Proto` header and `request.secure`\r\n\r\n## Background Context\r\n\r\nI found myself writing this filter over and over so I thought it might be nice to have it in a core module.\r\n\r\nI am willing to write some documentation for the changes I made but thought I'd wait for some feedback first.\r\n Redirect plain http to https filter >>> 0"
151,nan use akka-stream Compression in gzip filters and gzip flow >>> 1
152,"## Purpose\r\n\r\nRemoves `play.Configuration` in favor of using the `Config` class, from Typesafe Config lib.\r\n\r\n Reduce usage of deprecated play.Configuration class >>> 1"
153,Fixes #6817 Handle null ConfigOrigins in Configuration.configError >>> 1
154,## Status\r\n\r\nReady to review and merge.\r\n\r\n* [X] Have you squashed your commits?\r\n* [X] Have you updated the documentation for both Scala and Java sections?\r\n* [X] Review tests related to parsing secret configuration\r\n* [X] Update migration guide to document new/deprecated configurations\r\n\r\n## Purpose\r\n\r\nThis removes last `Crypto` related classes in favor of `HttpConfiguration.secret`. The main point is to remove `Crypto.cookieSigner` usages because of global state. Remove Crypto.cookieSigner from Flash and Session >>> 1
155,`Evolutions.applyFor` can also be done directly by the `EvolutionsApi`. Deprecate global state in evolutions >>> 1
156,## Fixes\r\n\r\nFixes #6782.\r\n\r\n## Purpose\r\n\r\n1. Uses JavaConverters' methods compatible with both Scala 2.11 and 2.12.\r\n2. Update interplay to ensure that proper `crossScalaVersions` is used in sbt plugins\r\n\r\n## References\r\n\r\nSee playframework/interplay#22 and playframework/interplay#23. Fix cross build compilation and publishing >>> 1
157,"This allows us to set the application mode globally when the app starts, so we can easily get a logger that only logs in a particular mode.\r\n\r\nI have some concerns about dealing with the edge case of multiple apps running in the same JVM with different modes. I can't think of a good reason that should happen so I just warn there. Provide global mechanism to specify application mode for logging >>> 1"
158,We can remove all of these since akka 2.4.16 comes with Source.lazily which makes the same thing tha MaterializeOnDemandPublisher.\r\nLuckily all classes are `private[play]`. removes MaterializeOnDemandPublisher and all his dependent classes >>> 1
159,## Purpose\r\n\r\nDocuments how to test file uploads in Java. Add docs about how to test file uploads >>> 1
160,"Hibernate validator *itself* will by default interpolate the error messages you define in a constraint (e.g. `@Required(message=""I will be interpolated"")`). See the [docs](https://docs.jboss.org/hibernate/validator/5.2/reference/en-US/html_single/#validator-gettingstarted-uel):\r\n> Hibernate Validator requires an implementation of the Unified Expression Language (JSR 341) for evaluating dynamic expressions **in constraint violation messages**\r\n\r\nSo you *could* [use expressions like](https://docs.jboss.org/hibernate/validator/5.2/reference/en-US/html_single/#_examples) `${validatedValue}` and `${formatter.format('%1$.2f', validatedValue)}` or [reference the attribute values of the constraint](https://docs.jboss.org/hibernate/validator/5.2/reference/en-US/html_single/#section-interpolation-with-message-expressions).\r\n\r\nSo for this message interpolation we did include the `javax.el` dependency: Otherwise we would get `ClassNotFoundExceptions` as soon a Play app (containing constraints) starts up - even when we never ever use these interpolation feature(s). **And that's the point:**\r\nAs it turns out Play cooks it's own soup on how to handle constraint error messages. Play's concept completely bypasses the way Hibernate validator retrieves error messages. Play uses the constraint message only to ""transport"" message keys of it's `conf/messages` file. What happens is that after a form got validated, Play builds it's own `ValidationError` objects containing the message key (and it's arguments) which it [retrieved (interpolated already) from Hibernate validator](https://github.com/playframework/playframework/blob/2.5.10/framework/src/play-java/src/main/java/play/data/Form.java#L320) and uses these objects for further processing ([rendering the error messages (by looking up in the `conf/messages`)](https://www.playframework.com/documentation/2.5.x/JavaForms#handling-binding-failure), etc.).\r\n\r\nGiving this, I don't see the need for interpolation these constraint messages (keys) before passing it to Play. [By providing](https://docs.jboss.org/hibernate/validator/5.2/reference/en-US/html_single/#section-validator-factory-message-interpolator) Hibernate Validator our own simple custom `MessageInterpolater` you see in this PR, we can just pass the message keys through. (May also improve performance).\r\n\r\nSure, someone could just write plain error messages into constraint's `message` attributes (instead of message keys) and *may* want to interpolate them the Hibernate validator way, but I think this is not what Play enforces you to do (giving all the documention is based on the message key concept - plus think of the `ValidationError` object concept). Also bad for i18n. Un-interpolated error messages allows removal of javax.el >>> 1"
161,actually this marks akka-http backend as non experimental\r\nand changes the gzip decoder from akka-http to akka-stream so that we can\r\nonly depend on the akka-http-core model\r\nAkkaHttpServer and ModelConversions were moved to play.core.server to be on par with the netty backend. Moves/Renames akka-http related files to be on par with netty and changes gzip decoder >>> 1
162,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [n/a] Have you added copyright headers to new files? No new files\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [n/a] Have you added tests for any changed functionality? AFAIK, sbt scripted test plugin doesn't allow to have test the reading key stroke from the console, i guess that's the reason why there is no existing tests for this part of the plugin.\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #2671\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\nThis PR allows the use if the key Enter to stop the play run command.\r\n\r\n## Background Context\r\n\r\nUsing Ctrl-D for stopping the run command is risky since Ctrl-D is also used for terminating SBT. I regularly terminate SBT when I only mean to stop the run command.\r\n\r\nWe should allow Enter to be used to stop the run command. SBT already uses Enter to terminate other interactive commands, such as ~compile, ~test, etc. To make the transition easy for users we can still support Ctrl-D in addition to Enter, but I suggest we only advertise Enter to new users.\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\nhttps://github.com/playframework/playframework/issues/2671\r\n Use Enter key to stop play run command (#2671) >>> 1"
163,Backport of #6801 [2.5.x] parses the path and the query string from our uri (#6801) (#6803) >>> 1
164,nan Fix mima issues in 2.5.x >>> 1
165,"For users developing their own `Formatter`s wanting to re-use the logic in the `parsing` method, please make this method non-private.\r\n\r\n# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Make Formats.parsing method non-private >>> 1"
166,## Purpose\r\n\r\nFixes a small problem at the documentation server. The `/api` were not being delivery inline because the `fileMimeTypes` was matching against the filename instead of file extension. Delivery /api inline at documentation server >>> 1
167,"## Fixes\r\n\r\nFixes #6849\r\n\r\n## Purpose\r\n\r\nFixes #6849 and add more tests to ensure that Strict, Streamed and Chunked results are covered. Preserve headers, cookies, flash and session when gzipping >>> 1"
168,## Purpose\r\n\r\nThis PR updates the documentation. It adds reference to [play-redis](https://github.com/KarelCemus/play-redis) module implementing a redis based cache. It implements common `CacheAPI` and some additional redis commands including the support of collections. The API is implemented in both blocking and asynchronous manner.\r\n Reference to play-redis module in Module directory in the documentation >>> 1
169,Favoring `sbt new` over activator where possible.\r\n\r\nRelated to #6713. [doc]: update docs with more information about sbt new >>> 1
170,"I don't believe this helper is used that often, since typically we recommend a third-party library for handling authentication/authorization, so I've just deprecated the code that reads from configuration. Remove global state from play.api.mvc.Security >>> 1"
171,Fixes #6828\r\n use Json.mapper().getFactory() instead of creating a new JsonFactory >>> 1
172,## Purpose\r\n\r\nIt stills keep some global state to keep some compatibility.\r\n\r\nThe remaining global state usage was marked as deprecated and should be removed later. Removes global state from CSRF components >>> 1
173,nan Add references to components in filters documentation >>> 1
174,## Purpose\r\n\r\nMain upgrade are:\r\n\r\n1. Akka-Http to version 10.0.1\r\n2. Guava to version 20.0\r\n3. Hibernate EntityManager to version 5.2.6.Final\r\n4. Spring to version 4.3.5.RELEASE\r\n5. Guice to version 4.1.0\r\n6. Netty to version 4.0.42\r\n\r\nAlso remove some minor deprecated calls. Upgrade dependencies >>> 1
175,1. How to update play and sbt\r\n2. Changes to cache apis\r\n3. Better order of migration steps Add more details to migration guide >>> 1
176, - Add `globalApplicationEnabled` lazy val to Application to make it easier to determine or set if the global app is enabled.\r\n - Change `Play.start` to only call `Play.stop` on the current app if we need to replace the existing global application instance.\r\n - Add `GuiceApplicationBuilder#globalApp(Boolean)` to make it easy to enable or disable global state in a test.\r\n - Change a test as a proof of concept. Improve global application configuration and handling >>> 1
177,Removes Play-WS code and adds a wrapper with Play specific functionality on top of a standalone WS client. Make Play-WS depend on playframework/play-ws >>> 1
178,Currently Akka does not provide a way to set a supervision strategy via typesafe-config.\r\nSo to get around that I provided a PlaySupervisionProvider where you can set a decider\r\nthat can actually be used to set a superivison strategy\r\n\r\n- [x] Java Stuff\r\n- [x] Tests\r\n- [x] Documentation\r\n adds a way to configure a supervision strategy in play #6872 >>> 0
179,So it matches the `sbt-eclipse` default `managedClassDirectories` setting:\r\nhttps://github.com/typesafehub/sbteclipse/blob/v5.1.0/src/main/scala/com/typesafe/sbteclipse/core/EclipsePlugin.scala#L48\r\n@benmccann [agrees with me on this change](https://github.com/typesafehub/sbteclipse/pull/327#issuecomment-268654889) (compile in Test) should also be a default in sbt-eclipse preTasks >>> 1
180,nan Remove global state from Multipart helpers >>> 1
181,"## Purpose\r\n\r\nThe main change here is to solve a small bug at `DefaultSyncCacheApi.getOrElseUpdate` which was ignoring the `expiration` parameter.\r\n\r\nTests were added to the Java version of Cache APIs and other small improvements were made.\r\n\r\n## References\r\n\r\nAfter merging, #4972 could be closed.\r\n Honors the expiration time in DefaultSyncCacheApi >>> 1"
182,## Purpose\r\n\r\nRemove global state from `RoutingDsl`. This deprecates both RoutingDls's default constructor and `build` methods. Remove global state from RoutingDsl >>> 1
183,This actually fixes a bug introduced on the master branch. We need to append the provided list of cookies to the existing list. Allow multiple withCookies calls in Java API >>> 1
184,"Fixes #6888 by reading the raw max-age value from the cookie and setting that directly instead of using `Math.max(maxAge, 0)`. It's valid to have negative `Max-Age` values, and this is needed to properly set `Expires` since `maxAge` is used to set both.\r\n\r\nNeeds backport to 2.5.x. Fix cookie max-age computation >>> 1"
185,Bumps fluentlenium version to 3.1.1 and adjusts classes accordingly. This seems fit since scalatestplus-play already uses Selenium 3 while the current fluentlenium version 0.10.9 still uses Selenium 2\r\n Update Fluentlenium to latest version >>> 1
186,## Fixes\r\n\r\nFixes #6797 and #6870.\r\n\r\n## References\r\n\r\n```bash\r\ncd documentation\r\nsbt validateExternalLinks\r\n``` Docs: validate and update external links >>> 1
187,"## Purpose\r\n\r\nAdds format method to `FormError`.\r\n## Background Context\r\n\r\nIf you're using a form error in Twirl, then the way to do it is:\r\n\r\n```\r\n@Messages(error.messages, error.args)\r\n```\r\n\r\nhttps://www.playframework.com/documentation/2.5.x/ScalaForms#Displaying-errors-in-a-view-template\r\n\r\nSince you have to pass in an implicit Messages anyway (again, from the docs above):\r\n\r\n```\r\n@(userForm: Form[UserData])(implicit messages: Messages)\r\n```\r\n\r\nand the Messages singleton object takes that same implicit messages, there's a whole bunch of typing that could just as well be:\r\n\r\n```\r\n@error.format\r\n```\r\n\r\nSo... that's what this PR does.\r\n Add format methods to FormError >>> 1"
188,@domdorn founded a user group in Austria.\r\nI put it beneath Berlin and Cologne because Vienna is german speaking as well :wink:  Add Play User Group Austria >>> 1
189,Backporting #6890 to 2.5.x branch. [2.5.x]: Fix cookie max-age computation >>> 1
190,Just a simple addition to #5565 so that the `withRequestLocale` lambda also spans across the `validator.validate(...)` method - just to make sure spring's `LocaleContextHolder.getLocale()` is set inside the validate logic if needed by spring (or by user code). Actually I thought I did this in the original PR already... Make (spring) locale of current request available in validate() >>> 1
191,Users should now use play.cache.SyncCacheApi or play.cache.AsynCacheApi.\r\n\r\n## References\r\n\r\nhttps://github.com/playframework/playframework/pull/6865#pullrequestreview-17119846\r\n Deprecate play.cache.CacheApi >>> 1
192,"Hi,\r\nthe play-guard module already has several users, so I thought it might be worth to be included in the module directory. \r\nThanks\r\nSimon\r\n added play-guard (Scala) to module directory >>> 1"
193,Try out https://www.sourceclear.com/ Add sourceclear scanning >>> 1
194,Fixes https://github.com/playframework/playframework/issues/6893 Prevent reaper from being scheduled in tests >>> 1
195,## Fixes\r\n\r\nFixes #6907 at the master branch.\r\n\r\n## References\r\n\r\nSee #6908 for 2.5.x branch.\r\n Remove unnecessary println call >>> 1
196,nan Clean up Assets global state >>> 1
197,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality? Update Deploying.md >>> 1"
198,\r\n## References\r\nOriginal PR was https://github.com/playframework/playframework/pull/5405\r\n\r\n remove notice about not supported logging methods >>> 1
199,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nCould not tag a docker image as the latest using `docker:publish`/`docker:publishLocal` due to old `sbt-native-packager` version.\r\n\r\n## Purpose\r\n\r\nUpdates `sbt-native-packager` to the latest stable version. \r\n\r\n## Background Context\r\n\r\nI tried to use `dockerUpdateLatest in Docker := true`, but it was broken in the `sbt-native-packager` used in Play 2.5.12\r\n\r\n## References\r\n\r\nhttps://github.com/sbt/sbt-native-packager/releases/tag/v1.1.5\r\nhttps://github.com/sbt/sbt-native-packager/releases/tag/v1.1.3\r\nsbt/sbt-native-packager#845\r\nsbt/sbt-native-packager#818\r\nsbt/sbt-native-packager#838\r\n\r\n## Other\r\n\r\nI want this in the next 2.5.x release too, is that possible? How would I go about doing that (open a PR against the `2.5.x` branch?)? Upgrade sbt-native-packager to latest stable version >>> 1"
200,nan Upgrade Akka HTTP to 10.0.3 >>> 1
201,"## Purpose\r\n\r\nMoves the note about unreliable networks in WS documentation down to patterns and adds `CustomExecutionContext` as a class.\r\n\r\n## Background Context\r\n\r\nThe note is situated too high, where it can be skipped over as an introduction, rather than as a general pattern.\r\n\r\nThis needs an example and also needs a Java section that is equivalent. [doc] Move the unreliable network section >>> 1"
202,nan Update copyright year to 2017 >>> 1
203,## Purpose\n\nRemoves joda-time and joda-convert and uses Java8 API's instead.\n## References\n\nhttps://groups.google.com/forum/#!topic/play-framework/FEMIT-40tks\n Removes joda-time and joda-convert >>> 1
204,## Purpose\r\n\r\nRefactoring docs code to avoid some usages of deprecated APIs. Also add some missing tests.\r\n\r\n## Background Context\r\n\r\nWe have deprecated a lot of APIs but they are still being used by some documentation code. I don't want to remove them all at once to avoid big pull requests (which are harder to review). Some of the changes are just mechanical and some could add new methods to production code (like https://github.com/playframework/playframework/pull/6916/commits/5f36597c5abe761e9159da854a7ba298fca320b3 at this PR). [doc]: remove some deprecated API usages >>> 1
205,## Purpose\r\n\r\nRemoves sourceclear from the build because it errors and doesn't work with sbt in any case :-/\r\n Remove sourceclear from Travis >>> 1
206,"## Purpose\r\n\r\nPer [Mozilla](https://hacks.mozilla.org/2017/01/using-immutable-caching-to-speed-up-the-web/) and [Patrick McManus](https://bitsup.blogspot.com/2016/05/cache-control-immutable.html) we can add the ""immutable"" property to versioned, fingerprinted assets in Prod mode to make sure that Firefox and Chrome speed up reloads of assets.\r\n\r\n## References\r\n\r\n[Chromium Issue](https://bugs.chromium.org/p/chromium/issues/detail?id=611416)\r\n[Mozilla Issue](https://bugzilla.mozilla.org/show_bug.cgi?id=1267474) Add immutable header to aggressiveCacheControl >>> 1"
207,Fixes https://github.com/playframework/playframework/issues/6625 [doc] Move integration tests out of unit testing section >>> 1
208,"Extend the websocket timeout to 10 seconds, re-enable the ping tests now we have Netty upgraded. Extend timeout and enable ping in websockets >>> 1"
209,"The websocket integration test is flaky, and will sometimes fail.  \n\nAdd some code to retry only websocket specs up to 5 times, with 100 msec delay. Retry the websocket spec >>> 1"
210,Correct typo Update SerializableResultSpec.scala >>> 0
211,Removes global state from PlayMagicForJava. Fix template magic for java >>> 1
212,Backports #6927 [2.5.x] Add immutable header to aggressiveCacheControl >>> 1
213,## Purpose\r\n\r\nAdds Scalate as a library module to the module directory.  This gives HAML and Velocity type support as an alternative to Twirl. Add Scalate as a library for Play support >>> 1
214,## Purpose\r\n\r\nRemoves console images that are still making reference to activator.\r\n\r\n## References\r\n\r\n#6856. Remove images that related to activator >>> 1
215,Backport of #6946 [Backport 2.5.x] Remove images that related to activator >>> 1
216,## Purpose\r\n\r\nRemove wrong app directory from console docs\r\n\r\n## Background Context\r\n\r\nThis was wrongly introduced at #6946.\r\n\r\n## References\r\n\r\nMust be merged before #6948 and cherry picked there. Remove wrong app directory from console docs >>> 1
217,1. Docs consistent with /download page\r\n2. Removes activator references in favor of using plain sbt\r\n\r\n## References\r\n\r\n1. Related to #6713 \r\n2. Backports #6856 \r\n [2.5.x]: Update docs with more information about sbt new (#6856) >>> 1
218,## Purpose\r\n\r\nRemoves `play.api.libs.concurrent.Execution` usage from documentation code.\r\n\r\n## References\r\n\r\nSee #6916 for more background context. [doc]: Remove play.api.libs.concurrent.Execution from documentation code >>> 1
219,"This includes new features like the `include required(""foo"")` syntax. Should be compatible with 1.3.0. [2.5.x] Update Typesafe Config to 1.3.1 >>> 1"
220,"Instead of writing our `JsValue`s to a string and then to bytes when writing them out, let's just write directly to a byte array.\n\nOne small API change here is that the implicit `Writeable` for `JsValue` always uses UTF-8. I decided to make the implicit `Writeable` not take a `Codec` and always write to UTF-8 and make another non-implicit one that allows setting a codec. The reasoning is that almost always you want to write as UTF-8, and since JSON is always Unicode, you would produce an invalid response if you happened to have the wrong codec in scope.\n Update Writeable to write JsValue to bytes with play-json 2.6.0-M2 >>> 1"
221,"* Updates Play-WS dependency to M3 (which does not contain unshaded deps)\r\n* Add option to create WSClient with HTTP caching (disabled by default) Update to Play-WS m3, add caching option >>> 1"
222,This will enable users to avoid deprecated `Action` and use `ActionBuilder` while keeping the same idiom. Add trait to keep Action idiom while creating routers >>> 0
223,See https://gist.github.com/wsargent/d72985c3ce8dfca718b3c23a579620a7 Fixes bad scala/javadoc >>> 1
224,"I'm marking this as experimental because this is to be discussed before evolving or even discarding later.\r\n\r\nBasically, while removing deprecated code from our docs, most of them were related to remove usage of the Action object. It is being used by most of the controllers and the actual solution is to migrate away from play.api.mvc.Controller and use play.api.mvc.AbstractController, which is more verbose. That means that user code, in order to avoid deprecation warnings, have to be rewritten from:\r\n\r\n    class MyController extends Controller\r\n\r\nto:\r\n\r\n    class MyController(components: ControllerComponents) extends AbstractController(components)\r\n\r\nThis PR uses Guice field injection to maintain the former syntax while partially avoiding global state. The drawbacks here are:\r\n\r\n1. Mixing constructor and field injection, which could be unclear\r\n2. Test code that manually instantiate controllers will need to call setControllerComponents to avoid the warning message.\r\n3. Compile Time DI code needs to call setControllerComponents. Experimental/WIP: field injection of components in controllers >>> 0"
225,Forward port of #6944 [master] Add Scalate as a library for Play support (#6944) >>> 1
226,## Fixes\r\n\r\nFixes #2178.\r\n\r\n## Purpose\r\n\r\nAdd a Java API for response headers\r\n\r\n## References\r\n\r\nSee #2178 and related issues.\r\n Add a Java API for response headers >>> 1
227,Changes the naming of `PlayBodyParsersImpl` to `DefaultPlayBodyParsers`. Also removes the call by name parameters and creates a special case for `BodyParsers.parse` Refactor PlayBodyParsers >>> 1
228,The documentation code was using \r\nreturn Optional.of(null); instead of Optional.empty();\r\nof(null) would throw a NullPointerException\r\n [2.5.x]: Fix documentation code NPE >>> 1
229,"The data and error fields of Java Forms are currently mutable.\r\nThis commit changes them to immutable maps (using the Guava data types).\r\n\r\nthis addresses issue #6232\r\n\r\nThe changes are not backwards-compatible.\r\nThere were two votes in issue #6232 for breaking  backwards-compatible.\r\nKnown backwards-compatibility issues are:\r\n\r\n- form.data() and form.errors() now return immutable maps, so code trying to modify them will get an exception (this change also required some changes in the unit tests)\r\n- the mutator-methods `reject` and `discardErrors` have been deprecated (I added new  `withError` methods similar to what Scala forms offer)\r\n- `Form` constructors will create immutable copies of passed arguments, so mutating arguments later no longer has an effect on the Forms\r\n- subclasses of `Form` and `DynamicForm` might break\r\n\r\nPerformance:\r\n\r\nI tried to use the immutable data types also internally to avoid creating too many new collections.\r\nHowever, performance is probably a bit worse with these changes.\r\nIn particular `reject` is no longer O(1). \r\n\r\n\r\n(this is my first pull request here, so I would be thankful for detailed feedback) Make Java Forms immutable >>> 0"
230,The documentation code was using \r\nreturn Optional.of(null); instead of Optional.empty();\r\nof(null) would throw a NullPointerException\r\n\r\n//cc @marcospereira #6966\r\n Fix documentation code NPE >>> 1
231,Fixes #6967 [WIP] Move from play.libs.Json to injectable ObjectMapper >>> 0
232,Changes the default implementation of Play's engine from Netty to Akka HTTP. Use Akka HTTP by default >>> 1
233,## Purpose\r\n\r\nUpdates Netty and remove deprecated API calls.\r\n\r\n## References\r\n\r\nhttp://netty.io/news/2017/01/30/4-0-44-Final-4-1-8-Final.html Update netty to version 4.1.8.final >>> 1
234,Add `synchronized` block to `Play.stop` and `Play.start` to make sure the current app is accessed correctly across threads. Make Play.start and Play.stop synchronized >>> 1
235,Updates Play to use Play-WS 1.0.0-M4 and updates the Java provider to use the injected ObjectMapper. Update to play-ws M4 >>> 1
236,nan Update dependencies to prepare for M1 release >>> 1
237,"The microbenchmarks have been added in a new Play subproject called Play-Microbenchmark. The microbenchmarks are executed using JMH via the sbt-jmh plugin. They can be run with the `jmh:run` command. I've added a few simple benchmarks to show how they are implemented.\n\nCc @gmethvin, @wsargent, @ktoso.\n Add some microbenchmarks >>> 1"
238,nan Fix some typos and small improvements to migration guide >>> 1
239,nan Fix small file headers inconsistencies >>> 1
240,nan Small correction for routing dsl deprecation warning >>> 1
241,## Purpose\r\n\r\nReferences logback documentation about `<file>` tag when using log rolling.\r\n\r\n## References\r\n\r\nSee #6364 for discussion and more information. /cc @ToBeHH.\r\n Add note and link about file tag for log configuration >>> 1
242,## Fixes\r\n\r\nFixes #6980.\r\n\r\n## References\r\n\r\nhttps://github.com/playframework/play-iteratees/blob/master/README.md\r\n Migration details about play-iteratees being a standalone project >>> 1
243,"Additionally, fix case of `openId` import in the OpenID documentation, and add a Java sbt import snippet. Add note about modularized OpenID support to 2.6 migration guide. >>> 1"
244,"The reference.conf refers to `request-timeout` but the code refers to `requestTimeout`\r\n\r\nThe idleTimeout settings are left undefined and not in AkkaHttp.\r\n\r\nSee https://groups.google.com/d/msg/play-framework/imf51EsYU08/tDk0sEfPCgAJ for user experience\r\n\r\nDocumentation at https://github.com/playframework/playframework/blob/master/documentation/manual/working/commonGuide/configuration/SettingsAkkaHttp.md could also use some love\r\n\r\n@ktoso and @jrudolph are looking at akka-http already, so not going to go into cleaning up the `.getOptional` calls... Ensure akka-http has valid reference.conf >>> 1"
245,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Update ModuleDirectory.md >>> 1"
246,nan Case Typo >>> 1
247,There are some tests that fail repeatedly on CI server even when they work locally. Add a skip on slow CI server flag on specs >>> 1
248,Add twirl injectable templates to the 2.6 highlights\r\n\r\nAlso see https://github.com/playframework/twirl/pull/135 Add injectable templates to highlights >>> 1
249,This also makes the API consistent with other APIs that define a default of `Mode.Test`. The default Assets controller is likely to be used in testing environments so this makes sense. Use test mode by default in StaticAssetsMetadata >>> 1
250,"This post describes many ""gotachas"" associated with upgrading from play 2.3 to play 2.5 and some of the things that were learned while upgrading.\r\n\r\nI've been working with @wsargent to get this blog post published. He initially suggested that it might be good to link to it on this page.\r\n\r\n~~_I haven't signed the CLA yet, I'm working on that now._~~ Add link to upgrading blog post >>> 1"
251,Fixes https://github.com/playframework/playframework/issues/6995 Add stub controller components >>> 1
252,nan play-ahc-ws should not depends on play-test >>> 1
253,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?  (N/A)\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?  (N/A)\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #4757 \r\n\r\n## Purpose\r\n\r\nWe have added clearAll function in cacheApi to remove all cached item at once. \r\n\r\n\r\n## References\r\n\r\n#5912 \r\n#4757  Added clear all function to cacheApi to remove all cached items. >>> 1"
254,This just migrates from using tags to attributes for storing the CSRF token info on the request.\r\n\r\nAll the existing tests still pass. I still have to look at what's happening with #6977. Migrate CSRF to request attributes >>> 1
255,nan JSON formats for Play Lang >>> 0
256,## Purpose\r\n\r\nAdds a link https://www.lucidchart.com/techblog/2017/02/22/upgrading-play-framework-2-3-play-2-5/ to Migration25.\r\n\r\n## Background Context\r\n\r\nThis gives more context for people going through the upgrade and makes it more visible than it would be on the tutorials page.\r\n\r\n**Needs backport to 2.5.x branch** Add Lucidchart Migration Blog Guide to Migration25 >>> 1
257,"## Purpose\r\n\r\nAdds blake2 as the default MAC algorithm for cookie signing, adds a ""mac"" configuration property.\r\n\r\n## Background Context\r\n\r\nWith a SHA-1 collision already found on http://security.googleblog.com/2017/02/announcing-first-sha1-collision.html it's clear it's time to migrate everything off SHA-1 based algorithms.  SHA1 HMAC is not implicated, but the trend is inexorable http://valerieaurora.org/hash.html and with 2.6.x coming out, it's a good time to change the default.  Blake2b is faster than SHA-1 https://blake2.net/ and is perfectly suited for MAC.\r\n\r\nThis should not affect most projects, but it is possible that projects which poke into the session handling cookie may break.  \r\n\r\nThere may be a problem with rolling upgrades, because session cookies will not validate as users roll between servers.  Maybe it'd be better to have fallback behavior, but that would be something you'd only want during the rollover.\r\n\r\nThe old behavior is still available if projects need it, by setting `play.http.secret.mac=HmacSHA1` in application.conf.\r\n\r\nWIP: Needs notes in Migration26.md, possibly add this to Highlights26.md [WIP] Add blake2 as default cookie signer >>> 0"
258,Backport of #7008 [2.5.x] Add note to Migration25 about Lucidchart blog (#7008) >>> 1
259,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n## Purpose\r\n\r\nThis adds type ascription to `path` and `withMultipartFormDateBody` method  in `play.api.test.FakeRequest` and `play.api.test.RequestTarget`. \r\nAnd remove useless method.\r\n Add type ascription in play.api.test.Fakes.scala, and remove useless method >>> 1"
260,"## Fixes\r\n\r\nAdds controller components to BuiltInComponentsFromContext\r\n\r\n## Purpose\r\n\r\nMakes it easier to use in compile time DI.\r\n\r\n## Background Context\r\n\r\nThis is important because any compile time DI that uses AbstractController depends on it:\r\n\r\n```\r\nclass MyComponents(context: ApplicationLoader.Context) \r\n  extends BuiltInComponentsFromContext(context)\r\n  with AhcWSComponents\r\n  with _root_.controllers.AssetsComponents {\r\n\r\n  lazy val parsers: PlayBodyParsers = playBodyParsers\r\n  lazy val actionBuilder: ActionBuilder[Request, AnyContent] = defaultActionBuilder\r\n\r\n  lazy val homeController = new _root_.controllers.HomeController(controllerComponents)\r\n\r\n  lazy val router: Router = new _root_.router.Routes(httpErrorHandler, homeController, assets)\r\n}\r\n```\r\n Add builtin controller components >>> 1"
261,This pull request removes some of the core Play JSON documentation from the Play project. The documentation will be added into the Play JSON project in a PR in that project.\r\n\r\nThe change is pretty simple—just remove some files. The only interesting piece here is the list of files to assume are present when markdown validation is run. Move most JSON docs to play-json project >>> 1
262,VS Code is becoming more popular and I'm trialling it now. This tiny commit tells Git to ignore the files VS Code adds to the root directory of the repository. Ignore files created by Visual Studio code editor >>> 1
263,"Right now we use a `Map<String,List<ValidationError>>` to store validation errors inside a `play.data.Form` instance.\r\nBut this absolutely doesn't make any sense at all and makes the code just very confusing. A `Map` entry's key will always be the same as the keys of the `ValidationError`s the `Map` entry stores anyway. It seemed the only ""sense"" to store the `ValidationError`s in a map was to access it ""more easy"" via `.get(""somekey"")` - but since we are using Java >= 8 we can simple use stream methods (`filter`, `findFirst` , etc.) to access the desired error(s).\r\n\r\nAlso it turns out the use of a `Map` here is a leftover from one of Play 2 first commits ever - where a `ValidationError` [didn't have a `key` field yet](https://github.com/playframework/playframework/commit/8d644fe0f629bb5213f0dbcd393d419329fba664#diff-5ca281da229236db4cc36a9058ce1238) (so using a map back than seems reasonable).\r\n\r\nPlus: The Scala `Form` uses [a simple `List`](https://github.com/playframework/playframework/blob/2.6.0-M1/framework/src/play/src/main/scala/play/api/data/Form.scala#L35) as well - so we just bring [the Java implemention on par with it](https://github.com/playframework/playframework/blob/2.6.0-M1/framework/src/play/src/main/scala/play/api/data/Form.scala#L174). Use List instead of Map to manage java-form errors >>> 1"
264,"1. Added removeAll function in AsyncCacheApi to remove all values from cache.\r\n2. Added related documentation..\r\n\r\n# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files? (N/A)\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #4757 \r\n\r\n## Purpose\r\n\r\nWe have added removeAll function in AsyncCacheApi to remove all cached item at once. \r\n\r\n\r\n## References\r\n\r\n#5912 \r\n#4757 \r\n#6985  Added removeAll function in AsyncCacheApi to remove all values from cache >>> 1"
265,## Purpose\r\n\r\nDeprecated test helpers methods that were using global state. Deprecate methods using global state in test Helpers >>> 1
266,"This PR adds a list of default filters to Play that is loaded automatically using SBT AutoPlugin.\r\n\r\nThis change means that new Play projects, with nothing defined, will have the filters library auto loaded, and the following filters enabled:\r\n\r\n```\r\nplay.filters.enabled = [\r\n    play.filters.csrf.CSRFFilter,\r\n    play.filters.headers.SecurityHeadersFilter,\r\n    play.filters.hosts.AllowedHostsFilter\r\n]\r\n```  \r\n\r\nNote that the default list is empty in the root Play `reference.conf`, and so if you disable the filters plugin, it functionally works the same as `NoHttpFilters`. Add default filters to Play >>> 1"
267,nan Update play-json to 2.6.0-M4 >>> 1
268,Fixes #6977 \r\n\r\nThis fixes the `SignedTokenProvider` so it correctly compares signed tokens. It also fixes the CSRF tests so they correctly catch this type of error. Correctly compare signed tokens >>> 1
269,"This fixes an incorrect mime type for XHTML, and adds constants for the XHTML mime type (`application/xhtml+xml`). Fix XHTML mime type >>> 1"
270,Because I needed it just now and people did ask for it in the past as well...\r\nhttps://groups.google.com/d/topic/play-framework/FIHBAS6qNms/discussion\r\nhttps://groups.google.com/d/topic/play-framework/knA9AzU7i3Q/discussion\r\n...I thought it would be nice to have it built-in. Add @helper.repeatWithIndex(...) template helper >>> 1
271,Fixes #7030  Fix logging on redirects and add status utility methods >>> 1
272,Backport of #7025 [2.5.x] Add @helper.repeatWithIndex(...) helper >>> 1
273,Fixes #5992 \r\n\r\nReplaces the `validate()` method with `JSR-303`/`JSR-349` compliant class-level constraints.\r\nPlay out-of-the-box now provides three generic class-level constraints which can be seen as almost drop in replacements for the current `validate()` method with it's different return types (only simple migration steps are necessary).\r\n\r\nThis is all possible because[ since Hibernate Validator v5.3](http://in.relation.to/2016/01/18/hibernate-validator-530-alpha1-out/#dynamic-payloads-for-constraints) we can now pass payloads (in our case `ValidationError` objects) from within constraints to Play's internal validation code.\r\n\r\nFor further explanation just have a look at the migration guide and docs I added in the PR. (I put the `Advanced validation` section at the bottom of the applicable documentation page so it gets out of the way for new users who just want to quickly know how basic validation works.) Provide generic class-level constraints and deprecate validate() >>> 1
274,nan Fix version in @deprecated >>> 1
275,"Removes the embedded play20 welcome page.  \n\nThis has been moved out to the play-starter templates:\n\nhttps://github.com/playframework/play-java-starter-example/blob/master/app/views/welcome.scala.html\n\nSo that new users have a better experience.  It's not used internally, and is confusing to new users. Delete welcome page >>> 1"
276,"This deprecates obsolete methods in java api. Methods such as getFile(String relativePath) , resource(String relativePath) and resourceAsStream(String relativePath) in Appliaction.java use deprecated methods from Appliaction.scala, so should be deprecated too.\r\n Deprecation of methods (getFile(), resource() and resourceAsStream() … >>> 1"
277,nan #7050 added note on migrating deprecated Forms code >>> 0
278,Build for branch 2.4.x is failing. This fix the test that is causing the failure. [2.4.x]: Fix build test >>> 1
279,"Defines a JWT cookie encoding using SHA256 for signing.  There is a fallback encoding that can read both urlencoded-SHA1-signed cookies or JWT, and write out a JWT.\r\n\r\nThis is based on JJWT as it depends on Jackson 2.8 and has no other dependencies.\r\n\r\nThe encode / decode methods are overridden to leverage JWT to and from Map[String, String].  The issued-at and not-before claims are written out, and there's an optional id.  There is an expiration option, which will render the JWT useless past a certain date.  This is neat, as it gives us the benefits of browser expiration and server side token expiration automatically.\r\n\r\nFor session, JWT expiration mapped to the same as `maxAge`.  For Flash, there is no JWT expiration.\r\n\r\nThe cookie information is a custom claim that can be defined through configuration.  The JWT configuration is broken out so that people who want custom cookies can leverage functionality without having to add it in themselves.\r\n\r\nThere is also some minor clean up of Flash and Session traits, making method returns explicit and removing SessionConfiguration from code and defining the domain explicitly.\r\n\r\nI note that we don't include anything about `CookieBaker` and the benefit from being able to float a custom type to and from cookie format -- we do this for Play JSON through `Reads` and `Writes` so it's a bit strange it isn't mentioned in the docs and we make people work with raw `Cookie`.  Add JWT cookie encoding >>> 1"
280,"Just had this case: When validating a form I want to make sure that all integers [inside a list submitted by the user](https://www.playframework.com/documentation/2.5.x/JavaFormHelpers#Handling-repeated-values) are positiv. This can be done with Hibernate validator via:\r\n\r\n```java\r\nimport play.data.validation.Constraints.Min;\r\n\r\n@Valid\r\nprivate List<@Min(0) Integer> values;\r\n```\r\n\r\nUnfortunataly all our constraints defined in [`play.data.validation.Constraints`](https://github.com/playframework/playframework/blob/2.6.0-M1/framework/src/play-java-forms/src/main/java/play/data/validation/Constraints.java) are only allowed  on fields currently. **So right now this doesn't work in Play.**\r\nHowever Hibernate validator allows you to put constraints on type arguments (`TYPE_USE`) like needed in the case above. For other purposes you could put constraints on methods, constructors and other annotations as well which is all described in the hibernate validator docs.\r\n[All](https://github.com/hibernate/hibernate-validator/tree/master/engine/src/main/java/org/hibernate/validator/constraints) the built-in Hibernate validator constraints use [these](https://github.com/hibernate/hibernate-validator/blob/master/engine/src/main/java/org/hibernate/validator/constraints/NotEmpty.java#L38) set of `@Targets` by default - I was actually wondering why Play doesn't. Add additional @Targets to constraints >>> 1"
281,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n\r\n## Purpose\r\n\r\nUpgrade mysql connector java to 5.1.41\r\n\r\n Update mysql connector java to  5.1.41 >>> 1"
282,## Fixes\r\n\r\nFixes #7028.\r\n\r\n## References\r\n\r\nSee also #7029.\r\n Return empty map when parsing uris with empty query strings >>> 1
283,"WIP NEEDS BENCHMARKING\r\n\r\n## Fixes\r\n\r\nFixes https://github.com/playframework/playframework/issues/6958\r\n\r\n## Purpose\r\n\r\nThis PR removes the SLF4J-JUL bridge and configuration from LogbackLoggerConfigurator\r\n\r\n## Background Context\r\n\r\nJUL is not used internally, and if there are any performance considerations, they'll come out in the build.  \r\n\r\nShould run through microbenchmarks and prune before merging this.\r\n Add LevelChangePropagator >>> 1"
284,## Purpose\r\n\r\n1. Add Netty 4.1 upgrade information\r\n2. Correct some format details\r\n3. Reword a phrase of Akka migration. Improvements and small corrections to migration guide >>> 1
285,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nThese lines are no longer needed in recent versions of sbt-assembly, as explained here: https://github.com/sbt/sbt-assembly/blob/master/Migration.md#upgrading-with-bare-buildsbt Update assembly.sbt >>> 1"
286,nan Fix compilation error in test >>> 1
287,"## Fixes\r\n\r\nFixes #4878.\r\n\r\n## Purpose\r\n\r\nAs suggested by @gmethvin (https://github.com/playframework/playframework/issues/4878#issuecomment-125174212), `HEAD` request against ws endpoints will now respond with a Bad Request (`400`).\r\n\r\n## References\r\n\r\nSee #7057 and #7059.\r\n WebSocket endpoints returning 400 for HEAD requests >>> 1"
288,"## Background Context\r\n\r\nIt's not clear in current documentation how controllers are allocated\r\nduring request handling. Also the current words ""prototype controller""\r\nare not clear to general developers as that's a spring specific term.\r\nThis change is to make things explicit.\r\n Component lifecycle clarification in DI chapter of doc >>> 0"
289,Some minor cleanup in `publishLocal` warnings. Clean up some warnings in doc >>> 1
290,## Purpose\r\n\r\nReturns an empty Map when parsing URIs with empty query string.\r\n\r\n## Fixes\r\n\r\nFixes #7028. [2.5.x]: Fix empty query string parse >>> 1
291,"## Fixes\r\n\r\nFixes #4878.\r\n\r\n## Purpose\r\n\r\nAs suggested by @gmethvin (https://github.com/playframework/playframework/issues/4878#issuecomment-125174212), `HEAD` request against ws endpoints will now respond with a Bad Request (`400`). [2.5.x]: WebSocket endpoints returning 400 for HEAD requests >>> 1"
292,## Fixes\r\n\r\nRelated to #4878.\r\n\r\n## Purpose\r\n\r\nBackport of #7057 to branch 2.4.x. [2.4.x]: WebSocket endpoints returning 400 for HEAD requests >>> 1
293,* Use `CompileOrder.JavaThenScala` when compiling java-forms tests to workaround [Scala 2.11 bug](https://github.com/playframework/playframework/pull/7055#issuecomment-285376543)\r\n* Had therefore move `Task.scala` to `Task.java` because `HttpFormsTest.JAVA` does depend on it (but gets compiled before now) Fix for 2.11 crossbuild >>> 1
294,"The Travis CI build is not building 2.11 effectively.\r\n\r\nSee https://github.com/playframework/playframework/pull/7055 for breakage.\r\n\r\nIn addition, with 2.11 we are running into https://github.com/playframework/playframework/issues/6782 again\r\n\r\n```\r\n[info] Resolving org.scala-sbt#sbt-launch;0.13.13 ...\r\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\r\n[warn] \t::          UNRESOLVED DEPENDENCIES         ::\r\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\r\n[warn] \t:: com.typesafe.sbt#sbt-twirl;1.3.0: not found\r\n[warn] \t:: com.typesafe.sbt#sbt-native-packager;1.1.5: not found\r\n[warn] \t:: com.typesafe.sbt#sbt-web;1.3.0: not found\r\n[warn] \t:: com.typesafe.sbt#sbt-js-engine;1.1.3: not found\r\n[warn] \t::::::::::::::::::::::::::::::::::::::::::::::\r\n[warn] \r\n[warn] \tNote: Some unresolved dependencies have extra attributes.  Check that these dependencies exist with the requested attributes.\r\n[warn] \t\tcom.typesafe.sbt:sbt-twirl:1.3.0 (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-native-packager:1.1.5 (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-web:1.3.0 (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-js-engine:1.1.3 (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \r\n[warn] \tNote: Unresolved dependencies path:\r\n[warn] \t\tcom.typesafe.sbt:sbt-twirl:1.3.0 (scalaVersion=2.11, sbtVersion=0.13) (/home/wsargent/work/playframework/master/framework/build.sbt#L172)\r\n[warn] \t\t  +- com.typesafe.play:sbt-plugin:2.6.0-SNAPSHOT (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-native-packager:1.1.5 (scalaVersion=2.11, sbtVersion=0.13) (/home/wsargent/work/playframework/master/framework/build.sbt#L172)\r\n[warn] \t\t  +- com.typesafe.play:sbt-plugin:2.6.0-SNAPSHOT (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-web:1.3.0 (scalaVersion=2.11, sbtVersion=0.13) (/home/wsargent/work/playframework/master/framework/build.sbt#L172)\r\n[warn] \t\t  +- com.typesafe.play:sbt-plugin:2.6.0-SNAPSHOT (scalaVersion=2.11, sbtVersion=0.13)\r\n[warn] \t\tcom.typesafe.sbt:sbt-js-engine:1.1.3 (scalaVersion=2.11, sbtVersion=0.13) (/home/wsargent/work/playframework/master/framework/build.sbt#L172)\r\n[warn] \t\t  +- com.typesafe.play:sbt-plugin:2.6.0-SNAPSHOT (scalaVersion=2.11, sbtVersion=0.13)\r\n[trace] Stack trace suppressed: run last SBT-Plugin/*:update for the full output.\r\n[error] (SBT-Plugin/*:update) sbt.ResolveException: unresolved dependency: com.typesafe.sbt#sbt-twirl;1.3.0: not found\r\n[error] unresolved dependency: com.typesafe.sbt#sbt-native-packager;1.1.5: not found\r\n[error] unresolved dependency: com.typesafe.sbt#sbt-web;1.3.0: not found\r\n[error] unresolved dependency: com.typesafe.sbt#sbt-js-engine;1.1.3: not found\r\n[error] Total time: 26 s, completed Mar 13, 2017 5:09:58 PM\r\n``` Fix Travis CI build for 2.11 crossbuild >>> 1"
295,"A filter that redirects from HTTP to HTTPS -- this is quite popular, so it makes sense to be internal.\r\n\r\n* https://www.clever-cloud.com/blog/engineering/2015/12/01/redirect-to-https-in-play/\r\n* https://stackoverflow.com/questions/19147147/how-to-force-play-framework-2-to-always-use-ssl\r\n* https://github.com/kaliber-scala/play-https-redirect-filter\r\n\r\nStarted with https://github.com/playframework/playframework/pull/6825\r\n\r\nThere's still some work to do -- for example, what to do if there is an explicit HTTPS port, and getting it to work nicely with Play has HTTP / HTTPS servers working. Redirect plain http to https filter (rebased #6825) >>> 1"
296,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Typo fixed from CRSF.getToken to CSRF.getToken >>> 1"
297,Forward porting from 2.5 branch. See https://github.com/playframework/playframework/commit/c490f506f2a9c136ed15070b54cc1f919ac01b69\r\n Typo: CRSF -> CSRF >>> 1
298,Seems weird that play.Environment does not have a binding but play.api.Environment does. Make play.Environment injectable >>> 1
299,nan Use play-file-watch library >>> 1
300,Adds referrer-policy to the list of security headers.\n\nSee https://scotthelme.co.uk/a-new-security-header-referrer-policy/ for more information. Adds referrer policy security header >>> 1
301,"This PR provides a reasonable default for Strict-Transport-Security, but only enables it in production.mode.  \n\nThis means that applications only have to enable the filter, and don't have to also explicitly configure HSTS. Better HSTS behavior in RedirectHttpsFilter >>> 1"
302,"This should allow people to use the helpers to run tests in parallel automatically, as long as they disable global state.\r\n\r\nNot sure how much configurability is needed here. This retains the existing functionality for those using global state so it shouldn't break existing apps. Only use PlayRunners.mutex for apps with global app enabled >>> 1"
303,Checkout https://github.com/blog/2335-open-source-license-descriptions-and-metadata\r\nI tought it's nice to make use of this GitHub feature.\r\nAlso IMHO the sentence in the README is just redundant (and not part of what the APPENDIX in the license says).\r\n\r\nFeel free to close this PR if you don't want this. Added LICENSE file to make use of GitHub goodness >>> 1
304,## Purpose\r\n\r\nRemove some more deprecated APIs calls from the documentation and sample code. Remove some more deprecated api usage from docs >>> 1
305,## Purpose\r\n\r\nUpdates HikariCP to its new version.\r\n\r\n## References\r\n\r\nSee HikariCP changelog for 2.6.0 and 2.6.1 to better understand what changed for this update:\r\n\r\nhttps://github.com/brettwooldridge/HikariCP/blob/dev/CHANGES\r\n Update HikariCP to version 2.6.1 >>> 1
306,nan Add compile time filter components for RedirectHttpsFilter >>> 1
307,## Fixes\r\n\r\nFixes #5815.\r\n\r\n## Purpose\r\n\r\nMost of the documentation was already updated with examples using Dependency Injection. This just add some small points to make it more clear and complete. Make clear that action composition can use Dependency Injection >>> 1
308,nan Add 406 and 415 error codes to Results.java >>> 1
309,"Some log statements  (especially trace- and debug-level) use parameters that are costly to compute. However, such logging levels are disabled in production. ""Lazy"" logging allows to log such big messages without explicitly checking if the requested log level is enabled. Still not supported by slf4j logging facade, although log4j 2 and JUL realization provide such methods. Override some log statements in Logger.java to provide lazy logging >>> 1"
310,"## Purpose\r\n\r\nMinor versions update.\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAkka Http is one of the important updates here, mainly because this release has ""groundwork to enable Play to make use of Akka HTTP and the new Akka Streams materializer in the upcoming Play 2.6"":\r\n\r\nhttp://akka.io/news/2017/03/17/akka-http-10.0.5-released.html\r\n Update dependencies >>> 1"
311,This is a rebased version of https://github.com/playframework/playframework/pull/6328 -- the only change here is to move to `GraphStage` in the CSRFAction so it doesn't use a deprecated type. Make CSRFActions use GraphStage (rebased #6328) >>> 0
312,"Fixes #7070 POST, PUT, PATCH should ignore querystring params when building form >>> 1"
313,The command as written right now doesn't actually run for me. I found that changing it to single quotes fixed the issue Change double quotes to single quotes to fix command >>> 1
314,## Purpose\r\n\r\nImprove the migration guide with information about Java Configuration API changes.\r\n\r\n## References\r\n\r\nSee discussion here:\r\n\r\nhttps://groups.google.com/forum/#!msg/play-framework/PInYQIyQ32M/Ev7CDs4hBwAJ Docs about the changes in Java Configuration API >>> 1
315,## Purpose\r\n\r\nThis follows a suggestion made by @ktoso and give Akka Streams Inlet/Outlet because it is better for debugging purposes. Change inlet/outlet stream names to improve debug >>> 1
316,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added tests for any changed functionality?\r\n\r\nDon't think this actually changed functionality.\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nThis is not related to any issue.\r\n\r\nI dive into the source code every now and then to look up how things are done and I recently found a small thing where I thought ""that's not the ideal way to use a for loop"". Seeing as how reading the source code had been helpful for me, I thought I'd do some static analysis, find some stuff to clean up and then submit said cleanup. Kinda like the boy scout rule, which is mentioned in the 'contributing' page.\r\n\r\nThis PR limits itself to the `play-java` sub-project.\r\n\r\n## References\r\n\r\nNo references\r\n\r\nSo yeah. general cleanup. It makes the code a bit cleaner. I fully understand if you're all like ""we don't actually need this right now"".\r\n\r\nThings that changed:\r\n* \* imports changed to fully qualified imports.\r\n* Seemingly pointless `matcher.matches();` calls removed.\r\n* 2 duplicate primitive assignments fixed\r\n* for loop into while loop, which seems more logical for the intended functionality of that bit of code\r\n* Use of JDK8's `computeIfAbsent` functionality, which is expressly intended for ""check if map index is null, if so add something at that index"" scenarios.\r\n* Removing redundant escapes in a regex. That's according to Intellij.\r\n Static analysis of play-java subproject. >>> 1"
317,"actually request methods were not available in AbstractController\r\n\r\nwhich is a little bit inconvienient since somebody would need\r\nto write them by itself\r\n\r\nTODO:\r\n* [ ] Tests\r\n\r\nNot sure if a better place might be `RequestExtractors` besides it's not a Extractor in ""Scala"" sense, but I think it can be called a RequestExtractor also. adds a mixin trait for request methods >>> 1"
318,Fix classloading lock issue discussed #6939  use thread local document builder >>> 0
319,"Fixes #7086 \r\n\r\nThis adds the [`SameSite`](https://tools.ietf.org/html/draft-west-first-party-cookies-07) attribute to the cookie models, and sets it to `Lax` by default for the session and flash cookies. Add cookie SameSite attribute and use for session and flash >>> 1"
320,nan Replace deprecated user with username >>> 1
321,"This is a first stab at the lowest-hanging fruits for improving akka-http backend performance in play (we have a few more ideas).\r\n\r\nTwo things in here:\r\n * don't generate a `Content-Length: 0` header for GET requests => `BodyParser.default` will now correctly detect that a GET request has no body and return a `DoneAccumulator` => this will prevent any expensive stream logic to run for any GET requests => throughput more than doubles for this very common case\r\n * optimize `DoneAccumulator` to prevent scheduling combinator callbacks if the given Future has already been successfully completed, on top of the other change this improves throughput for me by another 5-10% and also simplifies profiling a lot as regular stacktraces become much simpler than before First akka-http backend performance improvement >>> 1"
322,The statement doesn't match the [code](https://github.com/playframework/playframework/blob/2.5.x/framework/src/play-java/src/main/java/play/data/Form.java#L451-L457).\r\n\r\nSee https://github.com/playframework/playframework/pull/7038#discussion_r107654150\r\n\r\nI will tackle this in #7038 for the `master` branch. Fix statement about empty List when using validate() >>> 1
323,Fixes a small wording issue in the documentation about managing library dependencies. Small fix to wording in SBTDependencies.md >>> 1
324,"In messages.default the default messages for forms validation are defined. \r\nIn order to overwrite the validation messages with language-specific messages, you need to know under which alias the messages are defined.\r\nThe reference might save users some time searching for the aliases that play uses. reference messages.default in ScalaI18N.md >>> 0"
325,This makes it so that:\r\n* Reloader no longer depends on Twirl and is more generic\r\n* We don't try to parse a source file as Twirl unless the extension matches\r\n* It's easier to support other generated sources in the future More generic handling of generated sources >>> 1
326,Fixes https://github.com/playframework/playframework/issues/6041 and https://github.com/playframework/playframework/issues/7037 Only call entityManagerContext.pop if the EntityManager is set >>> 1
327,This is the first step towards supporting Maven + Play along with https://github.com/playframework/playframework/pull/7093 Remove SBT dependency from reloader >>> 1
328,"I'd like to refactor more of the Reloader into reusable components if possible. There's a bit of code that's shared between here and Lagom and it'd be nice to have it be an easily callable library.\r\n\r\nI couldn't find anything that was using this. Lagom doesn't support it, so it brings the two reloaders a little closer. It also simplifies the code by not needing to support unused functionality. Let me know if there's some external project that depends on it somehow. I did some searches and couldn't find any. Remove unused runSbtTask >>> 1"
329,## Purpose\r\n\r\nThis refactors `Form.bind` method into a set of smaller and more digestible methods.\r\n\r\n## References\r\n\r\nFollow up of the work done by @mkurz in #7038. See discussion there about this refactoring. Refactoring forms >>> 1
330,nan JSON formats for Play Lang >>> 1
331,"`SourceModificationWatch` internally is quite a bit more complicated than what we really are using it for. It calls `sleep` internally, tracks a quiet waiting period, etc. and really all we want to know is have any files been updated.\r\n\r\nOnce I simplified this implementation it became much clearer to walk through the test and see why it was failing. The issue is that the sleep statements need to be in between compiling and modifying the file.\r\n\r\nI'm wondering if a better solution though wouldn't be to setup a second watcher for the classpath using the file watch service. Do we really want the expected behavior of the user modifying `application.conf` within 1s of a compilation to be that the modification is ignored? Simplify timestamp checking and fix dev-mode tests >>> 1"
332,"Refactors Futures to remove the Timer, and use a global actorSystem under the hood.  All methods are deprecated (so we can remove them post 2.6.x) and non-static methods / stateless methods taking actorSystem have been added to `Timeout` interface.\r\n\r\nFixes https://github.com/playframework/playframework/issues/7117 Make play.libs.concurrent.Futures timeout use ActorSystem >>> 1"
333,Added an explanation of the Play versioning scheme and API compatibility guarantees. Update Play releases page to explain versioning and API compatibility >>> 1
334,Fixes #7142\r\n update dependencies related to webdriver >>> 1
335,"Reasons why:\r\n* Removing makes it easier to share code with Lagom and other projects\r\n* Doesn't work if you have a compile error in project\r\n* Docs are not searchable, so are hard to navigate compared to web\r\n* It's undocumented, so few know it exists\r\n* Required if we want to move to Paradox\r\n\r\nThe main benefit to the feature is that it makes the docs available offline, but we should be able to add a downloadable link to the docs artifact to fulfill this purpose Remove docs from dev mode >>> 1"
336,Centralizes logback dependencies inside specs and junit builds.\nRemoves an obsolete application-logger.xml file. Small refactoring for logback in tests >>> 1
337,Expand on the Highlights26.md document. Add more info to highlights >>> 1
338,"```\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for name\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for value\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for maxAge\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for path\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for domain\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for secure\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1987: warning: no @param for httpOnly\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for name\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for value\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for maxAge\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for path\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for domain\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for secure\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for httpOnly\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1971: warning: no @param for sameSite\r\n[warn]         public Cookie(String name, String value, Integer maxAge, String path,\r\n[warn]                ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play/src/main/java/play/mvc/Http.java:1786: warning: no @param for sameSite\r\n[warn]         public void setCookie(\r\n[warn]                     ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:473: warning - Tag @link: can't find route(Application, RequestBuilder) in play.test.Helpers\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:496: warning - Tag @link: can't find route(Application, RequestBuilder, long) in play.test.Helpers\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:345: warning - Tag @link: can't find routeAndCall(Application, Class, RequestBuilder, long) in play.test.Helpers\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:313: warning - Tag @link: missing '#': ""routeAndCall(Application, RequestBuilder, long)""\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:313: warning - Tag @link: can't find routeAndCall(Application, RequestBuilder, long) in play.test.Helpers\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:381: warning - Tag @link: can't find routeAndCall(Application, Router, RequestBuilder) in play.test.Helpers\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-test/src/main/java/play/test/Helpers.java:407: warning - Tag @link: can't find routeAndCall(Application, RequestBuilder, long) in play.test.Helpers\r\n[success] Total time: 65 s, completed Mar 28, 2017 3:54:04 PM\r\nmodel contains 87 documentable templates\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-filters-helpers/src/main/scala/play/filters/cors/CORSActionBuilder.scala:29: Could not find any member to link for ""play.api.mvc.ActionBuilder"".\r\n[warn] /**\r\n[warn] ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-filters-helpers/src/main/scala/play/filters/cors/CORSActionBuilder.scala:14: Could not find any member to link for ""play.api.mvc.ActionBuilder"".\r\n[warn] /**\r\n[warn] ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-filters-helpers/src/main/scala/play/filters/cors/CORSFilter.scala:15: Could not find any member to link for ""play.api.mvc.Filter"".\r\n[warn] /**\r\n[warn] ^\r\n[warn] /home/travis/build/playframework/playframework/framework/src/play-filters-helpers/src/main/scala/play/filters/cors/CORSConfig.scala:98: Could not find any member to link for ""play.api.Configuration"".\r\n[warn]   /**\r\n[warn]   ^\r\n``` Fix javadoc warnings >>> 1"
339,nan Update migration guides to reflect selenium update >>> 1
340,"Turns out my impl from yesterday was fine, just that something else on my other WIP branch was causing a failure.\r\n\r\nThis unblocks us from upgrading to Akka 2.5 and the new materialzer with it 👍  Rewrite CSRF BodyParser, unlock upgrading to Akka 2.5 >>> 1"
341,"Two optimisations.\r\n\r\nOne: to check that if contentLength is given but only if > 0 then we need to materialize things\r\nTwo: To avoid copying all headers into strings all the time but only when really we have to.\r\nThe special cased ContentLength, ContentType and uri help a bit too.\r\n\r\nI got a few failures locally but did not think they're related, so want to check on travis too while I look into those. \tSpecialized Headers impl backed by Akka Http >>> 0"
342,"With regards to https://github.com/playframework/playframework/issues/7154 there isn't an explicit warning that default filters may impact sites that are upgrading.\r\n\r\nI'll make the logs more explicit as well, so a security failure results in a log with SECURITY marker. Add a note about default filters potentially blocking  >>> 1"
343,On top of the `hasBody` change this removes unncessary scheduling of\r\nfuture combinator code. It should bring an additional 5-10% increase of\r\nthroughput in the request-without-body case. Optimize DoneAccumulator to bypass dispatcher in the happy strict case >>> 1
344,"It looks like Logback has some internal state that causes NPE on the build server. \n\nStill not sure what is causing this, but we can at least flag it as a known problem. Add exception to track down Logback error >>> 1"
345,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n\r\nThis PR resolved #7016.\r\n@richdougherty please take a look. \r\nI will be happy if you can suggest what can be improved since I've just started to learn Scala. (btw, thanks for the handy `newbie` label) allows to attach single attribute to the request >>> 1"
346,"The goal is to make it easier to use JDBC in Play's highly concurrent setting. Unlike a traditional servlet environment we don't have a thread-per-request, so the JDBC blocking API is easy to misuse.\r\n\r\nThis is a WIP for early review. I have a lot of changes planned which I will add as extra comments. I would welcome extra feedback before I start making these changes and polishing the code up more. WIP: Make JDBC work better with thread pools >>> 0"
347,## Fixes\r\n\r\nFixes #5722\r\n\r\n## Purpose\r\n\r\nThis adds some missing conversions between Java and Scala types. This issue is also one of the steps to have Java components. More consistent asJava and asScala methods >>> 1
348,nan Add to nightly snapshots documentation >>> 1
349,Fixes #7051 Add Helpers.httpContext method for creating new Http.Context >>> 1
350,Replaces #6964 because most of the work was done in my previous pull requests regarding Java Forms already anyway. Deprecate form.reject() and form.discardErrors() >>> 1
351,# Status\r\n\r\nThis is ready to be reviewed.\r\n\r\n* [x] Documentation (Hightlights and how to use the components)\r\n* [x] Javadocs for all the components\r\n* [x] Java version for `play.api.BuiltInComponentsFromContext`\r\n* [x] Filters components like `play.api.NoHttpFiltersComponents` (maybe this should be done in a serie of smaller pull requests)\r\n* [x] `*Components` for subprojects\r\n\r\n## Purpose\r\n\r\nJava components APIs similar to what we have in Scala for Compile Time Injection. Java components for compile time injection >>> 1
352,Depends on https://github.com/playframework/playframework/pull/7153\r\nThen rebase and apply.\r\n\r\nPutting it into the validation queue though right away Update Akka to 2.5.0-RC2 >>> 1
353,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nWhat does this PR do? Makes the documentation around running evolutions with compile time DI a little clearer. Describes what imports and dependencies are necessary to make it work.\r\n\r\n improve documentation for running evolutions using compile-time DI >>> 1"
354,Adding beta sourceclear support to master. Add sourceclear >>> 1
355,Addresses #7146 Keep form.data() backward compatible but introduce rawData() >>> 1
356,"Fixes #7174. Basically, this makes sure if we add new cookies, any existing cookies with the same name are removed from the list. Make Result#withCookies override existing cookies with the same name >>> 1"
357,It seems to make more sense for `AssetsMetadata` to contain and provide an `AssetsFinder` rather than extending it. This way we avoid type conflicts when using Macwire and it's harder to accidentally use one instead of the other. Use composition instead of inheritance for AssetsFinder/AssetsMetadata >>> 1
358,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n\r\n# Helpful things\r\n\r\nGoing over some more code, I noticed an opportunity for EnumMap to be used.\r\n\r\nEnumMap documentation: https://docs.oracle.com/javase/8/docs/api/java/util/EnumMap.html\r\n\r\nEnumMap is basically a Map specifically implemented for when the Map's key is an Enum. It's faster and has a smaller memory footprint than a HashMap. Extremely compact and performant.\r\n\r\nI made this little bit of code with some HashMap vs EnumMap loops, for people who want to test it: https://ideone.com/y5mT3d\r\n\r\nI scanned the entirety of the playframework github project for use of Maps, this instance is the only one in the entire project that has an Enum for a key, far as I call tell.\r\n\r\nEDIT: XmlBodyParser unit test failed for Scala version 2.11.8, something else for 2.12.1. Not sure how it's related. Using EnumMap for mapping enums. >>> 1"
359,## Purpose\r\n\r\nAdd missing file headers and also a task to check them.\r\n\r\n## References\r\n\r\nhttps://github.com/sbt/sbt-header\r\n Add sbt-headers and missing file headers >>> 1
360,I've noticed that we don't have compile time dependency injection support for `ObjectMapper` (and `Json`) initialization. Add missing module for ObjectMapper >>> 1
361,Follow up for #7163 and #5722. Add missing asJava/asScala conversion >>> 1
362,Fixes Issue #7169\r\n\r\nSee details/discussion on the issue's page Upgrade logback dependency to fix 5 secs delays in unittests >>> 1
363,We upgraded to 1.2.3 in the 2.5.x branch. Should make master match\r\n Upgrade logback to 1.2.3 >>> 1
364,"It should be clear to people what filters are enabled on server startup in development, and this is not obvious from the logs.  \r\n\r\nThis PR ensures that filters are printed out to the log on the first URL request, using a webcommand hook.\r\n\r\nThis is one of a series of ""filter visibility"" PRs, see https://github.com/playframework/playframework/issues/7148 as well List enabled filters on startup >>> 1"
365,"# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n\r\nThis is not final, as I need feedback before making it nice. Feel free to also fork this. \r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nExposes Issue https://github.com/playframework/playframework/issues/7194 \r\n\r\n## Background Context\r\n\r\nI found the place where both Netty and Akka HTTP are tested. But I am not sure this is the right place for this.\r\n\r\nI'm just adding this as an automated test case for reference as my time is somewhat limited to go much further but yet I'd like this problem not to appear in the 2.6.0 release.\r\n\r\n## References\r\n\r\n#7194\r\n Empty body result failing testcase >>> 0"
366,"Background:\n\nAkka HTTP buffers request bodies as ""strict"" entities in certain\nsituations. These bodies are available immediately, and don't need\nstream materialization to handle them.\n\nPlay's action design requires that all request bodies be handled as\nstreams, since an action is defined to be a Sink that materializes to a\nfuture of the response, so the only way to pass a body into a Play\naction is to feed a Source of the body into that Sink. Even if that body\nis a 100 byte body that has already been buffered by Akka HTTP as a\nstrict entity, and even if actions handling is to simply fold all the\nbyte strings received together, Play's design still requires that this\nin memory body be wrapped in a Source.single, and then fed into a\nSink.fold(_ ++ _). No matter how much Akka streams improves the\nperformance of its materialization, this materialization will always be\nunnecessarily done and a burden on Play's performance.\n\nPlay has already added a fast path for when there is no request body,\nand the action is not interested in consuming a request body (such as\nfor GET requests), in the form of a DoneAccumulator, which both the\nNetty and Akka HTTP server implementations use by invoking this fast\npath when there is no body.\n\nFix:\n\nThis change adds a StrictAccumulator implementation of Accumulator, which\nallows an accumulator to specify a fast path that avoids using streams\n(and hence materialization) for when there is just one or zero elements to\nhandle. It deletes the DoneAccumulator, and replaces its usage with\nStrictAccumulator, since StrictAccumulator can achieve everything that\nthe DoneAccumulator can. Both of these classes are private\nimplementation details of accumulator, there is no public API that has\nbeen broken to make these changes, the factory methods for creating a\ndone accumulator still exist, just implemented using the struct\naccumulator.\n\nThe body parsers that buffer the full body have been modified to use the\nnew strict accumulator, and the Akka HTTP server implementation has been\nmodified to take advantage of the new run method that has been added to\nAccumulator which triggers the StrictAccumulator fast path.\n\nThe Netty server implementation hasn't been modified at all - there's no\npoint, since it does not yet support buffering small bodies in a single\nmessage, and can only be handled with streams.\n\nStill to be done:\n\n* Tests for the Java accumulator have yet to be written\n* The Java body parsers do not yet use this. [WIP] Akka HTTP performance improvement for strict requests >>> 1"
367,"This PR adds additional support for brotli, bz2 and lzma (xz) to the assets controller.\r\nThe searched compression types can be configured through application.conf. Also their\r\nsearch-order can be influenced there. \r\nThis is a follow up PR to \r\nhttps://github.com/playframework/playframework/pull/6552\r\n Assets: configurable brotli, bz2 + lzma2 support >>> 1"
368,"## Fixes\r\n\r\nFixes #6045.\r\n\r\n## Purpose\r\n\r\nThis adds the Java version for LoggerConfigurator. This becomes more evident while documenting #7170.\r\n\r\n## Background Context\r\n\r\nThis will enable Java users to both implement their LoggerConfigurators using Java and have a Java API to access Scala LoggerConfigurators. In fact, the Java version of LoggerConfigurator inherits from the Scala version.\r\n\r\n## References\r\n\r\nThis is part of #7170. Add Java version for LoggerConfigurator >>> 1"
369,"If you run several integration tests in parallel, then logback configuration will\ndie in interesting ways as the configuration is not thread-safe.\n\nAdds synchronized block on LogbackLoggerConfigurator. Fix NPE in LogbackLoggerConfigurator >>> 1"
370,nan Bump versions of Hibernate and Jackson >>> 1
371,Note: this won't work yet since it depends on playframework/play-file-watch#6\r\n\r\nSee #7076 Add run-support project for scala 2.11/2.12 >>> 0
372,nan Add explanation as to why JPA limits async usage >>> 1
373,nan Simplify run-support dependencies >>> 1
374,"This PR adds a security marker context, and adds warning log messages tagged with an SLF4J marker when a request fails a security filter.\r\n\r\nThis serves several purposes:\r\n\r\n1. Make people aware why a particular request is failing and why they're getting a 400 code\r\n2. Allow security failures to be marked out distinct from normal Play logging\r\n3. Allow developers to disable security messages with a turbo filter.\r\n\r\nMay need to add some documentation to show how to turn off the warnings -- essentially in logback.xml you add:\r\n\r\n```xml\r\n<turboFilter class=""ch.qos.logback.classic.turbo.MarkerFilter"">\r\n    <Marker>SECURITY</Marker>\r\n    <OnMatch>DENY</OnMatch>\r\n  </turboFilter>\r\n```\r\n\r\nor triggering an email if you're in production (see https://roman-ivanov.blogspot.com/2015/03/neutral-filters-and-evaluator-ordering.html for a good example) Add security marker context, tag filter failures >>> 1"
375,"## Fixes\r\n\r\nCookie encoding behavior was not modularized before.  This PR breaks out cookie support into modules and documents clearly how to replace cookie encoding functionality so that users have the option to fallback to URL encoded cookies.\r\n\r\nAlso moves the CSRFTokenSigner to the CSRF module, as it is not in the core functionality. Add legacy cookie support >>> 1"
376,nan Fix some deprecation warnings >>> 1
377,## References\r\n\r\nSee:\r\n\r\n1. http://developer.lightbend.com/blog/2017-04-10-sbt-01315-JDK9-support-and-offline-installation/\r\n2. http://www.scala-sbt.org/0.13/docs/sbt-0.13-Tech-Previews.html#sbt+0.13.15 Update to sbt 0.13.15 >>> 1
378,Fixes #7213.\r\n\r\nUses `JavaHelpers.createResult` to incorporate the changes to `Http.Context.current()` into the final `Result`. Include changes to Http.Context in Result from RoutingDsl >>> 1
379,## Fixes\r\n\r\nFixes #7194.\r\n\r\n## Purpose\r\n\r\nFix Akka Server version to correctly deliver empty files.\r\n\r\n## References\r\n\r\nSee issue #7194. Tests were added by @ScalaWilliam in #7195 and cherry-picked here. Fix akka server empty response body >>> 1
380,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [x] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n## Purpose\r\n\r\nThe version of sbt-buildinfo in this project is outdated, so it should be upgraded to latest version.\r\n\r\nThis bumped the version of sbt-buildinfo to `0.6.1`.\r\n\r\n## Reference\r\n\r\n- [official migration guide of sbt-buildinfo](https://github.com/sbt/sbt-buildinfo/blob/master/Migration.md)\r\n Upgrade sbt-buildinfo to latest version >>> 1"
381,"This has been backported from #7214, and I've raised a PR rather than backporting directly because the backport wasn't trivial. Include changes to Http.Context in Result from RoutingDsl >>> 1"
382,nan Update to Akka 2.5.0 final >>> 1
383,Supersedes #7155.\r\n\r\n/cc @ktoso  Provide optimized implementation of Headers for Akka Http Backend >>> 1
384,This will be one of the APIs that will support Http2 in the future. Change Akka Http backend to use Http().bindAndHandleAsync >>> 1
385,For simple requests all the Futures are already completed so avoiding any\r\noverhead of using ExecutionContext brings a ~ 4% improvement. Use FastFuture in Akka Http backend result processing >>> 1
386,"Under some conditions (seen in examples) the EnabledFilters can print repeatedly, and it should be a @Singleton in any case. Make EnabledFilters a singleton >>> 1"
387,nan link to sbt-native-packager 1.1.5 docs (#7217) >>> 1
388,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n\r\nNot sure if there's an equivalent Scala part that requires updating.\r\n\r\n# Helpful things\r\n\r\n## Purpose\r\n\r\nCode analysis turned up this `tail recursion`, where the very last statement of a piece of code calls the same method again. Can be replaced by a loop, which is stated to be considerably faster.\r\n\r\n## Background Context\r\n\r\nIt is both easier to read and avoids method calls. The analysis stated that some JVMs perform this change at compilation time and others don't. In either case, having it in place anyway makes for more readable code. Replaced tail recursion in java-test with loop. >>> 0"
389,RequestHeaderSpec will cover this change because it runs concurrently.\r\n\r\nFixes #7229 Reuse AbsoluteUri for parsing host in RequestHeader #7229 >>> 1
390,This implements parsing for the Accept-Encoding header. The idea is we can use to determine encoding preference for assets as discussed in #7198. Accept-Encoding parsing >>> 1
391,## Fixes\r\n\r\nFixes #7217\r\n\r\n## Purpose\r\n\r\nBackports #7227 to branch 2.5.x. [2.5.x]: Backport #7227 >>> 1
392,"## Purpose\r\n\r\nThis PR adds informations about a new plugin I wrote ""play-webpack"", see https://github.com/BowlingX/play-webpack. added play-webpack plugin informations >>> 1"
393,"Fixes #7185. Makes sure RoutesPrefix is generated regardless of whether reverse routes are enabled, since it is used in the generated forwards router.\r\n\r\nOne of the scripted tests was failing for me locally but it wasn't clear why. Maybe someone more familiar with those tests can help. Make it possible to disable the reverse router >>> 1"
394, - Connection header handling is done in akka-http\r\n - No need to parse headers into akka-http header model Improve result header handling for Akka Http backend >>> 1
395,Backport of #7189 [2.5.x] Make it possible to disable the reverse router >>> 1
396,"Replaces instances of ""new FileInputStream"" and ""new FileOutputStream"" with Files.newInputStream and Files.newOutputStream, respectively.\n\nSee https://www.cloudbees.com/blog/fileinputstream-fileoutputstream-considered-harmful for the extended version -- the basic idea is that it reduces GC pauses. Remove FileInputStream/FileOutputStream >>> 1"
397,## Fixes\r\n\r\nFixes outdated documentation.\r\n\r\n## References\r\n\r\nFrom https://groups.google.com/d/msg/play-framework/HlpHPEyPbrU/q_BJYQNtAgAJ\r\n [doc] Replace DB.getConnection with db.getConnection >>> 1
398,"Otherwise, it gets regenerated and reformatted with every compilation. Fix generated PlayVersion.scala formatting >>> 1"
399,Please double check:\r\n```\r\ngrep -r '2\.11' .\r\ngrep -r '2\.12' .\r\n``` Upgrade scala to 2.12.2/2.11.11 >>> 0
400,"## Purpose\r\n\r\nBackports #7244 to branch 2.5.x. [2.5.x]: Backport ""Remove FileInputStream/FileOutputStream"" >>> 1"
401,"This provides a new controller `InjectedController` that uses method injection. It also refactors the controller hierarchy to remove duplication, makes `BaseController` the actual base controller trait with abstract `ControllerComponents`, and adds additional information to the migration guide. Add InjectedController and refactor controller hierarchy >>> 1"
402,"## Purpose\r\n\r\nThis PR updates the Database and JPA, ThreadPool and JavaAsync pages to incorporate use of `CustomExecutionContext` in the examples and explain why it matters.\r\n\r\nAlso adds CustomExecutionContext to the highlights and ensures the example JPA template gets called out.\r\n\r\nAlso discusses keeping JPA classes segregated from the rest of the app to help isolate persistence and remove pernicious assumptions like lazy loading or ""unit of work"" assumptions that do not translate well across multiple threads.\r\n\r\nAlso discusses sizing the database connection pool according to the HikariCP suggestions.\r\n\r\n## Background Context\r\n\r\nThe documentation discusses thread pool configuration and blocking IO, but does not provide practical worked examples of databases or blocking ORMs using a custom execution context or use of asynchronous boundaries.  Even worse, the JPA documentation assumes all database operations happen inside the controller, where it's that much harder to keep track of which execution context is used for what. [doc] Document CustomExecutionContext  >>> 1"
403,This just separates the cache module into two modules `play-cache` (`PlayImport.cacheApi`) and `play-ehcache` (`PlayImport.ehcache`).\r\n\r\nThis is the first step to better supporting alternate implementations.\r\n\r\nI also made some corrections in the documentation. Split cache API and ehcache implementation into separate modules >>> 1
404,"## Fixes\r\n\r\nFixes #6690.\r\n\r\n## Purpose\r\n\r\nThis updates Scala versions to 2.12.2 and 2.11.11 and also removes most of the unused imports from the generate route files. There are still some corner cases where unused imports could be added, like when splitting routes in multiple files, all the imports in `RoutesKeys.routesImport` are added for each file, even if not used.\r\n\r\nBut users have better control over that and the unused imports added by the framework are now removed.\r\n\r\n## Background Context\r\n\r\nThis cherry-picks the commit made by @mkurz in #7250. I'm doing that to avoid merging #7250 and have a unstable build while updating Scala versions.\r\n\r\n## References\r\n\r\nSee #7250, #6690 and #6302. Updates Scala versions and avoid unused imports in routes files >>> 1"
405,Bumps fluentlenium from 3.1.1 to 3.2.0 Update fluentlenium to 3.2.0 >>> 1
406,"Fixes #7262\n\nThis reverts commit 4e67bd27dbf31d1a12826a7e1e26719704ecf7d8, ensuring that the right in memory URL is used, by setting it as a system property. It removes the dependence on Play's global state by using an already set system property if detected - this means the JNDI provider can still be customised using system properties. Fixed JNDI initial context initialisation >>> 0"
407,"Resolves #7256, I have verified on the reporters reproducer however would be good to add a test covering this too I guess =pfh #7256 fix illegal double push in CSRF handler rewrite >>> 1"
408,## Fixes\r\nFixes a wrong URL in the documentation \r\n\r\n## Pull Request Checklist\r\n* I have  signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)\r\n [doc] Corrected URL to match the example >>> 1
409,## Purpose\r\n\r\nAdd example of timeout to Play WS and Scala WS pages. [doc] Update WS timeout with examples >>> 1
410,This PR closes the loop on the Messages refactorings by adding a FormAction/FormRequest that makes dealing with form processing much easier.  See https://github.com/playframework/playframework/pull/6438 for the details.\r\n\r\nNote the removal of docs from ScalaForms.md as all the code is implicit.\r\n\r\n/cc @cb372 Add a FormAction and FormRequestHeader >>> 1
411,## Purpose\r\n\r\nProvides example of using a marker context across different execution contexts.\r\n [doc] Provide marker context async example >>> 0
412,Replaces with references to sbt and sbt shell.\r\n\r\nFixes #7268. Remove references to `play` command >>> 1
413,"Updates sbt-web (and plugins) dependencies, since we already made some releases already. This is related to issue #7045. Update sbt-web dependencies >>> 1"
414,"`play-file-watch` was having issues with cross-compilation, 1.0.1 is published so we can upgrade to it. Upgrade play-file-watch to 1.0.1 >>> 0"
415,This documentation change adds more examples of MarkerContext usage to Highlights and logging.\r\n\r\nThis was previously https://github.com/playframework/playframework/pull/7269 but that PR was misfiled as a playframework branch instead of a wsargent branch. More explicit MarkerContext examples >>> 1
416,"If `hikariConfig.setInitializationFailFast` is called from code, it spits out a runtime warning which is awkward, considering it's in `reference.conf` and therefore always called.\r\n\r\nThis PR makes the `hikaricp.initializationFailFast` property optional and removes it from configuration, so that there is no warning from HikariCP by default. Make initializationFailFast optional >>> 1"
417,"The example in Highlights was using an action, but was not a controller.  Fixed to extend AbstractController. Fix example timeout >>> 1"
418,"## Fixes\r\n\r\nTry to move files atomically if supported and, if not, fallback to regular moving. It was added as a new API to avoid breaking existent uses of `moveTo` which does not use atomic moves.\r\n\r\n## References\r\n\r\nThis was adapted from Kafka: https://github.com/apache/kafka/blob/d345d53/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L608-L626 API to atomically move the files or fallback if not supported >>> 1"
419,This makes sure the new Configuration API is mentioned in the highlights and migration guide. I included a small amount of detail and refer to the Scala configuration docs for more information. Add Scala configuration changes to highlights and migration guide >>> 1
420,"This PR makes the `TokenInfo` class lazy, so we don't have to even generate the token if the page never chooses to render it. This is a performance optimization in the simple case of rendering a page that doesn't need a CSRF token.\r\n\r\nThis also performs some other refactoring of our CSRF utilities. There's still a lot of things that could be improved, but the main goal here was to move complexity to the `CSRFActionHelper` that's shared among all the CSRF utilities.\r\n\r\n/cc @richdougherty  Only add CSRF token if it has been rendered >>> 1"
421,nan Additional documentation for dev mode >>> 1
422,This is useful mainly for test purpose and is also supported (and documented) by Play 2.5.x. LoggerConfigurator should work with empty app config >>> 1
423,This change will allow us to delete Lagom's `build-link` project when Lagom is upgraded to Play 2.6 Align the BuildLink interface with Lagom >>> 1
424,"## Current status\r\n\r\nThis is feature complete but could probably do with some tests.\r\n\r\n## Purpose\r\n\r\nThis PR closes the loop on adding HTTP Caching to Play WS, fixing https://github.com/playframework/playframework/issues/5931.\r\n\r\n* Upgrades EhCache to 2.8.3 and adds JCache adapter to ehcache module dependencies.\r\n* Removes Play-Java dependency from PlayAhcWS.\r\n* Creates a cache through the JCache API if defined, using the app classloader, and provides it as a caching layer to AsyncHttpClient.\r\n\r\nThe end result should be that if there's a cache implementation available, then Play WS uses caching for HTTP.  If there isn't a caching API available, then it doesn't.  You can turn off caching even if it is available, and tune and rename cache configuration parameters.\r\n\r\n## Background Context\r\n\r\nThis PR is a bit fiddly, because it depends on another library, play-standalone-ws, which by its nature can't depend on any Play APIs.  So we use JCache as the intermediary.  We also can't use play.api.SyncCacheApi, because the key is a String.  So, we use the JCache implementation, but that's not bound and provided through dependency injection, but resolved through the classloader.  So that's not ideal.\r\n\r\nSo what we do is rely on JSR 107 all the way through, allowing a different caching provider to be used, but using the default if there's only one available. Enable HTTP Caching in Play WS >>> 1"
425,"As far as I can tell these settings have never been used and just add extra complication. These are not present in the Lagom reloader, which is a little cleaner\r\n\r\nI also reordered to parameters to `startDevMode` to match the ordering in Lagom, which I think was nicer, as it makes it easier for me to follow the logic across the two\r\n\r\nAfter this is merged, I have just one or maybe two changes left to the reload functionality and we will then be able to use the same reloader for Play and Lagom. After the cleanup is done it may make sense to have this be a separate project with its own lifecycle Remove unused classloader settings >>> 1"
426,## Purpose\r\n\r\nRenames MessagesAction to MessagesActionBuilder.\r\n\r\n## Background Context\r\n\r\nShould be inline with the other action builders. Rename MessagesAction to MessagesActionBuilder >>> 1
427,Copies in changes from https://github.com/lagom/lagom/pull/137\r\n\r\nLagom should be able to use this Reloader instead of having its own after this change\r\n\r\nThis should help for adding Maven support to Play as well because it will be easier to copy what Lagom is doing by aligning the codebases a bit more\r\n\r\nI'd appreciate a new milestone release after this is merged if that's not too hard. Lagom is already working on upgrading to Play 2.6 and that would make it slightly easier for me to test out some of these changes with Lagom Enable sharing Reloader with Lagom >>> 1
428,"## Purpose\r\n\r\nInstead of using Scala `BuiltInComponents` in `Server.forRouter`, now uses the Java version.\r\n\r\n## References\r\n\r\nSee #7170.\r\n Use Java version of BuiltInComponents in Server.forRouter >>> 1"
429,## Purpose\r\n\r\nAdds documentation example to WS on timeout for futures.\r\n\r\n## References\r\n\r\nFutures functionality needs example. [doc] Update WS timeout with examples >>> 1
430,## Fixes\r\n\r\nFixes a problem happening in https://github.com/playframework/play-java-dagger2-example/pull/10.\r\n\r\n## Purpose\r\n\r\nRemove global state access from `Results.bakeCookie`. Remove access to global state while baking result cookies >>> 1
431,Akka HTTP 10.0.6 released: http://akka.io/news/2017/05/03/akka-http-10.0.6-released.html Upgrade to Akka HTTP 10.0.6 >>> 1
432,"## Purpose\r\n\r\nhttps://www.playframework.com/documentation/2.6.x/ScalaI18N does not specify the use of custom MessagesApi objects\r\n\r\nWe need to specify how i18n can be tested, and add the changes to Highlights26.md and JavaI18n / ScalaI18n pages.\r\n\r\nAlso see https://github.com/playframework/playframework/blob/master/framework/src/play/src/test/scala/play/api/i18n/MessagesSpec.scala [WIP] Add documentation for testing MessagesApi >>> 1"
433,"Previously, we were using the implicit conversion to convert an `Int` status code to a `Status`, but that doesn't work for all possible values.\r\n\r\nFixes #7299 Support non-standard response codes on Akka HTTP >>> 1"
434,"## Purpose\r\n\r\nWhile this API is internal and wouldn’t be used by users creating application, it could be useful to people integrating with Play.\r\n\r\nIt will helps us to move from this code:\r\n\r\n```java\r\npublic class Foo {\r\n    public static void main(String[] args) {\r\n        play.Environment env = play.Environment.simple();\r\n        play.ApplicationLoader.Context context = play.ApplicationLoader.create(env);\r\n\r\n        play.api.ApplicationLoader loader = play.api.ApplicationLoader$.MODULE$.apply(context.underlying());\r\n\r\n        play.api.Application application = loader.load(context.underlying());\r\n        play.core.ApplicationProvider applicationProvider = play.core.ApplicationProvider$.MODULE$.apply(application);\r\n\r\n        scala.Option<play.api.mvc.Result> resultOption = applicationProvider.handleWebCommand(someKindOfRequestTranslation());\r\n        \r\n        if (resultOption.isDefined()) {\r\n            // application was able to handle the result\r\n            play.api.mvc.Result result = resultOption.get();\r\n        } else {\r\n            // no handler was found\r\n            play.api.mvc.Result result = play.api.mvc.Results$.MODULE$.NotFound();\r\n        }\r\n    }\r\n\r\n    private static play.api.mvc.RequestHeader someKindOfRequestTranslation() {\r\n        return ...;\r\n    }\r\n}\r\n```\r\n\r\nTo this new version using only Java APIs:\r\n\r\n```java\r\npublic class Foo {\r\n    public static void main(String[] args) {\r\n        play.Environment env = play.Environment.simple();\r\n        play.ApplicationLoader.Context context = play.ApplicationLoader.create(env);\r\n\r\n        play.ApplicationLoader loader = play.ApplicationLoader.apply(context);\r\n\r\n        play.Application application = loader.load(context);\r\n        play.server.ApplicationProvider applicationProvider = new play.server.ApplicationProvider(application);\r\n\r\n        play.mvc.Result result = applicationProvider\r\n                .handleWebCommand(someKindOfRequestTranslation())\r\n                .orElse(play.mvc.Results.notFound());\r\n    }\r\n\r\n    // This would translate from AWS Lambda request to a Play request.\r\n    private static play.mvc.Http.RequestHeader someKindOfRequestTranslation() {\r\n        return ...;\r\n    }\r\n}\r\n```\r\n\r\n## References\r\n\r\nSee discussion here: https://twitter.com/sapessi/status/859450726551924736 Improve Java app loader/provider >>> 1"
435,"Typo  ""You should se your changes.."" =>  ""You should see your changes..""\r\n\r\n# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Typo  ""You should se your changes.."" =>  ""You should see your changes.."" >>> 1"
436,nan Upgrade to Netty 4.1.10 >>> 1
437,* Adds details about i18nsupport migration needing an implicit request.\r\n* Adds details of using `MessagesRequest` in migration. [doc] Add more explicit migration notes. >>> 1
438,Fixes https://github.com/playframework/playframework/issues/7317 Remove implicit from defaultLang >>> 1
439,"There was an error in line 85 so I changed ""It"" to ""In""\r\n\r\n# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [ ] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Line 85, changed ""It"" to ""In"" >>> 1"
440,Takes out a dependency and reference to FakeRequest for type enrichment. Remove Filters dependency on Play-Test >>> 1
441,Adds an example of using Accumulator with `through` to show usage of Flow. [doc] Add accumulator flow example >>> 1
442,## Purpose\r\n\r\nForward port changes made at #7321 and #7305 to master branch. Also correct some other typos. Correct some typos >>> 1
443,## Fixes\r\n\r\nFixes #7309 \r\n\r\n## Purpose\r\n\r\nTo improve documentation about testing with databases Added dependency to documentation about testing with database >>> 1
444,Replace deprecated java Docker container with openjdk\r\nhttps://github.com/sbt/sbt-native-packager/releases Upgrade sbt-native-packager version. >>> 1
445,"The `play.mvc.Http.Context#withRequest` method should maintain the original `Session`, `Flash`, and `Response` references, so all changes can be read by `JavaAction`. Otherwise, modifications made inside the action will not be propagated up the chain.\r\n\r\nThis looks like it has actually been a bug for a while, but we only recently noticed it because we started using this method in `AuthenticatedAction`.\r\n\r\nFixes #7328. Make sure context.withRequest maintains original session and flash >>> 1"
446,I noticed we had no tests for clearing the session in the Java API. Add test for session clear >>> 1
447,"Remove some deprecated singleton access in the Helpers.\n\nInterestingly enough, even if I call `new Session()` the singleton object deprecation warning still is triggered by the compiler.\n\nThink this is bug. Refactor some singleton access >>> 1"
448,There were some 2.5.x deprecated Lang methods still hanging around.\n\nThis PR removes them.  Note that the 2.5.x deprecated Play.current() accessors are still around. Remove deprecated lang methods >>> 1
449,"## Purpose\r\n\r\nWe have these tests for Scala helpers, but not for Java. This was a side effect of another PR and was sitting in my machine for awhile. Add tests for Java test Helpers >>> 1"
450,"## Fixes\r\n\r\nFixes #7335.\r\n\r\n## Purpose\r\n\r\nConfigurable websocket buffer limit. Also add a reference value, which was hardcoded for both Akka and Netty servers.\r\n\r\nThis should be backported to 2.5.x. Configurable websocket frame max length >>> 1"
451,## Fixes\r\n\r\nFixes #7322.\r\n\r\n## Purpose\r\n\r\nAdd documentation about how to schedule recurring tasks out-of-the-box. Also add a new module section to list task schedulers. Add docs about how to schedule tasks >>> 1
452,Fixes #7262. Closes #7263. Fix JNDI initial context initialization >>> 1
453,Backport of #7332 [2.5.x] Make sure context.withRequest maintains original session and flash >>> 1
454,Fixes #7304 Add AssetsComponents to migration guide >>> 1
455,## Purpose\r\n\r\nThis will both keeps better consistency with other sections of our documentation but also avoid an annoying warning that I’ve noticed while writing #7346. Here is an example of this warning in our build:\r\n\r\nhttps://travis-ci.org/playframework/playframework/jobs/232592636#L3259 Rename package from java to javaguide >>> 1
456,"This PR adds a BaseControllerHelpers trait that wires everything in ControllerHelpers to ControllerComponents.\n\nThen, it redefines BaseController as BaseControllerHelpers with the Action trait.\n\nFinally, it adds a MessagesAbstractController that extends BaseControllerHelpers, and a MessagesControllerComponents.  \n\nThis means that if you want to use MessagesRequest as the base type with an abstract controller, you can do it without interfering with BaseController inheritance or rewiring ControllerHelpers from the ground up. Add MessagesAbstractController >>> 1"
457,This change supports the latest versions of `node` and `npm` Upgrade sbt-js-engine to 1.2.0 >>> 1
458,"Adds the stub helpers so that AbstractController is documented.\r\n\r\nAlso add a couple of missing pages from the testing overview page and add it to the highlights.\r\n\r\nNote that this doesn't cover scalatestplus-play as the documentation is in nother repository, and it doesn't cover changes in the Java test helpers (if any)... Add stub helpers into documentation >>> 1"
459,## Fixes\r\n\r\nFixes #7338. Fix docs to correct state Akka HTTP as the default server >>> 1
460,"Add messages implicit not found and more forms documentation.\n\nThis gives an error message like the following:\n\n```\n   [play-scala-seed] $ compile\n    [info] Compiling 10 Scala sources and 1 Java source to\n   /home/wsargent/work/play-scala-seed/target/scala-2.12/classes...\n    [error]\n    /home/wsargent/work/play-scala-seed/app/views/user/form.scala.html:9: An\n   implicit MessagesProvider instance was not found.  Please see\n    https://www.playframework.com/documentation/latest/ScalaForms#passing-messages-to-form-helpers\n    [error]   @helper.inputText(userForm(""name""))\n    [error]                    ^\n    [error]\n    /home/wsargent/work/play-scala-seed/app/views/user/form.scala.html:10: An\n    implicit MessagesProvider instance was not found.  Please see\n    https://www.playframework.com/documentation/latest/ScalaForms#passing-messages-to-form-helpers\n    [error]   @helper.inputText(userForm(""age""))\n    [error]                    ^\n    [error] two errors found\n    [error] (compile:compileIncremental) Compilation failed\n    [error] Total time: 0 s, completed May 16, 2017 4:29:00 PM\n``` Better MessagesProvider implicitNotFound >>> 1"
461,Fixes https://github.com/playframework/playframework/issues/7333 Add form helper stubs >>> 1
462,"To enable HTTP/2, you can enable the `PlayAkkaHttp2Support` module on your project. This will add the `play-akka-http2-support` module and add the Java agent for ALPN support.\r\n\r\nSee also playframework/play-scala-tls-example#30\r\n\r\nTo work in dev mode, the user has to manually add a Java agent for ALPN support. I hope to improve the user experience for this in the future. Add HTTP/2 support module and documentation >>> 1"
463,"This backports several PRs (#6157, #6774, #7153, #7257) to the 2.5.x branch, to enable compatibility with Akka 2.5. [2.5.x] Unlock Akka 2.5 upgrade >>> 1"
464,nan Upgrade to play-json 2.6.0-RC1 >>> 1
465,## Purpose\r\n\r\nThe main point here is to reduce the dependency on `Injector` for Java components at the same time that makes is more on par with what we have for Scala compile-time DI. So we have a new implementation of `JavaHandlerComponents` which does not depend on the `Injector` anymore.\r\n\r\nWe will then have components added to the injector (Java and some Scala which are eventually needed by Java delegates/wrappers). Remove injector from Java compile time dependency injection components >>> 1
466,"The RequestBuilder has a remote connection of 127.0.0.1 but does not pass the AllowedHostsFilter as it does not have a Host header.  This PR adds the Host header directly to the RequestBuilder constructor.\r\n\r\nNote that `Helpers.fakeRequest()` does contain the appropriate host header, but in testing it's just as valid to call `new Http.RequestBuilder()` as it is to call `Headers.fakeRequest()`. Initialize Http.RequestBuilder with localhost >>> 1"
467,Fixes https://github.com/playframework/playframework/issues/7240\r\n\r\nAlso fixes some outdated documentation in Futures. Add Timeout and make it deprecated >>> 1
468,"I'm going to remove https://github.com/playframework/play-joda. Since this is just a legacy support module that is unlikely to change, I don't think it makes sense to have a separate project. It's much easier if we have simple submodules (play-joda-forms, play-json-joda) that are associated with each project. Add joda forms module >>> 1"
469,Adds bindings for `CacheManager` JCache module implementation >>> 1
470,Updates the highlights and the AkkaHttpServer docs about HTTP/2. Add HTTP/2 to highlights >>> 1
471,Fixes #7325. Add note about play iteratees as a standalone project >>> 1
472,"It should allow white spaces occurred in the start of each line in the routes file. So we can adjust the indentation of each line to make it more prettier. \r\nCurrently, if white spaces occurs in the start of lines: \r\n```\r\n# Application\r\n   GET    /index         controllers.Application.index\r\n```\r\nIt will complain one error:\r\n```\r\n[error] D:\idea-projects\qizhi-cloud\conf\routes:5: Compilation error[string matching regex `\z' expected but `G' found]\r\n```\r\nWith this pull request, the following routes passes the compilation:\r\n```\r\n# Application\r\n  GET    /index         controllers.Application.index   \r\n  GET    /login         controllers.Application.login\r\n# GET    /test          controllers.Application.test\r\n  GET    /logout        controllers.Application.logout\r\n\r\n# Sub Routes   \r\n  ->     /product       product.Routes\r\n```\r\nAnd it looks more prettier than before. Allowing white spaces occurred in the start of each line in the routes file. >>> 0"
473,## Purpose\r\n\r\nUpdate play-ws to release candidate 1. Update play-ws to RC1 >>> 1
474,"Futures were being evaluated immediately, instead of calling at the later point. Use call by name on Futures >>> 1"
475,"## Purpose\r\n\r\nThe `Future.delayed` method took a `CompletionStage<T>` instead of a `Callable<CompetionStage<A>>`, which meant that an active completion stage was being passed in, instead of only being executed at the time on the delay.  This was pre-existing functionality, as the previous call took a `Supplier` which had the same effect, but `Supplier` is semantically more of an idempotent operation while `Callable` indicates processing.\r\n\r\nAlso added tests, fixed documentation. and cleaned up javadoc with examples.\r\n\r\n## References\r\n\r\nJava version of https://github.com/playframework/playframework/pull/7380\r\n Make Futures.delayed take Callable<CompletionStage<T>> >>> 1"
476,"This adds a syntax to ""tag"" routes to modify the behavior of that route. The tags are simply strings, but it's theoretically possible to extract the tags from the `HandlerDef` and implement more complex behavior (e.g. `key=value`).\r\n\r\nRoute modifiers are introduced using a line beginning with a `+` symbol before the route definition:\r\n\r\n```\r\n+ NOCSRF\r\nPOST /api/foo/bar ApiController.foobar\r\n```\r\n(leading/trailing spaces are optional)\r\n\r\nFixes #5882, by defining a new `NOCSRF` modifier that can be applied to the route (this can be configured in the CSRF config).\r\n\r\nI decided not to use comments as suggested in #5882, since I suspect others are already using comments and did not want to interfere with custom implementations already out there. Traditionally comments should not change the behavior of your app. I'm open to suggestions on the syntax of route modifiers though. Add route modifier tags and NOCSRF >>> 1"
477,This just got added I'm guessing by mistake. Feel free to close if it was added purposefully\r\n Remove semicolon at end of comment >>> 1
478,For @richdougherty per https://github.com/playframework/playframework/pull/7381#pullrequestreview-39891391 Add futures.delay() >>> 1
479,Some javadoc lint errors in the build. Fix loggerconfigurator javadoc >>> 1
480,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [x] Have you added copyright headers to new files?\r\n* [x] Have you checked that both Scala and Java APIs are updated?\r\n* [x] Have you updated the documentation for both Scala and Java sections?\r\n* [x] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes  \r\n\r\nFixes #5193 \r\n\r\n## Purpose\r\n\r\nBackports #6580 to branch 2.5.x. [2.5.x]: Add a CORS config param to allow pass-through of forbidden origins (backports #6580) >>> 1"
481,"I cleaned up the injector instance used in `BuiltInComponents`, removing an override in `BuiltInComponentsFromContext` (which seems unnecessary) and some components I could not find usages of in our APIs. I also added Scaladoc/Javadoc to clarify how the injector is set up. Simplify compile-time DI Injector and add comments >>> 1"
482,## Purpose\r\n\r\nUpdate Akka HTTP to version 10.0.7 and remove snapshot resolvers. Update akka-http to 10.0.7 >>> 1
483,Adds documentation for route modifiers in general (though nocsrf is the only one we support now) and the nocsrf modifier in both the routing documentation and the CSRF documentation.\r\n\r\nFixes #7383. Add docs for nocsrf and route modifiers >>> 1
484,nan Update dependencies >>> 1
485,"## Fixes\r\n\r\nThe example code for [Combining body parsers](https://www.playframework.com/documentation/2.5.x/ScalaBodyParsers#Combining-body-parsers) does not compile when copied into another Controller, because it uses a `file` method that is not part of the example. \r\nSince the method is a one-liner and not used elsewhere I inlined it in the example. [doc] make combined-body-parser example compile >>> 1"
486,"# Pull Request Checklist\n- [*] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\n- [*] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\n- [*] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\n- [*] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\n- [*] Have you added copyright headers to new files?\n- [*] Have you checked that both Scala and Java APIs are updated?\n- [*] Have you updated the documentation for both Scala and Java sections?\n- [*] Have you added tests for any changed functionality?\n# Helpful things\n## Fixes\n\nFixes #xxxx\n## Purpose\n\nFix dependency download issue when the client is behind http proxy that requires user/password authentication.\n## Background Context\n\nI found that HTTP_PROXY environment is not enough for clients that are behind http proxy that requires user/password authentication.\n## References\n\n…thentication.\n Add description when behind http proxy that requires user/password au… >>> 1"
487,Forgot in #7393 Update jQuery >>> 1
488,(I think) the gzip filter is the only play-provided  filter left which can't be configured solely via `application.conf`.\r\n\r\nWould be great if this makes it in before 2.6 final. gzip filter can be configured via black-/whitelist in application.conf >>> 1
489,It's better to set these programmatically than use reference-overrides. Remove unnecessary reference overrides for Akka HTTP server >>> 1
490,"## Purpose\r\n\r\nThe idea here is to better detect deprecated APIs or broken examples. Also, I'm adding some Java examples where it is needed (for example, using Java compile-time Dependency Injection with Filters).\r\n\r\nThere are also some small changes that needed when coding the samples, which is a good way to validate the APIs. Extract documentation code samples >>> 1"
491,"## Purpose\r\n\r\nThere are a number of files missing the license header. This adds the header for Scala and Java files, including the docs. Add missing headers >>> 1"
492,"Make sure we bind to the named cache for all cache types, and also bind the default implementation to the default named cache.\r\n\r\nFixes #7401 Fix CacheApi bindings >>> 1"
493,## Fixes\r\n\r\nFixes #7356.\r\n\r\n## Purpose\r\n\r\nCorrectly handles form `bindFromRequest` when using a max lengthed body parsers. Proper test was not possible without adding a `Writeable` to handle `MultipartFormData` so this PR also adds this.\r\n\r\n## References\r\n\r\nThe `Writeable` is based on both jroper/playframework@1cb3fdb and https://goo.gl/DTgGkj. Correctly bind form from request when using multipart form with max length >>> 1
494,"## Purpose\r\n\r\nAll the other built-in filters are already implemented as `EssentialFilter`s, which is more performant. This PR refactors the `CORSFilter` to be an `EssentialFilter` too. Refactor CORSFilter to be an EssentialFilter >>> 1"
495,It should be okay to remove `HttpRequestHandlerActionCreator` since it's not mentioned anywhere in the docs. Remove ActionCreator#wrapAction and HttpRequestHandlerActionCreator >>> 1
496,"This is a small problem I've noticed in playframework/play-java-dagger2-example#21. Basically, this method declaration is causing a conflict with the default implementation present in `AkkaComponents`. Remove unnecessary method declaration >>> 1"
497,nan Update to Twirl 1.3.1 >>> 1
498,"A user doesn't have to set `TwirlKeys.constructorAnnotations` by him/herself. It's done by Play already:\r\nhttps://github.com/playframework/playframework/blob/2.6.0-RC1/framework/src/sbt-plugin/src/main/scala/play/sbt/PlaySettings.scala#L57\r\n\r\nWith the note users will end up annotating the generated scala class twice (checked it myself):\r\n`class SomeView @javax.inject.Inject()@javax.inject.Inject() ...` Remove unnecessary ""@Inject"" twirl setting from 2.6 highlights >>> 1"
499,nan Upgrade twirl once more >>> 1
500,executionContent -> executionContext executionContent -> executionContext >>> 1
501,This was a mistake introduced by me.\r\n`clazz.isInstance(obj)` actually checks if `obj` is an instance of `clazz` (and not the other way around).\r\nSo in our code correct would be: `Validatable.class.isInstance(result.getTarget())`.\r\n\r\nHowever to avoid any confusion I just use the `instanceof` operator now which is much more clearer what we want to express.\r\n\r\nI found out about this problem because I got the warnings displayed from line 487 altough I switched everything to the new validate approach. [Bug] Fix instanceof >>> 1
502,nan Upgrade sbt-native-packager >>> 1
503,"- changed the location of the `application.log` and `access.log` to the standard `logs/` location of a Play application\r\n- fixed reference to incorrect ACCESS_FILE_APPENDER\r\n\r\n# Pull Request Checklist\r\n\r\n* [ ] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [ ] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nFixes #xxxx\r\n\r\n## Purpose\r\n\r\nWhat does this PR do?\r\n\r\n## Background Context\r\n\r\nWhy did you take this approach?\r\n\r\n## References\r\n\r\nAre there any relevant issues / PRs / mailing lists discussions?\r\n Fixed appender references >>> 1"
504,nan Upgrade jackson >>> 1
505,"# Pull Request Checklist\r\n\r\n* [x] Have you read [How to write the perfect pull request](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)?\r\n* [x] Have you read through the [contributor guidelines](https://www.playframework.com/contributing)?\r\n* [x] Have you signed the [Lightbend CLA](https://www.lightbend.com/contribute/cla)?\r\n* [ ] Have you [squashed your commits]? (Optional, but makes merge messages nicer.)\r\n* [ ] Have you added copyright headers to new files?\r\n* [ ] Have you checked that both Scala and Java APIs are updated?\r\n* [ ] Have you updated the documentation for both Scala and Java sections?\r\n* [ ] Have you added tests for any changed functionality?\r\n\r\n# Helpful things\r\n\r\n## Fixes\r\n\r\nRemoved duplicated `can`.\r\n\r\n## Purpose\r\n\r\nCorrect a typo in `SecurityHeaders.md`.\r\n\r\n## Background Context\r\n\r\nFound a typo while reading `SecurityHeaders.md`.\r\n\r\n## References\r\n\r\nThere are no relevant issues / PRs / mailing lists discussions.\r\n Correct a typo in SecurityHeaders.md >>> 1"
506,"Fixes #7406\r\n\r\nSince this is just for the templates, I think it's probably justified to use `ToScalaImplicits` and `ToJavaImplicits` from scala 2.12.\r\n\r\nI decided to create a trait that's mixed into `PlayMagicForJava` since there are other implicit conversions there, but I could move to a separate object if people think it's a good idea. Don't use JavaConversions in scala 2.12 template imports >>> 1"
507,"Fixes #7359.\r\n\r\nIn this case, we leave it up to the user to decide what to do. If Akka HTTP can't parse the query string, we leave the params map empty and still provide the raw query string in case the user wishes to parse it manually. Don't send 500 error if query string fails to parse >>> 1"
508,## Fixes\r\n\r\nFixes #7432\r\n\r\n## Purpose\r\n\r\nThe Java to Scala cookies conversion was not considering the `SameSite` attribute. Consider SameSite when converting cookies from Java to Scala >>> 1
509,## Purpose\r\n\r\nThis updates the docs to reference the correct version of sbt-native-packager.\r\n\r\n## References\r\n\r\nSee #7424. Update docs to the used version of sbt-native-packager >>> 1
510,nan Upgrade HikariCP >>> 1
511,"Fixes https://github.com/playframework/playframework/issues/7435\n\nSimpleModule was incorrectly declared with braces instead of using a Seq in the constructor parameters, and so the bindings never took effect.  This didn't show up in the tests, because the binding was done directly instead of through the module.\n\nChanged the tests, and made the filter and provider be @Singleton for housekeeping. Fix module in RedirectHttpsFilter >>> 1"
512,Very simple enhancement: We want to be able to pass the `host` explicitly to `JavaScriptReverseRouter.create` - right now the host is solely determined from the current Http.Context. There can be use cases where you may want to use a different host other than the one from the request or if you want to pre-render and cache stuff where an Http.Context isn't available. Overload JavaScriptReverseRouter.create to pass host explicitly >>> 1
513,Fixes #7348 Improve highlights and migration guide for play-json >>> 1
514,"This is just a minor update to the docs.\r\n\r\nI found it weird to display ""an error"" after a successful save. Docs: change ""an error"" to ""a message"" >>> 1"
515,Applies changes from #7420 on `master`. [master] Fixed appender references (#7420) >>> 1
516,## Fixes \r\n\r\nFixes #7440 \r\n\r\n## Purpose\r\n\r\nCookies for session and flash respecting the sameSite configuration.\r\n\r\nThis adds tests and also a small refactoring to use `asScala` idiom instead of having a helper method to convert Java cookies to Scala cookies.\r\n\r\n## References\r\n\r\nSee #7432 and #7433.\r\n Session and Flash cookies respecting the sameSite configuration >>> 1
517,The goal here is to provide sensible defaults for using Play components in a unit testing context. This is done by defaulting constructor parameters and apply methods to empty or singleton versions when possible.\r\n\r\nI know this mainly affects the Scala API. This is much more difficult to do with the Java API due to lack of default values.\r\n\r\nThere are more components we could do this for but I wanted to get an idea of whether this makes sense before I do more. Provide better defaults for manually instantiating components >>> 1
518,"PR for https://github.com/playframework/play-ws/pull/124 to add typed BodyReadables and BodyWritables.\r\n\r\nAll of the existing methods on WSRequest and WSResponse are around, but deprecated.\r\n\r\nUpdated for documentation. Update for play-ws with BodyReadable/BodyWritables. >>> 1"
519,For both Java and Scala versions. [docs]: Extract code samples for I18n documentation >>> 1
520,"## Purpose\r\n\r\nIn Java, lists are easier to manipulate (append, filter, etc) than arrays. Use java.util.List instead of an array for Java list of filters >>> 1"
521,"## Purpose\r\n\r\nFor both Java and Scala. Also, tries to keep both the docs in sync. Java docs were lacking some sections that are present in Scala's. [docs]: Update async actions docs >>> 1"
522,This corrects a small error in the code and rewords for clarity. Make some clarifications to Java config migration guide >>> 1
523,Fixes https://github.com/playframework/playframework/issues/7398 by updating the Javadoc and adding more tests. Update AuthenticatedBuilder javadoc >>> 1
524,The migration document was written before MessagesAbstractController was added.  Revised so that it is included. Update messages migration with MessagesAbstractController >>> 1
525,Removes the deprecated controller references from specs and from example documentation\n\nFixes https://github.com/playframework/playframework/issues/7390 [doc] Remove controller references >>> 1
526,nan Upgrade akka >>> 1
527,"## Purpose\r\n\r\nReorders some sections to show first more important changes, like the Scala controllers change. Some reordering and additions to Migration26 and Highlights26 >>> 1"
528,Fixes https://github.com/playframework/playframework/issues/7306\r\n\r\nThe documentation was missing references to MessagesRequest and the stubs.\r\n\r\nAlso found a couple of implicit conversion that were missing and some awkwardness in the Java construction. Fill out the i18n documentation with tests >>> 1
529,This is a small follow up after #6623. [docs]: Small improvements of sbt docs >>> 1
530,## Fixes\r\n\r\nAkka HTTP server backend was not considering configuration `play.server.akka.requestTimeout`. We don't have a requestTimeout setting for Netty server backend. This is why the specs here are Akka only. Correct requestTimeout configuration in Akka Server >>> 1
531,"## Fixes\r\n\r\nFixes any leftovers of #2240.\r\n\r\n## Purpose\r\n\r\nThis is both consistent with the Scala play.api.mvc.Headers (part of it since this Java version is not as complete as the Scala one) and also with the headers APIs in play-ws.\r\n\r\nThis Headers API can evolve later, but I kept it small now to avoid bigger changes (since we are very close to release 2.6.0).\r\n\r\n## References\r\n\r\nSee discussion in #2240 and playframework/play-ws#101.\r\n Consistent API to get headers from play.mvc.Http.RequestHeader >>> 1"
532,## Fixes\r\n\r\nFixes #7172.\r\n\r\n## Purpose\r\n\r\nNow `play.server.akka.http.idleTimeout` is derived from `play.server.http.idleTimeout` so that we these existing options will be respected by Akka HTTP server backend. The docs were also improved to clarify these with Akka HTTP specific settings (in the namespace\r\n`akka.http.server`).\r\n\r\n## References\r\n\r\nSee also #7067.\r\n Consistent configuration for idleTimeout in Akka HTTP server backend >>> 1
533,Updates several dependencies to newer versions Update dependencies >>> 1
534,Fixes https://github.com/playframework/playframework/issues/6812 Add notes on TemporaryFile to Highlights26.md >>> 1
535,Upgrades to play-ws 1.0.0 Upgrade to play-ws 1.0.0 >>> 1
536,## Fixes\r\n\r\nFixes #7067 #7364. [docs]: Document Akka HTTP timeouts in the migration guide >>> 1
537,Explain in detail what each filter does and what to do about it. Add notes to migration about filters >>> 1
538,Fills out Migration26 with more links and fixes some typos. Add links to migration document >>> 1
539,Clarifies what the `removeAll` method is for and how to use it. Update CacheApi docs and fix test >>> 1
540,Fixes https://github.com/playframework/playframework/issues/7444 [2.5.x] Fix url encoded query parameter in WSRequest >>> 1
541,referred to the 2.5 migration guide in case users still depended on removed\r\nForms functionality.\r\n #7052 added note on migrating deprecated Forms code >>> 1
542,"Fill out the Java CSRF documentation and add HttpExecutionContext as a thing, rather than calling Controller.request() Add more links and docs to java csrf >>> 1"
543,"Amazingly enough, we don't mention global state in the highlights!\n\nAlso, it looks like `play.allowGlobalApplication` is also not mentioned in docs, so putting it front and center here. Add allowGlobalApplication to highlights >>> 1"
544,"Add more CSRF documentation, fill out migration classes with links and fix some broken links Add more links and docs >>> 1"
0,nan Add missing curved bracket >>> 1
1,R: @jeffposnick @petele \n- polyfill isntructions\n- BC demo updates b/c of https://bugs.chromium.org/p/chromium/issues/detail?id=645180\n Updates to the CE/SD v1 articles and BroadcastChannel demo >>> 1
2,FYI failing CI test can be ignored\n add docs for lighthouse offline audits >>> 1
3,nan add lighthouse security docs >>> 1
4,@addyosmani WDYT?\n [lighthouse] add doc for progressive enhancement audit >>> 1
5,nan Convert #3471 >>> 1
6,nan Enr1 >>> 0
7,R: @petele  \n Add npm post install script to run gulp build >>> 1
8,nan Housing case study >>> 1
9,Fixes #372 \n Recording Audio #372 >>> 1
10,"Heyo. This is a first-pass on PWA Architecture. I've tried to link to relevant /web docs where it made sense, take advantage of devsite styling for callouts and ensure `gulp test` passed. Images have all been run through ImageOptim.\n\nTodos:\n- [ ]  Tech writer review\n- [ ]  Add Vivian to the contributors list and add her to the author list on this doc\n- [ ]  Investigate other improvements I can make to the content\n Fixes #3460 - Adds Intro to Progressive Web App Architecture >>> 1"
11,cc @RByers @patrickhlauke\n\nThoughts on these changes? Anything we should be adding in?\n Tweaks to touch docs >>> 0
12,"nan add lighthouse intro, fixes #3446 >>> 1"
13,This article is about the basics of UX and covers topics such as wire-framing and user testing etc. It is related to https://github.com/google/WebFundamentals/issues/579\n Basics of ux article >>> 0
14,This PR addresses issue: https://github.com/google/WebFundamentals/issues/3511\n check in of web storage overview page >>> 1
15,Fixes #3459 \n\n**Notes:**\n- Initially ported from https://www.polymer-project.org/1.0/toolbox/server\n- Added an Introduction that sets the stage for why PRPL is needed\n- Generalized prose to suggest it could be accomplished using other libraries/tools (Polymer App Toolbox is just an excellent example of how to achieve this)\n- Added call outs to check out the Shop demo and a Timeline breakdown over remote-debugging of the key performance moments to keep an eye on\n- Added conclusion with thoughts on the need for architectures that focus on interactivity\n- Added link to Kevin's talk and an experimental label near the top (as previously discussed)\n- Pete: intentionally didn't hook this up to the nav yet. Assumed we would do similar to #3525. \n\n**Reviewers:**\nMessaging: @graynorton @tjsavage @slightlyoff \n/web inclusion: @petele \n Adds the PRPL pattern to /web >>> 1
16,@PaulKinlan LGTY?\n\nI also changed the title to make it moar action-y\n RAIL-ify /web/perf... fixes #3519 >>> 1
17,Closes #3464\n\nThis is almost a full copy of @igrigorik article on HPBN. I mostly stripped the very low-level parts at the bottom.\n\n@Meggin @addyosmani PTAL\n Port HTTP/2 article from HPBN  >>> 1
18,What are the next steps?\n Service worker lifecycle article >>> 1
19,"R: @petele @google/webfundamentals \n\nTry it in a page:\n\n```\n<pre class=""prettyprint"">\n{% includecode github_path=""GoogleChrome/chromium-dashboard/blob/master/scripts/deploy_site.sh"" %}\n</pre>\n```\n Adds support for includecode github_path >>> 1"
20,cc @jpmedley \n Fixing samples >>> 1
21,nan Push hackathon >>> 1
22,"nan check in Addy's ""Offline Storage for PWAs"" article >>> 0"
23,R: @petele @PaulKinlan @robdodson \n Live demos for CE and SD article >>> 1
24,"nan Add ""Architecting a PWA with the App Shell Model"" >>> 1"
25,As per @igrigorik’s [comment](https://github.com/google/WebFundamentals/pull/3547#issuecomment-250797047)\n Add disclaimer to http2 introduction >>> 1
26,nan Localized ID content for next-verion >>> 0
27,"I'm assuming our framebox code doesn't run on live, and its intent is to emulate live.\n\nThis builds on top of https://github.com/google/WebFundamentals/pull/3579 and adds a `sandbox` attribute. This means the iframe runs on a different origin, more like it does on live.\n Framebox emulate origin >>> 1"
28,nan Fullscreen Article #3583 >>> 1
29,I've made it up to `dom-order-matters.md` in the Focus section. Still got a ways to go. I'm doing a ton of editing and cleanup\n A11y playbook >>> 1
30,See #3599.\n Change heading to use HTML entities >>> 1
31,"Updated the article: ""Your First Progressive Web App"" (ZH-CN)\n [l10n] ZH-CN: Your First Progressive Web App >>> 1"
32,"I haven't proof-read this myself, and I don't really have an ending yet, but here it is.\n Article: async functions >>> 0"
33,nan Fixes two small typos >>> 1
34,Article on the upcoming ResizeObserver in M54. \n\nThis still needs a review as well as a copy edit.\n\nRelated sample PR: GoogleChrome/samples#424\n\ncc @atotic as an FYI\n /update: ResizeObserver >>> 1
35,@paullewis @paulirish @addyosmani @brendankenny here's the last of the reference docs for the currently-active Lighthouse audits\n\ncc @jpmedley in case you want to review\n [lighthouse] add moar audit references >>> 0
36,nan Edits >>> 1
37,"@petele I added a section to your ""responsive web design basics"" doc on using DevTools to view how a page looks under different media queries. Looks good to you?\n\nFYI I also cleaned up the accompanying DevTools sections that I link off to while I was at it.\n add devtools section to responsive basics doc >>> 1"
38,"As discussed, here’s a resource for how to include videos.\n Write including-videos resource >>> 0"
39,nan Add external macro to many links in ko/. >>> 0
40,nan removes max-width for videos - see #3668 >>> 1
41,@gauntface & @PaulKinlan I added some tools goodness to the web app install banner doc\n add lighthouse and devtools to a2hs doc >>> 1
42,@gauntface & @PaulKinlan I also added tooling goodies to the web app manifest doc\n add devtools and lighthouse goodness to web app manifest doc >>> 1
43,As discussed in #3668 \n Inline styles for videos in resizeobserver update article >>> 1
44,…ield name.\n\nPaymentResponse details for credit cards `cardholderNumber` field should be replaced with `cardNumber` reflecting Chrome 53 on Android implementation and the [W3C spec](https://www.w3.org/TR/payment-method-basic-card/#basiccardresponse).\n Replaced incorrect `cardholderNumber` with `cardNumber` credit card f… >>> 1
45,nan ci: deploy to/from master instead of next-version >>> 1
46,@petele I've double-checked this for other macros.\n Add external macro to many links in ko/. >>> 1
47,R: @petele @arthurevans @jpmedley \n shadow dom: fix typo >>> 1
48,"I've changed the examples to use `srcObject`, which is widely implemented now.\n\nCouple of nitpicks: the `capture` attribute is boolean, and the stream object from a `getUserMediaI()` call is a `MediaStream`.\n\nA few other suggestions below.\n- Maybe add catch clauses?\n- Might be worth mentioning the video option for the <input> method, i.e: \n\n`<input type=""file"" accept=""video/*"" capture />`\n- Maybe mention that you can set the canvas dimensions from videoWidth and videoHeight — and it's probably better to set the canvas dimensions in JavaScript rather than in the HTML.\n- With the examples (e.g. upload and image on a canvas to a server) I wonder if it would be worth adding how-to suggestions?\n- You might want to link to samples:\n  - [simpl.info/mediacapture](http://simpl.info/mediacapture/)\n  - [webrtc.github.io/samples/src/content/getusermedia/gum](https://webrtc.github.io/samples/src/content/getusermedia/gum/)\n  - [webrtc.github.io/samples/src/content/getusermedia/canvas](https://webrtc.github.io/samples/src/content/getusermedia/canvas/)\n Tweaks to capturing-images article >>> 1"
49,fixing broken link reported from community --> https://twitter.com/bradydodd/status/786585438525857792\n add redirect for borked link >>> 1
50,@igrigorik LGTY?\n add lighthouse (and devtools note) to crp docs >>> 1
51,nan Heading updates based on test of edit-assist script. >>> 1
52,nan Add external macro to many links in it/. >>> 1
53,Fixes #3689 \n Update README.md >>> 1
54,"@petele this is good to go\n\nFYI this is a duplicate of #3631... that PR was set up against `next-version`... I tried to set the base to `master` but was getting some weird merge conflicts... It was faster to just open a new PR against `master`.\n\nIn #3631 I was waiting if any of the LH team wanted to provide feedback, but that ain't gonna happen, so we good to merge here\n add moar lighthouse references >>> 1"
55,"Hi @petele, \nCC: @tyohan, @mychaelgo,\n\nI have finished my next Indonesian Translation of Web - Building for Billions /web/billions/ section. \n\nI have tested it running on my local development version. Here's the screenshot:\n![web-billions](https://cloud.githubusercontent.com/assets/15354/19414058/d3c26b08-936b-11e6-99d4-b4820c66fd73.png)\n\nPlease review it.\n [Bahasa Indonesia] - ID Translation of /web/billions/ >>> 1"
56,"Hi @petele, \nCC: @abdshomad\nI have finished my Indonesian Translation of Web - Understanding Low Bandwidth and High Latency /web/fundamentals/performance/poor-connectivity/ section.\n\nplease review it \n [Bahasa Indonesia] ID transalation fundamentals/performance/poor-connectivity >>> 1"
57,"Hi @petele, \nCC: @tyohan, @mychaelgo, \n\nI have finished my next Indonesian Translation of Web Fundamentals for fundamentals/getting-started/ section. They consists of 16 files, where 14/16 files are include files. \n\nI have tested it running on my local development version. Here's the screenshot:\n![getting-started](https://cloud.githubusercontent.com/assets/15354/19411521/91e6f0ac-932d-11e6-9595-8a2dd6cfdece.png)\n\nPlease review it. \n [Bahasa Indonesia] - ID Translation of fundamentals/getting-started/ >>> 1"
58,"Hi @petele, \nCC: @tyohan, @mychaelgo,\n\nI have finished my next Indonesian Translation of Web - Building for Billions /web/billions/ section.\n\nI have tested it running on my local development version. Here's the screenshot:\n![your-first-pwapp](https://cloud.githubusercontent.com/assets/15354/19419091/72a99ece-93fa-11e6-9be5-5df48af473c5.png)\n\nPlease review it.\n [Bahasa Indonesia] - ID Translation of /fundamentals/..../your-first-pwapp/ >>> 1"
59,nan More updates from edit assist testing. >>> 1
60,nan Add external macro to file in hi/. >>> 1
61,nan Add exernal macro to links in he/ and fr/. >>> 0
62,nan Update sending-messages.md >>> 1
63,@marcacohen I added a DevTools section to your web storage doc. LGTY?\n add devtools section to storage doc >>> 1
64,Rebased for master and fixed a few minor issues.\n async article - replaces #3617 >>> 1
65,nan Adding some caveats to bugs the team found #3714 >>> 1
66,This is the post to support the Origin Trial of `navigator.share`\n Navigator.Share post >>> 1
67,"This supercedes existing PR (https://github.com/google/WebFundamentals/pull/3560),\nbecause next-version branch on which it was based got merged into master and a\nfresh PR seemed less hassle than the rebase drill.\n check in revised version of Addy's ""Offline Storage for PWAs"" article >>> 1"
68,nan Add exernal macro to links in he/ and fr/. >>> 1
69,"R: @petele \n\nArticle to accompany https://www.chromestatus.com/feature/5663174342737920, shipping in Chrome 55.\n\nThis has been internally reviewed by @jpmedley and the relevant engineering team.\n auxclick /web/updates article >>> 1"
70,nan tweaks to sharing update >>> 1
71,nan Fixing a minor set of nits from eng about sharing. >>> 1
72,PTAL @jpmedley.\n captureStream() post  >>> 1
73,"/web/updates article to accompany https://www.chromestatus.com/feature/5630331130478592, launching in Chrome 55.\n once support in addEventListener() >>> 1"
74,"I add houdini.md zh-cn version,thanks\n add houdini.md zh-cn version >>> 0"
75,"The content of the /web/updates post to accompany https://www.chromestatus.com/feature/4504699138998272, along with an entry in the contributors file for sgomes.\n\nI'm leaving things as a `<table>` rather than trying to find the markdown equivalents.\n Pointer Events updates post >>> 1"
76,nan Correcting babel reference >>> 1
77,nan Create chrome-55-deprecations.md >>> 1
78,Addressing some feedback from Twitter: https://twitter.com/RReverser/status/789160090624557056\n Tweaks to the addEventListener once post >>> 1
79,nan Correct more titles using script. >>> 1
80,nan Add ext macro to remaining zh-tw/ links. >>> 1
81,nan Adding touch action post >>> 1
82,nan Minor fixes to navigator-share.md. >>> 1
83,nan Copy edit pointer-events.md >>> 1
84,nan Nit: added apostrophe >>> 1
85,nan Updated Persistent Storage Update for removing from Origin Trial. >>> 1
86,The original video had audio (new video capture app and didn't realise it recorded any audio on device). New video has audio removed.\n Update touch-action.md >>> 1
87,As mentioned/commented on https://plus.google.com/u/0/+PatrickLauke/posts/XWqH69h7h75\n Fix/clarify Pointer Events update >>> 1
88,"The sentence wasn't really clear to me at first, and it took me a while to figure out the meaning of it. It may be just that English is not my native language, but I figured I'd suggest a more explicit version of the same sentence.\n\nHope this doesn't change the intended meaning and that it's written in correct English\nGreat article though,\nKeep up!\n [Async functions] Make return values explaination more explicit >>> 0"
89,"There were some last minute changes that came in from the Chrome team (not to the API), but\nrather some clarifications and a shorter URL for the orgin trail.\n navigator.share late changes and clarifications from Chrome team >>> 1"
90,"It's a really small change, but I hope it can help!\n\nKeep up!\n [Italian] - Translation of the ""caution: article may be outdated"" paragraph >>> 1"
91,I'll leave this open for a bit in case anyone on DevTools team wants to review...\n add oct 2016 devtools digest >>> 1
92,First work on Developer Feedback page\n New /web/feedback section >>> 1
93,nan Manage Hyphens with CSS >>> 1
94,See https://mathiasbynens.be/notes/async-analytics-snippet#async.\n Remove needless `scriptElement.async = true` >>> 1
95,Just replacing the slightly old screenshots with fresh ones. This should be good to go.\n update screenshots on devtools homepage >>> 1
96,@petele PTAL!\n Adding an avatar for sgomes. >>> 1
97,nan Clarify non-blocking in async await article >>> 1
98,nan Clarifying return behaviour >>> 1
99,"@petele This lacks a contributor because the author has yet to add himself to the contributors. We need to go ahead and publish this.\n Add the article titled, ""Avoiding the Not Secure Warning in Chrome"". >>> 1"
100,nan Add ericlawrence to Contributors >>> 1
101,Added a little content to readme\n Update README.md >>> 1
102,nan Fix broken image link. >>> 1
103,Add an image for use on CSS articles and any other place it may be appropriate.\n Add styles image for CSS articles on updates/ >>> 1
104,nan remove email from rss feeds >>> 0
105,nan Update gulp link >>> 1
106,nan Fixes #3777. Fix broken link >>> 1
107,@petele PTAL\n Let non-GDG groups add their events to the map >>> 1
108,"Since writing the original article, we've landed a few tweaks to Canary. My updates keep the WebFundamentals article in sync with what we're shipping.\n Update ""Avoid Not Secure Warning"" with latest text >>> 1"
109,Changed cross-origin to cross-site to reflect the eTLD+1 mitigation.\nAdded the criteria which specifies that only cross-site scripts are subject to the intervention.\n Cross-site criteria >>> 1
110,"Hey hey @petele I'm piloting interactive tutorials in the DevTools docs. Pretty excited about that. Did I use `framebox` correctly here? Also, I added a little styling to make inline images display more nicely. I'd like to add this as a general rule but I believe I'm blocked on that (you can ping me if you don't know what I'm talking about). \n\ncc @jpmedley in case you want to writer's review\n\nNotes for posterity:\n- I'm also testing out using less screenshots and more emphasis on UI labels / small inline images of relevant UI icons. If the docs are still usable like this, then this will aid doc longevity.\n introduce interactive tutorial in DOM breakpoints section >>> 1"
111,It looks like new anchors include `_` instead of `-`. \nThis patch fixes WebUSB article.\n\nR @petele \n Update anchor >>> 1
112,"I translated to italian the description of the udacity course: Critical Rendering Path. \n\nIt relates to #3769 \n\nPlease let me know if there's something wrong with how I performed the PR, I'm still relatively new to this\n Translate to italian: udacity critical rendering path course >>> 1"
113,We had a few messages saying it was a little confusing so I have clarified the usage. Clarifying the use of text or url in share article >>> 1
114,"nan Fix typo in the word ""notification"" >>> 1"
115,undefined variable in error handler. Fixes #3807 - undefined variable >>> 1
116,Added links to a couple tools that help in finding mixed content when CSP is not viable.\n Offer links to CSP alternatives? >>> 1
117,r: @addyosmani @jpmedley \r\nfyi: @samccone performance/prpl: Adjust wording around HTTP/2 push >>> 1
118,"As discussed in #3811 ([comment](https://github.com/google/WebFundamentals/pull/3811#issuecomment-257853212)).\r\n\r\nr: @addyosmani @igrigorik  performance/prpl: Use `<link rel=""preload"">` instead of ""Resource hints"" >>> 1"
119,"R: @kaycebasques \r\n\r\nAdds LH to the tools page and updates logos to newer versions\r\n\r\n<img width=""1082"" alt=""screen shot 2016-11-01 at 3 26 39 pm"" src=""https://cloud.githubusercontent.com/assets/238208/19910011/a609d564-a047-11e6-9fd8-82af11e9401b.png"">\r\n\r\n<img width=""681"" alt=""screen shot 2016-11-01 at 3 43 25 pm"" src=""https://cloud.githubusercontent.com/assets/238208/19910405/f1c89efc-a049-11e6-9551-a1cbfe576c9d.png"">\r\n\r\n Add Lighthouse to tools landing page >>> 1"
120,R: @petele \r\n\r\nMissed pushing the polymer logo update Update polymer logo to 2016 version >>> 1
121,R: @scheib Add new flag in bluetooth article >>> 1
122,…me Dev Summit check in Dru Knox' recommended changes in support of his talk at Chro… >>> 1
123,R: @petele  Update transferable objects post >>> 1
124,nan Refresh the Libraries landing page. >>> 1
125,nan Fix typo. >>> 1
126,nan Add comments widget. >>> 1
127,nan Add links to the sample. >>> 1
128,nan Refresh library landing page (replacement) >>> 1
129,"It is a PR for Japanese translation. :jp: I fixed it because there were differences between ja and en. See original sentence below:\r\n\r\nEnglish:\r\n\r\n```\r\nThe CSSOM and DOM are independent data structures!\r\n```\r\n\r\nJapanese:\r\n\r\n```\r\nCSSOM と CSSOM は独立したデータ構造です。\r\n```\r\n\r\nIn Japanese sentence, 2nd ""CSSOM"" should be ""DOM"" so I fixed it.\r\nPlease review it. \r\n\r\ncc:/ @agektmr  Fix Japanese Translation >>> 1"
130,"Not sure when this was changed, but this worked as of Chrome 53, if not earlier.\r\n\r\nAnother good FAQ would be whether this works on desktop Chrome. theme_color does work on any site now >>> 1"
131,nan Fix the animation. >>> 1
132,nan Add links to Credential Management API article >>> 1
133,cc @jpmedley @petele Would like to get this landed as soon as possible. Updating Push Codelab >>> 1
134,cc @RByers @patrickhlauke @jpmedley  Adding changes from previous PR >>> 1
135,Added new article. Awaiting review.  Adds offline UX guide >>> 1
136,nan Corrected typo in description >>> 1
137,"I'm going to do a round of bug fixes, but wanted to get this out for review as soon as possible. Adding new credential management docs >>> 1"
138,nan Fix tools link on /web landing page. >>> 1
139,nan Add links to the cm api integration guide >>> 1
140,Translating this article to brazilian portuguese\r\n\r\nAddress of the issue https://github.com/google/WebFundamentals/issues/3851\r\n\r\nThe name I used to sign the CLA is Osvaldo Abel. Create why-https.md >>> 0
141,"nan Typo in ""vendor"" >>> 1"
142,R: @g-ortuno\r\n\r\n![screenshot 2016-11-14 at 12 08 04 pm](https://cloud.githubusercontent.com/assets/634478/20262574/0f83b81e-aa63-11e6-8196-8680809eaba6.png)\r\n Add warning for bluetooth device disconnection >>> 1
143,nan Strict date checking on wf_updated|published_on >>> 1
144,nan Fix a few typos. >>> 1
145,R: @g-ortuno \r\n\r\n@petele Please wait for @g-ortuno to LGTM before merging ;)  Fix bluetooth nits >>> 1
146,nan Fix some inconsistency in usage of 'Payment Request'. >>> 1
147,"nan Change ""Code Labs"" to ""Codelabs"" per style guide >>> 1"
148,nan Adds PWAR SEA banner >>> 1
149,"@petele I plan to do more of this kind of thing. I stumbled on this and thought, it's (1) a critical issue and (2) the vids are a pretty apples-to-apples swap. Swap last years CDS presentation for this years. >>> 1"
150,"Fixes https://googlesamples.github.io/web-fundamentals/fundamentals/design-and-ui/input/touch/touch-demo-1.html and https://googlesamples.github.io/web-fundamentals/fundamentals/design-and-ui/input/touch/touch-demo-2.html (linked quite prominently from https://developers.google.com/web/fundamentals/design-and-ui/input/touch/) which are currently completely borked on touch-events browsers.\r\n\r\nFurther fixes a small typo ""None selectable"" > ""Non-Selectable"" (though I don't think that demo's actually linked to from anywhere) Fix web fundamentals design and UI input touch demos >>> 1"
151,nan Change link and fix typos. >>> 1
152,nan Fix typos. >>> 1
153,nan NIC55 post >>> 1
154,nan Add HTTP Udacity course to the other ones >>> 1
155,Authored these a while ago but forgot to PR... add lighthouse manifest audits docs >>> 1
156,cc @brendankenny  add note about input latency probabilities >>> 1
157,@Meggin please review add debugging js overview and get started >>> 1
158,"nan Moved Payment Request stuff to ""Discovery and Monetization"" >>> 1"
159,nan Fix broken paragraph per Owen's edit. >>> 1
160,"I got some feedback that the purpose of this section was confusing, so I updated the section title and note. I may need to change it again, so I'm using an include.\r\n\r\n@petele the [includes](https://developers.google.com/web/resources/widgets#includes) references says that an include must be HTML, and that Markdown is not supported. However when I run the site locally the Markdown is working just fine. So, is that doc outdated? Can I use Markdown? update section title in lighthouse references >>> 1"
161,"'Twas in a nonsensical location in the IA (under the ""Inspect CSS and HTML"" section, when it should be top-level) and included redundant mouse info and irrelevant Chrome browser shortcuts. clean up devtools shortcuts doc >>> 1"
162,"R: @petele \r\n\r\nRe: #3883, here's an article with guidance about the timing of your service worker registration.\r\n\r\nI'm going to leave #3883 to track updating our other samples/articles to start using this new pattern when appropriate.\r\n\r\nThe text has been previously reviewed, and @jpmedley has already given it an edit. ""Service Worker Registration"" article for /fundamentals >>> 1"
163,"This started in an email thread with @tdresser. The RAIL doc was confusingly referring to a budget of 16ms for animations. This PR rephrases the stuff to make it clear that animation code should finish in 10ms, so that the browser has time to paint the new frame to the screen within the 16ms budget. \r\n\r\nPR also includes some minor copyedit updates\r\n\r\n@tdresser please review rephrase rail guidelines >>> 1"
164,Thanks to @amedina for spotting these Fix blatant typos >>> 1
165,PTAL @PaulKinlan  progressive-web-apps/checklist: Fix typo >>> 1
166,nan Fix link that prevents publishing. >>> 1
167,https://github.com/google/WebFundamentals/issues/3566 Fix format names. >>> 1
168,nan add passive event listener audit reference >>> 1
169,nan Fix typo: dispalyed --> displayed >>> 1
170,misspelt the word queue 👎  Fixed a spelling mistake in the image >>> 1
171,PTAL @petele @PaulKinlan  Adds parallax article >>> 1
172,Forgot to add a redirect to Android Pay article thanks to @afitz0 for letting me know. Redirect Android Pay integration guide >>> 1
173,"@PaulKinlan @paullewis \r\n\r\nFYI and if one of you could give me an approval so I can merge it, I'd be appreciative. Parallax edit >>> 1"
174,@PaulKinlan @petele @jpmedley @beaufortfrancois @miguelao  PTAL.\r\n Added ImageCapture WF Update article >>> 1
175,nan Fix Markdown formatting and Find capitalization >>> 1
176,"Changes are described in [this doc](https://docs.google.com/document/d/1I8ha1ySrPWhx80EB4CVPmThkD4ILFM017AfOA5gEFg4/edit#).\r\n\r\nIn addition to above,\r\n- Changed the link to Android Pay sign-up page\r\n- Fixed the wording around `abort()` as per described in #3153\r\n\r\ncc: @jpmedley @zkoch @asieke @rsolomakhin Reflect Payment Request API updates in M56 >>> 1"
177,FYI: @paullewis \r\nr: @PaulKinlan please also redeploy so we can start sharing :) Add live demo to parallaxing article >>> 1
178,"nan Fix typos, punctuation, style. Link to Network Information API. >>> 1"
179,R: @petele @jpmedley \r\n\r\nSmall update to @gauntface's SW primer to call out the pattern of waiting until the page has loaded to perform SW registration.\r\n\r\nFixes #3883 Tweak to the SW primer to use the delayed SW registration pattern. >>> 1
180,nan Fix 404 sw-offline-google-analytics >>> 1
181,"Fixes https://github.com/google/WebFundamentals/issues/3920\r\n\r\nRemoves drop-off sentence, that seems out of place in the current paragraph. Update index.md >>> 1"
182,nan A2HS guideline to intercept the install prompt event >>> 1
183,"The green/red arrow in the flow chart are completely indistinguishable for me, and others with some level of colorblindness. The purposed change is more colorblind friendly. Update promise-flow.svg to be colorblind friendly >>> 1"
184,https://github.com/google/WebFundamentals/pull/3922 Change text to match color-blind friendly image. >>> 1
185,nan [DRAFT] Deprecations and Removals in Chrome 56 >>> 1
186,nan Fix typo. >>> 1
187,nan Fix typo. >>> 1
188,nan Fix title formatting. >>> 1
189,PTAL: @jpmedley  Adding position:sticky article for #3928 >>> 1
190,"ok @ebidel you're up, plz review add do better web audit references >>> 1"
191,"Specifically, ^#[^#].* was catching CSS id selectors as well as legitimate markdown. We will probably need to revisit this. Eliminate false posatives on test for titles. >>> 1"
192,@petele \r\n\r\nThese were also causing test failures when I tried to publish. I judged that 'sticky' was too specific because we don't have other CSS rules and selectors in the approved list. 'Parallax' and '3d' seemed better candidates. Fix tags for new articles. >>> 1
193,"This reverts commit cb21b3faf409b8cd2faaaab0fd0cdbbf5777fa55.\r\n\r\nDevsite doesn't like script tags. I'm resubmitting the article in a separate CL so that I can publish. Revert ""Added ImageCapture WF Update article (#3907)"" >>> 1"
194,nan Fix typo smack in the lead sentence >>> 1
195,nan Adding author to position sticky >>> 1
196,"PTAL @PaulKinlan @surma.\r\n\r\nI've not added this to _book.yaml yet, just while it gets a final look over. Adds WebVR section >>> 1"
197,The current sample code requires JS to execute completely. Modified it to assume it is overloading a standard anchor. Added specific note about this practice for developers so it is clear how they should do it in their own apps without reading the complete code sample. Improve docs and sample code for progressive enhancement. >>> 1
198,- iframe[alllowpaymentrequest]\r\n- removed `currencySystem`\r\n@rsolomakhin @zkoch @jpmedley  Additional changes in PR API for M56 >>> 1
199,"Updating Bug ID, as per Scott Main's request. Update _project.yaml >>> 1"
200,Fixes some old screenshots and instructions.\r\n\r\nAlso includes a feedback approach that I'm experimenting with. update devtools remote debugging get started doc >>> 1
201,nan Remove extra line break. >>> 1
202,Just updating an image that was incorrect.  updated image for double diamond >>> 1
203,Variable names are mixed up: 'payment' and 'request' are used interchangeably. Changed 'payment' instances to 'request'. Inconsistent var's: change 'payment' to 'request' >>> 1
204,nan Move wrong `external` flag in toc >>> 1
205,nan [lighthouse] fix doc title and link in toc >>> 1
206,nan Add 'URL Bar Resizing' article. >>> 0
207,nan [devtools] add cpu profile migration post >>> 1
208,"Two lists have nested (level 2) items, but these are incorrectly displayed as level 1. See b/33678820. Fix typos and formatting >>> 1"
209,"<img width=""478"" alt=""screen shot 2016-12-15 at 16 32 24"" src=""https://cloud.githubusercontent.com/assets/248078/21247417/17414fe6-c2e4-11e6-9191-3866ed11b58d.png"">\r\n\r\nNote that the the live Web**Fundamentals** website does not specify a favicon in the HTML, and falls back to the domain's favicon. Add Retina resolution to favicon. >>> 1"
210,nan Added bokan's contributor photo >>> 1
211,"nan Update images for ""Avoiding the Not Secure Warning in Chrome"". >>> 1"
212,nan Added bokan to _contributors.yaml >>> 1
213,"To find unused images I ran:\r\n\r\n```sh\r\n#!/bin/bash\r\n\r\n# Execute this from /src/content/en/tools/chrome-devtools\r\n\r\nfunction search {\r\n  # Provides just the filename of each image, e.g. `pause.png`\r\n  for img in $(ls $1); do\r\n    # Check if any of the files mention the image filename\r\n    if ! grep -qR $img *; then\r\n      rm $1$img\r\n    fi\r\n  done\r\n}\r\n\r\n# Recursively search for all directories named `imgs` or `images`\r\n# All of the image directories use one of these two names\r\nfor dir in $(ls -d -R */*im[ag][gs]*/); do\r\n  search $dir\r\ndone\r\n```\r\n\r\nExecuted from `/src/content/en/tools/chrome-devtools/`.\r\n\r\nI then ran a local server, and checked each page with DevTools open. DevTools didn't produce any 404s so I'm reasonably confident that the script worked as intended\r\n\r\nProduces a 9M reduction in the directory size. I was hoping for more, but hey, every bit counts. Removing all those unused files should help reduce cognitive burden for docs maintainers, too [devtools] delete old images >>> 1"
214,"R: @kaycebasques @petele \r\n\r\nThis should be merged after we've cut LH 1.3.0. I'll let ya'll know when that is. Feel free to leave comments in the meantime.\r\n\r\n@kaycebasques, eventually we'll want to update the screenshots in https://developers.google.com/web/tools/lighthouse/. Lighthouse 1.3.0 update >>> 1"
215,"* quotes weren't displaying properly in the title so I reworded the title to avoid the problem completely\r\n* framebox wasn't displaying properly b/c I forgot the `width=""auto"" height=""auto""` attributes [devtools] fix title and feedback box in cpu profiling migration post >>> 1"
216,nan Fixed typos & added Alt Text >>> 1
217,* added a link to bottom of each doc to the next doc in the series\r\n* using google analytics events to track how much the links are clicked\r\n* (unrelated) cleaned up the ToC a little [crp] add links to next docs >>> 1
218,nan Fix typos in Understanding resource timing >>> 1
219,nan Fix code block >>> 1
220,@petele Can you review/merge when you have some time? Promote pointerup User Gesture event >>> 1
221,"The PR closes #3980 where the typo in file Web-apps-that-talk-Introduction-to-the-Speech-Synthesis-API has been corrected. Closes #3980, Fixed the example code >>> 1"
222,Fixes requested in https://github.com/google/WebFundamentals/pull/3963 since Joe is out and I can't modify that PR. Add 'URL Bar Resizing' article >>> 1
223,"devtools team changed the layout on performance panel a day after I published 😁 \r\n\r\nI also updated images to include figure caption text, to align with DevRel doc style. Need to start getting used to this. Added some custom CSS so that the captions are centered\r\n\r\n@petele if this makes it in before the holidays, great, if not then a few peeps will prob notice but should be ok [devtools] update cpu profile migration post >>> 1"
224,nan Fix demo link in 'URL bar resizing' >>> 1
225,@Meggin here's another batch of devtools docs refactors fer yer review\r\n\r\nI'll add some notes to the files in a min [devtools] refactor network docs >>> 1
226,nan Minor corrections to SW article >>> 1
227,nan Fixes typos >>> 1
228,"Adding these features to the PWA checklist as requirements to hit the 'showcase' bar, if using them is appropriate for the use case, in accordance to internal discussion with sabineb and agektmr.\r\n\r\nNote these are not required for the baseline bar.\r\n\r\nI've not squashed these as I hear GitHub has a new squashing capability when accepting PRs, but let me know if you'd like them squashed on my end.\r\n Add Credential Management and Payment Request to PWA checklist >>> 1"
229,@ebidel would you plz review the app cache doc? add appcache audit & rename template >>> 1
230,R: @petele \r\n\r\nSee https://twitter.com/ChromiumDev/status/814386096876486656 Mention new Web Bluetooth module for Angular >>> 1
231,"Since Chrome 55, `startNotifications` returns `characteristic`. This patch updates notifications sample to reflect this change.\r\n\r\nR: @petele  Fixed nit in notifications sample >>> 1"
232,"nan Fix link ""Interact from Command Line"" >>> 1"
233,nan correct Chinese language short stand >>> 1
234,nan Add a URL that works. >>> 1
235,nan Add dtapuska contributor info >>> 1
236,nan Should fix #4008 - CSS is visible in the desc >>> 1
237,R: @petele heads up Adding wcorg post >>> 1
238,https://github.com/google/WebFundamentals/issues/4017 Add test for js files. >>> 1
239,FYI: @petele @jpmedley  Scrolling Intervention post. >>> 1
240,nan Fix typo. 📝 >>> 1
241,Copyediting for days. 😎 More typos. 📝 >>> 1
242,nan Should fix issue #3998 >>> 1
243,nan Fix typo. >>> 1
244,fixes #4023  add redirects for common 404s in tools directory >>> 1
245,nan URL bar resizing: add fullscreen scenarios >>> 1
246,hey @robdodson \r\n\r\nThis is the general direction that I'm planning on taking with the a11y audit references. Plz review and let me know if I need to change major course on anything [lighthouse] add a11y audit >>> 1
247,Adds various updates to the Payment doc.\r\nCan you please do a final review and approve? @rsolomakhin\r\nOne change I might be missing is if I should note that you can't ignore `shippingoptionchange` event. Can you confirm? Payment request update >>> 1
248,nan Fix grammar. >>> 1
249,Typo fix. Update app-shell.md >>> 1
250,nan Offline Storage for Progressive Web Apps. Should fix #4001 >>> 1
251,nan New episode >>> 1
252,Markdown required indentation level of 4 to correctly parse sub-lists. Resource timing list indentation fix (issue #3986) >>> 1
253,nan Add new stat to AliExpress showcase >>> 1
254,nan Update interstitial guidance >>> 1
255,@garanj @inexorabletash - added links to my libs. Are they worth adding? Promise I won't be offended if you disagree. Adding links to small key-val stores & promise wrappers >>> 1
256,This PR is still WIP and is used to get feedback from engineers.\r\n\r\nWDYT @mounirlamouri @xxyzzzq? Add WIP Media Session API updates article >>> 1
257,nan Chinese translation samples for index on DevTools >>> 1
258,"R: @kaycebasques @petele \r\n\r\nThis PR updates https://developers.google.com/web/tools/lighthouse/ with newer screenshots, adds and install and file a bug call to action, and adds new section on updated LH features.\r\nIt also updates the /tools landing page to raise LH above the fold.\r\n\r\ncc @brendankenny @paulirish @patrickhlauke \r\n\r\n![screen shot 2017-01-18 at 10 09 46 pm](https://cloud.githubusercontent.com/assets/238208/22095582/ee31598c-ddca-11e6-89a6-6d5521424016.png)\r\n\r\n![screen shot 2017-01-18 at 10 09 51 pm](https://cloud.githubusercontent.com/assets/238208/22095583/ee31719c-ddca-11e6-9fe4-f6e3caf43ec9.png)\r\n Update Lighthouse page >>> 1"
259,"@petele check this out\r\n\r\nMy goal is to create a reusable feedback widget that I can inline at the end of DevTools tutorial sections. This will give me much more granular information about where exactly people are failing.\r\n\r\nI've put the widget inside of a framebox because I want to disable the buttons after the user clicks one of them, as a means of preventing them from clicking both buttons and distorting the data.\r\n\r\nI made it pretty customizable, because I think it'll get boring if the success and response items say the same thing every time. Switching up the button / response texts should help keep the user engaged and participating.\r\n\r\nWhen I run this on the local server, the `{% setvar %}` tags aren't working properly, but I think I'm doing it correctly. See #4041 \r\n\r\nDitto for the framebox. I think I'm doing that one correctly, too, though. I think the local server just isn't set up to support these features\r\n\r\nGiven this, I'm not sure if the widget is going to display correctly. If this pattern looks OK to you, we can just check it when it's on staging\r\n\r\nNote: I also moved some common styles to another file and removed some comments. These aren't related to the feedback widget [devtools] add reusable feedback widget >>> 1"
260,"I was listening to the HTTP 203 podcast and I struggled to control the audio because the player was too small. So I make it bigger :)\r\n\r\n<img width=""914"" alt=""small"" src=""https://cloud.githubusercontent.com/assets/4459232/22158538/fd5c787e-df3c-11e6-8f83-07094bc7ff67.png"">\r\n\r\n<img width=""922"" alt=""big"" src=""https://cloud.githubusercontent.com/assets/4459232/22158537/fd593a2e-df3c-11e6-9703-27914a4c7b0e.png"">\r\n\r\nLet me know if I put the css on the wrong file, I couldn't find a better place. :)\r\n\r\nCloses #4064 Make the podcast audio player bigger >>> 0"
261,@robdodson here's the rest of the a11y docs... plz review [lighthouse] add docs for a11y audits >>> 1
262,Step-over executes the code on the current line rather than the code on the next line. Update step-code.md >>> 1
263,"cleans up the homepage and fixes #4040  [devtools] add some sweet, sweet polish >>> 1"
264,nan New In Chrome 56 updates post >>> 1
265,"This addresses some feedback I got about this doc, which is really just a reference on some common UI elements. The updated title and intro should clarify the purpose of the doc. I also polished up the sections and moved some sections to more appropriate places in other docs. [devtools] polish the UI reference >>> 1"
266,Here's what I've got for the landing page.\r\n\r\n![screen shot 2017-01-25 at 1 17 41 pm](https://cloud.githubusercontent.com/assets/718038/22303336/da624c1e-e300-11e6-901d-f0e5cfc213e5.png)\r\n Dev Feedback on Home Page >>> 1
267,To use the `gulp build` and `gulp test` commands described on the README file was necessary to have gulp installed globally.\r\n\r\nSo I added two npm scripts to simplify the usage and to use the gulp version defined as a dependency.\r\n\r\nCloses #4065 Simplify the Gulp usage described on the README >>> 1
268,"Otherwise, the sentence reads like ""Developer"" is the subject and ""defined"" is the verb,\r\nmaking the sentence as a whole ungrammatical. scrolling-intervention: Add missing dash in adjective ""Developer-defined"" >>> 1"
269,@rsolomakhin Can you review and approve these changes? Thanks! Payment Request changes for M57 >>> 1
270,nan Fixed typo in landing first paragraph >>> 1
271,@petele @jpmedley I believe this is kinda urgent as the article is linked to by the upcoming Chrome blog post. I’d appreciate if we can make this go live ASAP. Add css-grid update article >>> 1
272,R: @petele Update Web Bluetooth article for M56 >>> 1
273,Fix nudge variable & add missing closing brace for switch statement. Fix #4086 sample code is incorrect >>> 1
274,nan [WIP] Deprecations and Removals in Chrome 57 >>> 1
275,@petele any advice on image resource location change sync between EN and CN? The problem should be the same in other translations. update with original update   >>> 1
276,"We would like the ability to link to WebVR specific showcases from the WebVR section of fundamentals.\r\n\r\nTo that end I have extended the `tags` feature of `/updates/` so that it will work for `/showcase/` too.\r\n\r\nActual showcases with the `webvr` tag are still incoming, but for testing you can add `{# wf_tags: webvr #}` (or whatever tags you like) to random case studies and rebuild.\r\n\r\nR: @petele @PaulKinlan \r\n\r\n@meganlindsay: FYI. The resulting pages will be just like https://developers.google.com/web/updates/tags/chrome55 only they will say ""Showcases"" instead of ""Updates"" at the top. Extend the tags system for /updates/ to work for /showcase/ >>> 1"
277,nan Move deprecation policy to canned widget >>> 1
278,@jpmedley - missed this one in chrome 54 deprecations. Remove missed deprecation policy blurb >>> 1
279,dear Fundanmentals.\r\nNow i complete translation lighthouse overview into korean.\r\nnext i'll translate audits sections.\r\n\r\nPTAL.\r\nthanks. complete translation lighthouse overview into korean >>> 0
280,dear Fundanmentals.\r\nNow i complete translation lighthouse overview into korean.\r\nnext i'll translate audits sections.\r\n\r\nPTAL.\r\nthanks. complete translation lighthouse overview into korean >>> 1
281,R: @petele   Fixes #4101 - broken links in CDS update >>> 1
282,It's been a while Pete. Translated the overview of Engage-and-Retain in Korean >>> 1
283,Forgot to add my credit in the article. Add a credit >>> 1
284,dear fundamentals.\r\nnow complete translate overview for devTools.\r\nPTAL. translate overview within devTools >>> 1
285,Fixes #3948. Updated/fixed simpl.info links >>> 1
286,"Hello @jpmedley,\r\n\r\nThis PR fixes some nits in the `BluetoothDevice.uuids` deprecation article. Fix BluetoothDevice.uuids deprecation nits >>> 1"
287,"@agektmr, this PR fixes nits. Fix nits >>> 1"
288,FIX: https://github.com/google/WebFundamentals/issues/4118 Rickrolled all artwork images >>> 1
289,Fixes #4098. Fixed links >>> 1
290,@owencm @petele PTAL. Improved add to home screen >>> 1
291,These are the showcases for the WebVR launch. Preview at https://wf-staging-mscales.appspot.com/web/showcase/2017/ WebVR showcases >>> 1
292,nan Updates to the FAQ for improved A2HS based on feedback >>> 1
293,translated browser-customization into korean. complete translation browser-customization into korean >>> 1
294,"nan correct Console panel ""Multi-line entry"" shortcut >>> 1"
295,"* Updates screenshots to use the new standard.\r\n* Uses consistent, general step-by-step instructions on how to set each type of breakpoint.\r\n* Adds some overview information to top of doc explaining when to use each type.\r\n* Updates link text and URLs in other docs.\r\n* Changes the DOM change and exception breakpoint sections back to the standard format for guide sections. For a while I was experimenting with an interactive tutorial format for each of these sections. These are largely unnecessary now, now that the Get Started Debugging JS guide is shipped. It was also weird to have sections that were pretty much tutorials within a guide.\r\n* Changes title of doc to ""Breakpoints Guide"". Eventually, all the DevTools docs will follow this format.\r\n* Renames URL to `breakpoints` from `add-breakpoints` and redirects the old to the new. Kinda unnecessary, but I really like clean URLs [devtools] polish breakpoints doc >>> 1"
296,FIX: https://github.com/google/WebFundamentals/issues/4140 Improve bluetooth request device documentation >>> 1
297,What's new?\r\n\r\n- `new Uint8Array([...])` -> `Uint8Array.of(...)`\r\n- `acceptAllDevices` key\r\n- shim in noble\r\n\r\nR: @g-ortuno @scheib Update bluetooth article with latest news from M56 >>> 1
298,Staged here: https://milkbone.googleplex.com/web/fundamentals/getting-started/primers/media-source-extensions Media Source Extensions Article >>> 1
299,Sign up link for Android Pay web needs to go to: https://goo.gl/forms/dga1yH1MYcA6QR8x2 Updating Android Pay registration link >>> 1
300,* cache-contains-start_url\r\n* contrast-ratio translate some audits into korea >>> 1
301,also added the translator info on the bottom translated the rail model in Korean >>> 1
302,nan Test result as PR Review >>> 0
303,R: @jpmedley @petele\r\nFYI: @mounirlamouri @xxyzzzq?\r\n Add one more implementation note about artwork >>> 1
304,nan [devtools] fix a bunch of little bugs >>> 1
305,"Update links to point to the published version (as opposed to the draft), fix\r\nthe code to actually work. webfont optimization: fix code, update links >>> 1"
306,nan Further changes adding more guidance for developers on apks >>> 1
307,R: @jpmedley  Fix some nits >>> 1
308,"@samdutton Now that the Media Session article is up, I thought we should link it from your existing article at https://developers.google.com/web/updates/2015/07/media-notifications\r\n\r\nWDYT?\r\n\r\nFYI @mounirlamouri Add link to Media Session article >>> 1"
309,R: @petele  Fix Francois Beaufort contributor links >>> 1
310,"Building off of #4042, this adds more inline feedback to the Debug JS tutorial.\r\n\r\nNote: these widgets don't work from the local development server (`startappengine.sh`). This is a limitation of the server. The widgets aren't broken.\r\n\r\n [devtools] add more inline feedback to JS debugging tutorial >>> 1"
311,nan Clarify Web Bluetooth description. >>> 1
312,Ready for review! Navigation preload article >>> 1
313,R: @kaycebasques \r\n\r\nGDoc: https://goo.gl/vxhXTc\r\n\r\nAlso added `devsite-crx-install` to the LH page so we're prepped for the inline install flow that's landing soon in devsite! Lighthouse jan 2017 update >>> 1
314,nan Improve phrasing in PRPL doc >>> 1
315,nan fix 404 link >>> 1
316,From This issue: https://github.com/google/WebFundamentals/issues/4172 Incorporate feedback into MSE primer. >>> 1
317,"Following https://github.com/google/WebFundamentals/pull/4147, I've omitted `self` in Service Worker code snippets. Omit self in SW >>> 1"
318,"* Updates screenshots.\r\n* Moves all of the debugging features to a single task-based ""reference"". They were previously scattered across a bunch of small docs. Uses a fairly consistent and much more concise style to describe each feature.\r\n* Deprecates the old docs by putting warnings at top of each section, along with links to new content.\r\n* Adds ""deprecation"" status to old docs in ToC. [devtools] refactor js debugging docs >>> 1"
319,nan New test infrastructure >>> 1
320,nan Minor edits to MSE for Audio Update >>> 1
321,This section seems to be incomplete (?).\r\n\r\n/cc @PaulKinlan  Remove incomplete (?) section >>> 1
322,nan Add a missed deprecation. >>> 1
323,nan Add appropriate tags. >>> 1
324,FYI: @mounirlamouri \r\n\r\nTry it live at https://pr-4190-dot-web-central.appspot.com/web/updates/2017/02/media-session#notifications_everywhere Fix Wear notification and add more samples >>> 1
325,nan Fix IntersectionObserver sample code >>> 1
326,This works differently on live to how it works in the dev server. On live the `>` is escaped to `&gt;` breaking the layout. Fix escaping bug >>> 1
327,* Organizes the reference to use a task-based structure.\r\n* Adds a lot of visual aids. [devtools] refactor network reference >>> 1
328,"nan Update location of Lighthouse ""Open in Viewer"" >>> 1"
329,* common query\r\n* common implementation-heading\r\n* aria-allowed-attributes\r\n* content-sized-correctly-for-viewport\r\n* critical-request-chains\r\n* valid-aria-attributes Translate some lighthouse audits into korean >>> 0
330,nan Translated PRPL pattern in Korean >>> 0
331,nan Fixes #4209 A2HS warning for /web/update >>> 1
332,- Replaced older images\r\n- Added note about support for ISO4217\r\ncc: @rsolomakhin @zkoch Payment request update >>> 1
333,There was a [late merge to 57](https://bugs.chromium.org/p/chromium/issues/detail?id=688586#c19). Update chrome-57-deprecations.md >>> 1
334,Some people may take offense to the linked article. Update service-workers.md >>> 1
335,nan Minor spelling fix >>> 1
336,Firfox -> Firefox Spelling fix >>> 1
337,更正错别字 Fixes typos in zh-cn fundamentals/getting-started/codelabs/offline/index.md >>> 1
338,nan Minor edit. >>> 1
339,"* Removes hardcoding of `url` from the main feedback logic.\r\n* Makes `question` optional. If omitted, then it's assumed that all of needed context will be provided in the button text. See `web/tools/chrome-devtools/javascript` on the staging site for an example.\r\n* Makes the fail response more flexible.\r\n* Prefixes the labels with numbers, so that I can keep track of the ordering of questions in Google Analytics\r\n* Updates doc [devtools] update inline feedback >>> 1"
340,nan Business Values on PWA Landing >>> 0
341,nan Add late deprecation to article. >>> 1
342,nan Trans: zh-cn fundamentals/performance/index.md >>> 1
343,Fix some grammatical errors and a few minor style/clarity issues. Grammar fixes >>> 1
344,nan Warn end users about enabling unknown sources >>> 1
345,Before merging:\r\n* [ ] Update video link New In Chrome 57 Updates post >>> 1
346,\r\n![screenshot 2017-03-01 at 11 47 44 am](https://cloud.githubusercontent.com/assets/634478/23456800/39d4ff9a-fe75-11e6-9c22-85d1564e3cd7.png)\r\n\r\n\r\nFYI @paullewis  Add Media Session video >>> 1
347,nan New podcast episode >>> 1
348,R: @petele  Add inline CWS install button support >>> 1
349,"nan [devtools] add figure numbers, clean up docs references >>> 1"
350,Because of: https://bugs.chromium.org/p/chromium/issues/detail?id=468806#c39 Remove 'Mouse on Android stops firing TouchEvents'. >>> 1
351,Still needs reviewing but here's the initial commit.\r\n\r\ncc @petele @PaulKinlan @addyosmani  Web push book >>> 1
352,Added a note about quota limitation of calling `canMakePayment()`.\r\ncc: @rsolomakhin Payment request update re: `canMakePayment()` >>> 0
353,Added a note about quota limitation of calling `canMakePayment()`.\r\ncc: @rsolomakhin Added a note about calling `canMakePayment()` >>> 1
354,Fixes #4288 Correct typo >>> 1
355,@petele or @jpmedley can either of you plz review this in the next two or three days? DevTools eng wants to ship this in Canary soon\r\n\r\nNote that the Chrome DevTools UI uses`release-notes-preview.png`. The release notes don't use that image add devtools release notes for m58 >>> 1
356,nan [devtools] fix description on homepage >>> 1
357,nan Deprecations and Removals for Chrome 58. >>> 1
358,Correcting a spelling error Updating style-guide.md >>> 1
359,"Adding a new podcast to the shows section, this will be updated once the videos go live.  Designer vs developer podcast >>> 1"
360,nan Background tabs >>> 1
361,nan Redo use of revokeObjectURL(). >>> 1
362,nan Fix typos and links. >>> 1
363,Adding feedburner links and updating feed. adding feedburner link >>> 1
364,nan Fix typo. >>> 1
365,nan Add missed item to 57 deps/rems. >>> 1
366,nan Copy edit. >>> 1
367,Not ready to merge just yet. Tech writer has reviewed but still waiting on review from Alice. Wanted to get the PR setup though :) Add a11y audit article >>> 1
368,"I'm running an experiment to see how this change affects engagement rates. My hunch is that readers who are successfully completing the tutorial don't really have an incentive to continue to respond to the questions. If they realize how helpful it is to us, they may be more inclined to continue to engage\r\n\r\nnote to self: mark the date when this change goes live (not when it's merged) [devtools] make explicit request to reader in first inline feedback question >>> 1"
369,inline feedback ftw [devtools] update fail response per #4311 >>> 1
370,"Adds a feedback section to the bottom of every audit reference, asking if the user found the page helpful.\r\n\r\nGooglers can an example at `<staging site>.com/web/tools/lighthouse/audits/alt-attribute` [lighthouse] add inline feedback to audit docs >>> 1"
371,nan Fix anchor links >>> 1
372,cc @kaycebasques  Lighthouse: avoid a recursive link >>> 1
373,Adding link to the video that just launched!!  Designer vs developer adding video link >>> 1
374,"Preview at https://pr-4335-dot-web-central.appspot.com/web/updates/2017/03/chrome-58-media-updates\r\nR: @mounirlamouri Add ""Media Updates in Chrome 58"" updates post >>> 1"
375,R: @petele  Fixed deprecated next-version links >>> 1
376,nan Removed outdated note on background sync support >>> 1
377,nan Add Mixing Streams. >>> 1
378,nan Add timeline-viewer reference >>> 1
379,Updates to deps/rems 57. Add Late Deprecations and Removals >>> 1
380,nan Copy edit. >>> 1
381,"Following https://github.com/google/WebFundamentals/pull/4337, what do you think @jpmedley?  Reworded ControlsList API paragraph >>> 1"
382,PTAL @PaulKinlan @petele  Adds article on performant expand and collapse anims >>> 1
383,"I’m currently linking to my personal blog as the Animation Worklet API is still volatile. Once it stabilizes and is properly spec’d by the Houdini TF, I’ll move an updated version of the article to /updates.\r\n\r\nPTAL @petele  houdini: Link to Animation Worklet article >>> 1"
384,This patch addresses @samdutton feedback raised at https://github.com/google/WebFundamentals/pull/4335#pullrequestreview-28322376 Address @samdutton feedback >>> 1
385,nan Change to standard TL;DR >>> 1
386,This a PR so that reviewers and others can have a look during work in progress.\r\n\r\nPreview at https://pr-4299-dot-web-central.appspot.com/web/fundamentals/getting-started/primers/mobile-web-video-playback [WIP] Add Mobile Web Video Playback primer article >>> 0
387,nan Convert dialogs policy article. >>> 1
388,"Videos in this article are unplayable on Android. I added WebM versions and enabled controls, because I can’t get autoplay to work currently.\r\n\r\n@petele please merge and deploy as soon as possible.\r\n\r\ncc @paullewis  [URGENT] Fix videos for expando article >>> 1"
389,* polish all the things\r\n* fixes up https://github.com/GoogleChrome/lighthouse/issues/1842 [lighthouse] spruce up the homepage >>> 1
390,Adding episode two  Designer vs developer podcast ep02 >>> 1
391,TBR: @jpmedley  Linked to mixing-streams article >>> 1
392,Updated the link with angular2 instead of angularjs. What about Angular2? >>> 1
393,nan Add missing metadata. >>> 1
394,__Please don’t merge/deploy before Monday :)__\r\n\r\nNew article. \r\nI use math SVGs that have been rendered using LaTeX. I kept the LaTeX source code as comments in the document so that future editors can edit the SVGs as well if necessary. Let me know if that is okay or if I should remove them.\r\n\r\n@petele @jpmedley PTAL. Add custom-scrollbar article >>> 1
395,nan Create Media Section >>> 1
396,nan Remove 'external' macro from internal link. >>> 1
397,"The dialog policy article was written a while ago. This removes the\r\n“proposed” adjective (because it is published and no longer proposed),\r\nand updates the list of changes to be up-to-date. Bring dialog policy up-to-date. >>> 1"
398,nan Minor clarification between Chrome 57 & 58 >>> 1
399,nan add Santa Tracker PWA showcase article >>> 1
400,FYI @mounirlamouri\r\nPreview at https://pr-4368-dot-web-central.appspot.com/web/updates/2017/03/chrome-58-media-updates#autoplay Improved autoplay scope explanation >>> 1
401,nan [devtools] add load performance tutorial for performance panel >>> 0
402,nan Remove new 'comma' splice. >>> 1
403,nan fix broken link >>> 1
404,As coauthor for https://github.com/google/WebFundamentals/pull/4365 Add plegner as new contributor >>> 1
405,"use https, drop G+ update samthor contributor links >>> 1"
406,Updating video links  adding links >>> 1
407,"this PR also includes another translation wich is in another pending PR #4331 Translate updates/dialogs-policy.md into Chinese, resolve #4383 >>> 1"
408,cc @petele  Adding common issues and debugging tips >>> 1
409,"The snippet isn't showing up - I think probably because I'm using an em dash rather than just boring characters. add plegner, make snippet ASCII >>> 1"
410,Can we merge this on Friday? I am away so will have difficulty making edits as I don't have access to my laptop.  Designer vs developer podcast ep03 >>> 1
411,nan Fixed a type >>> 1
412,Article metadata tags don't work if you split them across multiple lines\r\n\r\n@samthor @petele  Fix Santa showcase meta tag >>> 1
413,nan DevTools: m57 isn't canary any longer. removing note. >>> 1
414,nan [devtools] fix todo in doc description >>> 1
415,nan [devtools] fix 404s >>> 1
416,nan Fixes small typo >>> 1
417,nan Fix variable in code sample. >>> 1
418,nan Add note about infinite scroll case >>> 1
419,"One of my fellow writers plz review ASAP, I'm timing this with the Chrome release.\r\n\r\nIt doesn't need to be perfect (barring any critical problems)... I can always update later. [devtools] add what's new for m59 >>> 1"
420,nan Update localhost URL in Web USB article to use code formatting. >>> 1
421,nan Update index.md >>> 1
422,fixing minor issues in JA documentations. fix typo and nit layout issues >>> 1
423,nan Update service-workers.md >>> 1
424,"This change makes the use of promises in the code example clear, and safe when people copy-paste.\r\n\r\nA lot of readers will be unclear about the important differences between `registration.then(mapSuccess, mapError)` and `registration.then(mapSuccess).catch(mapError)`, namely that in the second example `mapError` handles both registration errors *and* errors from the `mapSuccess` handler.\r\n\r\nThis change makes the comment `// registration failed :(` true regardless of changes made to the currently safe `mapSuccess` function. The error handler will now indeed run if-and-only-if the registration itself fails. Clarify promise usage in service worker example >>> 1"
425,This was a doozy\r\n\r\nThe new doc is [index.md](https://github.com/google/WebFundamentals/compare/master...kaycebasques:cdt-perf?expand=1#diff-dd6e42295cd623684916dbe974b717b7)... in my preview GitHub isn't loading large diffs by default anymore [devtools] add runtime perf tutorial >>> 1
426,nan Fix run-on sentences and inconsistent capitalization >>> 0
427,nan Update promises.md >>> 1
428,nan Weekly updates >>> 1
429,feedback from bgauslin landing page fix >>> 1
430,nan Update service-workers.md >>> 1
431,update note on customized built in support Fixes #4206 >>> 1
432,This was confusing - I want to make it clear that `onscroll` doesn't really need a passive event listener. I think our documentation makes it sound like even the basic scroll event is something that users should worry about setting passive. add note about scroll event not needing to be passive >>> 1
433,nan New in Chrome 58 >>> 1
434,"Hey, Eiji\r\n\r\nHere's a new version of the credential management API docs.\r\n\r\nNote that I didn't create new animations yet, as I suspect there will be a new sample to work with for IO. \r\n\r\nI'll confirm this with you, and we can push this PR once it is reviewed first, and add new animations later.\r\n\r\nMeggin new version of credential api docs >>> 1"
435,nan Update lorem ipsum text in PWA ILT Gulp slide intro para >>> 1
436,Just minor updates so that the reference doc titles match the audit titles pretty much verbatim [lighthouse] make doc titles match audit titles >>> 1
437,nan [devtools] fix descrip & add link on home >>> 1
438,* remove workspaces 2.0... it was reverted out of the release\r\n* add more code coverage info [devtools] update m59 release notes >>> 1
439,"Updates the section that explains how to use DevTools to analyze JS execution by fixing the screenshots and descriptions. [devtools] update ""optimize js execution"" >>> 1"
440,Waiting on a screenshot from Voot. Voot.com case study. >>> 1
441,To match new art done for https://developer.android.com/develop/quality-guidelines/building-for-billions.html. Update some /billions images >>> 1
442,"nan add ""unoptimized images"" audit doc >>> 1"
443,Adding episode 4 Designer vs developer podcast ep04 >>> 1
444,nan Update guide for including videos >>> 1
445,The order of items in the menu of ILT topics should match the course/video order ([here](https://www.youtube.com/watch?v=63YUj-7qxl0&list=PLNYkxOF6rcIAdnzEsWkg0KpMn2WJwMBmN&index=5)).\r\n\r\nI've also tweaked one or two headings so they fit in a single line. Update ILT menu to match video numbering order >>> 1
446,cc @flackr as FYI Add Rob Flack to contributors >>> 1
447,cc @zkoch\r\n\r\nThis adds the updated PaymentRequest API docs to Web Fundamentals. Adds Deep Dive for Payment request API >>> 1
448,nan Update getting-started-pwa.md >>> 1
449,"As the warning mentions, it's largely redundant with the new runtime perf tutorial [devtools] deprecate the old forced synchronous layouts tutorial >>> 1"
450,nan Update index.md >>> 1
451,indentation increased as explained in my previous issue: https://github.com/google/WebFundamentals/issues/4422 Update index.md >>> 1
452,"https://developers.google.com/web/updates/2011/11/Quota-Management-API-Fast-Facts\r\n\r\nDrive-by English fixes.\r\n\r\nCode blocks didn't show:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/33569/25363223/eaef4908-290d-11e7-9a8f-26fafa00ce67.png)\r\n\r\nCC @agektmr  Fix English and formatting, date article >>> 0"
453,nan DevTools m58 changelog post: credit sroussey >>> 1
454,"nan [devtools] make ""get involved"" section more descriptive >>> 1"
455,changed &lt;JWTHeader&gt;.&lt;Payload&gt;.&lt;Signature&gt; to \r\n<JWTHeader>.<Payload>.<Signature> Fixed typo in introduction-to-push-notifications.md >>> 1
456,"nan [lighthouse] add ""oversize images"" reference >>> 1"
457,I don't think many readers will know what ILT means. PWA ILT: changed titles to Progressive Web Apps Training >>> 1
458,Seems like `cache-polyfill` is not needed any more and should be removed: https://github.com/dominiccooney/cache-polyfill\r\n\r\nStyle guide suggests using 4 spaces / 1 tab for code blocks instead of <code>```</code> but both are working on DevSite! Maybe the guide should be updated. Update index.md >>> 1
459,nan Update style-guide.md >>> 1
460,cc @jpmedley @Meggin @kosamari @petele \r\n\r\nWould be good to get this landed as soon as possible. Adding the macos notification update post >>> 1
461,nan [devtools] fix versioning note >>> 1
462,"nan [lighthouse] add the ""theme-color meta tag"" audit reference >>> 1"
463,This PR fixes two problems with the file.\r\n\r\n* It contained several items from Chrome 59 that were erroneously included.\r\n* It had an unescaped <iframe> tag. Fix M58 deprecations. >>> 1
464,nan Update airhorner.md >>> 1
465,R: @petele \r\n\r\nIncreases the timeout before the autosizing iframe sets its source. Ref: https://b/37207644 Fixes deep link race condition with browser bug searcher >>> 1
466,nan [devtools] mention the url for checking versions >>> 1
467,cc @jpmedley @petele Fixing some out-of-date text >>> 1
468,"Hey @petele,\r\n\r\nthis is my current state for adding a section for “HowTo: Components” to WebFun’s architecture section.\r\n\r\nIt’s not ready to launch, so **please don’t merge**.\r\n\r\nPlease take a look and tell me\r\n\r\n1. If the content files are okay this way (lots of inline HTML for the literate programming view)\r\n2. If the 2 stylesheets (`main.css` and `prism-solarizedlight.css`) are okay to put into the folders\r\n3. How to proceed with the one JavaScript file that will need to run on WebFun, as it is responsible to to resize the demo’s iframe to an appropriate size.\r\n\r\n@robdodson @devnook as an FYI Add HowTo Components section >>> 0"
469,- Mention bluetooth-internals page\r\n- Add Bluetooth Descriptors section\r\n- Updated implementation status page URL\r\n\r\nDoes that look good you @scheib @g-ortuno?\r\n\r\n Update bluetooth article >>> 1
470,nan Weekly updates for 0427 >>> 1
471,"This PR contains two things:\r\n\r\n* Fixes to previous deps/rems articles, specifically the one's for 57 and 58.\r\n* A new article for Chrome 59 deps and rems. Deprecations and Remvoals >>> 1"
472,"This test checks for `Must only redirect from paths below ""pathname""` error when deploying the site Add test for _redirect.yaml >>> 1"
473,"There are some images on the Media > Video section that are not appearing on translations.\r\nThis is one of them:\r\n\r\nwrong path in i10n:\r\nhttps://developers.google.com/web/fundamentals/media/img/Accept-Ranges-Chrome-Dev-Tools.png\r\nright path in English file:\r\nhttps://developers.google.com/web/fundamentals/media/images/Accept-Ranges-Chrome-Dev-Tools.png\r\n\r\nI've replaced the wrong ""img/"" path with the correct ""images/"" in all i10n files.\r\nIssue similar to #4404  Fixed wrong image path for not English translations in Media > Video … >>> 1"
474,nan [lighthouse] add missing ToC entry >>> 1
475,nan Fix featured image on voot case study. >>> 1
476,nan Update optimize-encoding-and-transfer.md >>> 1
477,R: @petele @drufball @PaulKinlan \r\n\r\n- [x] Tech review \r\n- [x] Tech writer review  Headless Chrome update >>> 1
478,"Note to reviewers: Tech Review [check], Tech Writer Review [check].... Checked on a staging environment.... [Nope]\r\n\r\nI am on Chrome OS and it's nearly impossible to test this... but I think it is all ok.\r\n\r\n[Note: all is ok] Getrelated installed apps article >>> 1"
479,Fixes a number of formatting errors and cleans up a few sentences. Fix getInstalledRelatedApps blog post >>> 1
480,"Fixed erroneous link from ""debugging-service-workers/"" to ""push-notifications/"" in i10n ""Getting-started > Welcome"" page. Wrong link in ""Getting-started"" page >>> 1"
481,nan Updating notes based on final feedback. >>> 1
482,"Currently, an `{% include""xyz.html"" %}` of a non-existent file crashes the development server. This patch fixes the behavior to how it was obviously intended – but wasn’t implemented correctly. Fix `include` implementation >>> 1"
483,Removes HTML for note: Fixes #4507 - markdown issue in note. >>> 1
484,"There is a new format for podcast images (minimum size of 1400 x 1400 pixels and a maximum size of 3000 x 3000 pixels), they need to be much larger then we currently make them (300x300 pixels). \r\n\r\nIve created larger images and referenced them in the correct places.  Des vs dev fixing podcast images >>> 1"
485,nan Fix typo in credentials management >>> 1
486,"'View slides on Google Docs' links on pages such as [this](https://developers.google.com/web/ilt/pwa/your-audience-your-content) currently give a 403 error.\r\n\r\nMy fix seems to work, but there may be a better way. PWA ILT: Fix to 'View slides on Google Docs' link >>> 1"
487,"R: @kaycebasques \r\n\r\nGetting  some stuff in for hacker news folks. cc @kosamari  Headless chrome updates, links, faq >>> 1"
488,"A tweak on the cri description. It doesn't really go high-level, just builds APIs based on the protocol.  headless-chrome: wording tweak >>> 1"
489,Since #4462 got too complicated. Deprecate article per @agektmr >>> 1
490,adding changes to the gulp build Des vs dev fixing podcast images >>> 1
491,Updates sign-up link to pre-fill the Feature drop-down. Update getinstalledrelatedapps.md >>> 1
492,"There is an itunes tag called <itunes:explicit> that is required for new podcast submissions. I have added <itunes:explicit>no</itunes:explicit> into the template file by default.  adding another itunes tag, required for new podcast submissions, basi… >>> 1"
493,which is https://developers.google.com/web/updates/2017/04/headless-chrome add Japanese translation for the Headless Chrome post >>> 1
494,nan Headless article: fix link and typo >>> 1
495,Context: https://webmasters.stackexchange.com/questions/42960/how-to-test-the-speed-of-a-page-that-is-guarded-behind-login-page/105954#105954 Mention ability to audit logged-in pages >>> 1
496,Updating offline article with a link to a code example and a visual example that shows the user when content has been updated.  Offline ux update >>> 1
497,nan Fixes small grammatical error >>> 1
498,nan Add missing styles and formatting >>> 1
499,OS X is the old name. macOS is the official name nowadays. Replace “(Mac) OS X” with “macOS” >>> 1
500,From some later feedback. https://github.com/google/WebFundamentals/pull/4492#event-1064639543 Two fixes from foolip. >>> 1
501,"Related issue: #4539 \r\n\r\nThis solution is based on W3C articles:\r\n\r\n1. [Creating HTML Pages in Arabic, Hebrew and Other Right-to-left Scripts (tutorial)](https://www.w3.org/International/tutorials/bidi-xhtml)\r\n2. [Structural markup and right-to-left text in HTML](https://www.w3.org/International/articles/inline-bidi-markup/uba-basics)\r\n\r\nThere're other articles from W3C to read about this, but I'm tired & busy to read them now and wanted to share what I have with you. Maybe someone could see this fix as an appropriate fix and add to it. Or, it could be inapplicable for the project and I should stop!\r\n\r\nKnown issues:\r\n1. no margins in the right toc.\r\n2. In RTL languages (tested with Arabic), lines with English text only flow from RTL! (Could be fixed by wrapping it with `<bdi>` or `<span>`, didn't test but should work. Or add `dir=auto` to every paragraph!\r\n3. English-only headings are from RTL in RTL languages. Either it should be translated or wrapped with `<bdi>` / span\r\n\r\nI just test Arabic and English text which look fine to me. Maybe other languages need extra treatments...\r\n\r\nSuggested reviewers:\r\n@PaulKinlan, @samdutton, @petele, @ianbarber (because you were involved with localisation & RTL issues) Add markups to support RTL languages (fixes #4539) >>> 0"
502,Episode 5 going out today Designer vs developer podcast ep05 >>> 1
503,nan [devtools] fix mailing list link >>> 1
504,nan Fixed small typo >>> 1
505,nan Fix typos. >>> 1
506,Removed apostrophe in PWAs Fixed small typo >>> 1
507,nan Small typo fix >>> 1
508,"R: anyone [lighthouse] add ""button name"" reference >>> 1"
509,Separated note containing bulleted list into two notes. Added spaces in front of notes in numbered list so the notes are indented and the numbers increment correctly. Removed double underscores. Fixed in source doc as well. Fixed issue with notes in numbered list >>> 0
510,Currently the development server crashes when the inline-content of a framebox contains non-ASCII characters. This PR fixes that. Encode strings with UTF-8 before hashing >>> 1
511,"#3578\r\nAddressed the issue, plus also addressed single long strings breaking the page. \r\n**Mutual issue**\r\n\r\n![screenshot from 2017-05-02 23-57-10](https://cloud.githubusercontent.com/assets/24438869/25633137/c5b81c3c-2f93-11e7-86d0-bc63d0001ead.png)\r\n\r\n**PR's commit also solves this issue**\r\n![screenshot from 2017-05-02 23-58-53](https://cloud.githubusercontent.com/assets/24438869/25633163/decd0d18-2f93-11e7-835f-c39e3e22e848.png)\r\n\r\n Fix #3578: Author photo's get squashed when the bio is long >>> 1"
512,nan Add support for class= for framebox >>> 1
513,nan updated tools to add Workbox >>> 1
514,nan Update _contributors.yaml >>> 0
515,As per closed PR #4560. Updated /web/billions video >>> 1
516,playing catch-up with recent lighthouse changes [lighthouse] refactor theme color audits >>> 1
517,New comprehensive reference on DevTools features related to performance analysis\r\n\r\nFYI the new (huge) doc is [src/content/en/tools/chrome-devtools/evaluate-performance/reference.md](https://github.com/google/WebFundamentals/compare/master...kaycebasques:cdt-perfpanel?expand=1#diff-ecf859259434f89a7dc0ce74a2424f62)... when I look at this PR GitHub doesn't display it... you need to click **Load Diff** to see it. [devtools] add perf reference >>> 1
518,@kaycebasques PTAL Update contrast ratio lighthouse doc >>> 1
519,Fixed broken images on https://developers.google.com/web/fundamentals/getting-started/codelabs/push-notifications/?hl=ja according to [original](https://github.com/google/WebFundamentals/blob/master/src/content/en/fundamentals/getting-started/codelabs/push-notifications/index.md). Fix image paths >>> 1
520,nan fix indentation >>> 1
521,nan ja: Update 'closed mode' translation >>> 1
522,![image](https://cloud.githubusercontent.com/assets/445333/26243804/181318b0-3cc8-11e7-81e0-3e2532166548.png)\r\n ja: fix markdown >>> 1
523,"`exec()` spawns a shell, which means that unsanitised user input must not be passed. The example suggests doing exactly that. Presumably people will use Lighthouse, but nevertheless I don't think such a potentially dangerous approach should be shown as an example.\r\n\r\nThis commit changes the example to use `execFile()`. This does not spawn a shell, avoiding the security risks of `exec()`. headless chrome: use execFile() >>> 1"
524,R: all Add Lighthouse I/O video to docs >>> 1
525,"Hello @petele,\r\nMay you also sync `_code` folder when merging this PR?\r\n\r\nThanks in advance,\r\nFrancois. s/paused/playing in Mobile Web Video Playback sample >>> 1"
526,Fix for #4582 - correct payment details Fix for #4582 - correct payment details >>> 1
527,I wrote a new page for the VR fundamentals section. I already got eng feedback but could do with a tech writer review.\r\n Add a new WebVR page about how it is different from regular web >>> 1
528,nan [devtools] add what's new for m60 >>> 1
529,Slides source folder:\r\n\r\nhttps://drive.google.com/corp/drive/folders/0B1-BUeTLlQTOQ0JyZE1JTWlLeE0 Update slide id >>> 0
530,Slides source folder:\r\n\r\nhttps://drive.google.com/corp/drive/folders/0B1-BUeTLlQTOQ0JyZE1JTWlLeE0 Update slide id >>> 0
531,Slides source folder:\r\n\r\nhttps://drive.google.com/corp/drive/folders/0B1-BUeTLlQTOQ0JyZE1JTWlLeE0 Update slide id >>> 0
532,nan update m60 whats new >>> 1
533,This updates the list of dialog changes with the latest info about the effective dates and milestones.\r\n\r\nI'm open to phrasing changes to make the three paragraphs in this section flow more smoothly. Update status list with new info >>> 1
534,Latest episode  Designer vs developer podcast ep06 >>> 1
535,Updating podcast links.  Designer vs developer podcast ep06 >>> 1
536,"Repro:\r\n\r\n1. Add this to a page:\r\n\r\n       {% framebox width=""auto"" height=""auto"" enable_widgets=""true"" %}\r\n         <p>this is my framebox</p>\r\n       {% endframebox %}\r\n\r\n2. Run `start-appengine.sh`\r\n3. Access the page fix framebox emulator >>> 1"
537,nan New in Chrome 59 >>> 1
538,"nan [lighthouse] add ""offscreen images"" reference >>> 1"
539,"Originally published [here](https://developers.google.com/web/updates/2016/12/imagecapture), needs an update because is being shipped in M59 and some things have changed.\r\n\r\nNeeds some polishing of the literature, but I'm uploading it anyhow so that (any of) you guys can take a look.\r\n\r\nPTAL:\r\n+@beaufortfrancois (all your comments from the early review taken in except the `revokeURL()`, thanks!)\r\n+@samdutton\r\n\r\nFYI:\r\n+@petele\r\n+@owencm Update ImageCapture web fundamentals entry  >>> 1"
540,Fixes #4626 :)\r\n\r\n_I've signed the CLA._\r\nName: Mihir Karandikar Fix typo >>> 1
541,"This PR adds a section for the HowTo: Components to `/web/fundamentals/architecture`.\r\n\r\nCurrently, there is no components, because off of them are still under review. However, the project is being referenced in an upcoming “Designers vs Developers” video, so I want to the section to be live and have something to link to.\r\n\r\n@petele @jpmedley PTAL (this time, this actually needs a tech writer review 😄)\r\n\r\n@devnook @robdodson FYI Add section for HowTo: Components >>> 1"
542,"Pete, let me know if I've missed anything that needs to be done to get this published. Also let me know if you think updates is the best place for this (a lot of the content here is very much ""fundamentals"", imo). Add the performance metrics article >>> 1"
543,"nan add video to ""what's new in devtools m59"" post >>> 1"
544,"Issue observed on: [https://developers.google.com/web/ilt/pwa/lab-auditing-with-lighthouse](https://developers.google.com/web/ilt/pwa/lab-auditing-with-lighthouse).\r\n\r\nIn mainfest.json the app's name is 'Blog' \r\n```\r\n  ""short_name"": ""Blog"",\r\n```\r\nwhile in meta tags it is 'PSK'  and 'Polymer Starter Kit'\r\n```\r\n<meta name=""apple-mobile-web-app-title"" content=""Polymer Starter Kit"">\r\n<meta name=""application-name"" content=""PSK"">\r\n```\r\n\r\nThis pull request names them all 'Blog'. \r\n Update application name based on manifest.json >>> 1"
545,nan Translate 'Media/Mobile Web Video Playback' into Japanese >>> 1
546,"nan add ""offscreen images"" to ToC >>> 1"
547,nan Update measure-crp.md >>> 1
548,"Translate the latest article about devtools under updates category into Chineses Translate devtools-release-notes.md into Chinese, resolve #4330 >>> 1"
549,"this PR also contains changes of pending PR #4331, #4384 Translate `what's new for m59` into Simplified Chinese, resolve #4420 >>> 0"
550,I want to propose me as an Italian translator for your website. I'm Italian mother-language with tech background (B.S. in Computer Science). \r\nPlease accept this initial contribution in Web Fundamentals Homepage and Getting started pages. proposed Italian translation of Home and Getting Started >>> 0
551,Reported externally here:\r\nhttps://twitter.com/AdamTReineke/status/870745685171126272 Fix typos >>> 1
552,Not as urgent as the last one :)\r\n\r\n@petele @jpmedley PTAL Add first 2 articles to howto components >>> 1
553,nan Draft dependencies/removals article for Chrome 60. >>> 1
554,nan Fix IntersectionObserver options object >>> 1
555,nan App 101 and cheat sheet. >>> 1
556,nan Add article on object rest & spread properties >>> 1
557,nan fix metrics update typo >>> 1
558,nan Fixed stringify typo (fixes #4643) >>> 1
559,nan Typo: Command to Ctrl (fixes #4634) >>> 1
560,nan Fix formatting issues and update analytics code >>> 1
561,nan Corrected Udacity URLs  (fixes #4654) >>> 1
562,"Hey @petele check it out:\r\n\r\n* by storing the question / response data in JSON and populating the framebox with a little JS, we can support questions with any number of choices. see [evaluate-performance/index.md](https://github.com/google/WebFundamentals/compare/master...kaycebasques:wf-multichoicefeedback?expand=1#diff-dd6e42295cd623684916dbe974b717b7) for an example of a question with three choices (v1 of the widget is hard-coded to only support two choices)\r\n* this refactor can also replace the v1 widget. it handles all of v1's features 100%. see [javascript/index.md](https://github.com/google/WebFundamentals/compare/master...kaycebasques:wf-multichoicefeedback?expand=1#diff-f755648971e7f7ec554c20e5ff5454cd) for example\r\n* I like that I can include the JSON directly in the page. In v1 there's too much indirection and it's tedious to author\r\n* I like the flexibility of a JS solution\r\n\r\nIn general I'm very happy with this new workflow. Let me know if you're cool with it make inline feedback widget more flexible >>> 1"
563,nan Missing quote >>> 1
564,cc @PaulKinlan @owencm  Adding budget api post >>> 1
565,nan some typo >>> 1
566,"POC for now to see how things can work on staging. Purposefully not creating any pointer links. \r\n\r\nThis also updates our template files to set a correct meta viewport. Add ""Try Lighthouse"" page >>> 1"
567,nan [devtools] add css tutorial >>> 1
568,nan Site wide Udacity link fix >>> 1
569,nan [devtools] fix analytics values >>> 1
570,nan [devtools] fix figure numbers >>> 1
571,Ready for review! Adding credential management updates guide >>> 1
572,R: @petele \r\n\r\nSwitch to `async/await` to make examples easier to read. Updates to headless chrome article >>> 1
573,@jpmedley would you mind reviewing this? It's already been reviewed by members of the IndexedDB eng team. Add an IndexedDB best practices article >>> 1
574,"Please merge ASAP, demos are currently broken:\r\n\r\n<img width=""928"" alt=""screen shot 2017-06-12 at 11 28 57 am"" src=""https://user-images.githubusercontent.com/234957/27029985-782dda3a-4f62-11e7-9a78-f3e2a7df2ec8.png"">\r\n Fix HowTo Components demos >>> 1"
575,nan Cheat Sheet Corrections. >>> 1
576,"nan [lighthouse] add ""text compression"" reference >>> 1"
577,nan Fix typos. >>> 1
578,nan Experiment with enhanced teaser text. >>> 1
579,formatting is hard :| fix feedback >>> 1
580,nan New case study for the WebVR team >>> 0
581,R: @mounirlamouri\r\nPreview at https://pr-4688-dot-web-central.appspot.com/web/updates/2017/06/play-request-was-interrupted\r\nBackground: https://bugs.chromium.org/p/chromium/issues/detail?id=728294#c25 Add play-request-was-interrupted update article >>> 1
582,@kaycebasques  Headless: Testing on Headless Chrome with Karma/Mocha/Chai >>> 1
583,nan Fix typo. >>> 1
584,nan Fix typo in credential management >>> 1
585,"nan [lighthouse] add ""custom splash screen"" reference >>> 1"
586,Chris took his demo down so I'm replacing it with one from Mozilla. Update push demos. >>> 1
587,nan Fix typo >>> 1
588,"Started at https://github.com/google/WebFundamentals/pull/4688, this PR addresses @mounirlamouri feedback.\r\n\r\nWhen LGTM, please merge. Address mounirlamouri feedback >>> 1"
589,"BTW, is there a reason the markup in this paragraph is HTML rather than Markdown? Maybe `Dogfood:` doesn't support Markdown? Less abrupt link to the shim >>> 1"
590,Developers [have asked if this was a standard](https://developers.google.com/web/updates/2016/07/payment-request?google_comment_id=z13fjbk5pzbbg30tg23ltlwbfuzucndlg) or a Chrome API. Mention PaymentRequest is a W3C standard >>> 0
591,"I've submitted a PR to caniuse, but [you can't add the Android Chrome versions via PRs](https://github.com/Fyrd/caniuse/pull/3516#issuecomment-308557990). Add PaymentRequest availability >>> 1"
592,"nan [lighthouse] add ""fast enough on 3G"" audit and update feedback stuff >>> 1"
593,The title says it all. [devtools] document keyboard shortcut for Command Menu >>> 1
594,"Added commas to fix syntax error in JS code, removed '' from publicKey to be consistent with other fields. Fix syntax error in JS examples >>> 1"
595,`framebox` demos that include arrow functions need to be moved to external files... the DevSite Markdown parser converts the greater-than characters to HTML encodings...\r\n\r\nAlso fixes logic bugs that I encountered when testing out the changes on the real staging server [devtools] fix css demos >>> 1
596,nan Fix typo on JavaScript >>> 1
597,nan Shadow dom: update lingo fixes >>> 1
598,I don't know why this PR is showing changes to the cheatsheet. Those have already been [merged to master](https://github.com/google/WebFundamentals/pull/4681). Draft of file manipulation article. >>> 0
599,I realized the error message wasn't the exact one displayed in Chrome. Here it is.\r\nFeel free to merge and push live when possible.\r\n\r\nFYI @mounirlamouri Fix error message >>> 1
600,nan Fix anchor in eme article >>> 1
601,FIX: #4722\r\nR: @jakearchibald  Update browser support for async functions >>> 1
602,nan [CM API update] Add featured image and a link to the migration guide >>> 1
603,nan Fix Typo >>> 0
604,nan Fix polyfill link >>> 1
605,#polish [devtools] update the homepage >>> 1
606,nan Fix shim markup error and add Apple Pay JS wrapper >>> 1
607,"nan [lighthouse] add ""install prompt"" reference >>> 1"
608,nan typo fix >>> 1
609,"Pulled out the right image names from English version and replaced them with Bash script in pt-br, ko, id, es and zh-ch.\r\n\r\nScript:\r\n```\r\n#!/bin/bash\r\n\r\nfor URL in src/content/pt-br/fundamentals/getting-started/codelabs/push-notifications/index.md src/content/ko/fundamentals/getting-started/codelabs/push-notifications/index.md src/content/id/fundamentals/getting-started/codelabs/push-notifications/index.md src/content/es/fundamentals/getting-started/codelabs/push-notifications/index.md src/content/zh-cn/fundamentals/getting-started/codelabs/push-notifications/index.md\r\ndo\r\n\tsed -i -e 's/49b343a07e2c92d.png/4525ec369fc2ae47.png/g' $URL\r\n\tsed -i -e 's/d670cb813f3a7575.png/6b698d7c7bbf1bc0.png/g' $URL\r\n\tsed -i -e 's/d712c8726928ca4.png/de3ceca91043d278.png/g' $URL\r\n\tsed -i -e 's/b9787463acb7e3fb.png/15f6375617c11974.png/g' $URL\r\n\tsed -i -e 's/5b8a7e9905f2237b.png/227cea0abe03a5b4.png/g' $URL\r\n\tsed -i -e 's/bfdc9f92e001934a.png/8fe2b1b110f87b34.png/g' $URL\r\n\tsed -i -e 's/c69f7428408c5bbc.png/8775071d7fd66432.png/g' $URL\r\n\tsed -i -e 's/e36f921fa7598419.png/2b5314607196f4e1.png/g' $URL\r\n\tsed -i -e 's/c0fab1022906d01f.png/2b089bdf10a8a945.png/g' $URL\r\n\tsed -i -e 's/cb84e05044ad029f.png/cf0e71f76cb79cc4.png/g' $URL\r\n\tsed -i -e 's/e320ddd1b66682e6.png/a12fbfdc08233592.png/g' $URL\r\n\tsed -i -e 's/3c5c0d5d1c3deda4.png/2973c2b818ca9324.png/g' $URL\r\n\tsed -i -e 's/fdedaa6d10e2f43e.png/75b1fedbfb7e0b99.png/g' $URL\r\n\tsed -i -e 's/32acb7ec17ef75a8.png/33dd89c437c17c97.png/g' $URL\r\ndone\r\n\r\n```\r\n Corrects wrong image src to reflect en and ja versions (Fixes #4713) >>> 1"
610,nan Fix typos. >>> 1
611,"With launcher shipped solo, we can clean up some more of this.\r\n\r\ncc @samccone  Headless guide: Lighthouse -> chrome-launcher >>> 1"
612,"nan [lighthouse] add ""webp"" reference >>> 1"
613,"nan [lighthouse] add ""optimize images"" reference >>> 1"
614,nan Aligning Input Events. >>> 1
615,We were missing details on what the Push timings mean. And I augmented definitions of two others.\r\n\r\ncc @caseq  DevTools: augment network timing details incl Push >>> 1
616,"I swept the existing cred management docs to update them in line with the changes in Chrome 60, as described in this update doc: https://developers.google.com/web/updates/2017/06/credential-management-updates#password\r\n\r\nWe may still want to add a section on credentials shared from sub-domains. Any thoughts on this much appreciated. Now reflecting Chrome 60 cred management API updates >>> 0"
617,This is my WIP (not ready at all) article for video preload. Let's use it to get feedback.\r\n\r\nLive preview at https://pr-4744-dot-web-central.appspot.com/web/fundamentals/media/video-preload Add media video preload article >>> 0
618,"Here's an update to [CM API the update article](https://developers.google.com/web/updates/2017/06/credential-management-updates).\r\n\r\ncc: @Meggin  [Credential Management API] Adding feature detection note, correction for `create()` >>> 1"
619,"nan [lighthouse] add ""network payloads"" reference >>> 1"
620,Safari 10.1 shipped custom elements v1 Update custom-elements browser support >>> 1
621,nan Fix typos. >>> 1
622,nan Adding further tweaks to the article >>> 1
